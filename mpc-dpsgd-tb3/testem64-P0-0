entering iterations
Epoch 1.1: Loss = 2.30209
Epoch 1.2: Loss = 2.27839
Epoch 1.3: Loss = 2.25566
Epoch 1.4: Loss = 2.20818
Epoch 1.5: Loss = 2.14189
Epoch 1.6: Loss = 2.05964
Epoch 1.7: Loss = 1.95646
Epoch 1.8: Loss = 1.8347
Epoch 1.9: Loss = 1.72505
Epoch 1.10: Loss = 1.5921
Epoch 1.11: Loss = 1.57212
Epoch 1.12: Loss = 1.48535
Epoch 1.13: Loss = 1.41546
Epoch 1.14: Loss = 1.35918
Epoch 1.15: Loss = 1.31499
Epoch 1.16: Loss = 1.30996
Epoch 1.17: Loss = 1.24454
Epoch 1.18: Loss = 1.21448
Epoch 1.19: Loss = 1.16449
Epoch 1.20: Loss = 1.18867
Epoch 1.21: Loss = 1.10298
Epoch 1.22: Loss = 1.07234
Epoch 1.23: Loss = 1.07067
Epoch 1.24: Loss = 1.13076
Epoch 1.25: Loss = 1.05566
Epoch 1.26: Loss = 1.08003
Epoch 1.27: Loss = 1.00099
Epoch 1.28: Loss = 1.01379
Epoch 1.29: Loss = 0.973225
Epoch 1.30: Loss = 0.94683
Epoch 1.31: Loss = 1.03738
Epoch 1.32: Loss = 0.947988
Epoch 1.33: Loss = 0.844146
Epoch 1.34: Loss = 0.974987
Epoch 1.35: Loss = 0.99287
Epoch 1.36: Loss = 0.945625
Epoch 1.37: Loss = 0.928974
Epoch 1.38: Loss = 0.895716
Epoch 1.39: Loss = 0.89432
Epoch 1.40: Loss = 0.908393
Epoch 1.41: Loss = 0.919269
Epoch 1.42: Loss = 0.886917
Epoch 1.43: Loss = 0.878484
Epoch 1.44: Loss = 0.886665
Epoch 1.45: Loss = 0.90514
Epoch 1.46: Loss = 0.907908
Epoch 1.47: Loss = 0.811662
Epoch 1.48: Loss = 0.799108
Epoch 1.49: Loss = 0.883988
Epoch 1.50: Loss = 0.81873
Epoch 1.51: Loss = 0.742111
Epoch 1.52: Loss = 0.841883
Epoch 1.53: Loss = 0.894171
Epoch 1.54: Loss = 0.688187
Epoch 1.55: Loss = 0.804069
Epoch 1.56: Loss = 0.815766
Epoch 1.57: Loss = 0.844404
Epoch 1.58: Loss = 0.838098
Epoch 1.59: Loss = 0.819173
Epoch 1.60: Loss = 0.883671
Epoch 1.61: Loss = 0.723461
Epoch 1.62: Loss = 0.85854
Epoch 1.63: Loss = 0.713798
Epoch 1.64: Loss = 0.741492
Epoch 1.65: Loss = 0.778438
Epoch 1.66: Loss = 0.755538
Epoch 1.67: Loss = 0.698268
Epoch 1.68: Loss = 0.847285
Epoch 1.69: Loss = 0.809004
Epoch 1.70: Loss = 0.793726
Epoch 1.71: Loss = 0.673994
Epoch 1.72: Loss = 0.774909
Epoch 1.73: Loss = 0.851944
Epoch 1.74: Loss = 0.780411
Epoch 1.75: Loss = 0.694985
Epoch 1.76: Loss = 0.761147
Epoch 1.77: Loss = 0.749286
Epoch 1.78: Loss = 0.738539
Epoch 1.79: Loss = 0.678867
Epoch 1.80: Loss = 0.709672
Epoch 1.81: Loss = 0.678033
Epoch 1.82: Loss = 0.725655
Epoch 1.83: Loss = 0.762384
Epoch 1.84: Loss = 0.721516
Epoch 1.85: Loss = 0.73476
Epoch 1.86: Loss = 0.773461
Epoch 1.87: Loss = 0.742588
Epoch 1.88: Loss = 0.674681
Epoch 1.89: Loss = 0.807112
Epoch 1.90: Loss = 0.710722
Epoch 1.91: Loss = 0.821353
Epoch 1.92: Loss = 0.747861
Epoch 1.93: Loss = 0.774829
Epoch 1.94: Loss = 0.688979
Epoch 1.95: Loss = 0.720334
Epoch 1.96: Loss = 0.674835
Epoch 1.97: Loss = 0.570739
Epoch 1.98: Loss = 0.71834
Epoch 1.99: Loss = 0.731297
Epoch 1.100: Loss = 0.665859
Epoch 1.101: Loss = 0.769467
Epoch 1.102: Loss = 0.719269
Epoch 1.103: Loss = 0.710135
Epoch 1.104: Loss = 0.673175
Epoch 1.105: Loss = 0.674563
Epoch 1.106: Loss = 0.782979
Epoch 1.107: Loss = 0.756367
Epoch 1.108: Loss = 0.738665
Epoch 1.109: Loss = 0.696155
Epoch 1.110: Loss = 0.75799
Epoch 1.111: Loss = 0.6619
Epoch 1.112: Loss = 0.666956
Epoch 1.113: Loss = 0.666047
Epoch 1.114: Loss = 0.695129
Epoch 1.115: Loss = 0.704481
Epoch 1.116: Loss = 0.635361
Epoch 1.117: Loss = 0.75943
Epoch 1.118: Loss = 0.60371
Epoch 1.119: Loss = 0.712897
Epoch 1.120: Loss = 0.698964
TRAIN LOSS = 0.949773
TRAIN ACC = -327.666 % (39421/60000)
Reducing learning rate to 0.4
entering iterations
Epoch 2.1: Loss = 0.742368
Epoch 2.2: Loss = 0.664529
Epoch 2.3: Loss = 0.743678
Epoch 2.4: Loss = 0.647869
Epoch 2.5: Loss = 0.653104
Epoch 2.6: Loss = 0.788456
Epoch 2.7: Loss = 0.683011
Epoch 2.8: Loss = 0.783518
Epoch 2.9: Loss = 0.594227
Epoch 2.10: Loss = 0.515748
Epoch 2.11: Loss = 0.78047
Epoch 2.12: Loss = 0.69331
Epoch 2.13: Loss = 0.720402
Epoch 2.14: Loss = 0.699211
Epoch 2.15: Loss = 0.677495
Epoch 2.16: Loss = 0.721561
Epoch 2.17: Loss = 0.644514
Epoch 2.18: Loss = 0.681425
Epoch 2.19: Loss = 0.653988
Epoch 2.20: Loss = 0.749067
Epoch 2.21: Loss = 0.612484
Epoch 2.22: Loss = 0.533007
Epoch 2.23: Loss = 0.717999
Epoch 2.24: Loss = 0.762986
Epoch 2.25: Loss = 0.675505
Epoch 2.26: Loss = 0.612162
Epoch 2.27: Loss = 0.712224
Epoch 2.28: Loss = 0.678041
Epoch 2.29: Loss = 0.70117
Epoch 2.30: Loss = 0.645824
Epoch 2.31: Loss = 0.743443
Epoch 2.32: Loss = 0.6726
Epoch 2.33: Loss = 0.576515
Epoch 2.34: Loss = 0.767077
Epoch 2.35: Loss = 0.681115
Epoch 2.36: Loss = 0.685525
Epoch 2.37: Loss = 0.713325
Epoch 2.38: Loss = 0.666727
Epoch 2.39: Loss = 0.741336
Epoch 2.40: Loss = 0.673979
Epoch 2.41: Loss = 0.713541
Epoch 2.42: Loss = 0.664419
Epoch 2.43: Loss = 0.685894
Epoch 2.44: Loss = 0.628866
Epoch 2.45: Loss = 0.725593
Epoch 2.46: Loss = 0.781229
Epoch 2.47: Loss = 0.641833
Epoch 2.48: Loss = 0.608052
Epoch 2.49: Loss = 0.711911
Epoch 2.50: Loss = 0.673425
Epoch 2.51: Loss = 0.545806
Epoch 2.52: Loss = 0.702646
Epoch 2.53: Loss = 0.748763
Epoch 2.54: Loss = 0.519783
Epoch 2.55: Loss = 0.670814
Epoch 2.56: Loss = 0.664074
Epoch 2.57: Loss = 0.730353
Epoch 2.58: Loss = 0.669693
Epoch 2.59: Loss = 0.744332
Epoch 2.60: Loss = 0.689872
Epoch 2.61: Loss = 0.600385
Epoch 2.62: Loss = 0.729285
Epoch 2.63: Loss = 0.60298
Epoch 2.64: Loss = 0.605797
Epoch 2.65: Loss = 0.709497
Epoch 2.66: Loss = 0.624507
Epoch 2.67: Loss = 0.638061
Epoch 2.68: Loss = 0.776566
Epoch 2.69: Loss = 0.685214
Epoch 2.70: Loss = 0.70371
Epoch 2.71: Loss = 0.545869
Epoch 2.72: Loss = 0.671343
Epoch 2.73: Loss = 0.766408
Epoch 2.74: Loss = 0.67518
Epoch 2.75: Loss = 0.589461
Epoch 2.76: Loss = 0.662648
Epoch 2.77: Loss = 0.654022
Epoch 2.78: Loss = 0.663358
Epoch 2.79: Loss = 0.614596
Epoch 2.80: Loss = 0.602013
Epoch 2.81: Loss = 0.597793
Epoch 2.82: Loss = 0.618101
Epoch 2.83: Loss = 0.677323
Epoch 2.84: Loss = 0.602515
Epoch 2.85: Loss = 0.653286
Epoch 2.86: Loss = 0.701528
Epoch 2.87: Loss = 0.651916
Epoch 2.88: Loss = 0.598192
Epoch 2.89: Loss = 0.74442
Epoch 2.90: Loss = 0.648488
Epoch 2.91: Loss = 0.743844
Epoch 2.92: Loss = 0.686562
Epoch 2.93: Loss = 0.709408
Epoch 2.94: Loss = 0.606759
Epoch 2.95: Loss = 0.658512
Epoch 2.96: Loss = 0.620713
Epoch 2.97: Loss = 0.526608
Epoch 2.98: Loss = 0.650601
Epoch 2.99: Loss = 0.707737
Epoch 2.100: Loss = 0.625281
Epoch 2.101: Loss = 0.692758
Epoch 2.102: Loss = 0.670746
Epoch 2.103: Loss = 0.628608
Epoch 2.104: Loss = 0.574198
Epoch 2.105: Loss = 0.597373
Epoch 2.106: Loss = 0.722486
Epoch 2.107: Loss = 0.728709
Epoch 2.108: Loss = 0.728465
Epoch 2.109: Loss = 0.687738
Epoch 2.110: Loss = 0.720504
Epoch 2.111: Loss = 0.636527
Epoch 2.112: Loss = 0.625327
Epoch 2.113: Loss = 0.630333
Epoch 2.114: Loss = 0.654689
Epoch 2.115: Loss = 0.631003
Epoch 2.116: Loss = 0.589205
Epoch 2.117: Loss = 0.678788
Epoch 2.118: Loss = 0.551883
Epoch 2.119: Loss = 0.677516
Epoch 2.120: Loss = 0.630322
TRAIN LOSS = 0.66783
TRAIN ACC = 109.222 % (46857/60000)
Reducing learning rate to 0.4
entering iterations
Epoch 3.1: Loss = 0.659076
Epoch 3.2: Loss = 0.607181
Epoch 3.3: Loss = 0.687001
Epoch 3.4: Loss = 0.589007
Epoch 3.5: Loss = 0.615892
Epoch 3.6: Loss = 0.754404
Epoch 3.7: Loss = 0.638048
Epoch 3.8: Loss = 0.747853
Epoch 3.9: Loss = 0.523804
Epoch 3.10: Loss = 0.462063
Epoch 3.11: Loss = 0.745055
Epoch 3.12: Loss = 0.628161
Epoch 3.13: Loss = 0.687487
Epoch 3.14: Loss = 0.648403
Epoch 3.15: Loss = 0.638826
Epoch 3.16: Loss = 0.68344
Epoch 3.17: Loss = 0.574408
Epoch 3.18: Loss = 0.66249
Epoch 3.19: Loss = 0.598446
Epoch 3.20: Loss = 0.710943
Epoch 3.21: Loss = 0.546081
Epoch 3.22: Loss = 0.469294
Epoch 3.23: Loss = 0.649958
Epoch 3.24: Loss = 0.705296
Epoch 3.25: Loss = 0.641317
Epoch 3.26: Loss = 0.5263
Epoch 3.27: Loss = 0.625916
Epoch 3.28: Loss = 0.625024
Epoch 3.29: Loss = 0.665712
Epoch 3.30: Loss = 0.617319
Epoch 3.31: Loss = 0.705357
Epoch 3.32: Loss = 0.619532
Epoch 3.33: Loss = 0.54672
Epoch 3.34: Loss = 0.722146
Epoch 3.35: Loss = 0.647925
Epoch 3.36: Loss = 0.654278
Epoch 3.37: Loss = 0.678329
Epoch 3.38: Loss = 0.615085
Epoch 3.39: Loss = 0.710817
Epoch 3.40: Loss = 0.633051
Epoch 3.41: Loss = 0.668864
Epoch 3.42: Loss = 0.617837
Epoch 3.43: Loss = 0.63738
Epoch 3.44: Loss = 0.534204
Epoch 3.45: Loss = 0.688844
Epoch 3.46: Loss = 0.75725
Epoch 3.47: Loss = 0.63127
Epoch 3.48: Loss = 0.567897
Epoch 3.49: Loss = 0.654147
Epoch 3.50: Loss = 0.645824
Epoch 3.51: Loss = 0.492853
Epoch 3.52: Loss = 0.666872
Epoch 3.53: Loss = 0.687487
Epoch 3.54: Loss = 0.478444
Epoch 3.55: Loss = 0.633989
Epoch 3.56: Loss = 0.61831
Epoch 3.57: Loss = 0.710305
Epoch 3.58: Loss = 0.604934
Epoch 3.59: Loss = 0.733407
Epoch 3.60: Loss = 0.631801
Epoch 3.61: Loss = 0.559757
Epoch 3.62: Loss = 0.691196
Epoch 3.63: Loss = 0.571886
Epoch 3.64: Loss = 0.552856
Epoch 3.65: Loss = 0.696223
Epoch 3.66: Loss = 0.553578
Epoch 3.67: Loss = 0.59543
Epoch 3.68: Loss = 0.767691
Epoch 3.69: Loss = 0.624033
Epoch 3.70: Loss = 0.633145
Epoch 3.71: Loss = 0.505689
Epoch 3.72: Loss = 0.631
Epoch 3.73: Loss = 0.714641
Epoch 3.74: Loss = 0.631099
Epoch 3.75: Loss = 0.56228
Epoch 3.76: Loss = 0.601663
Epoch 3.77: Loss = 0.610672
Epoch 3.78: Loss = 0.641179
Epoch 3.79: Loss = 0.571404
Epoch 3.80: Loss = 0.577473
Epoch 3.81: Loss = 0.564057
Epoch 3.82: Loss = 0.582222
Epoch 3.83: Loss = 0.653831
Epoch 3.84: Loss = 0.562028
Epoch 3.85: Loss = 0.613875
Epoch 3.86: Loss = 0.666432
Epoch 3.87: Loss = 0.614728
Epoch 3.88: Loss = 0.574219
Epoch 3.89: Loss = 0.715392
Epoch 3.90: Loss = 0.626568
Epoch 3.91: Loss = 0.714218
Epoch 3.92: Loss = 0.648322
Epoch 3.93: Loss = 0.663267
Epoch 3.94: Loss = 0.579294
Epoch 3.95: Loss = 0.649015
Epoch 3.96: Loss = 0.579892
Epoch 3.97: Loss = 0.499019
Epoch 3.98: Loss = 0.62386
Epoch 3.99: Loss = 0.660306
Epoch 3.100: Loss = 0.605375
Epoch 3.101: Loss = 0.643934
Epoch 3.102: Loss = 0.645349
Epoch 3.103: Loss = 0.590906
Epoch 3.104: Loss = 0.535836
Epoch 3.105: Loss = 0.554069
Epoch 3.106: Loss = 0.682127
Epoch 3.107: Loss = 0.653571
Epoch 3.108: Loss = 0.750843
Epoch 3.109: Loss = 0.656374
Epoch 3.110: Loss = 0.685355
Epoch 3.111: Loss = 0.618794
Epoch 3.112: Loss = 0.606607
Epoch 3.113: Loss = 0.622536
Epoch 3.114: Loss = 0.646864
Epoch 3.115: Loss = 0.594472
Epoch 3.116: Loss = 0.560382
Epoch 3.117: Loss = 0.641527
Epoch 3.118: Loss = 0.531214
Epoch 3.119: Loss = 0.631076
Epoch 3.120: Loss = 0.592965
TRAIN LOSS = 0.627191
TRAIN ACC = 0 % (48280/60000)
Reducing learning rate to 0.4
entering iterations
Epoch 4.1: Loss = 0.593816
Epoch 4.2: Loss = 0.582125
Epoch 4.3: Loss = 0.653102
Epoch 4.4: Loss = 0.554207
Epoch 4.5: Loss = 0.59874
Epoch 4.6: Loss = 0.743505
Epoch 4.7: Loss = 0.598946
Epoch 4.8: Loss = 0.720432
Epoch 4.9: Loss = 0.487978
Epoch 4.10: Loss = 0.42907
Epoch 4.11: Loss = 0.706248
Epoch 4.12: Loss = 0.598066
Epoch 4.13: Loss = 0.655273
Epoch 4.14: Loss = 0.618644
Epoch 4.15: Loss = 0.646314
Epoch 4.16: Loss = 0.658647
Epoch 4.17: Loss = 0.540842
Epoch 4.18: Loss = 0.640258
Epoch 4.19: Loss = 0.564478
Epoch 4.20: Loss = 0.680515
Epoch 4.21: Loss = 0.517661
Epoch 4.22: Loss = 0.446933
Epoch 4.23: Loss = 0.607746
Epoch 4.24: Loss = 0.672444
Epoch 4.25: Loss = 0.631797
Epoch 4.26: Loss = 0.491582
Epoch 4.27: Loss = 0.590642
Epoch 4.28: Loss = 0.597559
Epoch 4.29: Loss = 0.632881
Epoch 4.30: Loss = 0.600888
Epoch 4.31: Loss = 0.672877
Epoch 4.32: Loss = 0.572456
Epoch 4.33: Loss = 0.537317
Epoch 4.34: Loss = 0.686167
Epoch 4.35: Loss = 0.629343
Epoch 4.36: Loss = 0.634846
Epoch 4.37: Loss = 0.666592
Epoch 4.38: Loss = 0.591012
Epoch 4.39: Loss = 0.70189
Epoch 4.40: Loss = 0.60843
Epoch 4.41: Loss = 0.641516
Epoch 4.42: Loss = 0.599188
Epoch 4.43: Loss = 0.623543
Epoch 4.44: Loss = 0.489888
Epoch 4.45: Loss = 0.667815
Epoch 4.46: Loss = 0.725253
Epoch 4.47: Loss = 0.621231
Epoch 4.48: Loss = 0.547256
Epoch 4.49: Loss = 0.636938
Epoch 4.50: Loss = 0.632397
Epoch 4.51: Loss = 0.469465
Epoch 4.52: Loss = 0.6385
Epoch 4.53: Loss = 0.655693
Epoch 4.54: Loss = 0.460271
Epoch 4.55: Loss = 0.613847
Epoch 4.56: Loss = 0.592857
Epoch 4.57: Loss = 0.698029
Epoch 4.58: Loss = 0.566947
Epoch 4.59: Loss = 0.717342
Epoch 4.60: Loss = 0.616566
Epoch 4.61: Loss = 0.538823
Epoch 4.62: Loss = 0.663987
Epoch 4.63: Loss = 0.550077
Epoch 4.64: Loss = 0.520793
Epoch 4.65: Loss = 0.68255
Epoch 4.66: Loss = 0.523302
Epoch 4.67: Loss = 0.56069
Epoch 4.68: Loss = 0.773505
Epoch 4.69: Loss = 0.589606
Epoch 4.70: Loss = 0.607976
Epoch 4.71: Loss = 0.487782
Epoch 4.72: Loss = 0.614015
Epoch 4.73: Loss = 0.688964
Epoch 4.74: Loss = 0.611858
Epoch 4.75: Loss = 0.552181
Epoch 4.76: Loss = 0.572038
Epoch 4.77: Loss = 0.595828
Epoch 4.78: Loss = 0.629177
Epoch 4.79: Loss = 0.549891
Epoch 4.80: Loss = 0.57496
Epoch 4.81: Loss = 0.55772
Epoch 4.82: Loss = 0.575286
Epoch 4.83: Loss = 0.642108
Epoch 4.84: Loss = 0.543482
Epoch 4.85: Loss = 0.594548
Epoch 4.86: Loss = 0.649968
Epoch 4.87: Loss = 0.593898
Epoch 4.88: Loss = 0.552151
Epoch 4.89: Loss = 0.687159
Epoch 4.90: Loss = 0.623648
Epoch 4.91: Loss = 0.682298
Epoch 4.92: Loss = 0.62107
Epoch 4.93: Loss = 0.628316
Epoch 4.94: Loss = 0.56837
Epoch 4.95: Loss = 0.636282
Epoch 4.96: Loss = 0.565784
Epoch 4.97: Loss = 0.48708
Epoch 4.98: Loss = 0.609473
Epoch 4.99: Loss = 0.633263
Epoch 4.100: Loss = 0.593854
Epoch 4.101: Loss = 0.620748
Epoch 4.102: Loss = 0.63685
Epoch 4.103: Loss = 0.572084
Epoch 4.104: Loss = 0.527882
Epoch 4.105: Loss = 0.533941
Epoch 4.106: Loss = 0.674259
Epoch 4.107: Loss = 0.608535
Epoch 4.108: Loss = 0.771539
Epoch 4.109: Loss = 0.638436
Epoch 4.110: Loss = 0.667173
Epoch 4.111: Loss = 0.605341
Epoch 4.112: Loss = 0.599582
Epoch 4.113: Loss = 0.610203
Epoch 4.114: Loss = 0.64729
Epoch 4.115: Loss = 0.573909
Epoch 4.116: Loss = 0.540385
Epoch 4.117: Loss = 0.62604
Epoch 4.118: Loss = 0.521741
Epoch 4.119: Loss = 0.604024
Epoch 4.120: Loss = 0.576385
TRAIN LOSS = 0.605558
TRAIN ACC = 109.222 % (48993/60000)
Reducing learning rate to 0.4
entering iterations
Epoch 5.1: Loss = 0.551822
Epoch 5.2: Loss = 0.570664
Epoch 5.3: Loss = 0.635081
Epoch 5.4: Loss = 0.536211
Epoch 5.5: Loss = 0.596245
Epoch 5.6: Loss = 0.740286
Epoch 5.7: Loss = 0.575726
Epoch 5.8: Loss = 0.704806
Epoch 5.9: Loss = 0.473375
Epoch 5.10: Loss = 0.409011
Epoch 5.11: Loss = 0.68494
Epoch 5.12: Loss = 0.580695
Epoch 5.13: Loss = 0.639045
Epoch 5.14: Loss = 0.598451
Epoch 5.15: Loss = 0.640018
Epoch 5.16: Loss = 0.64451
Epoch 5.17: Loss = 0.527016
Epoch 5.18: Loss = 0.629798
Epoch 5.19: Loss = 0.544754
Epoch 5.20: Loss = 0.66063
Epoch 5.21: Loss = 0.505445
Epoch 5.22: Loss = 0.437983
Epoch 5.23: Loss = 0.585301
Epoch 5.24: Loss = 0.652508
Epoch 5.25: Loss = 0.619389
Epoch 5.26: Loss = 0.473857
Epoch 5.27: Loss = 0.56734
Epoch 5.28: Loss = 0.582045
Epoch 5.29: Loss = 0.612121
Epoch 5.30: Loss = 0.594408
Epoch 5.31: Loss = 0.650267
Epoch 5.32: Loss = 0.548888
Epoch 5.33: Loss = 0.536631
Epoch 5.34: Loss = 0.667672
Epoch 5.35: Loss = 0.616728
Epoch 5.36: Loss = 0.621752
Epoch 5.37: Loss = 0.662377
Epoch 5.38: Loss = 0.575107
Epoch 5.39: Loss = 0.699861
Epoch 5.40: Loss = 0.589895
Epoch 5.41: Loss = 0.623106
Epoch 5.42: Loss = 0.592309
Epoch 5.43: Loss = 0.610835
Epoch 5.44: Loss = 0.467098
Epoch 5.45: Loss = 0.652698
Epoch 5.46: Loss = 0.713274
Epoch 5.47: Loss = 0.619294
Epoch 5.48: Loss = 0.534794
Epoch 5.49: Loss = 0.633408
Epoch 5.50: Loss = 0.6262
Epoch 5.51: Loss = 0.455567
Epoch 5.52: Loss = 0.616917
Epoch 5.53: Loss = 0.638946
Epoch 5.54: Loss = 0.452541
Epoch 5.55: Loss = 0.600391
Epoch 5.56: Loss = 0.580803
Epoch 5.57: Loss = 0.68345
Epoch 5.58: Loss = 0.544384
Epoch 5.59: Loss = 0.703629
Epoch 5.60: Loss = 0.612275
Epoch 5.61: Loss = 0.531093
Epoch 5.62: Loss = 0.647686
Epoch 5.63: Loss = 0.533327
Epoch 5.64: Loss = 0.501204
Epoch 5.65: Loss = 0.668847
Epoch 5.66: Loss = 0.500466
Epoch 5.67: Loss = 0.535286
Epoch 5.68: Loss = 0.780092
Epoch 5.69: Loss = 0.571678
Epoch 5.70: Loss = 0.597264
Epoch 5.71: Loss = 0.484289
Epoch 5.72: Loss = 0.611073
Epoch 5.73: Loss = 0.672705
Epoch 5.74: Loss = 0.602171
Epoch 5.75: Loss = 0.543517
Epoch 5.76: Loss = 0.565431
Epoch 5.77: Loss = 0.589876
Epoch 5.78: Loss = 0.611983
Epoch 5.79: Loss = 0.540045
Epoch 5.80: Loss = 0.573155
Epoch 5.81: Loss = 0.559128
Epoch 5.82: Loss = 0.579001
Epoch 5.83: Loss = 0.63167
Epoch 5.84: Loss = 0.532715
Epoch 5.85: Loss = 0.584325
Epoch 5.86: Loss = 0.637717
Epoch 5.87: Loss = 0.582174
Epoch 5.88: Loss = 0.542135
Epoch 5.89: Loss = 0.668681
Epoch 5.90: Loss = 0.626334
Epoch 5.91: Loss = 0.660512
Epoch 5.92: Loss = 0.604683
Epoch 5.93: Loss = 0.601534
Epoch 5.94: Loss = 0.563696
Epoch 5.95: Loss = 0.620744
Epoch 5.96: Loss = 0.560897
Epoch 5.97: Loss = 0.480473
Epoch 5.98: Loss = 0.595958
Epoch 5.99: Loss = 0.614003
Epoch 5.100: Loss = 0.58825
Epoch 5.101: Loss = 0.605027
Epoch 5.102: Loss = 0.633396
Epoch 5.103: Loss = 0.561889
Epoch 5.104: Loss = 0.527897
Epoch 5.105: Loss = 0.522407
Epoch 5.106: Loss = 0.675777
Epoch 5.107: Loss = 0.597174
Epoch 5.108: Loss = 0.781362
Epoch 5.109: Loss = 0.628489
Epoch 5.110: Loss = 0.655186
Epoch 5.111: Loss = 0.597671
Epoch 5.112: Loss = 0.598114
Epoch 5.113: Loss = 0.5981
Epoch 5.114: Loss = 0.649843
Epoch 5.115: Loss = 0.561424
Epoch 5.116: Loss = 0.523289
Epoch 5.117: Loss = 0.618712
Epoch 5.118: Loss = 0.513968
Epoch 5.119: Loss = 0.588568
Epoch 5.120: Loss = 0.567585
TRAIN LOSS = 0.593386
TRAIN ACC = -109.222 % (49455/60000)
Reducing learning rate to 0.4
