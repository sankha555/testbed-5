entering iterations
Epoch 1.1: Loss = 2.30205
Epoch 1.2: Loss = 2.27853
Epoch 1.3: Loss = 2.25606
Epoch 1.4: Loss = 2.20923
Epoch 1.5: Loss = 2.14398
Epoch 1.6: Loss = 2.06293
Epoch 1.7: Loss = 1.96111
Epoch 1.8: Loss = 1.84026
Epoch 1.9: Loss = 1.73122
Epoch 1.10: Loss = 1.5981
Epoch 1.11: Loss = 1.57799
Epoch 1.12: Loss = 1.49078
Epoch 1.13: Loss = 1.42029
Epoch 1.14: Loss = 1.36414
Epoch 1.15: Loss = 1.3192
Epoch 1.16: Loss = 1.31364
Epoch 1.17: Loss = 1.24863
Epoch 1.18: Loss = 1.21808
Epoch 1.19: Loss = 1.16808
Epoch 1.20: Loss = 1.19164
Epoch 1.21: Loss = 1.10658
Epoch 1.22: Loss = 1.07614
Epoch 1.23: Loss = 1.07361
Epoch 1.24: Loss = 1.13329
Epoch 1.25: Loss = 1.05882
Epoch 1.26: Loss = 1.08148
Epoch 1.27: Loss = 1.00247
Epoch 1.28: Loss = 1.01611
Epoch 1.29: Loss = 0.97583
Epoch 1.30: Loss = 0.949829
Epoch 1.31: Loss = 1.03833
Epoch 1.32: Loss = 0.950546
Epoch 1.33: Loss = 0.846939
Epoch 1.34: Loss = 0.976654
Epoch 1.35: Loss = 0.994766
Epoch 1.36: Loss = 0.948364
Epoch 1.37: Loss = 0.930222
Epoch 1.38: Loss = 0.897186
Epoch 1.39: Loss = 0.895782
Epoch 1.40: Loss = 0.909515
Epoch 1.41: Loss = 0.921478
Epoch 1.42: Loss = 0.887863
Epoch 1.43: Loss = 0.879272
Epoch 1.44: Loss = 0.886551
Epoch 1.45: Loss = 0.905594
Epoch 1.46: Loss = 0.909012
Epoch 1.47: Loss = 0.813461
Epoch 1.48: Loss = 0.800613
Epoch 1.49: Loss = 0.884888
Epoch 1.50: Loss = 0.820435
Epoch 1.51: Loss = 0.744247
Epoch 1.52: Loss = 0.843506
Epoch 1.53: Loss = 0.895187
Epoch 1.54: Loss = 0.689911
Epoch 1.55: Loss = 0.805466
Epoch 1.56: Loss = 0.817047
Epoch 1.57: Loss = 0.845352
Epoch 1.58: Loss = 0.838654
Epoch 1.59: Loss = 0.820221
Epoch 1.60: Loss = 0.883942
Epoch 1.61: Loss = 0.72493
Epoch 1.62: Loss = 0.859207
Epoch 1.63: Loss = 0.714478
Epoch 1.64: Loss = 0.742691
Epoch 1.65: Loss = 0.779617
Epoch 1.66: Loss = 0.756836
Epoch 1.67: Loss = 0.699036
Epoch 1.68: Loss = 0.847717
Epoch 1.69: Loss = 0.809525
Epoch 1.70: Loss = 0.794281
Epoch 1.71: Loss = 0.675339
Epoch 1.72: Loss = 0.775131
Epoch 1.73: Loss = 0.852371
Epoch 1.74: Loss = 0.781647
Epoch 1.75: Loss = 0.696533
Epoch 1.76: Loss = 0.761246
Epoch 1.77: Loss = 0.749222
Epoch 1.78: Loss = 0.7388
Epoch 1.79: Loss = 0.679642
Epoch 1.80: Loss = 0.710403
Epoch 1.81: Loss = 0.679153
Epoch 1.82: Loss = 0.726089
Epoch 1.83: Loss = 0.762817
Epoch 1.84: Loss = 0.72226
Epoch 1.85: Loss = 0.73494
Epoch 1.86: Loss = 0.773727
Epoch 1.87: Loss = 0.743576
Epoch 1.88: Loss = 0.674805
Epoch 1.89: Loss = 0.806564
Epoch 1.90: Loss = 0.711685
Epoch 1.91: Loss = 0.821411
Epoch 1.92: Loss = 0.748047
Epoch 1.93: Loss = 0.774429
Epoch 1.94: Loss = 0.69014
Epoch 1.95: Loss = 0.720856
Epoch 1.96: Loss = 0.675446
Epoch 1.97: Loss = 0.571442
Epoch 1.98: Loss = 0.718979
Epoch 1.99: Loss = 0.730927
Epoch 1.100: Loss = 0.666748
Epoch 1.101: Loss = 0.769211
Epoch 1.102: Loss = 0.72023
Epoch 1.103: Loss = 0.710953
Epoch 1.104: Loss = 0.674515
Epoch 1.105: Loss = 0.674759
Epoch 1.106: Loss = 0.782028
Epoch 1.107: Loss = 0.75502
Epoch 1.108: Loss = 0.738617
Epoch 1.109: Loss = 0.696381
Epoch 1.110: Loss = 0.757294
Epoch 1.111: Loss = 0.661591
Epoch 1.112: Loss = 0.666473
Epoch 1.113: Loss = 0.666992
Epoch 1.114: Loss = 0.695709
Epoch 1.115: Loss = 0.704758
Epoch 1.116: Loss = 0.635803
Epoch 1.117: Loss = 0.760223
Epoch 1.118: Loss = 0.60437
Epoch 1.119: Loss = 0.712479
Epoch 1.120: Loss = 0.697815
TRAIN LOSS = 0.951172
TRAIN ACC = 65.6647 % (39401/60000)
Reducing learning rate to 0.399994
entering iterations
Epoch 2.1: Loss = 0.742081
Epoch 2.2: Loss = 0.665283
Epoch 2.3: Loss = 0.743805
Epoch 2.4: Loss = 0.647797
Epoch 2.5: Loss = 0.653641
Epoch 2.6: Loss = 0.788696
Epoch 2.7: Loss = 0.683182
Epoch 2.8: Loss = 0.783493
Epoch 2.9: Loss = 0.595398
Epoch 2.10: Loss = 0.516357
Epoch 2.11: Loss = 0.780106
Epoch 2.12: Loss = 0.694504
Epoch 2.13: Loss = 0.719833
Epoch 2.14: Loss = 0.699097
Epoch 2.15: Loss = 0.677994
Epoch 2.16: Loss = 0.721649
Epoch 2.17: Loss = 0.644653
Epoch 2.18: Loss = 0.681808
Epoch 2.19: Loss = 0.654099
Epoch 2.20: Loss = 0.74968
Epoch 2.21: Loss = 0.613159
Epoch 2.22: Loss = 0.533951
Epoch 2.23: Loss = 0.718277
Epoch 2.24: Loss = 0.762634
Epoch 2.25: Loss = 0.675049
Epoch 2.26: Loss = 0.611679
Epoch 2.27: Loss = 0.711075
Epoch 2.28: Loss = 0.677948
Epoch 2.29: Loss = 0.701035
Epoch 2.30: Loss = 0.646179
Epoch 2.31: Loss = 0.74379
Epoch 2.32: Loss = 0.672745
Epoch 2.33: Loss = 0.576721
Epoch 2.34: Loss = 0.766785
Epoch 2.35: Loss = 0.682068
Epoch 2.36: Loss = 0.685852
Epoch 2.37: Loss = 0.713791
Epoch 2.38: Loss = 0.667206
Epoch 2.39: Loss = 0.740845
Epoch 2.40: Loss = 0.674133
Epoch 2.41: Loss = 0.714218
Epoch 2.42: Loss = 0.665176
Epoch 2.43: Loss = 0.686569
Epoch 2.44: Loss = 0.629135
Epoch 2.45: Loss = 0.725693
Epoch 2.46: Loss = 0.781494
Epoch 2.47: Loss = 0.641342
Epoch 2.48: Loss = 0.608582
Epoch 2.49: Loss = 0.712738
Epoch 2.50: Loss = 0.673782
Epoch 2.51: Loss = 0.546677
Epoch 2.52: Loss = 0.702896
Epoch 2.53: Loss = 0.749786
Epoch 2.54: Loss = 0.520523
Epoch 2.55: Loss = 0.671555
Epoch 2.56: Loss = 0.664734
Epoch 2.57: Loss = 0.73056
Epoch 2.58: Loss = 0.670212
Epoch 2.59: Loss = 0.744385
Epoch 2.60: Loss = 0.690948
Epoch 2.61: Loss = 0.60025
Epoch 2.62: Loss = 0.728851
Epoch 2.63: Loss = 0.601852
Epoch 2.64: Loss = 0.605606
Epoch 2.65: Loss = 0.708328
Epoch 2.66: Loss = 0.6241
Epoch 2.67: Loss = 0.638062
Epoch 2.68: Loss = 0.776047
Epoch 2.69: Loss = 0.685379
Epoch 2.70: Loss = 0.704803
Epoch 2.71: Loss = 0.546265
Epoch 2.72: Loss = 0.670731
Epoch 2.73: Loss = 0.766663
Epoch 2.74: Loss = 0.675598
Epoch 2.75: Loss = 0.589737
Epoch 2.76: Loss = 0.663345
Epoch 2.77: Loss = 0.654968
Epoch 2.78: Loss = 0.663101
Epoch 2.79: Loss = 0.615387
Epoch 2.80: Loss = 0.60202
Epoch 2.81: Loss = 0.598831
Epoch 2.82: Loss = 0.618195
Epoch 2.83: Loss = 0.677689
Epoch 2.84: Loss = 0.602951
Epoch 2.85: Loss = 0.653
Epoch 2.86: Loss = 0.701523
Epoch 2.87: Loss = 0.652435
Epoch 2.88: Loss = 0.597488
Epoch 2.89: Loss = 0.742767
Epoch 2.90: Loss = 0.648987
Epoch 2.91: Loss = 0.744308
Epoch 2.92: Loss = 0.68663
Epoch 2.93: Loss = 0.708969
Epoch 2.94: Loss = 0.607117
Epoch 2.95: Loss = 0.657608
Epoch 2.96: Loss = 0.62114
Epoch 2.97: Loss = 0.52655
Epoch 2.98: Loss = 0.65033
Epoch 2.99: Loss = 0.706512
Epoch 2.100: Loss = 0.625
Epoch 2.101: Loss = 0.692749
Epoch 2.102: Loss = 0.671066
Epoch 2.103: Loss = 0.629196
Epoch 2.104: Loss = 0.574905
Epoch 2.105: Loss = 0.598633
Epoch 2.106: Loss = 0.723389
Epoch 2.107: Loss = 0.728195
Epoch 2.108: Loss = 0.727753
Epoch 2.109: Loss = 0.687363
Epoch 2.110: Loss = 0.719528
Epoch 2.111: Loss = 0.635529
Epoch 2.112: Loss = 0.624756
Epoch 2.113: Loss = 0.630066
Epoch 2.114: Loss = 0.654678
Epoch 2.115: Loss = 0.631561
Epoch 2.116: Loss = 0.590271
Epoch 2.117: Loss = 0.679352
Epoch 2.118: Loss = 0.5522
Epoch 2.119: Loss = 0.677124
Epoch 2.120: Loss = 0.630035
TRAIN LOSS = 0.667984
TRAIN ACC = 78.0777 % (46849/60000)
Reducing learning rate to 0.399994
entering iterations
Epoch 3.1: Loss = 0.659271
Epoch 3.2: Loss = 0.607712
Epoch 3.3: Loss = 0.687119
Epoch 3.4: Loss = 0.58905
Epoch 3.5: Loss = 0.616043
Epoch 3.6: Loss = 0.754013
Epoch 3.7: Loss = 0.638504
Epoch 3.8: Loss = 0.74823
Epoch 3.9: Loss = 0.524704
Epoch 3.10: Loss = 0.462692
Epoch 3.11: Loss = 0.745392
Epoch 3.12: Loss = 0.628967
Epoch 3.13: Loss = 0.6875
Epoch 3.14: Loss = 0.648605
Epoch 3.15: Loss = 0.638382
Epoch 3.16: Loss = 0.683746
Epoch 3.17: Loss = 0.574509
Epoch 3.18: Loss = 0.663208
Epoch 3.19: Loss = 0.59877
Epoch 3.20: Loss = 0.711914
Epoch 3.21: Loss = 0.546753
Epoch 3.22: Loss = 0.470001
Epoch 3.23: Loss = 0.650269
Epoch 3.24: Loss = 0.705551
Epoch 3.25: Loss = 0.639923
Epoch 3.26: Loss = 0.527084
Epoch 3.27: Loss = 0.625763
Epoch 3.28: Loss = 0.624863
Epoch 3.29: Loss = 0.666031
Epoch 3.30: Loss = 0.617722
Epoch 3.31: Loss = 0.706009
Epoch 3.32: Loss = 0.620026
Epoch 3.33: Loss = 0.546555
Epoch 3.34: Loss = 0.722565
Epoch 3.35: Loss = 0.648239
Epoch 3.36: Loss = 0.654587
Epoch 3.37: Loss = 0.678543
Epoch 3.38: Loss = 0.615967
Epoch 3.39: Loss = 0.710663
Epoch 3.40: Loss = 0.633072
Epoch 3.41: Loss = 0.669815
Epoch 3.42: Loss = 0.618515
Epoch 3.43: Loss = 0.638397
Epoch 3.44: Loss = 0.535095
Epoch 3.45: Loss = 0.687759
Epoch 3.46: Loss = 0.757889
Epoch 3.47: Loss = 0.630753
Epoch 3.48: Loss = 0.567993
Epoch 3.49: Loss = 0.654358
Epoch 3.50: Loss = 0.645767
Epoch 3.51: Loss = 0.493057
Epoch 3.52: Loss = 0.666489
Epoch 3.53: Loss = 0.688812
Epoch 3.54: Loss = 0.479019
Epoch 3.55: Loss = 0.634338
Epoch 3.56: Loss = 0.618881
Epoch 3.57: Loss = 0.710327
Epoch 3.58: Loss = 0.605728
Epoch 3.59: Loss = 0.733139
Epoch 3.60: Loss = 0.632446
Epoch 3.61: Loss = 0.560364
Epoch 3.62: Loss = 0.69162
Epoch 3.63: Loss = 0.570984
Epoch 3.64: Loss = 0.552841
Epoch 3.65: Loss = 0.695587
Epoch 3.66: Loss = 0.553421
Epoch 3.67: Loss = 0.595078
Epoch 3.68: Loss = 0.767319
Epoch 3.69: Loss = 0.62439
Epoch 3.70: Loss = 0.634262
Epoch 3.71: Loss = 0.506241
Epoch 3.72: Loss = 0.631073
Epoch 3.73: Loss = 0.714645
Epoch 3.74: Loss = 0.630951
Epoch 3.75: Loss = 0.561996
Epoch 3.76: Loss = 0.604141
Epoch 3.77: Loss = 0.611618
Epoch 3.78: Loss = 0.640152
Epoch 3.79: Loss = 0.571854
Epoch 3.80: Loss = 0.577087
Epoch 3.81: Loss = 0.564209
Epoch 3.82: Loss = 0.581833
Epoch 3.83: Loss = 0.653931
Epoch 3.84: Loss = 0.561554
Epoch 3.85: Loss = 0.612625
Epoch 3.86: Loss = 0.666275
Epoch 3.87: Loss = 0.614136
Epoch 3.88: Loss = 0.572144
Epoch 3.89: Loss = 0.712921
Epoch 3.90: Loss = 0.626358
Epoch 3.91: Loss = 0.714081
Epoch 3.92: Loss = 0.648209
Epoch 3.93: Loss = 0.662537
Epoch 3.94: Loss = 0.57959
Epoch 3.95: Loss = 0.6483
Epoch 3.96: Loss = 0.580734
Epoch 3.97: Loss = 0.498871
Epoch 3.98: Loss = 0.623795
Epoch 3.99: Loss = 0.659134
Epoch 3.100: Loss = 0.605042
Epoch 3.101: Loss = 0.643494
Epoch 3.102: Loss = 0.645309
Epoch 3.103: Loss = 0.59108
Epoch 3.104: Loss = 0.536087
Epoch 3.105: Loss = 0.554657
Epoch 3.106: Loss = 0.682983
Epoch 3.107: Loss = 0.654663
Epoch 3.108: Loss = 0.749847
Epoch 3.109: Loss = 0.656403
Epoch 3.110: Loss = 0.684464
Epoch 3.111: Loss = 0.617996
Epoch 3.112: Loss = 0.605942
Epoch 3.113: Loss = 0.622208
Epoch 3.114: Loss = 0.646622
Epoch 3.115: Loss = 0.595032
Epoch 3.116: Loss = 0.560867
Epoch 3.117: Loss = 0.641968
Epoch 3.118: Loss = 0.53154
Epoch 3.119: Loss = 0.6315
Epoch 3.120: Loss = 0.592239
TRAIN LOSS = 0.627304
TRAIN ACC = 80.4596 % (48278/60000)
Reducing learning rate to 0.399994
entering iterations
Epoch 4.1: Loss = 0.594238
Epoch 4.2: Loss = 0.58252
Epoch 4.3: Loss = 0.653275
Epoch 4.4: Loss = 0.554428
Epoch 4.5: Loss = 0.59877
Epoch 4.6: Loss = 0.743271
Epoch 4.7: Loss = 0.599274
Epoch 4.8: Loss = 0.719925
Epoch 4.9: Loss = 0.488525
Epoch 4.10: Loss = 0.429291
Epoch 4.11: Loss = 0.706543
Epoch 4.12: Loss = 0.598679
Epoch 4.13: Loss = 0.655579
Epoch 4.14: Loss = 0.619049
Epoch 4.15: Loss = 0.645447
Epoch 4.16: Loss = 0.6586
Epoch 4.17: Loss = 0.540756
Epoch 4.18: Loss = 0.640793
Epoch 4.19: Loss = 0.564346
Epoch 4.20: Loss = 0.681213
Epoch 4.21: Loss = 0.518112
Epoch 4.22: Loss = 0.447403
Epoch 4.23: Loss = 0.608124
Epoch 4.24: Loss = 0.672974
Epoch 4.25: Loss = 0.630203
Epoch 4.26: Loss = 0.491745
Epoch 4.27: Loss = 0.58989
Epoch 4.28: Loss = 0.597458
Epoch 4.29: Loss = 0.632889
Epoch 4.30: Loss = 0.601334
Epoch 4.31: Loss = 0.673141
Epoch 4.32: Loss = 0.573227
Epoch 4.33: Loss = 0.53714
Epoch 4.34: Loss = 0.686417
Epoch 4.35: Loss = 0.62973
Epoch 4.36: Loss = 0.635193
Epoch 4.37: Loss = 0.666626
Epoch 4.38: Loss = 0.591934
Epoch 4.39: Loss = 0.700821
Epoch 4.40: Loss = 0.608109
Epoch 4.41: Loss = 0.642426
Epoch 4.42: Loss = 0.599518
Epoch 4.43: Loss = 0.624283
Epoch 4.44: Loss = 0.490387
Epoch 4.45: Loss = 0.667953
Epoch 4.46: Loss = 0.726196
Epoch 4.47: Loss = 0.620773
Epoch 4.48: Loss = 0.547348
Epoch 4.49: Loss = 0.63681
Epoch 4.50: Loss = 0.632233
Epoch 4.51: Loss = 0.469742
Epoch 4.52: Loss = 0.639374
Epoch 4.53: Loss = 0.656647
Epoch 4.54: Loss = 0.460617
Epoch 4.55: Loss = 0.614288
Epoch 4.56: Loss = 0.59346
Epoch 4.57: Loss = 0.698044
Epoch 4.58: Loss = 0.567383
Epoch 4.59: Loss = 0.716995
Epoch 4.60: Loss = 0.616455
Epoch 4.61: Loss = 0.538971
Epoch 4.62: Loss = 0.663132
Epoch 4.63: Loss = 0.549225
Epoch 4.64: Loss = 0.520569
Epoch 4.65: Loss = 0.682266
Epoch 4.66: Loss = 0.523743
Epoch 4.67: Loss = 0.561218
Epoch 4.68: Loss = 0.773575
Epoch 4.69: Loss = 0.590347
Epoch 4.70: Loss = 0.608429
Epoch 4.71: Loss = 0.488251
Epoch 4.72: Loss = 0.61438
Epoch 4.73: Loss = 0.689468
Epoch 4.74: Loss = 0.612259
Epoch 4.75: Loss = 0.552094
Epoch 4.76: Loss = 0.57338
Epoch 4.77: Loss = 0.596252
Epoch 4.78: Loss = 0.629379
Epoch 4.79: Loss = 0.550339
Epoch 4.80: Loss = 0.574265
Epoch 4.81: Loss = 0.557999
Epoch 4.82: Loss = 0.574188
Epoch 4.83: Loss = 0.64238
Epoch 4.84: Loss = 0.543137
Epoch 4.85: Loss = 0.593628
Epoch 4.86: Loss = 0.649979
Epoch 4.87: Loss = 0.593719
Epoch 4.88: Loss = 0.551315
Epoch 4.89: Loss = 0.686096
Epoch 4.90: Loss = 0.623459
Epoch 4.91: Loss = 0.68251
Epoch 4.92: Loss = 0.620987
Epoch 4.93: Loss = 0.628006
Epoch 4.94: Loss = 0.568451
Epoch 4.95: Loss = 0.635956
Epoch 4.96: Loss = 0.566116
Epoch 4.97: Loss = 0.487244
Epoch 4.98: Loss = 0.609619
Epoch 4.99: Loss = 0.633087
Epoch 4.100: Loss = 0.593445
Epoch 4.101: Loss = 0.620346
Epoch 4.102: Loss = 0.636673
Epoch 4.103: Loss = 0.572342
Epoch 4.104: Loss = 0.527237
Epoch 4.105: Loss = 0.534393
Epoch 4.106: Loss = 0.674438
Epoch 4.107: Loss = 0.60936
Epoch 4.108: Loss = 0.770157
Epoch 4.109: Loss = 0.638702
Epoch 4.110: Loss = 0.666382
Epoch 4.111: Loss = 0.604568
Epoch 4.112: Loss = 0.598541
Epoch 4.113: Loss = 0.609985
Epoch 4.114: Loss = 0.647018
Epoch 4.115: Loss = 0.574173
Epoch 4.116: Loss = 0.541016
Epoch 4.117: Loss = 0.626678
Epoch 4.118: Loss = 0.522049
Epoch 4.119: Loss = 0.604492
Epoch 4.120: Loss = 0.575974
TRAIN LOSS = 0.605652
TRAIN ACC = 81.6483 % (48992/60000)
Reducing learning rate to 0.399994
entering iterations
Epoch 5.1: Loss = 0.552475
Epoch 5.2: Loss = 0.57106
Epoch 5.3: Loss = 0.635254
Epoch 5.4: Loss = 0.536987
Epoch 5.5: Loss = 0.596207
Epoch 5.6: Loss = 0.740173
Epoch 5.7: Loss = 0.575851
Epoch 5.8: Loss = 0.70459
Epoch 5.9: Loss = 0.473816
Epoch 5.10: Loss = 0.409302
Epoch 5.11: Loss = 0.685394
Epoch 5.12: Loss = 0.581177
Epoch 5.13: Loss = 0.639282
Epoch 5.14: Loss = 0.598618
Epoch 5.15: Loss = 0.639816
Epoch 5.16: Loss = 0.644775
Epoch 5.17: Loss = 0.527069
Epoch 5.18: Loss = 0.630371
Epoch 5.19: Loss = 0.544739
Epoch 5.20: Loss = 0.661423
Epoch 5.21: Loss = 0.5056
Epoch 5.22: Loss = 0.43837
Epoch 5.23: Loss = 0.58519
Epoch 5.24: Loss = 0.652847
Epoch 5.25: Loss = 0.618851
Epoch 5.26: Loss = 0.473877
Epoch 5.27: Loss = 0.567291
Epoch 5.28: Loss = 0.582169
Epoch 5.29: Loss = 0.612259
Epoch 5.30: Loss = 0.594574
Epoch 5.31: Loss = 0.650696
Epoch 5.32: Loss = 0.549454
Epoch 5.33: Loss = 0.535995
Epoch 5.34: Loss = 0.667999
Epoch 5.35: Loss = 0.617249
Epoch 5.36: Loss = 0.622162
Epoch 5.37: Loss = 0.662048
Epoch 5.38: Loss = 0.576035
Epoch 5.39: Loss = 0.699539
Epoch 5.40: Loss = 0.590195
Epoch 5.41: Loss = 0.623901
Epoch 5.42: Loss = 0.592407
Epoch 5.43: Loss = 0.611633
Epoch 5.44: Loss = 0.467636
Epoch 5.45: Loss = 0.651993
Epoch 5.46: Loss = 0.713181
Epoch 5.47: Loss = 0.618362
Epoch 5.48: Loss = 0.534851
Epoch 5.49: Loss = 0.632965
Epoch 5.50: Loss = 0.626175
Epoch 5.51: Loss = 0.455734
Epoch 5.52: Loss = 0.616928
Epoch 5.53: Loss = 0.639633
Epoch 5.54: Loss = 0.452744
Epoch 5.55: Loss = 0.600739
Epoch 5.56: Loss = 0.581253
Epoch 5.57: Loss = 0.683395
Epoch 5.58: Loss = 0.545013
Epoch 5.59: Loss = 0.703766
Epoch 5.60: Loss = 0.612335
Epoch 5.61: Loss = 0.530838
Epoch 5.62: Loss = 0.647675
Epoch 5.63: Loss = 0.532532
Epoch 5.64: Loss = 0.501099
Epoch 5.65: Loss = 0.66861
Epoch 5.66: Loss = 0.500381
Epoch 5.67: Loss = 0.535843
Epoch 5.68: Loss = 0.779755
Epoch 5.69: Loss = 0.572113
Epoch 5.70: Loss = 0.597656
Epoch 5.71: Loss = 0.484589
Epoch 5.72: Loss = 0.611237
Epoch 5.73: Loss = 0.673386
Epoch 5.74: Loss = 0.6026
Epoch 5.75: Loss = 0.543686
Epoch 5.76: Loss = 0.566345
Epoch 5.77: Loss = 0.589951
Epoch 5.78: Loss = 0.611588
Epoch 5.79: Loss = 0.540405
Epoch 5.80: Loss = 0.572754
Epoch 5.81: Loss = 0.559052
Epoch 5.82: Loss = 0.578201
Epoch 5.83: Loss = 0.632263
Epoch 5.84: Loss = 0.532547
Epoch 5.85: Loss = 0.583679
Epoch 5.86: Loss = 0.637405
Epoch 5.87: Loss = 0.582321
Epoch 5.88: Loss = 0.541306
Epoch 5.89: Loss = 0.668533
Epoch 5.90: Loss = 0.626083
Epoch 5.91: Loss = 0.660782
Epoch 5.92: Loss = 0.605331
Epoch 5.93: Loss = 0.602325
Epoch 5.94: Loss = 0.563568
Epoch 5.95: Loss = 0.62059
Epoch 5.96: Loss = 0.561508
Epoch 5.97: Loss = 0.480423
Epoch 5.98: Loss = 0.59549
Epoch 5.99: Loss = 0.613556
Epoch 5.100: Loss = 0.58757
Epoch 5.101: Loss = 0.604294
Epoch 5.102: Loss = 0.633286
Epoch 5.103: Loss = 0.561951
Epoch 5.104: Loss = 0.527405
Epoch 5.105: Loss = 0.522644
Epoch 5.106: Loss = 0.675369
Epoch 5.107: Loss = 0.597733
Epoch 5.108: Loss = 0.779739
Epoch 5.109: Loss = 0.628479
Epoch 5.110: Loss = 0.654526
Epoch 5.111: Loss = 0.596893
Epoch 5.112: Loss = 0.597427
Epoch 5.113: Loss = 0.597672
Epoch 5.114: Loss = 0.649719
Epoch 5.115: Loss = 0.561752
Epoch 5.116: Loss = 0.52388
Epoch 5.117: Loss = 0.618561
Epoch 5.118: Loss = 0.514206
Epoch 5.119: Loss = 0.589218
Epoch 5.120: Loss = 0.566818
TRAIN LOSS = 0.593445
TRAIN ACC = 82.4097 % (49448/60000)
Reducing learning rate to 0.399994
