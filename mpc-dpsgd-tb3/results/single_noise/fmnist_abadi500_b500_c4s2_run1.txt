Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.35092
Epoch 1.2: Loss = 2.28423
Epoch 1.3: Loss = 2.18495
Epoch 1.4: Loss = 2.08806
Epoch 1.5: Loss = 2.02562
Epoch 1.6: Loss = 1.98611
Epoch 1.7: Loss = 1.91064
Epoch 1.8: Loss = 1.83791
Epoch 1.9: Loss = 1.79829
Epoch 1.10: Loss = 1.72183
Epoch 1.11: Loss = 1.72388
Epoch 1.12: Loss = 1.65402
Epoch 1.13: Loss = 1.6057
Epoch 1.14: Loss = 1.57451
Epoch 1.15: Loss = 1.5054
Epoch 1.16: Loss = 1.49275
Epoch 1.17: Loss = 1.49963
Epoch 1.18: Loss = 1.42993
Epoch 1.19: Loss = 1.38829
Epoch 1.20: Loss = 1.40671
Epoch 1.21: Loss = 1.33795
Epoch 1.22: Loss = 1.29904
Epoch 1.23: Loss = 1.27829
Epoch 1.24: Loss = 1.33363
Epoch 1.25: Loss = 1.29709
Epoch 1.26: Loss = 1.21417
Epoch 1.27: Loss = 1.18266
Epoch 1.28: Loss = 1.17224
Epoch 1.29: Loss = 1.179
Epoch 1.30: Loss = 1.1494
Epoch 1.31: Loss = 1.18103
Epoch 1.32: Loss = 1.14484
Epoch 1.33: Loss = 1.04588
Epoch 1.34: Loss = 1.1189
Epoch 1.35: Loss = 1.16843
Epoch 1.36: Loss = 1.1273
Epoch 1.37: Loss = 1.10577
Epoch 1.38: Loss = 1.06363
Epoch 1.39: Loss = 1.04602
Epoch 1.40: Loss = 1.04239
Epoch 1.41: Loss = 1.07816
Epoch 1.42: Loss = 0.99765
Epoch 1.43: Loss = 0.982895
Epoch 1.44: Loss = 0.956619
Epoch 1.45: Loss = 1.00095
Epoch 1.46: Loss = 1.00319
Epoch 1.47: Loss = 0.969971
Epoch 1.48: Loss = 0.93605
Epoch 1.49: Loss = 0.987366
Epoch 1.50: Loss = 0.942902
Epoch 1.51: Loss = 0.907288
Epoch 1.52: Loss = 0.978897
Epoch 1.53: Loss = 0.982361
Epoch 1.54: Loss = 0.8582
Epoch 1.55: Loss = 0.926544
Epoch 1.56: Loss = 0.943649
Epoch 1.57: Loss = 0.958939
Epoch 1.58: Loss = 0.924377
Epoch 1.59: Loss = 0.902145
Epoch 1.60: Loss = 0.949631
Epoch 1.61: Loss = 0.851456
Epoch 1.62: Loss = 0.93866
Epoch 1.63: Loss = 0.794037
Epoch 1.64: Loss = 0.834198
Epoch 1.65: Loss = 0.856689
Epoch 1.66: Loss = 0.853394
Epoch 1.67: Loss = 0.784195
Epoch 1.68: Loss = 0.903885
Epoch 1.69: Loss = 0.852798
Epoch 1.70: Loss = 0.843033
Epoch 1.71: Loss = 0.781921
Epoch 1.72: Loss = 0.799316
Epoch 1.73: Loss = 0.884048
Epoch 1.74: Loss = 0.878128
Epoch 1.75: Loss = 0.797806
Epoch 1.76: Loss = 0.828247
Epoch 1.77: Loss = 0.807846
Epoch 1.78: Loss = 0.810287
Epoch 1.79: Loss = 0.75502
Epoch 1.80: Loss = 0.805832
Epoch 1.81: Loss = 0.760605
Epoch 1.82: Loss = 0.777512
Epoch 1.83: Loss = 0.825867
Epoch 1.84: Loss = 0.794479
Epoch 1.85: Loss = 0.770279
Epoch 1.86: Loss = 0.836914
Epoch 1.87: Loss = 0.829361
Epoch 1.88: Loss = 0.709015
Epoch 1.89: Loss = 0.840195
Epoch 1.90: Loss = 0.768402
Epoch 1.91: Loss = 0.808487
Epoch 1.92: Loss = 0.783157
Epoch 1.93: Loss = 0.79068
Epoch 1.94: Loss = 0.762711
Epoch 1.95: Loss = 0.805603
Epoch 1.96: Loss = 0.746292
Epoch 1.97: Loss = 0.662338
Epoch 1.98: Loss = 0.758102
Epoch 1.99: Loss = 0.767258
Epoch 1.100: Loss = 0.741714
Epoch 1.101: Loss = 0.808319
Epoch 1.102: Loss = 0.779877
Epoch 1.103: Loss = 0.785873
Epoch 1.104: Loss = 0.731903
Epoch 1.105: Loss = 0.70546
Epoch 1.106: Loss = 0.845459
Epoch 1.107: Loss = 0.759598
Epoch 1.108: Loss = 0.771271
Epoch 1.109: Loss = 0.746811
Epoch 1.110: Loss = 0.753693
Epoch 1.111: Loss = 0.684723
Epoch 1.112: Loss = 0.677567
Epoch 1.113: Loss = 0.741898
Epoch 1.114: Loss = 0.745239
Epoch 1.115: Loss = 0.742661
Epoch 1.116: Loss = 0.664719
Epoch 1.117: Loss = 0.804443
Epoch 1.118: Loss = 0.673141
Epoch 1.119: Loss = 0.715042
Epoch 1.120: Loss = 0.701691
TRAIN LOSS = 1.04321
TRAIN ACC = 64.8972 % (38940/60000)
Loss = 0.681671
Loss = 0.787308
Loss = 0.757111
Loss = 0.69223
Loss = 0.684128
Loss = 0.833633
Loss = 0.839767
Loss = 0.788147
Loss = 0.731369
Loss = 0.698868
Loss = 0.787674
Loss = 0.782425
Loss = 0.747375
Loss = 0.786896
Loss = 0.722504
Loss = 0.784164
Loss = 0.718979
Loss = 0.757507
Loss = 0.806198
Loss = 0.730453
TEST LOSS = 0.75592
TEST ACC = 389.4 % (7328/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.742172
Epoch 2.2: Loss = 0.715927
Epoch 2.3: Loss = 0.787247
Epoch 2.4: Loss = 0.656509
Epoch 2.5: Loss = 0.718765
Epoch 2.6: Loss = 0.79509
Epoch 2.7: Loss = 0.724243
Epoch 2.8: Loss = 0.779694
Epoch 2.9: Loss = 0.659195
Epoch 2.10: Loss = 0.587051
Epoch 2.11: Loss = 0.782608
Epoch 2.12: Loss = 0.716537
Epoch 2.13: Loss = 0.710678
Epoch 2.14: Loss = 0.718399
Epoch 2.15: Loss = 0.705307
Epoch 2.16: Loss = 0.756851
Epoch 2.17: Loss = 0.700089
Epoch 2.18: Loss = 0.723404
Epoch 2.19: Loss = 0.695801
Epoch 2.20: Loss = 0.780273
Epoch 2.21: Loss = 0.648453
Epoch 2.22: Loss = 0.621582
Epoch 2.23: Loss = 0.698837
Epoch 2.24: Loss = 0.794235
Epoch 2.25: Loss = 0.712402
Epoch 2.26: Loss = 0.636475
Epoch 2.27: Loss = 0.692688
Epoch 2.28: Loss = 0.684128
Epoch 2.29: Loss = 0.70636
Epoch 2.30: Loss = 0.686081
Epoch 2.31: Loss = 0.749847
Epoch 2.32: Loss = 0.680298
Epoch 2.33: Loss = 0.619537
Epoch 2.34: Loss = 0.74614
Epoch 2.35: Loss = 0.737442
Epoch 2.36: Loss = 0.742172
Epoch 2.37: Loss = 0.738373
Epoch 2.38: Loss = 0.69136
Epoch 2.39: Loss = 0.755753
Epoch 2.40: Loss = 0.695389
Epoch 2.41: Loss = 0.739471
Epoch 2.42: Loss = 0.684708
Epoch 2.43: Loss = 0.676331
Epoch 2.44: Loss = 0.619446
Epoch 2.45: Loss = 0.701614
Epoch 2.46: Loss = 0.765274
Epoch 2.47: Loss = 0.648163
Epoch 2.48: Loss = 0.631454
Epoch 2.49: Loss = 0.73317
Epoch 2.50: Loss = 0.686676
Epoch 2.51: Loss = 0.598633
Epoch 2.52: Loss = 0.703827
Epoch 2.53: Loss = 0.75647
Epoch 2.54: Loss = 0.580704
Epoch 2.55: Loss = 0.680542
Epoch 2.56: Loss = 0.699295
Epoch 2.57: Loss = 0.741699
Epoch 2.58: Loss = 0.701355
Epoch 2.59: Loss = 0.695435
Epoch 2.60: Loss = 0.706619
Epoch 2.61: Loss = 0.637405
Epoch 2.62: Loss = 0.716537
Epoch 2.63: Loss = 0.585007
Epoch 2.64: Loss = 0.606567
Epoch 2.65: Loss = 0.667007
Epoch 2.66: Loss = 0.644196
Epoch 2.67: Loss = 0.63269
Epoch 2.68: Loss = 0.74501
Epoch 2.69: Loss = 0.675369
Epoch 2.70: Loss = 0.697098
Epoch 2.71: Loss = 0.593231
Epoch 2.72: Loss = 0.660568
Epoch 2.73: Loss = 0.753937
Epoch 2.74: Loss = 0.710648
Epoch 2.75: Loss = 0.61998
Epoch 2.76: Loss = 0.649338
Epoch 2.77: Loss = 0.655197
Epoch 2.78: Loss = 0.66864
Epoch 2.79: Loss = 0.641464
Epoch 2.80: Loss = 0.625534
Epoch 2.81: Loss = 0.623581
Epoch 2.82: Loss = 0.608994
Epoch 2.83: Loss = 0.699036
Epoch 2.84: Loss = 0.628601
Epoch 2.85: Loss = 0.650528
Epoch 2.86: Loss = 0.710114
Epoch 2.87: Loss = 0.695969
Epoch 2.88: Loss = 0.588654
Epoch 2.89: Loss = 0.733826
Epoch 2.90: Loss = 0.662231
Epoch 2.91: Loss = 0.705475
Epoch 2.92: Loss = 0.670593
Epoch 2.93: Loss = 0.687073
Epoch 2.94: Loss = 0.638321
Epoch 2.95: Loss = 0.677567
Epoch 2.96: Loss = 0.629044
Epoch 2.97: Loss = 0.557648
Epoch 2.98: Loss = 0.629959
Epoch 2.99: Loss = 0.648285
Epoch 2.100: Loss = 0.641022
Epoch 2.101: Loss = 0.71843
Epoch 2.102: Loss = 0.677429
Epoch 2.103: Loss = 0.652634
Epoch 2.104: Loss = 0.609634
Epoch 2.105: Loss = 0.604889
Epoch 2.106: Loss = 0.74292
Epoch 2.107: Loss = 0.693924
Epoch 2.108: Loss = 0.713745
Epoch 2.109: Loss = 0.67691
Epoch 2.110: Loss = 0.66481
Epoch 2.111: Loss = 0.597168
Epoch 2.112: Loss = 0.600586
Epoch 2.113: Loss = 0.646439
Epoch 2.114: Loss = 0.662613
Epoch 2.115: Loss = 0.646149
Epoch 2.116: Loss = 0.577957
Epoch 2.117: Loss = 0.712738
Epoch 2.118: Loss = 0.57373
Epoch 2.119: Loss = 0.635956
Epoch 2.120: Loss = 0.605743
TRAIN LOSS = 0.679443
TRAIN ACC = 76.5945 % (45959/60000)
Loss = 0.599319
Loss = 0.713135
Loss = 0.660782
Loss = 0.584015
Loss = 0.60199
Loss = 0.762878
Loss = 0.778641
Loss = 0.712219
Loss = 0.658508
Loss = 0.616928
Loss = 0.727921
Loss = 0.73497
Loss = 0.669662
Loss = 0.696503
Loss = 0.646118
Loss = 0.710464
Loss = 0.639359
Loss = 0.68988
Loss = 0.729813
Loss = 0.652634
TEST LOSS = 0.679287
TEST ACC = 459.589 % (7667/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.654907
Epoch 3.2: Loss = 0.651047
Epoch 3.3: Loss = 0.700577
Epoch 3.4: Loss = 0.569962
Epoch 3.5: Loss = 0.640213
Epoch 3.6: Loss = 0.737061
Epoch 3.7: Loss = 0.660599
Epoch 3.8: Loss = 0.742706
Epoch 3.9: Loss = 0.563751
Epoch 3.10: Loss = 0.505615
Epoch 3.11: Loss = 0.727707
Epoch 3.12: Loss = 0.644897
Epoch 3.13: Loss = 0.657959
Epoch 3.14: Loss = 0.641006
Epoch 3.15: Loss = 0.645767
Epoch 3.16: Loss = 0.700027
Epoch 3.17: Loss = 0.616684
Epoch 3.18: Loss = 0.65654
Epoch 3.19: Loss = 0.627777
Epoch 3.20: Loss = 0.725235
Epoch 3.21: Loss = 0.564011
Epoch 3.22: Loss = 0.529739
Epoch 3.23: Loss = 0.637283
Epoch 3.24: Loss = 0.746643
Epoch 3.25: Loss = 0.633118
Epoch 3.26: Loss = 0.559845
Epoch 3.27: Loss = 0.644058
Epoch 3.28: Loss = 0.63736
Epoch 3.29: Loss = 0.645981
Epoch 3.30: Loss = 0.642899
Epoch 3.31: Loss = 0.704514
Epoch 3.32: Loss = 0.611023
Epoch 3.33: Loss = 0.564362
Epoch 3.34: Loss = 0.713684
Epoch 3.35: Loss = 0.671539
Epoch 3.36: Loss = 0.695648
Epoch 3.37: Loss = 0.694229
Epoch 3.38: Loss = 0.653214
Epoch 3.39: Loss = 0.723694
Epoch 3.40: Loss = 0.646606
Epoch 3.41: Loss = 0.698547
Epoch 3.42: Loss = 0.626678
Epoch 3.43: Loss = 0.63588
Epoch 3.44: Loss = 0.576477
Epoch 3.45: Loss = 0.661224
Epoch 3.46: Loss = 0.734665
Epoch 3.47: Loss = 0.597244
Epoch 3.48: Loss = 0.579483
Epoch 3.49: Loss = 0.680252
Epoch 3.50: Loss = 0.656464
Epoch 3.51: Loss = 0.543701
Epoch 3.52: Loss = 0.669678
Epoch 3.53: Loss = 0.724762
Epoch 3.54: Loss = 0.525223
Epoch 3.55: Loss = 0.636475
Epoch 3.56: Loss = 0.653305
Epoch 3.57: Loss = 0.696869
Epoch 3.58: Loss = 0.638657
Epoch 3.59: Loss = 0.675949
Epoch 3.60: Loss = 0.669037
Epoch 3.61: Loss = 0.604294
Epoch 3.62: Loss = 0.68042
Epoch 3.63: Loss = 0.546539
Epoch 3.64: Loss = 0.544357
Epoch 3.65: Loss = 0.639694
Epoch 3.66: Loss = 0.600174
Epoch 3.67: Loss = 0.601364
Epoch 3.68: Loss = 0.733429
Epoch 3.69: Loss = 0.637329
Epoch 3.70: Loss = 0.662216
Epoch 3.71: Loss = 0.549698
Epoch 3.72: Loss = 0.637421
Epoch 3.73: Loss = 0.724258
Epoch 3.74: Loss = 0.677246
Epoch 3.75: Loss = 0.577072
Epoch 3.76: Loss = 0.611923
Epoch 3.77: Loss = 0.624161
Epoch 3.78: Loss = 0.638794
Epoch 3.79: Loss = 0.62059
Epoch 3.80: Loss = 0.587158
Epoch 3.81: Loss = 0.596863
Epoch 3.82: Loss = 0.566437
Epoch 3.83: Loss = 0.665039
Epoch 3.84: Loss = 0.585541
Epoch 3.85: Loss = 0.623489
Epoch 3.86: Loss = 0.670715
Epoch 3.87: Loss = 0.647827
Epoch 3.88: Loss = 0.561676
Epoch 3.89: Loss = 0.714981
Epoch 3.90: Loss = 0.638367
Epoch 3.91: Loss = 0.675659
Epoch 3.92: Loss = 0.631912
Epoch 3.93: Loss = 0.662155
Epoch 3.94: Loss = 0.606644
Epoch 3.95: Loss = 0.649429
Epoch 3.96: Loss = 0.587234
Epoch 3.97: Loss = 0.517914
Epoch 3.98: Loss = 0.604034
Epoch 3.99: Loss = 0.620926
Epoch 3.100: Loss = 0.604797
Epoch 3.101: Loss = 0.698929
Epoch 3.102: Loss = 0.64798
Epoch 3.103: Loss = 0.618713
Epoch 3.104: Loss = 0.580704
Epoch 3.105: Loss = 0.5746
Epoch 3.106: Loss = 0.717941
Epoch 3.107: Loss = 0.675385
Epoch 3.108: Loss = 0.702545
Epoch 3.109: Loss = 0.668884
Epoch 3.110: Loss = 0.636841
Epoch 3.111: Loss = 0.577286
Epoch 3.112: Loss = 0.583664
Epoch 3.113: Loss = 0.608932
Epoch 3.114: Loss = 0.642395
Epoch 3.115: Loss = 0.610718
Epoch 3.116: Loss = 0.557495
Epoch 3.117: Loss = 0.685562
Epoch 3.118: Loss = 0.547623
Epoch 3.119: Loss = 0.610489
Epoch 3.120: Loss = 0.583084
TRAIN LOSS = 0.636093
TRAIN ACC = 78.9352 % (47363/60000)
Loss = 0.572372
Loss = 0.69455
Loss = 0.628952
Loss = 0.56279
Loss = 0.587585
Loss = 0.747452
Loss = 0.770401
Loss = 0.69487
Loss = 0.643524
Loss = 0.59906
Loss = 0.725021
Loss = 0.733536
Loss = 0.654129
Loss = 0.68158
Loss = 0.631607
Loss = 0.681824
Loss = 0.623947
Loss = 0.678009
Loss = 0.710297
Loss = 0.637543
TEST LOSS = 0.662952
TEST ACC = 473.63 % (7833/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.626236
Epoch 4.2: Loss = 0.640427
Epoch 4.3: Loss = 0.668579
Epoch 4.4: Loss = 0.547653
Epoch 4.5: Loss = 0.618378
Epoch 4.6: Loss = 0.7061
Epoch 4.7: Loss = 0.625198
Epoch 4.8: Loss = 0.718796
Epoch 4.9: Loss = 0.529144
Epoch 4.10: Loss = 0.475204
Epoch 4.11: Loss = 0.720963
Epoch 4.12: Loss = 0.61824
Epoch 4.13: Loss = 0.633057
Epoch 4.14: Loss = 0.618042
Epoch 4.15: Loss = 0.616714
Epoch 4.16: Loss = 0.682541
Epoch 4.17: Loss = 0.585541
Epoch 4.18: Loss = 0.637878
Epoch 4.19: Loss = 0.596161
Epoch 4.20: Loss = 0.700928
Epoch 4.21: Loss = 0.526505
Epoch 4.22: Loss = 0.495041
Epoch 4.23: Loss = 0.611969
Epoch 4.24: Loss = 0.729813
Epoch 4.25: Loss = 0.602432
Epoch 4.26: Loss = 0.528915
Epoch 4.27: Loss = 0.616852
Epoch 4.28: Loss = 0.615265
Epoch 4.29: Loss = 0.620438
Epoch 4.30: Loss = 0.623108
Epoch 4.31: Loss = 0.678268
Epoch 4.32: Loss = 0.589981
Epoch 4.33: Loss = 0.542465
Epoch 4.34: Loss = 0.683853
Epoch 4.35: Loss = 0.656387
Epoch 4.36: Loss = 0.668594
Epoch 4.37: Loss = 0.681473
Epoch 4.38: Loss = 0.631378
Epoch 4.39: Loss = 0.684036
Epoch 4.40: Loss = 0.61409
Epoch 4.41: Loss = 0.676682
Epoch 4.42: Loss = 0.599228
Epoch 4.43: Loss = 0.60733
Epoch 4.44: Loss = 0.546021
Epoch 4.45: Loss = 0.64389
Epoch 4.46: Loss = 0.718124
Epoch 4.47: Loss = 0.574081
Epoch 4.48: Loss = 0.557327
Epoch 4.49: Loss = 0.637527
Epoch 4.50: Loss = 0.643311
Epoch 4.51: Loss = 0.516693
Epoch 4.52: Loss = 0.65036
Epoch 4.53: Loss = 0.693649
Epoch 4.54: Loss = 0.493057
Epoch 4.55: Loss = 0.608627
Epoch 4.56: Loss = 0.636368
Epoch 4.57: Loss = 0.695084
Epoch 4.58: Loss = 0.614349
Epoch 4.59: Loss = 0.674347
Epoch 4.60: Loss = 0.630371
Epoch 4.61: Loss = 0.577591
Epoch 4.62: Loss = 0.64978
Epoch 4.63: Loss = 0.530792
Epoch 4.64: Loss = 0.515823
Epoch 4.65: Loss = 0.624695
Epoch 4.66: Loss = 0.564316
Epoch 4.67: Loss = 0.581833
Epoch 4.68: Loss = 0.732346
Epoch 4.69: Loss = 0.613602
Epoch 4.70: Loss = 0.637878
Epoch 4.71: Loss = 0.534103
Epoch 4.72: Loss = 0.62323
Epoch 4.73: Loss = 0.702789
Epoch 4.74: Loss = 0.657608
Epoch 4.75: Loss = 0.552536
Epoch 4.76: Loss = 0.598511
Epoch 4.77: Loss = 0.610031
Epoch 4.78: Loss = 0.618835
Epoch 4.79: Loss = 0.599228
Epoch 4.80: Loss = 0.561966
Epoch 4.81: Loss = 0.574799
Epoch 4.82: Loss = 0.54924
Epoch 4.83: Loss = 0.649033
Epoch 4.84: Loss = 0.565323
Epoch 4.85: Loss = 0.602142
Epoch 4.86: Loss = 0.648193
Epoch 4.87: Loss = 0.613968
Epoch 4.88: Loss = 0.554214
Epoch 4.89: Loss = 0.697693
Epoch 4.90: Loss = 0.624359
Epoch 4.91: Loss = 0.65921
Epoch 4.92: Loss = 0.618454
Epoch 4.93: Loss = 0.644089
Epoch 4.94: Loss = 0.598679
Epoch 4.95: Loss = 0.635757
Epoch 4.96: Loss = 0.572296
Epoch 4.97: Loss = 0.504501
Epoch 4.98: Loss = 0.585632
Epoch 4.99: Loss = 0.603577
Epoch 4.100: Loss = 0.596008
Epoch 4.101: Loss = 0.676666
Epoch 4.102: Loss = 0.638809
Epoch 4.103: Loss = 0.593353
Epoch 4.104: Loss = 0.551956
Epoch 4.105: Loss = 0.552979
Epoch 4.106: Loss = 0.69519
Epoch 4.107: Loss = 0.653687
Epoch 4.108: Loss = 0.697998
Epoch 4.109: Loss = 0.669357
Epoch 4.110: Loss = 0.626984
Epoch 4.111: Loss = 0.561539
Epoch 4.112: Loss = 0.579269
Epoch 4.113: Loss = 0.595673
Epoch 4.114: Loss = 0.645157
Epoch 4.115: Loss = 0.588135
Epoch 4.116: Loss = 0.5513
Epoch 4.117: Loss = 0.670288
Epoch 4.118: Loss = 0.528305
Epoch 4.119: Loss = 0.592133
Epoch 4.120: Loss = 0.583786
TRAIN LOSS = 0.615189
TRAIN ACC = 80.0461 % (48030/60000)
Loss = 0.555115
Loss = 0.680847
Loss = 0.601028
Loss = 0.541519
Loss = 0.579269
Loss = 0.724747
Loss = 0.754059
Loss = 0.683365
Loss = 0.624847
Loss = 0.583328
Loss = 0.730286
Loss = 0.725098
Loss = 0.642181
Loss = 0.665451
Loss = 0.614365
Loss = 0.662582
Loss = 0.60527
Loss = 0.661102
Loss = 0.696777
Loss = 0.623154
TEST LOSS = 0.647719
TEST ACC = 480.299 % (7940/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.610657
Epoch 5.2: Loss = 0.620331
Epoch 5.3: Loss = 0.656693
Epoch 5.4: Loss = 0.536407
Epoch 5.5: Loss = 0.617661
Epoch 5.6: Loss = 0.693817
Epoch 5.7: Loss = 0.613464
Epoch 5.8: Loss = 0.717133
Epoch 5.9: Loss = 0.512711
Epoch 5.10: Loss = 0.467117
Epoch 5.11: Loss = 0.715607
Epoch 5.12: Loss = 0.607147
Epoch 5.13: Loss = 0.627319
Epoch 5.14: Loss = 0.612503
Epoch 5.15: Loss = 0.611877
Epoch 5.16: Loss = 0.676498
Epoch 5.17: Loss = 0.567154
Epoch 5.18: Loss = 0.633591
Epoch 5.19: Loss = 0.580429
Epoch 5.20: Loss = 0.697876
Epoch 5.21: Loss = 0.516983
Epoch 5.22: Loss = 0.487549
Epoch 5.23: Loss = 0.589096
Epoch 5.24: Loss = 0.724274
Epoch 5.25: Loss = 0.592743
Epoch 5.26: Loss = 0.512543
Epoch 5.27: Loss = 0.605225
Epoch 5.28: Loss = 0.603851
Epoch 5.29: Loss = 0.601349
Epoch 5.30: Loss = 0.61937
Epoch 5.31: Loss = 0.669373
Epoch 5.32: Loss = 0.57605
Epoch 5.33: Loss = 0.532623
Epoch 5.34: Loss = 0.681107
Epoch 5.35: Loss = 0.643082
Epoch 5.36: Loss = 0.665649
Epoch 5.37: Loss = 0.674866
Epoch 5.38: Loss = 0.624313
Epoch 5.39: Loss = 0.680313
Epoch 5.40: Loss = 0.603043
Epoch 5.41: Loss = 0.661224
Epoch 5.42: Loss = 0.59639
Epoch 5.43: Loss = 0.594177
Epoch 5.44: Loss = 0.534119
Epoch 5.45: Loss = 0.639999
Epoch 5.46: Loss = 0.70813
Epoch 5.47: Loss = 0.566483
Epoch 5.48: Loss = 0.544525
Epoch 5.49: Loss = 0.628616
Epoch 5.50: Loss = 0.630524
Epoch 5.51: Loss = 0.510208
Epoch 5.52: Loss = 0.636307
Epoch 5.53: Loss = 0.68045
Epoch 5.54: Loss = 0.486633
Epoch 5.55: Loss = 0.603409
Epoch 5.56: Loss = 0.632339
Epoch 5.57: Loss = 0.695099
Epoch 5.58: Loss = 0.596344
Epoch 5.59: Loss = 0.676743
Epoch 5.60: Loss = 0.621994
Epoch 5.61: Loss = 0.566956
Epoch 5.62: Loss = 0.639496
Epoch 5.63: Loss = 0.514557
Epoch 5.64: Loss = 0.494629
Epoch 5.65: Loss = 0.606369
Epoch 5.66: Loss = 0.541992
Epoch 5.67: Loss = 0.570679
Epoch 5.68: Loss = 0.728867
Epoch 5.69: Loss = 0.598297
Epoch 5.70: Loss = 0.620316
Epoch 5.71: Loss = 0.512177
Epoch 5.72: Loss = 0.610092
Epoch 5.73: Loss = 0.692841
Epoch 5.74: Loss = 0.639435
Epoch 5.75: Loss = 0.547028
Epoch 5.76: Loss = 0.588501
Epoch 5.77: Loss = 0.605042
Epoch 5.78: Loss = 0.617081
Epoch 5.79: Loss = 0.589798
Epoch 5.80: Loss = 0.551315
Epoch 5.81: Loss = 0.576889
Epoch 5.82: Loss = 0.54715
Epoch 5.83: Loss = 0.653137
Epoch 5.84: Loss = 0.552155
Epoch 5.85: Loss = 0.587601
Epoch 5.86: Loss = 0.638321
Epoch 5.87: Loss = 0.601624
Epoch 5.88: Loss = 0.545639
Epoch 5.89: Loss = 0.695709
Epoch 5.90: Loss = 0.619232
Epoch 5.91: Loss = 0.646576
Epoch 5.92: Loss = 0.601334
Epoch 5.93: Loss = 0.627029
Epoch 5.94: Loss = 0.59024
Epoch 5.95: Loss = 0.623138
Epoch 5.96: Loss = 0.559921
Epoch 5.97: Loss = 0.491333
Epoch 5.98: Loss = 0.579895
Epoch 5.99: Loss = 0.594101
Epoch 5.100: Loss = 0.582291
Epoch 5.101: Loss = 0.663406
Epoch 5.102: Loss = 0.632095
Epoch 5.103: Loss = 0.577789
Epoch 5.104: Loss = 0.540894
Epoch 5.105: Loss = 0.543564
Epoch 5.106: Loss = 0.684647
Epoch 5.107: Loss = 0.639053
Epoch 5.108: Loss = 0.69313
Epoch 5.109: Loss = 0.666214
Epoch 5.110: Loss = 0.620865
Epoch 5.111: Loss = 0.546555
Epoch 5.112: Loss = 0.569916
Epoch 5.113: Loss = 0.600906
Epoch 5.114: Loss = 0.636765
Epoch 5.115: Loss = 0.584503
Epoch 5.116: Loss = 0.534683
Epoch 5.117: Loss = 0.664963
Epoch 5.118: Loss = 0.514481
Epoch 5.119: Loss = 0.588333
Epoch 5.120: Loss = 0.574753
TRAIN LOSS = 0.605438
TRAIN ACC = 80.7144 % (48431/60000)
Loss = 0.537766
Loss = 0.671371
Loss = 0.576736
Loss = 0.520599
Loss = 0.570923
Loss = 0.708435
Loss = 0.755157
Loss = 0.673996
Loss = 0.610229
Loss = 0.563416
Loss = 0.726227
Loss = 0.724533
Loss = 0.629272
Loss = 0.652573
Loss = 0.5979
Loss = 0.655685
Loss = 0.594284
Loss = 0.641907
Loss = 0.678711
Loss = 0.61702
TEST LOSS = 0.635337
TEST ACC = 484.309 % (7984/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.58461
Epoch 6.2: Loss = 0.618057
Epoch 6.3: Loss = 0.633896
Epoch 6.4: Loss = 0.515518
Epoch 6.5: Loss = 0.612991
Epoch 6.6: Loss = 0.69371
Epoch 6.7: Loss = 0.596313
Epoch 6.8: Loss = 0.71286
Epoch 6.9: Loss = 0.497879
Epoch 6.10: Loss = 0.451416
Epoch 6.11: Loss = 0.70607
Epoch 6.12: Loss = 0.595169
Epoch 6.13: Loss = 0.619385
Epoch 6.14: Loss = 0.601364
Epoch 6.15: Loss = 0.606674
Epoch 6.16: Loss = 0.669632
Epoch 6.17: Loss = 0.558746
Epoch 6.18: Loss = 0.6371
Epoch 6.19: Loss = 0.573944
Epoch 6.20: Loss = 0.696213
Epoch 6.21: Loss = 0.506897
Epoch 6.22: Loss = 0.467285
Epoch 6.23: Loss = 0.581085
Epoch 6.24: Loss = 0.718536
Epoch 6.25: Loss = 0.578537
Epoch 6.26: Loss = 0.50415
Epoch 6.27: Loss = 0.59639
Epoch 6.28: Loss = 0.601013
Epoch 6.29: Loss = 0.592712
Epoch 6.30: Loss = 0.615936
Epoch 6.31: Loss = 0.66954
Epoch 6.32: Loss = 0.560242
Epoch 6.33: Loss = 0.525192
Epoch 6.34: Loss = 0.677109
Epoch 6.35: Loss = 0.634674
Epoch 6.36: Loss = 0.659805
Epoch 6.37: Loss = 0.676666
Epoch 6.38: Loss = 0.623764
Epoch 6.39: Loss = 0.683289
Epoch 6.40: Loss = 0.592911
Epoch 6.41: Loss = 0.662109
Epoch 6.42: Loss = 0.592636
Epoch 6.43: Loss = 0.595428
Epoch 6.44: Loss = 0.530228
Epoch 6.45: Loss = 0.643402
Epoch 6.46: Loss = 0.720108
Epoch 6.47: Loss = 0.567734
Epoch 6.48: Loss = 0.542542
Epoch 6.49: Loss = 0.626221
Epoch 6.50: Loss = 0.627441
Epoch 6.51: Loss = 0.499771
Epoch 6.52: Loss = 0.643036
Epoch 6.53: Loss = 0.680222
Epoch 6.54: Loss = 0.471069
Epoch 6.55: Loss = 0.597046
Epoch 6.56: Loss = 0.628357
Epoch 6.57: Loss = 0.689346
Epoch 6.58: Loss = 0.58345
Epoch 6.59: Loss = 0.680832
Epoch 6.60: Loss = 0.630508
Epoch 6.61: Loss = 0.580032
Epoch 6.62: Loss = 0.644379
Epoch 6.63: Loss = 0.516617
Epoch 6.64: Loss = 0.493729
Epoch 6.65: Loss = 0.620651
Epoch 6.66: Loss = 0.533066
Epoch 6.67: Loss = 0.570496
Epoch 6.68: Loss = 0.749573
Epoch 6.69: Loss = 0.604355
Epoch 6.70: Loss = 0.614059
Epoch 6.71: Loss = 0.523026
Epoch 6.72: Loss = 0.621933
Epoch 6.73: Loss = 0.689056
Epoch 6.74: Loss = 0.630051
Epoch 6.75: Loss = 0.547211
Epoch 6.76: Loss = 0.585648
Epoch 6.77: Loss = 0.600998
Epoch 6.78: Loss = 0.625534
Epoch 6.79: Loss = 0.583725
Epoch 6.80: Loss = 0.551941
Epoch 6.81: Loss = 0.582718
Epoch 6.82: Loss = 0.546463
Epoch 6.83: Loss = 0.663376
Epoch 6.84: Loss = 0.545898
Epoch 6.85: Loss = 0.593475
Epoch 6.86: Loss = 0.639191
Epoch 6.87: Loss = 0.604156
Epoch 6.88: Loss = 0.539154
Epoch 6.89: Loss = 0.70549
Epoch 6.90: Loss = 0.617081
Epoch 6.91: Loss = 0.655029
Epoch 6.92: Loss = 0.600113
Epoch 6.93: Loss = 0.617004
Epoch 6.94: Loss = 0.591568
Epoch 6.95: Loss = 0.619125
Epoch 6.96: Loss = 0.55481
Epoch 6.97: Loss = 0.496033
Epoch 6.98: Loss = 0.577469
Epoch 6.99: Loss = 0.595184
Epoch 6.100: Loss = 0.590393
Epoch 6.101: Loss = 0.658249
Epoch 6.102: Loss = 0.635132
Epoch 6.103: Loss = 0.582184
Epoch 6.104: Loss = 0.542542
Epoch 6.105: Loss = 0.54451
Epoch 6.106: Loss = 0.679123
Epoch 6.107: Loss = 0.63475
Epoch 6.108: Loss = 0.714737
Epoch 6.109: Loss = 0.664459
Epoch 6.110: Loss = 0.622009
Epoch 6.111: Loss = 0.547134
Epoch 6.112: Loss = 0.583176
Epoch 6.113: Loss = 0.600937
Epoch 6.114: Loss = 0.642639
Epoch 6.115: Loss = 0.581131
Epoch 6.116: Loss = 0.541275
Epoch 6.117: Loss = 0.669052
Epoch 6.118: Loss = 0.515182
Epoch 6.119: Loss = 0.593796
Epoch 6.120: Loss = 0.57634
TRAIN LOSS = 0.603409
TRAIN ACC = 81.0867 % (48654/60000)
Loss = 0.533493
Loss = 0.672028
Loss = 0.577835
Loss = 0.520981
Loss = 0.582458
Loss = 0.716385
Loss = 0.765594
Loss = 0.681015
Loss = 0.617508
Loss = 0.571411
Loss = 0.744751
Loss = 0.743652
Loss = 0.63913
Loss = 0.647995
Loss = 0.601837
Loss = 0.660782
Loss = 0.602112
Loss = 0.660461
Loss = 0.687012
Loss = 0.621902
TEST LOSS = 0.642417
TEST ACC = 486.539 % (8024/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.581253
Epoch 7.2: Loss = 0.621613
Epoch 7.3: Loss = 0.642044
Epoch 7.4: Loss = 0.524017
Epoch 7.5: Loss = 0.617493
Epoch 7.6: Loss = 0.689392
Epoch 7.7: Loss = 0.590317
Epoch 7.8: Loss = 0.715179
Epoch 7.9: Loss = 0.487564
Epoch 7.10: Loss = 0.443268
Epoch 7.11: Loss = 0.706451
Epoch 7.12: Loss = 0.592789
Epoch 7.13: Loss = 0.618164
Epoch 7.14: Loss = 0.595352
Epoch 7.15: Loss = 0.617111
Epoch 7.16: Loss = 0.678391
Epoch 7.17: Loss = 0.549255
Epoch 7.18: Loss = 0.639084
Epoch 7.19: Loss = 0.578186
Epoch 7.20: Loss = 0.706833
Epoch 7.21: Loss = 0.502441
Epoch 7.22: Loss = 0.477936
Epoch 7.23: Loss = 0.578537
Epoch 7.24: Loss = 0.724518
Epoch 7.25: Loss = 0.577728
Epoch 7.26: Loss = 0.497314
Epoch 7.27: Loss = 0.594574
Epoch 7.28: Loss = 0.600525
Epoch 7.29: Loss = 0.582428
Epoch 7.30: Loss = 0.615341
Epoch 7.31: Loss = 0.663239
Epoch 7.32: Loss = 0.551392
Epoch 7.33: Loss = 0.533249
Epoch 7.34: Loss = 0.677063
Epoch 7.35: Loss = 0.636093
Epoch 7.36: Loss = 0.664764
Epoch 7.37: Loss = 0.674423
Epoch 7.38: Loss = 0.622726
Epoch 7.39: Loss = 0.687454
Epoch 7.40: Loss = 0.578491
Epoch 7.41: Loss = 0.651001
Epoch 7.42: Loss = 0.599487
Epoch 7.43: Loss = 0.592072
Epoch 7.44: Loss = 0.523621
Epoch 7.45: Loss = 0.644684
Epoch 7.46: Loss = 0.722946
Epoch 7.47: Loss = 0.565063
Epoch 7.48: Loss = 0.532288
Epoch 7.49: Loss = 0.621704
Epoch 7.50: Loss = 0.624832
Epoch 7.51: Loss = 0.495102
Epoch 7.52: Loss = 0.640457
Epoch 7.53: Loss = 0.678802
Epoch 7.54: Loss = 0.467819
Epoch 7.55: Loss = 0.590714
Epoch 7.56: Loss = 0.613205
Epoch 7.57: Loss = 0.676636
Epoch 7.58: Loss = 0.575226
Epoch 7.59: Loss = 0.684799
Epoch 7.60: Loss = 0.629868
Epoch 7.61: Loss = 0.577301
Epoch 7.62: Loss = 0.637283
Epoch 7.63: Loss = 0.512451
Epoch 7.64: Loss = 0.491165
Epoch 7.65: Loss = 0.621262
Epoch 7.66: Loss = 0.522919
Epoch 7.67: Loss = 0.565399
Epoch 7.68: Loss = 0.758514
Epoch 7.69: Loss = 0.587616
Epoch 7.70: Loss = 0.610199
Epoch 7.71: Loss = 0.525146
Epoch 7.72: Loss = 0.628647
Epoch 7.73: Loss = 0.684006
Epoch 7.74: Loss = 0.622452
Epoch 7.75: Loss = 0.545578
Epoch 7.76: Loss = 0.586685
Epoch 7.77: Loss = 0.600693
Epoch 7.78: Loss = 0.614731
Epoch 7.79: Loss = 0.576126
Epoch 7.80: Loss = 0.560303
Epoch 7.81: Loss = 0.572113
Epoch 7.82: Loss = 0.542419
Epoch 7.83: Loss = 0.647705
Epoch 7.84: Loss = 0.538223
Epoch 7.85: Loss = 0.584244
Epoch 7.86: Loss = 0.629944
Epoch 7.87: Loss = 0.596451
Epoch 7.88: Loss = 0.53685
Epoch 7.89: Loss = 0.692444
Epoch 7.90: Loss = 0.623306
Epoch 7.91: Loss = 0.634155
Epoch 7.92: Loss = 0.599289
Epoch 7.93: Loss = 0.601395
Epoch 7.94: Loss = 0.588242
Epoch 7.95: Loss = 0.612549
Epoch 7.96: Loss = 0.550125
Epoch 7.97: Loss = 0.489258
Epoch 7.98: Loss = 0.57402
Epoch 7.99: Loss = 0.587082
Epoch 7.100: Loss = 0.58284
Epoch 7.101: Loss = 0.654129
Epoch 7.102: Loss = 0.642029
Epoch 7.103: Loss = 0.575119
Epoch 7.104: Loss = 0.535324
Epoch 7.105: Loss = 0.535873
Epoch 7.106: Loss = 0.666779
Epoch 7.107: Loss = 0.619858
Epoch 7.108: Loss = 0.708939
Epoch 7.109: Loss = 0.657562
Epoch 7.110: Loss = 0.620956
Epoch 7.111: Loss = 0.538025
Epoch 7.112: Loss = 0.578964
Epoch 7.113: Loss = 0.589828
Epoch 7.114: Loss = 0.640839
Epoch 7.115: Loss = 0.560501
Epoch 7.116: Loss = 0.531509
Epoch 7.117: Loss = 0.666718
Epoch 7.118: Loss = 0.514343
Epoch 7.119: Loss = 0.58992
Epoch 7.120: Loss = 0.574234
TRAIN LOSS = 0.599823
TRAIN ACC = 81.4972 % (48900/60000)
Loss = 0.523758
Loss = 0.656921
Loss = 0.563766
Loss = 0.509384
Loss = 0.574921
Loss = 0.691483
Loss = 0.757492
Loss = 0.680328
Loss = 0.602921
Loss = 0.564667
Loss = 0.731308
Loss = 0.72287
Loss = 0.632599
Loss = 0.634872
Loss = 0.587234
Loss = 0.641937
Loss = 0.59584
Loss = 0.647186
Loss = 0.66507
Loss = 0.618393
TEST LOSS = 0.630147
TEST ACC = 488.998 % (8069/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.566498
Epoch 8.2: Loss = 0.597809
Epoch 8.3: Loss = 0.635284
Epoch 8.4: Loss = 0.505127
Epoch 8.5: Loss = 0.609268
Epoch 8.6: Loss = 0.682587
Epoch 8.7: Loss = 0.588654
Epoch 8.8: Loss = 0.696014
Epoch 8.9: Loss = 0.489685
Epoch 8.10: Loss = 0.42598
Epoch 8.11: Loss = 0.695389
Epoch 8.12: Loss = 0.575333
Epoch 8.13: Loss = 0.604797
Epoch 8.14: Loss = 0.577225
Epoch 8.15: Loss = 0.614166
Epoch 8.16: Loss = 0.669296
Epoch 8.17: Loss = 0.540512
Epoch 8.18: Loss = 0.62735
Epoch 8.19: Loss = 0.568344
Epoch 8.20: Loss = 0.68576
Epoch 8.21: Loss = 0.495621
Epoch 8.22: Loss = 0.462784
Epoch 8.23: Loss = 0.564438
Epoch 8.24: Loss = 0.714722
Epoch 8.25: Loss = 0.569
Epoch 8.26: Loss = 0.481552
Epoch 8.27: Loss = 0.581482
Epoch 8.28: Loss = 0.58847
Epoch 8.29: Loss = 0.578278
Epoch 8.30: Loss = 0.616119
Epoch 8.31: Loss = 0.650864
Epoch 8.32: Loss = 0.552094
Epoch 8.33: Loss = 0.527557
Epoch 8.34: Loss = 0.664291
Epoch 8.35: Loss = 0.636307
Epoch 8.36: Loss = 0.654938
Epoch 8.37: Loss = 0.658936
Epoch 8.38: Loss = 0.618271
Epoch 8.39: Loss = 0.675674
Epoch 8.40: Loss = 0.57135
Epoch 8.41: Loss = 0.633118
Epoch 8.42: Loss = 0.595367
Epoch 8.43: Loss = 0.588776
Epoch 8.44: Loss = 0.524277
Epoch 8.45: Loss = 0.628189
Epoch 8.46: Loss = 0.707794
Epoch 8.47: Loss = 0.565399
Epoch 8.48: Loss = 0.526108
Epoch 8.49: Loss = 0.605804
Epoch 8.50: Loss = 0.606567
Epoch 8.51: Loss = 0.482239
Epoch 8.52: Loss = 0.627533
Epoch 8.53: Loss = 0.663452
Epoch 8.54: Loss = 0.462387
Epoch 8.55: Loss = 0.586441
Epoch 8.56: Loss = 0.616333
Epoch 8.57: Loss = 0.674927
Epoch 8.58: Loss = 0.557953
Epoch 8.59: Loss = 0.686905
Epoch 8.60: Loss = 0.63356
Epoch 8.61: Loss = 0.566467
Epoch 8.62: Loss = 0.632401
Epoch 8.63: Loss = 0.512009
Epoch 8.64: Loss = 0.483139
Epoch 8.65: Loss = 0.616867
Epoch 8.66: Loss = 0.502792
Epoch 8.67: Loss = 0.558365
Epoch 8.68: Loss = 0.773453
Epoch 8.69: Loss = 0.578934
Epoch 8.70: Loss = 0.60466
Epoch 8.71: Loss = 0.512299
Epoch 8.72: Loss = 0.623749
Epoch 8.73: Loss = 0.674377
Epoch 8.74: Loss = 0.61644
Epoch 8.75: Loss = 0.536911
Epoch 8.76: Loss = 0.573929
Epoch 8.77: Loss = 0.602509
Epoch 8.78: Loss = 0.600342
Epoch 8.79: Loss = 0.569092
Epoch 8.80: Loss = 0.556381
Epoch 8.81: Loss = 0.568085
Epoch 8.82: Loss = 0.542725
Epoch 8.83: Loss = 0.650436
Epoch 8.84: Loss = 0.53183
Epoch 8.85: Loss = 0.584213
Epoch 8.86: Loss = 0.621552
Epoch 8.87: Loss = 0.593445
Epoch 8.88: Loss = 0.534409
Epoch 8.89: Loss = 0.701202
Epoch 8.90: Loss = 0.623871
Epoch 8.91: Loss = 0.616592
Epoch 8.92: Loss = 0.57962
Epoch 8.93: Loss = 0.599304
Epoch 8.94: Loss = 0.588226
Epoch 8.95: Loss = 0.611313
Epoch 8.96: Loss = 0.5392
Epoch 8.97: Loss = 0.475143
Epoch 8.98: Loss = 0.56041
Epoch 8.99: Loss = 0.571899
Epoch 8.100: Loss = 0.581818
Epoch 8.101: Loss = 0.641235
Epoch 8.102: Loss = 0.634613
Epoch 8.103: Loss = 0.577042
Epoch 8.104: Loss = 0.535522
Epoch 8.105: Loss = 0.533325
Epoch 8.106: Loss = 0.658279
Epoch 8.107: Loss = 0.61618
Epoch 8.108: Loss = 0.717102
Epoch 8.109: Loss = 0.641861
Epoch 8.110: Loss = 0.601501
Epoch 8.111: Loss = 0.533066
Epoch 8.112: Loss = 0.580658
Epoch 8.113: Loss = 0.583252
Epoch 8.114: Loss = 0.637421
Epoch 8.115: Loss = 0.551498
Epoch 8.116: Loss = 0.530869
Epoch 8.117: Loss = 0.640884
Epoch 8.118: Loss = 0.50235
Epoch 8.119: Loss = 0.575531
Epoch 8.120: Loss = 0.569595
TRAIN LOSS = 0.591873
TRAIN ACC = 81.8329 % (49102/60000)
Loss = 0.516815
Loss = 0.645325
Loss = 0.557831
Loss = 0.508545
Loss = 0.564941
Loss = 0.688293
Loss = 0.760391
Loss = 0.667786
Loss = 0.596161
Loss = 0.564606
Loss = 0.735382
Loss = 0.708908
Loss = 0.628296
Loss = 0.626205
Loss = 0.577026
Loss = 0.631119
Loss = 0.591965
Loss = 0.644547
Loss = 0.661377
Loss = 0.604294
TEST LOSS = 0.62399
TEST ACC = 491.019 % (8119/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.544052
Epoch 9.2: Loss = 0.59668
Epoch 9.3: Loss = 0.633179
Epoch 9.4: Loss = 0.506989
Epoch 9.5: Loss = 0.611664
Epoch 9.6: Loss = 0.677948
Epoch 9.7: Loss = 0.575195
Epoch 9.8: Loss = 0.693314
Epoch 9.9: Loss = 0.481064
Epoch 9.10: Loss = 0.421936
Epoch 9.11: Loss = 0.682495
Epoch 9.12: Loss = 0.578033
Epoch 9.13: Loss = 0.597656
Epoch 9.14: Loss = 0.572083
Epoch 9.15: Loss = 0.607513
Epoch 9.16: Loss = 0.650848
Epoch 9.17: Loss = 0.529373
Epoch 9.18: Loss = 0.619705
Epoch 9.19: Loss = 0.55011
Epoch 9.20: Loss = 0.683151
Epoch 9.21: Loss = 0.490936
Epoch 9.22: Loss = 0.464386
Epoch 9.23: Loss = 0.563248
Epoch 9.24: Loss = 0.704865
Epoch 9.25: Loss = 0.572601
Epoch 9.26: Loss = 0.476303
Epoch 9.27: Loss = 0.57901
Epoch 9.28: Loss = 0.588516
Epoch 9.29: Loss = 0.566177
Epoch 9.30: Loss = 0.609894
Epoch 9.31: Loss = 0.650345
Epoch 9.32: Loss = 0.543686
Epoch 9.33: Loss = 0.519302
Epoch 9.34: Loss = 0.660156
Epoch 9.35: Loss = 0.632401
Epoch 9.36: Loss = 0.650467
Epoch 9.37: Loss = 0.653564
Epoch 9.38: Loss = 0.600281
Epoch 9.39: Loss = 0.674805
Epoch 9.40: Loss = 0.559174
Epoch 9.41: Loss = 0.622528
Epoch 9.42: Loss = 0.588379
Epoch 9.43: Loss = 0.577301
Epoch 9.44: Loss = 0.520157
Epoch 9.45: Loss = 0.631897
Epoch 9.46: Loss = 0.697098
Epoch 9.47: Loss = 0.558151
Epoch 9.48: Loss = 0.509872
Epoch 9.49: Loss = 0.597824
Epoch 9.50: Loss = 0.604492
Epoch 9.51: Loss = 0.475296
Epoch 9.52: Loss = 0.613083
Epoch 9.53: Loss = 0.658234
Epoch 9.54: Loss = 0.45285
Epoch 9.55: Loss = 0.590485
Epoch 9.56: Loss = 0.602432
Epoch 9.57: Loss = 0.666122
Epoch 9.58: Loss = 0.548706
Epoch 9.59: Loss = 0.674103
Epoch 9.60: Loss = 0.634872
Epoch 9.61: Loss = 0.558182
Epoch 9.62: Loss = 0.612335
Epoch 9.63: Loss = 0.503113
Epoch 9.64: Loss = 0.473022
Epoch 9.65: Loss = 0.598633
Epoch 9.66: Loss = 0.492493
Epoch 9.67: Loss = 0.543274
Epoch 9.68: Loss = 0.764191
Epoch 9.69: Loss = 0.576569
Epoch 9.70: Loss = 0.594193
Epoch 9.71: Loss = 0.512695
Epoch 9.72: Loss = 0.621857
Epoch 9.73: Loss = 0.660843
Epoch 9.74: Loss = 0.608521
Epoch 9.75: Loss = 0.536087
Epoch 9.76: Loss = 0.56073
Epoch 9.77: Loss = 0.591736
Epoch 9.78: Loss = 0.585342
Epoch 9.79: Loss = 0.556076
Epoch 9.80: Loss = 0.547699
Epoch 9.81: Loss = 0.561737
Epoch 9.82: Loss = 0.541534
Epoch 9.83: Loss = 0.630539
Epoch 9.84: Loss = 0.524612
Epoch 9.85: Loss = 0.575638
Epoch 9.86: Loss = 0.619247
Epoch 9.87: Loss = 0.588898
Epoch 9.88: Loss = 0.524536
Epoch 9.89: Loss = 0.689255
Epoch 9.90: Loss = 0.615982
Epoch 9.91: Loss = 0.618423
Epoch 9.92: Loss = 0.581451
Epoch 9.93: Loss = 0.585068
Epoch 9.94: Loss = 0.582031
Epoch 9.95: Loss = 0.608047
Epoch 9.96: Loss = 0.541382
Epoch 9.97: Loss = 0.476898
Epoch 9.98: Loss = 0.566055
Epoch 9.99: Loss = 0.563156
Epoch 9.100: Loss = 0.584305
Epoch 9.101: Loss = 0.637558
Epoch 9.102: Loss = 0.623047
Epoch 9.103: Loss = 0.576477
Epoch 9.104: Loss = 0.547394
Epoch 9.105: Loss = 0.539383
Epoch 9.106: Loss = 0.649231
Epoch 9.107: Loss = 0.62944
Epoch 9.108: Loss = 0.719116
Epoch 9.109: Loss = 0.637253
Epoch 9.110: Loss = 0.601624
Epoch 9.111: Loss = 0.547592
Epoch 9.112: Loss = 0.579514
Epoch 9.113: Loss = 0.592773
Epoch 9.114: Loss = 0.638809
Epoch 9.115: Loss = 0.548706
Epoch 9.116: Loss = 0.518509
Epoch 9.117: Loss = 0.653122
Epoch 9.118: Loss = 0.51387
Epoch 9.119: Loss = 0.581573
Epoch 9.120: Loss = 0.560425
TRAIN LOSS = 0.586411
TRAIN ACC = 82.1182 % (49273/60000)
Loss = 0.514175
Loss = 0.655807
Loss = 0.570221
Loss = 0.512924
Loss = 0.569077
Loss = 0.700912
Loss = 0.764053
Loss = 0.665085
Loss = 0.607452
Loss = 0.562607
Loss = 0.745392
Loss = 0.719025
Loss = 0.641937
Loss = 0.63237
Loss = 0.597061
Loss = 0.629517
Loss = 0.603134
Loss = 0.644836
Loss = 0.663116
Loss = 0.614777
TEST LOSS = 0.630674
TEST ACC = 492.729 % (8115/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.546188
Epoch 10.2: Loss = 0.610672
Epoch 10.3: Loss = 0.636948
Epoch 10.4: Loss = 0.500732
Epoch 10.5: Loss = 0.612869
Epoch 10.6: Loss = 0.680328
Epoch 10.7: Loss = 0.565125
Epoch 10.8: Loss = 0.688919
Epoch 10.9: Loss = 0.470657
Epoch 10.10: Loss = 0.416138
Epoch 10.11: Loss = 0.678421
Epoch 10.12: Loss = 0.578156
Epoch 10.13: Loss = 0.601654
Epoch 10.14: Loss = 0.567688
Epoch 10.15: Loss = 0.613525
Epoch 10.16: Loss = 0.662201
Epoch 10.17: Loss = 0.524582
Epoch 10.18: Loss = 0.615372
Epoch 10.19: Loss = 0.546875
Epoch 10.20: Loss = 0.684372
Epoch 10.21: Loss = 0.490509
Epoch 10.22: Loss = 0.463593
Epoch 10.23: Loss = 0.556625
Epoch 10.24: Loss = 0.691315
Epoch 10.25: Loss = 0.570999
Epoch 10.26: Loss = 0.467087
Epoch 10.27: Loss = 0.586411
Epoch 10.28: Loss = 0.584915
Epoch 10.29: Loss = 0.568008
Epoch 10.30: Loss = 0.602066
Epoch 10.31: Loss = 0.645523
Epoch 10.32: Loss = 0.547333
Epoch 10.33: Loss = 0.518753
Epoch 10.34: Loss = 0.661652
Epoch 10.35: Loss = 0.63063
Epoch 10.36: Loss = 0.643509
Epoch 10.37: Loss = 0.658279
Epoch 10.38: Loss = 0.592026
Epoch 10.39: Loss = 0.676498
Epoch 10.40: Loss = 0.568176
Epoch 10.41: Loss = 0.642609
Epoch 10.42: Loss = 0.589157
Epoch 10.43: Loss = 0.568039
Epoch 10.44: Loss = 0.518188
Epoch 10.45: Loss = 0.625839
Epoch 10.46: Loss = 0.69313
Epoch 10.47: Loss = 0.559479
Epoch 10.48: Loss = 0.505844
Epoch 10.49: Loss = 0.607529
Epoch 10.50: Loss = 0.602554
Epoch 10.51: Loss = 0.475586
Epoch 10.52: Loss = 0.622025
Epoch 10.53: Loss = 0.664017
Epoch 10.54: Loss = 0.458939
Epoch 10.55: Loss = 0.594254
Epoch 10.56: Loss = 0.602951
Epoch 10.57: Loss = 0.667953
Epoch 10.58: Loss = 0.545654
Epoch 10.59: Loss = 0.674805
Epoch 10.60: Loss = 0.627487
Epoch 10.61: Loss = 0.562225
Epoch 10.62: Loss = 0.615402
Epoch 10.63: Loss = 0.499466
Epoch 10.64: Loss = 0.472549
Epoch 10.65: Loss = 0.607758
Epoch 10.66: Loss = 0.496796
Epoch 10.67: Loss = 0.539093
Epoch 10.68: Loss = 0.76712
Epoch 10.69: Loss = 0.571457
Epoch 10.70: Loss = 0.598984
Epoch 10.71: Loss = 0.504303
Epoch 10.72: Loss = 0.625076
Epoch 10.73: Loss = 0.664185
Epoch 10.74: Loss = 0.616547
Epoch 10.75: Loss = 0.531784
Epoch 10.76: Loss = 0.554306
Epoch 10.77: Loss = 0.599716
Epoch 10.78: Loss = 0.593628
Epoch 10.79: Loss = 0.564056
Epoch 10.80: Loss = 0.549164
Epoch 10.81: Loss = 0.571716
Epoch 10.82: Loss = 0.544754
Epoch 10.83: Loss = 0.637497
Epoch 10.84: Loss = 0.531296
Epoch 10.85: Loss = 0.579315
Epoch 10.86: Loss = 0.616592
Epoch 10.87: Loss = 0.591125
Epoch 10.88: Loss = 0.529037
Epoch 10.89: Loss = 0.693848
Epoch 10.90: Loss = 0.618088
Epoch 10.91: Loss = 0.60553
Epoch 10.92: Loss = 0.5849
Epoch 10.93: Loss = 0.574875
Epoch 10.94: Loss = 0.587021
Epoch 10.95: Loss = 0.615875
Epoch 10.96: Loss = 0.545181
Epoch 10.97: Loss = 0.468887
Epoch 10.98: Loss = 0.568985
Epoch 10.99: Loss = 0.55687
Epoch 10.100: Loss = 0.582779
Epoch 10.101: Loss = 0.64122
Epoch 10.102: Loss = 0.628174
Epoch 10.103: Loss = 0.566055
Epoch 10.104: Loss = 0.543808
Epoch 10.105: Loss = 0.536026
Epoch 10.106: Loss = 0.640182
Epoch 10.107: Loss = 0.617828
Epoch 10.108: Loss = 0.72081
Epoch 10.109: Loss = 0.636932
Epoch 10.110: Loss = 0.595428
Epoch 10.111: Loss = 0.546463
Epoch 10.112: Loss = 0.583954
Epoch 10.113: Loss = 0.581955
Epoch 10.114: Loss = 0.638519
Epoch 10.115: Loss = 0.541382
Epoch 10.116: Loss = 0.52327
Epoch 10.117: Loss = 0.659561
Epoch 10.118: Loss = 0.501801
Epoch 10.119: Loss = 0.578156
Epoch 10.120: Loss = 0.567642
TRAIN LOSS = 0.586334
TRAIN ACC = 82.2906 % (49377/60000)
Loss = 0.508163
Loss = 0.652893
Loss = 0.566025
Loss = 0.512268
Loss = 0.565506
Loss = 0.685944
Loss = 0.769531
Loss = 0.675201
Loss = 0.598312
Loss = 0.562988
Loss = 0.747528
Loss = 0.719086
Loss = 0.650772
Loss = 0.633179
Loss = 0.585114
Loss = 0.627777
Loss = 0.602783
Loss = 0.64296
Loss = 0.669159
Loss = 0.616394
TEST LOSS = 0.629579
TEST ACC = 493.77 % (8136/10000)
