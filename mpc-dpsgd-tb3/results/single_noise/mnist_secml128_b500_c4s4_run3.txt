Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.30373
Epoch 1.2: Loss = 2.30304
Epoch 1.3: Loss = 2.2592
Epoch 1.4: Loss = 2.23317
Epoch 1.5: Loss = 2.19571
Epoch 1.6: Loss = 2.16986
Epoch 1.7: Loss = 2.15564
Epoch 1.8: Loss = 2.12865
Epoch 1.9: Loss = 2.0907
Epoch 1.10: Loss = 2.06378
Epoch 1.11: Loss = 1.98875
Epoch 1.12: Loss = 1.99084
Epoch 1.13: Loss = 1.92697
Epoch 1.14: Loss = 1.93077
Epoch 1.15: Loss = 1.97279
Epoch 1.16: Loss = 1.87207
Epoch 1.17: Loss = 1.84392
Epoch 1.18: Loss = 1.82434
Epoch 1.19: Loss = 1.78198
Epoch 1.20: Loss = 1.7375
Epoch 1.21: Loss = 1.68613
Epoch 1.22: Loss = 1.6805
Epoch 1.23: Loss = 1.59929
Epoch 1.24: Loss = 1.70264
Epoch 1.25: Loss = 1.60745
Epoch 1.26: Loss = 1.61694
Epoch 1.27: Loss = 1.5668
Epoch 1.28: Loss = 1.55852
Epoch 1.29: Loss = 1.56624
Epoch 1.30: Loss = 1.59421
Epoch 1.31: Loss = 1.43996
Epoch 1.32: Loss = 1.49217
Epoch 1.33: Loss = 1.40518
Epoch 1.34: Loss = 1.44196
Epoch 1.35: Loss = 1.35063
Epoch 1.36: Loss = 1.46698
Epoch 1.37: Loss = 1.31874
Epoch 1.38: Loss = 1.29364
Epoch 1.39: Loss = 1.27373
Epoch 1.40: Loss = 1.19962
Epoch 1.41: Loss = 1.22949
Epoch 1.42: Loss = 1.22263
Epoch 1.43: Loss = 1.16689
Epoch 1.44: Loss = 1.0862
Epoch 1.45: Loss = 1.21629
Epoch 1.46: Loss = 1.13599
Epoch 1.47: Loss = 1.09755
Epoch 1.48: Loss = 1.13586
Epoch 1.49: Loss = 1.07465
Epoch 1.50: Loss = 1.13832
Epoch 1.51: Loss = 0.971176
Epoch 1.52: Loss = 0.993256
Epoch 1.53: Loss = 1.02667
Epoch 1.54: Loss = 1.08054
Epoch 1.55: Loss = 1.04668
Epoch 1.56: Loss = 0.939285
Epoch 1.57: Loss = 0.877533
Epoch 1.58: Loss = 0.921143
Epoch 1.59: Loss = 0.957932
Epoch 1.60: Loss = 1.06517
Epoch 1.61: Loss = 0.964844
Epoch 1.62: Loss = 1.03703
Epoch 1.63: Loss = 1.01875
Epoch 1.64: Loss = 0.977966
Epoch 1.65: Loss = 1.01791
Epoch 1.66: Loss = 0.907227
Epoch 1.67: Loss = 0.912659
Epoch 1.68: Loss = 0.738358
Epoch 1.69: Loss = 0.821396
Epoch 1.70: Loss = 0.905807
Epoch 1.71: Loss = 0.822784
Epoch 1.72: Loss = 0.824203
Epoch 1.73: Loss = 0.860703
Epoch 1.74: Loss = 0.704315
Epoch 1.75: Loss = 0.830627
Epoch 1.76: Loss = 0.82193
Epoch 1.77: Loss = 0.794006
Epoch 1.78: Loss = 0.746994
Epoch 1.79: Loss = 0.737488
Epoch 1.80: Loss = 0.855392
Epoch 1.81: Loss = 0.678818
Epoch 1.82: Loss = 0.668152
Epoch 1.83: Loss = 0.830078
Epoch 1.84: Loss = 0.768005
Epoch 1.85: Loss = 0.876144
Epoch 1.86: Loss = 0.745682
Epoch 1.87: Loss = 0.647324
Epoch 1.88: Loss = 0.677902
Epoch 1.89: Loss = 0.776764
Epoch 1.90: Loss = 0.654739
Epoch 1.91: Loss = 0.761093
Epoch 1.92: Loss = 0.719772
Epoch 1.93: Loss = 0.738312
Epoch 1.94: Loss = 0.597656
Epoch 1.95: Loss = 0.68486
Epoch 1.96: Loss = 0.693405
Epoch 1.97: Loss = 0.540634
Epoch 1.98: Loss = 0.637772
Epoch 1.99: Loss = 0.722946
Epoch 1.100: Loss = 0.818069
Epoch 1.101: Loss = 0.742004
Epoch 1.102: Loss = 0.678131
Epoch 1.103: Loss = 0.575897
Epoch 1.104: Loss = 0.596603
Epoch 1.105: Loss = 0.681
Epoch 1.106: Loss = 0.711731
Epoch 1.107: Loss = 0.599777
Epoch 1.108: Loss = 0.643219
Epoch 1.109: Loss = 0.560471
Epoch 1.110: Loss = 0.645203
Epoch 1.111: Loss = 0.564987
Epoch 1.112: Loss = 0.521164
Epoch 1.113: Loss = 0.6035
Epoch 1.114: Loss = 0.535233
Epoch 1.115: Loss = 0.602234
Epoch 1.116: Loss = 0.585007
Epoch 1.117: Loss = 0.473007
Epoch 1.118: Loss = 0.435181
Epoch 1.119: Loss = 0.459961
Epoch 1.120: Loss = 0.436035
TRAIN LOSS = 1.12674
TRAIN ACC = 67.9077 % (40746/60000)
Loss = 0.626343
Loss = 0.633667
Loss = 0.774017
Loss = 0.713226
Loss = 0.752121
Loss = 0.634521
Loss = 0.605728
Loss = 0.775986
Loss = 0.761353
Loss = 0.697067
Loss = 0.360046
Loss = 0.510818
Loss = 0.384094
Loss = 0.577316
Loss = 0.431107
Loss = 0.464172
Loss = 0.439835
Loss = 0.234299
Loss = 0.441101
Loss = 0.652069
TEST LOSS = 0.573444
TEST ACC = 407.46 % (8207/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.55574
Epoch 2.2: Loss = 0.671432
Epoch 2.3: Loss = 0.638046
Epoch 2.4: Loss = 0.522858
Epoch 2.5: Loss = 0.537201
Epoch 2.6: Loss = 0.553268
Epoch 2.7: Loss = 0.601044
Epoch 2.8: Loss = 0.573959
Epoch 2.9: Loss = 0.581924
Epoch 2.10: Loss = 0.568909
Epoch 2.11: Loss = 0.563919
Epoch 2.12: Loss = 0.546616
Epoch 2.13: Loss = 0.494431
Epoch 2.14: Loss = 0.526337
Epoch 2.15: Loss = 0.645355
Epoch 2.16: Loss = 0.629379
Epoch 2.17: Loss = 0.643402
Epoch 2.18: Loss = 0.739838
Epoch 2.19: Loss = 0.525772
Epoch 2.20: Loss = 0.489746
Epoch 2.21: Loss = 0.476212
Epoch 2.22: Loss = 0.488342
Epoch 2.23: Loss = 0.52063
Epoch 2.24: Loss = 0.698563
Epoch 2.25: Loss = 0.573013
Epoch 2.26: Loss = 0.612839
Epoch 2.27: Loss = 0.622803
Epoch 2.28: Loss = 0.608414
Epoch 2.29: Loss = 0.67308
Epoch 2.30: Loss = 0.697113
Epoch 2.31: Loss = 0.452118
Epoch 2.32: Loss = 0.634354
Epoch 2.33: Loss = 0.539352
Epoch 2.34: Loss = 0.596588
Epoch 2.35: Loss = 0.58284
Epoch 2.36: Loss = 0.675797
Epoch 2.37: Loss = 0.468536
Epoch 2.38: Loss = 0.48053
Epoch 2.39: Loss = 0.528717
Epoch 2.40: Loss = 0.477768
Epoch 2.41: Loss = 0.545746
Epoch 2.42: Loss = 0.635208
Epoch 2.43: Loss = 0.488617
Epoch 2.44: Loss = 0.426941
Epoch 2.45: Loss = 0.5495
Epoch 2.46: Loss = 0.58252
Epoch 2.47: Loss = 0.499405
Epoch 2.48: Loss = 0.567749
Epoch 2.49: Loss = 0.534912
Epoch 2.50: Loss = 0.632706
Epoch 2.51: Loss = 0.482681
Epoch 2.52: Loss = 0.454269
Epoch 2.53: Loss = 0.517471
Epoch 2.54: Loss = 0.610657
Epoch 2.55: Loss = 0.542587
Epoch 2.56: Loss = 0.455612
Epoch 2.57: Loss = 0.456757
Epoch 2.58: Loss = 0.533157
Epoch 2.59: Loss = 0.611755
Epoch 2.60: Loss = 0.665924
Epoch 2.61: Loss = 0.592285
Epoch 2.62: Loss = 0.638992
Epoch 2.63: Loss = 0.676453
Epoch 2.64: Loss = 0.588333
Epoch 2.65: Loss = 0.7034
Epoch 2.66: Loss = 0.516479
Epoch 2.67: Loss = 0.590088
Epoch 2.68: Loss = 0.360825
Epoch 2.69: Loss = 0.440399
Epoch 2.70: Loss = 0.609238
Epoch 2.71: Loss = 0.469437
Epoch 2.72: Loss = 0.488174
Epoch 2.73: Loss = 0.543518
Epoch 2.74: Loss = 0.400635
Epoch 2.75: Loss = 0.629745
Epoch 2.76: Loss = 0.530472
Epoch 2.77: Loss = 0.4711
Epoch 2.78: Loss = 0.489182
Epoch 2.79: Loss = 0.531296
Epoch 2.80: Loss = 0.578735
Epoch 2.81: Loss = 0.434738
Epoch 2.82: Loss = 0.393326
Epoch 2.83: Loss = 0.568192
Epoch 2.84: Loss = 0.49736
Epoch 2.85: Loss = 0.735779
Epoch 2.86: Loss = 0.562286
Epoch 2.87: Loss = 0.389923
Epoch 2.88: Loss = 0.44812
Epoch 2.89: Loss = 0.596878
Epoch 2.90: Loss = 0.43399
Epoch 2.91: Loss = 0.564896
Epoch 2.92: Loss = 0.533661
Epoch 2.93: Loss = 0.575562
Epoch 2.94: Loss = 0.394211
Epoch 2.95: Loss = 0.48349
Epoch 2.96: Loss = 0.551392
Epoch 2.97: Loss = 0.373657
Epoch 2.98: Loss = 0.456299
Epoch 2.99: Loss = 0.579178
Epoch 2.100: Loss = 0.641602
Epoch 2.101: Loss = 0.615097
Epoch 2.102: Loss = 0.492966
Epoch 2.103: Loss = 0.424438
Epoch 2.104: Loss = 0.426376
Epoch 2.105: Loss = 0.525421
Epoch 2.106: Loss = 0.571503
Epoch 2.107: Loss = 0.433731
Epoch 2.108: Loss = 0.511978
Epoch 2.109: Loss = 0.396454
Epoch 2.110: Loss = 0.527206
Epoch 2.111: Loss = 0.390701
Epoch 2.112: Loss = 0.400452
Epoch 2.113: Loss = 0.452179
Epoch 2.114: Loss = 0.389633
Epoch 2.115: Loss = 0.425568
Epoch 2.116: Loss = 0.423965
Epoch 2.117: Loss = 0.311661
Epoch 2.118: Loss = 0.26535
Epoch 2.119: Loss = 0.366623
Epoch 2.120: Loss = 0.336136
TRAIN LOSS = 0.529694
TRAIN ACC = 83.6945 % (50219/60000)
Loss = 0.478088
Loss = 0.518692
Loss = 0.652756
Loss = 0.597885
Loss = 0.658463
Loss = 0.483749
Loss = 0.462906
Loss = 0.661362
Loss = 0.628693
Loss = 0.576828
Loss = 0.243851
Loss = 0.387222
Loss = 0.331421
Loss = 0.435791
Loss = 0.270126
Loss = 0.376114
Loss = 0.292374
Loss = 0.122971
Loss = 0.329086
Loss = 0.565231
TEST LOSS = 0.45368
TEST ACC = 502.19 % (8636/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.436356
Epoch 3.2: Loss = 0.561066
Epoch 3.3: Loss = 0.530991
Epoch 3.4: Loss = 0.384079
Epoch 3.5: Loss = 0.431427
Epoch 3.6: Loss = 0.451614
Epoch 3.7: Loss = 0.433243
Epoch 3.8: Loss = 0.427795
Epoch 3.9: Loss = 0.482956
Epoch 3.10: Loss = 0.482895
Epoch 3.11: Loss = 0.459869
Epoch 3.12: Loss = 0.452759
Epoch 3.13: Loss = 0.385284
Epoch 3.14: Loss = 0.415527
Epoch 3.15: Loss = 0.496246
Epoch 3.16: Loss = 0.524414
Epoch 3.17: Loss = 0.532333
Epoch 3.18: Loss = 0.658859
Epoch 3.19: Loss = 0.456146
Epoch 3.20: Loss = 0.388931
Epoch 3.21: Loss = 0.379654
Epoch 3.22: Loss = 0.368851
Epoch 3.23: Loss = 0.43132
Epoch 3.24: Loss = 0.625656
Epoch 3.25: Loss = 0.487915
Epoch 3.26: Loss = 0.592102
Epoch 3.27: Loss = 0.558075
Epoch 3.28: Loss = 0.548538
Epoch 3.29: Loss = 0.621872
Epoch 3.30: Loss = 0.609833
Epoch 3.31: Loss = 0.357544
Epoch 3.32: Loss = 0.553329
Epoch 3.33: Loss = 0.414093
Epoch 3.34: Loss = 0.521255
Epoch 3.35: Loss = 0.526474
Epoch 3.36: Loss = 0.584137
Epoch 3.37: Loss = 0.369537
Epoch 3.38: Loss = 0.386108
Epoch 3.39: Loss = 0.442566
Epoch 3.40: Loss = 0.381577
Epoch 3.41: Loss = 0.438324
Epoch 3.42: Loss = 0.590652
Epoch 3.43: Loss = 0.415787
Epoch 3.44: Loss = 0.364929
Epoch 3.45: Loss = 0.452454
Epoch 3.46: Loss = 0.482178
Epoch 3.47: Loss = 0.417542
Epoch 3.48: Loss = 0.48674
Epoch 3.49: Loss = 0.467697
Epoch 3.50: Loss = 0.544891
Epoch 3.51: Loss = 0.416275
Epoch 3.52: Loss = 0.375443
Epoch 3.53: Loss = 0.441742
Epoch 3.54: Loss = 0.558014
Epoch 3.55: Loss = 0.450851
Epoch 3.56: Loss = 0.408737
Epoch 3.57: Loss = 0.398956
Epoch 3.58: Loss = 0.436493
Epoch 3.59: Loss = 0.556671
Epoch 3.60: Loss = 0.587982
Epoch 3.61: Loss = 0.515137
Epoch 3.62: Loss = 0.550629
Epoch 3.63: Loss = 0.633728
Epoch 3.64: Loss = 0.559174
Epoch 3.65: Loss = 0.652939
Epoch 3.66: Loss = 0.452774
Epoch 3.67: Loss = 0.488815
Epoch 3.68: Loss = 0.284576
Epoch 3.69: Loss = 0.378738
Epoch 3.70: Loss = 0.533295
Epoch 3.71: Loss = 0.41275
Epoch 3.72: Loss = 0.418579
Epoch 3.73: Loss = 0.469467
Epoch 3.74: Loss = 0.334518
Epoch 3.75: Loss = 0.589264
Epoch 3.76: Loss = 0.475067
Epoch 3.77: Loss = 0.373062
Epoch 3.78: Loss = 0.444382
Epoch 3.79: Loss = 0.499817
Epoch 3.80: Loss = 0.478729
Epoch 3.81: Loss = 0.408539
Epoch 3.82: Loss = 0.338089
Epoch 3.83: Loss = 0.509552
Epoch 3.84: Loss = 0.448303
Epoch 3.85: Loss = 0.648178
Epoch 3.86: Loss = 0.513504
Epoch 3.87: Loss = 0.3535
Epoch 3.88: Loss = 0.412369
Epoch 3.89: Loss = 0.525452
Epoch 3.90: Loss = 0.367844
Epoch 3.91: Loss = 0.519714
Epoch 3.92: Loss = 0.493668
Epoch 3.93: Loss = 0.517807
Epoch 3.94: Loss = 0.340546
Epoch 3.95: Loss = 0.422348
Epoch 3.96: Loss = 0.514587
Epoch 3.97: Loss = 0.319214
Epoch 3.98: Loss = 0.402832
Epoch 3.99: Loss = 0.543213
Epoch 3.100: Loss = 0.602844
Epoch 3.101: Loss = 0.575134
Epoch 3.102: Loss = 0.449783
Epoch 3.103: Loss = 0.388748
Epoch 3.104: Loss = 0.391373
Epoch 3.105: Loss = 0.516205
Epoch 3.106: Loss = 0.590958
Epoch 3.107: Loss = 0.372284
Epoch 3.108: Loss = 0.485214
Epoch 3.109: Loss = 0.362625
Epoch 3.110: Loss = 0.489044
Epoch 3.111: Loss = 0.350571
Epoch 3.112: Loss = 0.361053
Epoch 3.113: Loss = 0.402313
Epoch 3.114: Loss = 0.353836
Epoch 3.115: Loss = 0.355804
Epoch 3.116: Loss = 0.404922
Epoch 3.117: Loss = 0.267303
Epoch 3.118: Loss = 0.252151
Epoch 3.119: Loss = 0.306931
Epoch 3.120: Loss = 0.3116
TRAIN LOSS = 0.459839
TRAIN ACC = 86.348 % (51811/60000)
Loss = 0.420547
Loss = 0.504944
Loss = 0.608231
Loss = 0.585022
Loss = 0.635529
Loss = 0.437622
Loss = 0.423706
Loss = 0.644287
Loss = 0.600357
Loss = 0.541748
Loss = 0.220551
Loss = 0.379349
Loss = 0.346085
Loss = 0.396408
Loss = 0.209473
Loss = 0.328369
Loss = 0.26947
Loss = 0.0853882
Loss = 0.290161
Loss = 0.57489
TEST LOSS = 0.425107
TEST ACC = 518.109 % (8756/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.384979
Epoch 4.2: Loss = 0.535751
Epoch 4.3: Loss = 0.490524
Epoch 4.4: Loss = 0.35257
Epoch 4.5: Loss = 0.39476
Epoch 4.6: Loss = 0.409439
Epoch 4.7: Loss = 0.376556
Epoch 4.8: Loss = 0.399796
Epoch 4.9: Loss = 0.441406
Epoch 4.10: Loss = 0.413605
Epoch 4.11: Loss = 0.449768
Epoch 4.12: Loss = 0.422699
Epoch 4.13: Loss = 0.362411
Epoch 4.14: Loss = 0.402405
Epoch 4.15: Loss = 0.451645
Epoch 4.16: Loss = 0.479218
Epoch 4.17: Loss = 0.521912
Epoch 4.18: Loss = 0.630417
Epoch 4.19: Loss = 0.437927
Epoch 4.20: Loss = 0.351227
Epoch 4.21: Loss = 0.377045
Epoch 4.22: Loss = 0.319427
Epoch 4.23: Loss = 0.386276
Epoch 4.24: Loss = 0.634506
Epoch 4.25: Loss = 0.454544
Epoch 4.26: Loss = 0.543365
Epoch 4.27: Loss = 0.522995
Epoch 4.28: Loss = 0.499619
Epoch 4.29: Loss = 0.561783
Epoch 4.30: Loss = 0.569748
Epoch 4.31: Loss = 0.336746
Epoch 4.32: Loss = 0.513657
Epoch 4.33: Loss = 0.391495
Epoch 4.34: Loss = 0.480896
Epoch 4.35: Loss = 0.480484
Epoch 4.36: Loss = 0.529144
Epoch 4.37: Loss = 0.332886
Epoch 4.38: Loss = 0.357727
Epoch 4.39: Loss = 0.400589
Epoch 4.40: Loss = 0.363419
Epoch 4.41: Loss = 0.385147
Epoch 4.42: Loss = 0.560089
Epoch 4.43: Loss = 0.357468
Epoch 4.44: Loss = 0.350296
Epoch 4.45: Loss = 0.429398
Epoch 4.46: Loss = 0.463287
Epoch 4.47: Loss = 0.389069
Epoch 4.48: Loss = 0.462128
Epoch 4.49: Loss = 0.441772
Epoch 4.50: Loss = 0.538361
Epoch 4.51: Loss = 0.368713
Epoch 4.52: Loss = 0.337112
Epoch 4.53: Loss = 0.410782
Epoch 4.54: Loss = 0.562592
Epoch 4.55: Loss = 0.431137
Epoch 4.56: Loss = 0.40181
Epoch 4.57: Loss = 0.401184
Epoch 4.58: Loss = 0.414352
Epoch 4.59: Loss = 0.54982
Epoch 4.60: Loss = 0.534286
Epoch 4.61: Loss = 0.495026
Epoch 4.62: Loss = 0.541092
Epoch 4.63: Loss = 0.61618
Epoch 4.64: Loss = 0.563812
Epoch 4.65: Loss = 0.620834
Epoch 4.66: Loss = 0.405884
Epoch 4.67: Loss = 0.474884
Epoch 4.68: Loss = 0.26767
Epoch 4.69: Loss = 0.363678
Epoch 4.70: Loss = 0.502487
Epoch 4.71: Loss = 0.389526
Epoch 4.72: Loss = 0.385208
Epoch 4.73: Loss = 0.478745
Epoch 4.74: Loss = 0.31546
Epoch 4.75: Loss = 0.636749
Epoch 4.76: Loss = 0.462265
Epoch 4.77: Loss = 0.356735
Epoch 4.78: Loss = 0.434677
Epoch 4.79: Loss = 0.494263
Epoch 4.80: Loss = 0.456757
Epoch 4.81: Loss = 0.400833
Epoch 4.82: Loss = 0.309982
Epoch 4.83: Loss = 0.5103
Epoch 4.84: Loss = 0.455521
Epoch 4.85: Loss = 0.63623
Epoch 4.86: Loss = 0.505234
Epoch 4.87: Loss = 0.3414
Epoch 4.88: Loss = 0.421753
Epoch 4.89: Loss = 0.515366
Epoch 4.90: Loss = 0.351654
Epoch 4.91: Loss = 0.515274
Epoch 4.92: Loss = 0.502838
Epoch 4.93: Loss = 0.513702
Epoch 4.94: Loss = 0.339142
Epoch 4.95: Loss = 0.410583
Epoch 4.96: Loss = 0.488159
Epoch 4.97: Loss = 0.322311
Epoch 4.98: Loss = 0.392014
Epoch 4.99: Loss = 0.537308
Epoch 4.100: Loss = 0.590057
Epoch 4.101: Loss = 0.594162
Epoch 4.102: Loss = 0.4272
Epoch 4.103: Loss = 0.375153
Epoch 4.104: Loss = 0.379044
Epoch 4.105: Loss = 0.505524
Epoch 4.106: Loss = 0.572662
Epoch 4.107: Loss = 0.367508
Epoch 4.108: Loss = 0.463409
Epoch 4.109: Loss = 0.359894
Epoch 4.110: Loss = 0.498581
Epoch 4.111: Loss = 0.341873
Epoch 4.112: Loss = 0.345947
Epoch 4.113: Loss = 0.396484
Epoch 4.114: Loss = 0.324295
Epoch 4.115: Loss = 0.328262
Epoch 4.116: Loss = 0.392746
Epoch 4.117: Loss = 0.249878
Epoch 4.118: Loss = 0.228119
Epoch 4.119: Loss = 0.328461
Epoch 4.120: Loss = 0.327133
TRAIN LOSS = 0.439011
TRAIN ACC = 87.5595 % (52538/60000)
Loss = 0.421585
Loss = 0.522476
Loss = 0.588882
Loss = 0.584198
Loss = 0.634308
Loss = 0.43071
Loss = 0.430115
Loss = 0.648621
Loss = 0.613083
Loss = 0.546768
Loss = 0.202271
Loss = 0.381195
Loss = 0.305801
Loss = 0.393295
Loss = 0.186829
Loss = 0.33165
Loss = 0.234146
Loss = 0.072403
Loss = 0.298782
Loss = 0.567337
TEST LOSS = 0.419723
TEST ACC = 525.378 % (8830/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.374466
Epoch 5.2: Loss = 0.525284
Epoch 5.3: Loss = 0.521317
Epoch 5.4: Loss = 0.353149
Epoch 5.5: Loss = 0.387711
Epoch 5.6: Loss = 0.433594
Epoch 5.7: Loss = 0.366684
Epoch 5.8: Loss = 0.389252
Epoch 5.9: Loss = 0.426361
Epoch 5.10: Loss = 0.420837
Epoch 5.11: Loss = 0.454712
Epoch 5.12: Loss = 0.414734
Epoch 5.13: Loss = 0.350708
Epoch 5.14: Loss = 0.397217
Epoch 5.15: Loss = 0.45108
Epoch 5.16: Loss = 0.496078
Epoch 5.17: Loss = 0.493179
Epoch 5.18: Loss = 0.714783
Epoch 5.19: Loss = 0.468536
Epoch 5.20: Loss = 0.372543
Epoch 5.21: Loss = 0.365799
Epoch 5.22: Loss = 0.304276
Epoch 5.23: Loss = 0.395844
Epoch 5.24: Loss = 0.602951
Epoch 5.25: Loss = 0.476608
Epoch 5.26: Loss = 0.617126
Epoch 5.27: Loss = 0.498001
Epoch 5.28: Loss = 0.524231
Epoch 5.29: Loss = 0.573807
Epoch 5.30: Loss = 0.600861
Epoch 5.31: Loss = 0.335831
Epoch 5.32: Loss = 0.485947
Epoch 5.33: Loss = 0.387772
Epoch 5.34: Loss = 0.467316
Epoch 5.35: Loss = 0.471802
Epoch 5.36: Loss = 0.522232
Epoch 5.37: Loss = 0.367996
Epoch 5.38: Loss = 0.343643
Epoch 5.39: Loss = 0.402191
Epoch 5.40: Loss = 0.375
Epoch 5.41: Loss = 0.407364
Epoch 5.42: Loss = 0.585907
Epoch 5.43: Loss = 0.331299
Epoch 5.44: Loss = 0.359329
Epoch 5.45: Loss = 0.430908
Epoch 5.46: Loss = 0.449005
Epoch 5.47: Loss = 0.407318
Epoch 5.48: Loss = 0.420105
Epoch 5.49: Loss = 0.425064
Epoch 5.50: Loss = 0.531784
Epoch 5.51: Loss = 0.37265
Epoch 5.52: Loss = 0.336639
Epoch 5.53: Loss = 0.432602
Epoch 5.54: Loss = 0.565277
Epoch 5.55: Loss = 0.474564
Epoch 5.56: Loss = 0.406555
Epoch 5.57: Loss = 0.419022
Epoch 5.58: Loss = 0.414139
Epoch 5.59: Loss = 0.538742
Epoch 5.60: Loss = 0.530151
Epoch 5.61: Loss = 0.449646
Epoch 5.62: Loss = 0.527435
Epoch 5.63: Loss = 0.63942
Epoch 5.64: Loss = 0.528
Epoch 5.65: Loss = 0.577393
Epoch 5.66: Loss = 0.400208
Epoch 5.67: Loss = 0.468552
Epoch 5.68: Loss = 0.265213
Epoch 5.69: Loss = 0.361938
Epoch 5.70: Loss = 0.50145
Epoch 5.71: Loss = 0.358932
Epoch 5.72: Loss = 0.347321
Epoch 5.73: Loss = 0.471008
Epoch 5.74: Loss = 0.345474
Epoch 5.75: Loss = 0.615921
Epoch 5.76: Loss = 0.476364
Epoch 5.77: Loss = 0.336792
Epoch 5.78: Loss = 0.447281
Epoch 5.79: Loss = 0.485992
Epoch 5.80: Loss = 0.446365
Epoch 5.81: Loss = 0.370331
Epoch 5.82: Loss = 0.300629
Epoch 5.83: Loss = 0.518677
Epoch 5.84: Loss = 0.458115
Epoch 5.85: Loss = 0.612335
Epoch 5.86: Loss = 0.508179
Epoch 5.87: Loss = 0.34642
Epoch 5.88: Loss = 0.453323
Epoch 5.89: Loss = 0.525742
Epoch 5.90: Loss = 0.361298
Epoch 5.91: Loss = 0.51622
Epoch 5.92: Loss = 0.538071
Epoch 5.93: Loss = 0.520905
Epoch 5.94: Loss = 0.336761
Epoch 5.95: Loss = 0.425339
Epoch 5.96: Loss = 0.505859
Epoch 5.97: Loss = 0.340088
Epoch 5.98: Loss = 0.40715
Epoch 5.99: Loss = 0.523682
Epoch 5.100: Loss = 0.593109
Epoch 5.101: Loss = 0.601166
Epoch 5.102: Loss = 0.421387
Epoch 5.103: Loss = 0.379654
Epoch 5.104: Loss = 0.366501
Epoch 5.105: Loss = 0.504745
Epoch 5.106: Loss = 0.583435
Epoch 5.107: Loss = 0.393539
Epoch 5.108: Loss = 0.476959
Epoch 5.109: Loss = 0.363464
Epoch 5.110: Loss = 0.496872
Epoch 5.111: Loss = 0.342285
Epoch 5.112: Loss = 0.342819
Epoch 5.113: Loss = 0.422867
Epoch 5.114: Loss = 0.338333
Epoch 5.115: Loss = 0.294891
Epoch 5.116: Loss = 0.37027
Epoch 5.117: Loss = 0.225464
Epoch 5.118: Loss = 0.233551
Epoch 5.119: Loss = 0.336288
Epoch 5.120: Loss = 0.347336
TRAIN LOSS = 0.440201
TRAIN ACC = 87.915 % (52751/60000)
Loss = 0.421066
Loss = 0.524689
Loss = 0.586395
Loss = 0.614685
Loss = 0.640762
Loss = 0.410767
Loss = 0.393143
Loss = 0.643951
Loss = 0.608307
Loss = 0.571335
Loss = 0.227219
Loss = 0.391266
Loss = 0.350632
Loss = 0.430405
Loss = 0.196609
Loss = 0.32077
Loss = 0.241989
Loss = 0.0782623
Loss = 0.267181
Loss = 0.588394
TEST LOSS = 0.425391
TEST ACC = 527.509 % (8848/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.39653
Epoch 6.2: Loss = 0.537857
Epoch 6.3: Loss = 0.536728
Epoch 6.4: Loss = 0.34201
Epoch 6.5: Loss = 0.387741
Epoch 6.6: Loss = 0.420486
Epoch 6.7: Loss = 0.359253
Epoch 6.8: Loss = 0.354141
Epoch 6.9: Loss = 0.409836
Epoch 6.10: Loss = 0.423569
Epoch 6.11: Loss = 0.423111
Epoch 6.12: Loss = 0.399933
Epoch 6.13: Loss = 0.333115
Epoch 6.14: Loss = 0.385345
Epoch 6.15: Loss = 0.450027
Epoch 6.16: Loss = 0.482681
Epoch 6.17: Loss = 0.505798
Epoch 6.18: Loss = 0.698837
Epoch 6.19: Loss = 0.466034
Epoch 6.20: Loss = 0.372406
Epoch 6.21: Loss = 0.38678
Epoch 6.22: Loss = 0.292816
Epoch 6.23: Loss = 0.379822
Epoch 6.24: Loss = 0.637405
Epoch 6.25: Loss = 0.455811
Epoch 6.26: Loss = 0.583328
Epoch 6.27: Loss = 0.493378
Epoch 6.28: Loss = 0.503433
Epoch 6.29: Loss = 0.544769
Epoch 6.30: Loss = 0.599991
Epoch 6.31: Loss = 0.334564
Epoch 6.32: Loss = 0.495255
Epoch 6.33: Loss = 0.393494
Epoch 6.34: Loss = 0.496964
Epoch 6.35: Loss = 0.462967
Epoch 6.36: Loss = 0.521912
Epoch 6.37: Loss = 0.360565
Epoch 6.38: Loss = 0.352493
Epoch 6.39: Loss = 0.376038
Epoch 6.40: Loss = 0.386276
Epoch 6.41: Loss = 0.423584
Epoch 6.42: Loss = 0.593689
Epoch 6.43: Loss = 0.339523
Epoch 6.44: Loss = 0.363541
Epoch 6.45: Loss = 0.482224
Epoch 6.46: Loss = 0.458801
Epoch 6.47: Loss = 0.416351
Epoch 6.48: Loss = 0.444504
Epoch 6.49: Loss = 0.449554
Epoch 6.50: Loss = 0.550079
Epoch 6.51: Loss = 0.370667
Epoch 6.52: Loss = 0.310638
Epoch 6.53: Loss = 0.429108
Epoch 6.54: Loss = 0.574265
Epoch 6.55: Loss = 0.501358
Epoch 6.56: Loss = 0.407272
Epoch 6.57: Loss = 0.456635
Epoch 6.58: Loss = 0.429367
Epoch 6.59: Loss = 0.531952
Epoch 6.60: Loss = 0.520233
Epoch 6.61: Loss = 0.463058
Epoch 6.62: Loss = 0.534241
Epoch 6.63: Loss = 0.645767
Epoch 6.64: Loss = 0.513107
Epoch 6.65: Loss = 0.604202
Epoch 6.66: Loss = 0.406952
Epoch 6.67: Loss = 0.431534
Epoch 6.68: Loss = 0.244675
Epoch 6.69: Loss = 0.353317
Epoch 6.70: Loss = 0.497406
Epoch 6.71: Loss = 0.390839
Epoch 6.72: Loss = 0.339706
Epoch 6.73: Loss = 0.463989
Epoch 6.74: Loss = 0.33255
Epoch 6.75: Loss = 0.694153
Epoch 6.76: Loss = 0.477875
Epoch 6.77: Loss = 0.319885
Epoch 6.78: Loss = 0.420181
Epoch 6.79: Loss = 0.542465
Epoch 6.80: Loss = 0.49968
Epoch 6.81: Loss = 0.423553
Epoch 6.82: Loss = 0.317688
Epoch 6.83: Loss = 0.568253
Epoch 6.84: Loss = 0.475067
Epoch 6.85: Loss = 0.647125
Epoch 6.86: Loss = 0.507248
Epoch 6.87: Loss = 0.31749
Epoch 6.88: Loss = 0.469162
Epoch 6.89: Loss = 0.500824
Epoch 6.90: Loss = 0.34523
Epoch 6.91: Loss = 0.5336
Epoch 6.92: Loss = 0.551895
Epoch 6.93: Loss = 0.504166
Epoch 6.94: Loss = 0.326462
Epoch 6.95: Loss = 0.446625
Epoch 6.96: Loss = 0.539261
Epoch 6.97: Loss = 0.332108
Epoch 6.98: Loss = 0.40416
Epoch 6.99: Loss = 0.564026
Epoch 6.100: Loss = 0.605789
Epoch 6.101: Loss = 0.593124
Epoch 6.102: Loss = 0.43454
Epoch 6.103: Loss = 0.407074
Epoch 6.104: Loss = 0.36795
Epoch 6.105: Loss = 0.526321
Epoch 6.106: Loss = 0.594482
Epoch 6.107: Loss = 0.388107
Epoch 6.108: Loss = 0.487396
Epoch 6.109: Loss = 0.380554
Epoch 6.110: Loss = 0.50795
Epoch 6.111: Loss = 0.308044
Epoch 6.112: Loss = 0.363037
Epoch 6.113: Loss = 0.402893
Epoch 6.114: Loss = 0.339371
Epoch 6.115: Loss = 0.300247
Epoch 6.116: Loss = 0.392105
Epoch 6.117: Loss = 0.234818
Epoch 6.118: Loss = 0.198151
Epoch 6.119: Loss = 0.307144
Epoch 6.120: Loss = 0.341965
TRAIN LOSS = 0.443558
TRAIN ACC = 88.2095 % (52928/60000)
Loss = 0.4272
Loss = 0.517944
Loss = 0.585251
Loss = 0.630951
Loss = 0.645355
Loss = 0.42778
Loss = 0.396896
Loss = 0.655014
Loss = 0.610458
Loss = 0.547272
Loss = 0.218933
Loss = 0.361633
Loss = 0.382919
Loss = 0.409134
Loss = 0.186493
Loss = 0.349426
Loss = 0.232788
Loss = 0.0817871
Loss = 0.271667
Loss = 0.568451
TEST LOSS = 0.425368
TEST ACC = 529.279 % (8907/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.403564
Epoch 7.2: Loss = 0.488754
Epoch 7.3: Loss = 0.551361
Epoch 7.4: Loss = 0.362961
Epoch 7.5: Loss = 0.355057
Epoch 7.6: Loss = 0.417068
Epoch 7.7: Loss = 0.352005
Epoch 7.8: Loss = 0.374237
Epoch 7.9: Loss = 0.390381
Epoch 7.10: Loss = 0.454117
Epoch 7.11: Loss = 0.439117
Epoch 7.12: Loss = 0.424225
Epoch 7.13: Loss = 0.324646
Epoch 7.14: Loss = 0.3759
Epoch 7.15: Loss = 0.435364
Epoch 7.16: Loss = 0.480667
Epoch 7.17: Loss = 0.529114
Epoch 7.18: Loss = 0.687988
Epoch 7.19: Loss = 0.463959
Epoch 7.20: Loss = 0.365463
Epoch 7.21: Loss = 0.38179
Epoch 7.22: Loss = 0.284576
Epoch 7.23: Loss = 0.391998
Epoch 7.24: Loss = 0.649734
Epoch 7.25: Loss = 0.462265
Epoch 7.26: Loss = 0.582748
Epoch 7.27: Loss = 0.53595
Epoch 7.28: Loss = 0.475235
Epoch 7.29: Loss = 0.556656
Epoch 7.30: Loss = 0.592575
Epoch 7.31: Loss = 0.365585
Epoch 7.32: Loss = 0.471954
Epoch 7.33: Loss = 0.392273
Epoch 7.34: Loss = 0.513672
Epoch 7.35: Loss = 0.406052
Epoch 7.36: Loss = 0.509262
Epoch 7.37: Loss = 0.347992
Epoch 7.38: Loss = 0.35466
Epoch 7.39: Loss = 0.353531
Epoch 7.40: Loss = 0.372482
Epoch 7.41: Loss = 0.404633
Epoch 7.42: Loss = 0.61673
Epoch 7.43: Loss = 0.32254
Epoch 7.44: Loss = 0.381195
Epoch 7.45: Loss = 0.445602
Epoch 7.46: Loss = 0.455093
Epoch 7.47: Loss = 0.408493
Epoch 7.48: Loss = 0.452911
Epoch 7.49: Loss = 0.457825
Epoch 7.50: Loss = 0.541122
Epoch 7.51: Loss = 0.346054
Epoch 7.52: Loss = 0.360764
Epoch 7.53: Loss = 0.432495
Epoch 7.54: Loss = 0.578613
Epoch 7.55: Loss = 0.540802
Epoch 7.56: Loss = 0.399765
Epoch 7.57: Loss = 0.457336
Epoch 7.58: Loss = 0.412247
Epoch 7.59: Loss = 0.543579
Epoch 7.60: Loss = 0.505325
Epoch 7.61: Loss = 0.457138
Epoch 7.62: Loss = 0.515457
Epoch 7.63: Loss = 0.683182
Epoch 7.64: Loss = 0.521072
Epoch 7.65: Loss = 0.632553
Epoch 7.66: Loss = 0.416656
Epoch 7.67: Loss = 0.449387
Epoch 7.68: Loss = 0.235489
Epoch 7.69: Loss = 0.341827
Epoch 7.70: Loss = 0.499466
Epoch 7.71: Loss = 0.374527
Epoch 7.72: Loss = 0.311951
Epoch 7.73: Loss = 0.452499
Epoch 7.74: Loss = 0.340866
Epoch 7.75: Loss = 0.680176
Epoch 7.76: Loss = 0.487778
Epoch 7.77: Loss = 0.315598
Epoch 7.78: Loss = 0.434921
Epoch 7.79: Loss = 0.573288
Epoch 7.80: Loss = 0.490555
Epoch 7.81: Loss = 0.413498
Epoch 7.82: Loss = 0.324753
Epoch 7.83: Loss = 0.549454
Epoch 7.84: Loss = 0.467163
Epoch 7.85: Loss = 0.671692
Epoch 7.86: Loss = 0.521667
Epoch 7.87: Loss = 0.311676
Epoch 7.88: Loss = 0.432053
Epoch 7.89: Loss = 0.522568
Epoch 7.90: Loss = 0.383301
Epoch 7.91: Loss = 0.543991
Epoch 7.92: Loss = 0.569885
Epoch 7.93: Loss = 0.519897
Epoch 7.94: Loss = 0.337753
Epoch 7.95: Loss = 0.449951
Epoch 7.96: Loss = 0.548843
Epoch 7.97: Loss = 0.353943
Epoch 7.98: Loss = 0.424103
Epoch 7.99: Loss = 0.555862
Epoch 7.100: Loss = 0.629227
Epoch 7.101: Loss = 0.619583
Epoch 7.102: Loss = 0.469818
Epoch 7.103: Loss = 0.396469
Epoch 7.104: Loss = 0.361877
Epoch 7.105: Loss = 0.540314
Epoch 7.106: Loss = 0.572662
Epoch 7.107: Loss = 0.397308
Epoch 7.108: Loss = 0.533707
Epoch 7.109: Loss = 0.371704
Epoch 7.110: Loss = 0.48439
Epoch 7.111: Loss = 0.302933
Epoch 7.112: Loss = 0.379517
Epoch 7.113: Loss = 0.422699
Epoch 7.114: Loss = 0.366608
Epoch 7.115: Loss = 0.297852
Epoch 7.116: Loss = 0.377655
Epoch 7.117: Loss = 0.228394
Epoch 7.118: Loss = 0.193771
Epoch 7.119: Loss = 0.338531
Epoch 7.120: Loss = 0.36322
TRAIN LOSS = 0.445877
TRAIN ACC = 88.6093 % (53168/60000)
Loss = 0.436691
Loss = 0.509781
Loss = 0.576126
Loss = 0.637909
Loss = 0.647903
Loss = 0.412506
Loss = 0.420349
Loss = 0.681793
Loss = 0.646164
Loss = 0.542938
Loss = 0.193832
Loss = 0.372086
Loss = 0.380936
Loss = 0.417664
Loss = 0.179184
Loss = 0.330475
Loss = 0.253174
Loss = 0.078064
Loss = 0.2966
Loss = 0.572433
TEST LOSS = 0.42933
TEST ACC = 531.679 % (8886/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.392273
Epoch 8.2: Loss = 0.506104
Epoch 8.3: Loss = 0.54393
Epoch 8.4: Loss = 0.343964
Epoch 8.5: Loss = 0.327957
Epoch 8.6: Loss = 0.418045
Epoch 8.7: Loss = 0.355347
Epoch 8.8: Loss = 0.373016
Epoch 8.9: Loss = 0.399826
Epoch 8.10: Loss = 0.449066
Epoch 8.11: Loss = 0.461182
Epoch 8.12: Loss = 0.425781
Epoch 8.13: Loss = 0.346649
Epoch 8.14: Loss = 0.361771
Epoch 8.15: Loss = 0.475388
Epoch 8.16: Loss = 0.508575
Epoch 8.17: Loss = 0.549316
Epoch 8.18: Loss = 0.703583
Epoch 8.19: Loss = 0.490433
Epoch 8.20: Loss = 0.398163
Epoch 8.21: Loss = 0.385391
Epoch 8.22: Loss = 0.280945
Epoch 8.23: Loss = 0.378159
Epoch 8.24: Loss = 0.611145
Epoch 8.25: Loss = 0.474884
Epoch 8.26: Loss = 0.594849
Epoch 8.27: Loss = 0.565689
Epoch 8.28: Loss = 0.497726
Epoch 8.29: Loss = 0.582214
Epoch 8.30: Loss = 0.611298
Epoch 8.31: Loss = 0.380829
Epoch 8.32: Loss = 0.462753
Epoch 8.33: Loss = 0.435745
Epoch 8.34: Loss = 0.517258
Epoch 8.35: Loss = 0.412567
Epoch 8.36: Loss = 0.530823
Epoch 8.37: Loss = 0.345764
Epoch 8.38: Loss = 0.378876
Epoch 8.39: Loss = 0.374039
Epoch 8.40: Loss = 0.422943
Epoch 8.41: Loss = 0.446518
Epoch 8.42: Loss = 0.624649
Epoch 8.43: Loss = 0.353729
Epoch 8.44: Loss = 0.390961
Epoch 8.45: Loss = 0.465897
Epoch 8.46: Loss = 0.475388
Epoch 8.47: Loss = 0.401337
Epoch 8.48: Loss = 0.470627
Epoch 8.49: Loss = 0.472656
Epoch 8.50: Loss = 0.588882
Epoch 8.51: Loss = 0.35228
Epoch 8.52: Loss = 0.355789
Epoch 8.53: Loss = 0.440567
Epoch 8.54: Loss = 0.587616
Epoch 8.55: Loss = 0.542267
Epoch 8.56: Loss = 0.418289
Epoch 8.57: Loss = 0.455811
Epoch 8.58: Loss = 0.402512
Epoch 8.59: Loss = 0.547165
Epoch 8.60: Loss = 0.529312
Epoch 8.61: Loss = 0.469086
Epoch 8.62: Loss = 0.536697
Epoch 8.63: Loss = 0.71022
Epoch 8.64: Loss = 0.534363
Epoch 8.65: Loss = 0.673584
Epoch 8.66: Loss = 0.40593
Epoch 8.67: Loss = 0.456039
Epoch 8.68: Loss = 0.275833
Epoch 8.69: Loss = 0.370499
Epoch 8.70: Loss = 0.522354
Epoch 8.71: Loss = 0.382736
Epoch 8.72: Loss = 0.314697
Epoch 8.73: Loss = 0.466476
Epoch 8.74: Loss = 0.362885
Epoch 8.75: Loss = 0.742813
Epoch 8.76: Loss = 0.51091
Epoch 8.77: Loss = 0.355453
Epoch 8.78: Loss = 0.463989
Epoch 8.79: Loss = 0.603806
Epoch 8.80: Loss = 0.47464
Epoch 8.81: Loss = 0.409195
Epoch 8.82: Loss = 0.326675
Epoch 8.83: Loss = 0.553802
Epoch 8.84: Loss = 0.482895
Epoch 8.85: Loss = 0.667969
Epoch 8.86: Loss = 0.541611
Epoch 8.87: Loss = 0.319214
Epoch 8.88: Loss = 0.446716
Epoch 8.89: Loss = 0.54805
Epoch 8.90: Loss = 0.415939
Epoch 8.91: Loss = 0.543289
Epoch 8.92: Loss = 0.565964
Epoch 8.93: Loss = 0.549454
Epoch 8.94: Loss = 0.333298
Epoch 8.95: Loss = 0.51506
Epoch 8.96: Loss = 0.548325
Epoch 8.97: Loss = 0.363815
Epoch 8.98: Loss = 0.485779
Epoch 8.99: Loss = 0.607941
Epoch 8.100: Loss = 0.659195
Epoch 8.101: Loss = 0.64917
Epoch 8.102: Loss = 0.50235
Epoch 8.103: Loss = 0.412338
Epoch 8.104: Loss = 0.398392
Epoch 8.105: Loss = 0.536331
Epoch 8.106: Loss = 0.622955
Epoch 8.107: Loss = 0.394241
Epoch 8.108: Loss = 0.552917
Epoch 8.109: Loss = 0.380447
Epoch 8.110: Loss = 0.493408
Epoch 8.111: Loss = 0.330505
Epoch 8.112: Loss = 0.369034
Epoch 8.113: Loss = 0.433197
Epoch 8.114: Loss = 0.347961
Epoch 8.115: Loss = 0.29451
Epoch 8.116: Loss = 0.399612
Epoch 8.117: Loss = 0.223679
Epoch 8.118: Loss = 0.214676
Epoch 8.119: Loss = 0.321823
Epoch 8.120: Loss = 0.386307
TRAIN LOSS = 0.459808
TRAIN ACC = 88.5803 % (53150/60000)
Loss = 0.438599
Loss = 0.541367
Loss = 0.626984
Loss = 0.656876
Loss = 0.675705
Loss = 0.461029
Loss = 0.440323
Loss = 0.706039
Loss = 0.65477
Loss = 0.588577
Loss = 0.212387
Loss = 0.403595
Loss = 0.361252
Loss = 0.434906
Loss = 0.197189
Loss = 0.31395
Loss = 0.257431
Loss = 0.0734406
Loss = 0.297424
Loss = 0.534317
TEST LOSS = 0.443808
TEST ACC = 531.499 % (8897/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.420074
Epoch 9.2: Loss = 0.520447
Epoch 9.3: Loss = 0.523544
Epoch 9.4: Loss = 0.352615
Epoch 9.5: Loss = 0.339859
Epoch 9.6: Loss = 0.427658
Epoch 9.7: Loss = 0.381256
Epoch 9.8: Loss = 0.370834
Epoch 9.9: Loss = 0.423553
Epoch 9.10: Loss = 0.468201
Epoch 9.11: Loss = 0.52504
Epoch 9.12: Loss = 0.456406
Epoch 9.13: Loss = 0.375046
Epoch 9.14: Loss = 0.36203
Epoch 9.15: Loss = 0.464783
Epoch 9.16: Loss = 0.515015
Epoch 9.17: Loss = 0.537018
Epoch 9.18: Loss = 0.7453
Epoch 9.19: Loss = 0.500763
Epoch 9.20: Loss = 0.384964
Epoch 9.21: Loss = 0.387558
Epoch 9.22: Loss = 0.289902
Epoch 9.23: Loss = 0.400192
Epoch 9.24: Loss = 0.650177
Epoch 9.25: Loss = 0.506912
Epoch 9.26: Loss = 0.56868
Epoch 9.27: Loss = 0.567307
Epoch 9.28: Loss = 0.523132
Epoch 9.29: Loss = 0.590714
Epoch 9.30: Loss = 0.627792
Epoch 9.31: Loss = 0.414032
Epoch 9.32: Loss = 0.497086
Epoch 9.33: Loss = 0.437149
Epoch 9.34: Loss = 0.546875
Epoch 9.35: Loss = 0.421371
Epoch 9.36: Loss = 0.545654
Epoch 9.37: Loss = 0.327316
Epoch 9.38: Loss = 0.386765
Epoch 9.39: Loss = 0.383179
Epoch 9.40: Loss = 0.415009
Epoch 9.41: Loss = 0.424255
Epoch 9.42: Loss = 0.661072
Epoch 9.43: Loss = 0.382111
Epoch 9.44: Loss = 0.408463
Epoch 9.45: Loss = 0.478333
Epoch 9.46: Loss = 0.506393
Epoch 9.47: Loss = 0.427902
Epoch 9.48: Loss = 0.512512
Epoch 9.49: Loss = 0.461334
Epoch 9.50: Loss = 0.572128
Epoch 9.51: Loss = 0.377548
Epoch 9.52: Loss = 0.385361
Epoch 9.53: Loss = 0.436691
Epoch 9.54: Loss = 0.589386
Epoch 9.55: Loss = 0.53334
Epoch 9.56: Loss = 0.376205
Epoch 9.57: Loss = 0.474533
Epoch 9.58: Loss = 0.474136
Epoch 9.59: Loss = 0.563797
Epoch 9.60: Loss = 0.543335
Epoch 9.61: Loss = 0.489944
Epoch 9.62: Loss = 0.522186
Epoch 9.63: Loss = 0.706985
Epoch 9.64: Loss = 0.547958
Epoch 9.65: Loss = 0.681732
Epoch 9.66: Loss = 0.431183
Epoch 9.67: Loss = 0.494858
Epoch 9.68: Loss = 0.273605
Epoch 9.69: Loss = 0.41008
Epoch 9.70: Loss = 0.498398
Epoch 9.71: Loss = 0.383987
Epoch 9.72: Loss = 0.333206
Epoch 9.73: Loss = 0.473862
Epoch 9.74: Loss = 0.383621
Epoch 9.75: Loss = 0.815598
Epoch 9.76: Loss = 0.501846
Epoch 9.77: Loss = 0.356949
Epoch 9.78: Loss = 0.490967
Epoch 9.79: Loss = 0.624435
Epoch 9.80: Loss = 0.508179
Epoch 9.81: Loss = 0.448364
Epoch 9.82: Loss = 0.329651
Epoch 9.83: Loss = 0.566635
Epoch 9.84: Loss = 0.502075
Epoch 9.85: Loss = 0.668793
Epoch 9.86: Loss = 0.560562
Epoch 9.87: Loss = 0.320801
Epoch 9.88: Loss = 0.458191
Epoch 9.89: Loss = 0.54599
Epoch 9.90: Loss = 0.374008
Epoch 9.91: Loss = 0.564682
Epoch 9.92: Loss = 0.558823
Epoch 9.93: Loss = 0.558548
Epoch 9.94: Loss = 0.345245
Epoch 9.95: Loss = 0.520844
Epoch 9.96: Loss = 0.560867
Epoch 9.97: Loss = 0.366516
Epoch 9.98: Loss = 0.502808
Epoch 9.99: Loss = 0.603897
Epoch 9.100: Loss = 0.615524
Epoch 9.101: Loss = 0.646439
Epoch 9.102: Loss = 0.515625
Epoch 9.103: Loss = 0.433578
Epoch 9.104: Loss = 0.392395
Epoch 9.105: Loss = 0.555481
Epoch 9.106: Loss = 0.62825
Epoch 9.107: Loss = 0.388214
Epoch 9.108: Loss = 0.550308
Epoch 9.109: Loss = 0.356155
Epoch 9.110: Loss = 0.522751
Epoch 9.111: Loss = 0.339981
Epoch 9.112: Loss = 0.408447
Epoch 9.113: Loss = 0.458679
Epoch 9.114: Loss = 0.378204
Epoch 9.115: Loss = 0.306564
Epoch 9.116: Loss = 0.421204
Epoch 9.117: Loss = 0.218445
Epoch 9.118: Loss = 0.233231
Epoch 9.119: Loss = 0.334305
Epoch 9.120: Loss = 0.401657
TRAIN LOSS = 0.471359
TRAIN ACC = 88.5773 % (53148/60000)
Loss = 0.44632
Loss = 0.53566
Loss = 0.635605
Loss = 0.625595
Loss = 0.674591
Loss = 0.477463
Loss = 0.425491
Loss = 0.763916
Loss = 0.670715
Loss = 0.592484
Loss = 0.218735
Loss = 0.361206
Loss = 0.369308
Loss = 0.419968
Loss = 0.190369
Loss = 0.357071
Loss = 0.247421
Loss = 0.0659027
Loss = 0.348633
Loss = 0.574371
TEST LOSS = 0.450041
TEST ACC = 531.479 % (8904/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.422714
Epoch 10.2: Loss = 0.532623
Epoch 10.3: Loss = 0.565002
Epoch 10.4: Loss = 0.339584
Epoch 10.5: Loss = 0.336792
Epoch 10.6: Loss = 0.412323
Epoch 10.7: Loss = 0.40329
Epoch 10.8: Loss = 0.398849
Epoch 10.9: Loss = 0.427216
Epoch 10.10: Loss = 0.492371
Epoch 10.11: Loss = 0.505875
Epoch 10.12: Loss = 0.497574
Epoch 10.13: Loss = 0.36412
Epoch 10.14: Loss = 0.365921
Epoch 10.15: Loss = 0.468735
Epoch 10.16: Loss = 0.532318
Epoch 10.17: Loss = 0.552704
Epoch 10.18: Loss = 0.745956
Epoch 10.19: Loss = 0.52182
Epoch 10.20: Loss = 0.398727
Epoch 10.21: Loss = 0.412628
Epoch 10.22: Loss = 0.300385
Epoch 10.23: Loss = 0.420975
Epoch 10.24: Loss = 0.650452
Epoch 10.25: Loss = 0.497375
Epoch 10.26: Loss = 0.600861
Epoch 10.27: Loss = 0.607162
Epoch 10.28: Loss = 0.524704
Epoch 10.29: Loss = 0.630386
Epoch 10.30: Loss = 0.620728
Epoch 10.31: Loss = 0.414688
Epoch 10.32: Loss = 0.535309
Epoch 10.33: Loss = 0.443222
Epoch 10.34: Loss = 0.545319
Epoch 10.35: Loss = 0.455566
Epoch 10.36: Loss = 0.568787
Epoch 10.37: Loss = 0.376007
Epoch 10.38: Loss = 0.352966
Epoch 10.39: Loss = 0.426422
Epoch 10.40: Loss = 0.383057
Epoch 10.41: Loss = 0.41423
Epoch 10.42: Loss = 0.690125
Epoch 10.43: Loss = 0.37413
Epoch 10.44: Loss = 0.422943
Epoch 10.45: Loss = 0.45813
Epoch 10.46: Loss = 0.52829
Epoch 10.47: Loss = 0.404587
Epoch 10.48: Loss = 0.527435
Epoch 10.49: Loss = 0.450012
Epoch 10.50: Loss = 0.588959
Epoch 10.51: Loss = 0.342743
Epoch 10.52: Loss = 0.412796
Epoch 10.53: Loss = 0.464172
Epoch 10.54: Loss = 0.580215
Epoch 10.55: Loss = 0.576828
Epoch 10.56: Loss = 0.438416
Epoch 10.57: Loss = 0.474075
Epoch 10.58: Loss = 0.455688
Epoch 10.59: Loss = 0.604935
Epoch 10.60: Loss = 0.561996
Epoch 10.61: Loss = 0.498703
Epoch 10.62: Loss = 0.544876
Epoch 10.63: Loss = 0.775055
Epoch 10.64: Loss = 0.561798
Epoch 10.65: Loss = 0.679779
Epoch 10.66: Loss = 0.464401
Epoch 10.67: Loss = 0.49939
Epoch 10.68: Loss = 0.31395
Epoch 10.69: Loss = 0.399979
Epoch 10.70: Loss = 0.547394
Epoch 10.71: Loss = 0.400192
Epoch 10.72: Loss = 0.377213
Epoch 10.73: Loss = 0.475388
Epoch 10.74: Loss = 0.397995
Epoch 10.75: Loss = 0.809006
Epoch 10.76: Loss = 0.511398
Epoch 10.77: Loss = 0.335587
Epoch 10.78: Loss = 0.473083
Epoch 10.79: Loss = 0.622757
Epoch 10.80: Loss = 0.487274
Epoch 10.81: Loss = 0.460205
Epoch 10.82: Loss = 0.338287
Epoch 10.83: Loss = 0.598816
Epoch 10.84: Loss = 0.502289
Epoch 10.85: Loss = 0.724594
Epoch 10.86: Loss = 0.524139
Epoch 10.87: Loss = 0.347565
Epoch 10.88: Loss = 0.503372
Epoch 10.89: Loss = 0.545715
Epoch 10.90: Loss = 0.361694
Epoch 10.91: Loss = 0.549591
Epoch 10.92: Loss = 0.564133
Epoch 10.93: Loss = 0.601486
Epoch 10.94: Loss = 0.354706
Epoch 10.95: Loss = 0.549927
Epoch 10.96: Loss = 0.572861
Epoch 10.97: Loss = 0.376678
Epoch 10.98: Loss = 0.51918
Epoch 10.99: Loss = 0.610901
Epoch 10.100: Loss = 0.616425
Epoch 10.101: Loss = 0.673065
Epoch 10.102: Loss = 0.485901
Epoch 10.103: Loss = 0.458725
Epoch 10.104: Loss = 0.405365
Epoch 10.105: Loss = 0.560623
Epoch 10.106: Loss = 0.636581
Epoch 10.107: Loss = 0.410736
Epoch 10.108: Loss = 0.520126
Epoch 10.109: Loss = 0.365005
Epoch 10.110: Loss = 0.532959
Epoch 10.111: Loss = 0.32251
Epoch 10.112: Loss = 0.397079
Epoch 10.113: Loss = 0.476364
Epoch 10.114: Loss = 0.355576
Epoch 10.115: Loss = 0.322586
Epoch 10.116: Loss = 0.454514
Epoch 10.117: Loss = 0.243942
Epoch 10.118: Loss = 0.253754
Epoch 10.119: Loss = 0.306839
Epoch 10.120: Loss = 0.415314
TRAIN LOSS = 0.481812
TRAIN ACC = 88.7756 % (53267/60000)
Loss = 0.423233
Loss = 0.549347
Loss = 0.63562
Loss = 0.649368
Loss = 0.767487
Loss = 0.476257
Loss = 0.397049
Loss = 0.764587
Loss = 0.664093
Loss = 0.582214
Loss = 0.240173
Loss = 0.33255
Loss = 0.444992
Loss = 0.434555
Loss = 0.192398
Loss = 0.357208
Loss = 0.277237
Loss = 0.0773621
Loss = 0.357681
Loss = 0.595612
TEST LOSS = 0.460951
TEST ACC = 532.669 % (8931/10000)
