Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.34525
Epoch 1.2: Loss = 2.29456
Epoch 1.3: Loss = 2.21939
Epoch 1.4: Loss = 2.10934
Epoch 1.5: Loss = 2.05756
Epoch 1.6: Loss = 2.008
Epoch 1.7: Loss = 1.95906
Epoch 1.8: Loss = 1.90773
Epoch 1.9: Loss = 1.85156
Epoch 1.10: Loss = 1.75172
Epoch 1.11: Loss = 1.79172
Epoch 1.12: Loss = 1.73468
Epoch 1.13: Loss = 1.67004
Epoch 1.14: Loss = 1.61076
Epoch 1.15: Loss = 1.58946
Epoch 1.16: Loss = 1.57011
Epoch 1.17: Loss = 1.53273
Epoch 1.18: Loss = 1.48415
Epoch 1.19: Loss = 1.43784
Epoch 1.20: Loss = 1.45427
Epoch 1.21: Loss = 1.39186
Epoch 1.22: Loss = 1.35213
Epoch 1.23: Loss = 1.33569
Epoch 1.24: Loss = 1.35437
Epoch 1.25: Loss = 1.30273
Epoch 1.26: Loss = 1.23058
Epoch 1.27: Loss = 1.22433
Epoch 1.28: Loss = 1.21848
Epoch 1.29: Loss = 1.19783
Epoch 1.30: Loss = 1.17871
Epoch 1.31: Loss = 1.19995
Epoch 1.32: Loss = 1.17308
Epoch 1.33: Loss = 1.08984
Epoch 1.34: Loss = 1.15045
Epoch 1.35: Loss = 1.16463
Epoch 1.36: Loss = 1.12521
Epoch 1.37: Loss = 1.10776
Epoch 1.38: Loss = 1.07158
Epoch 1.39: Loss = 1.0519
Epoch 1.40: Loss = 1.04297
Epoch 1.41: Loss = 1.0909
Epoch 1.42: Loss = 1.00345
Epoch 1.43: Loss = 1.0014
Epoch 1.44: Loss = 0.966522
Epoch 1.45: Loss = 1.00702
Epoch 1.46: Loss = 1.00378
Epoch 1.47: Loss = 0.981918
Epoch 1.48: Loss = 0.943604
Epoch 1.49: Loss = 0.989517
Epoch 1.50: Loss = 0.931488
Epoch 1.51: Loss = 0.896317
Epoch 1.52: Loss = 0.979492
Epoch 1.53: Loss = 0.965164
Epoch 1.54: Loss = 0.852432
Epoch 1.55: Loss = 0.921005
Epoch 1.56: Loss = 0.935913
Epoch 1.57: Loss = 0.938156
Epoch 1.58: Loss = 0.908524
Epoch 1.59: Loss = 0.901993
Epoch 1.60: Loss = 0.949493
Epoch 1.61: Loss = 0.82254
Epoch 1.62: Loss = 0.929199
Epoch 1.63: Loss = 0.792999
Epoch 1.64: Loss = 0.836197
Epoch 1.65: Loss = 0.852722
Epoch 1.66: Loss = 0.855637
Epoch 1.67: Loss = 0.793503
Epoch 1.68: Loss = 0.911606
Epoch 1.69: Loss = 0.85817
Epoch 1.70: Loss = 0.825958
Epoch 1.71: Loss = 0.770798
Epoch 1.72: Loss = 0.807953
Epoch 1.73: Loss = 0.875305
Epoch 1.74: Loss = 0.869629
Epoch 1.75: Loss = 0.798843
Epoch 1.76: Loss = 0.812881
Epoch 1.77: Loss = 0.784454
Epoch 1.78: Loss = 0.805832
Epoch 1.79: Loss = 0.746002
Epoch 1.80: Loss = 0.796097
Epoch 1.81: Loss = 0.758591
Epoch 1.82: Loss = 0.772888
Epoch 1.83: Loss = 0.82872
Epoch 1.84: Loss = 0.771866
Epoch 1.85: Loss = 0.767502
Epoch 1.86: Loss = 0.841537
Epoch 1.87: Loss = 0.82753
Epoch 1.88: Loss = 0.703735
Epoch 1.89: Loss = 0.827621
Epoch 1.90: Loss = 0.773254
Epoch 1.91: Loss = 0.827042
Epoch 1.92: Loss = 0.794952
Epoch 1.93: Loss = 0.785431
Epoch 1.94: Loss = 0.76384
Epoch 1.95: Loss = 0.79631
Epoch 1.96: Loss = 0.723511
Epoch 1.97: Loss = 0.654114
Epoch 1.98: Loss = 0.756897
Epoch 1.99: Loss = 0.749069
Epoch 1.100: Loss = 0.706924
Epoch 1.101: Loss = 0.797394
Epoch 1.102: Loss = 0.780975
Epoch 1.103: Loss = 0.773376
Epoch 1.104: Loss = 0.724136
Epoch 1.105: Loss = 0.697876
Epoch 1.106: Loss = 0.833755
Epoch 1.107: Loss = 0.7668
Epoch 1.108: Loss = 0.746597
Epoch 1.109: Loss = 0.740189
Epoch 1.110: Loss = 0.768784
Epoch 1.111: Loss = 0.671738
Epoch 1.112: Loss = 0.686752
Epoch 1.113: Loss = 0.738556
Epoch 1.114: Loss = 0.71611
Epoch 1.115: Loss = 0.739304
Epoch 1.116: Loss = 0.671524
Epoch 1.117: Loss = 0.808807
Epoch 1.118: Loss = 0.670456
Epoch 1.119: Loss = 0.724823
Epoch 1.120: Loss = 0.681595
TRAIN LOSS = 1.05193
TRAIN ACC = 64.9261 % (38957/60000)
Loss = 0.66629
Loss = 0.783661
Loss = 0.760315
Loss = 0.67392
Loss = 0.674408
Loss = 0.857086
Loss = 0.836868
Loss = 0.790161
Loss = 0.723282
Loss = 0.680222
Loss = 0.791656
Loss = 0.777588
Loss = 0.76123
Loss = 0.760162
Loss = 0.738663
Loss = 0.789795
Loss = 0.70108
Loss = 0.760361
Loss = 0.808746
Loss = 0.739777
TEST LOSS = 0.753763
TEST ACC = 389.569 % (7328/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.737839
Epoch 2.2: Loss = 0.719559
Epoch 2.3: Loss = 0.798126
Epoch 2.4: Loss = 0.648056
Epoch 2.5: Loss = 0.714752
Epoch 2.6: Loss = 0.798431
Epoch 2.7: Loss = 0.714691
Epoch 2.8: Loss = 0.787125
Epoch 2.9: Loss = 0.641037
Epoch 2.10: Loss = 0.558807
Epoch 2.11: Loss = 0.791809
Epoch 2.12: Loss = 0.741852
Epoch 2.13: Loss = 0.724304
Epoch 2.14: Loss = 0.714233
Epoch 2.15: Loss = 0.705994
Epoch 2.16: Loss = 0.762238
Epoch 2.17: Loss = 0.68248
Epoch 2.18: Loss = 0.727448
Epoch 2.19: Loss = 0.70195
Epoch 2.20: Loss = 0.809662
Epoch 2.21: Loss = 0.666412
Epoch 2.22: Loss = 0.628479
Epoch 2.23: Loss = 0.733505
Epoch 2.24: Loss = 0.772797
Epoch 2.25: Loss = 0.694107
Epoch 2.26: Loss = 0.635956
Epoch 2.27: Loss = 0.699234
Epoch 2.28: Loss = 0.710327
Epoch 2.29: Loss = 0.712051
Epoch 2.30: Loss = 0.677963
Epoch 2.31: Loss = 0.781525
Epoch 2.32: Loss = 0.68988
Epoch 2.33: Loss = 0.620285
Epoch 2.34: Loss = 0.776443
Epoch 2.35: Loss = 0.772369
Epoch 2.36: Loss = 0.737274
Epoch 2.37: Loss = 0.743835
Epoch 2.38: Loss = 0.692368
Epoch 2.39: Loss = 0.755447
Epoch 2.40: Loss = 0.692856
Epoch 2.41: Loss = 0.740112
Epoch 2.42: Loss = 0.698685
Epoch 2.43: Loss = 0.693695
Epoch 2.44: Loss = 0.623642
Epoch 2.45: Loss = 0.723938
Epoch 2.46: Loss = 0.768555
Epoch 2.47: Loss = 0.647186
Epoch 2.48: Loss = 0.646759
Epoch 2.49: Loss = 0.746002
Epoch 2.50: Loss = 0.689346
Epoch 2.51: Loss = 0.602112
Epoch 2.52: Loss = 0.735291
Epoch 2.53: Loss = 0.7854
Epoch 2.54: Loss = 0.58168
Epoch 2.55: Loss = 0.706909
Epoch 2.56: Loss = 0.722382
Epoch 2.57: Loss = 0.748535
Epoch 2.58: Loss = 0.713074
Epoch 2.59: Loss = 0.722733
Epoch 2.60: Loss = 0.736038
Epoch 2.61: Loss = 0.641449
Epoch 2.62: Loss = 0.753143
Epoch 2.63: Loss = 0.600342
Epoch 2.64: Loss = 0.630447
Epoch 2.65: Loss = 0.674561
Epoch 2.66: Loss = 0.666458
Epoch 2.67: Loss = 0.654388
Epoch 2.68: Loss = 0.786591
Epoch 2.69: Loss = 0.7229
Epoch 2.70: Loss = 0.730881
Epoch 2.71: Loss = 0.59613
Epoch 2.72: Loss = 0.683701
Epoch 2.73: Loss = 0.771988
Epoch 2.74: Loss = 0.730942
Epoch 2.75: Loss = 0.635483
Epoch 2.76: Loss = 0.669815
Epoch 2.77: Loss = 0.652725
Epoch 2.78: Loss = 0.698563
Epoch 2.79: Loss = 0.654022
Epoch 2.80: Loss = 0.646545
Epoch 2.81: Loss = 0.65007
Epoch 2.82: Loss = 0.629761
Epoch 2.83: Loss = 0.738251
Epoch 2.84: Loss = 0.639313
Epoch 2.85: Loss = 0.670959
Epoch 2.86: Loss = 0.742813
Epoch 2.87: Loss = 0.706467
Epoch 2.88: Loss = 0.594635
Epoch 2.89: Loss = 0.764771
Epoch 2.90: Loss = 0.685257
Epoch 2.91: Loss = 0.758636
Epoch 2.92: Loss = 0.6978
Epoch 2.93: Loss = 0.722534
Epoch 2.94: Loss = 0.679596
Epoch 2.95: Loss = 0.692261
Epoch 2.96: Loss = 0.651276
Epoch 2.97: Loss = 0.581726
Epoch 2.98: Loss = 0.676285
Epoch 2.99: Loss = 0.674026
Epoch 2.100: Loss = 0.636185
Epoch 2.101: Loss = 0.742889
Epoch 2.102: Loss = 0.694275
Epoch 2.103: Loss = 0.676544
Epoch 2.104: Loss = 0.632339
Epoch 2.105: Loss = 0.62294
Epoch 2.106: Loss = 0.760117
Epoch 2.107: Loss = 0.726166
Epoch 2.108: Loss = 0.711594
Epoch 2.109: Loss = 0.693512
Epoch 2.110: Loss = 0.692627
Epoch 2.111: Loss = 0.613602
Epoch 2.112: Loss = 0.634003
Epoch 2.113: Loss = 0.676361
Epoch 2.114: Loss = 0.642883
Epoch 2.115: Loss = 0.675797
Epoch 2.116: Loss = 0.611923
Epoch 2.117: Loss = 0.72998
Epoch 2.118: Loss = 0.596039
Epoch 2.119: Loss = 0.675629
Epoch 2.120: Loss = 0.62056
TRAIN LOSS = 0.694427
TRAIN ACC = 76.6327 % (45982/60000)
Loss = 0.614441
Loss = 0.748611
Loss = 0.68367
Loss = 0.590408
Loss = 0.619263
Loss = 0.818527
Loss = 0.811417
Loss = 0.751099
Loss = 0.691147
Loss = 0.621948
Loss = 0.766022
Loss = 0.762833
Loss = 0.708786
Loss = 0.704971
Loss = 0.699097
Loss = 0.748871
Loss = 0.652939
Loss = 0.730026
Loss = 0.78157
Loss = 0.705566
TEST LOSS = 0.71056
TEST ACC = 459.819 % (7634/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.684814
Epoch 3.2: Loss = 0.672546
Epoch 3.3: Loss = 0.741562
Epoch 3.4: Loss = 0.600403
Epoch 3.5: Loss = 0.653061
Epoch 3.6: Loss = 0.755432
Epoch 3.7: Loss = 0.671097
Epoch 3.8: Loss = 0.765259
Epoch 3.9: Loss = 0.56543
Epoch 3.10: Loss = 0.48587
Epoch 3.11: Loss = 0.768555
Epoch 3.12: Loss = 0.690979
Epoch 3.13: Loss = 0.691925
Epoch 3.14: Loss = 0.656921
Epoch 3.15: Loss = 0.654053
Epoch 3.16: Loss = 0.724014
Epoch 3.17: Loss = 0.620605
Epoch 3.18: Loss = 0.686783
Epoch 3.19: Loss = 0.654648
Epoch 3.20: Loss = 0.756058
Epoch 3.21: Loss = 0.590439
Epoch 3.22: Loss = 0.540848
Epoch 3.23: Loss = 0.69487
Epoch 3.24: Loss = 0.739563
Epoch 3.25: Loss = 0.652496
Epoch 3.26: Loss = 0.577271
Epoch 3.27: Loss = 0.660385
Epoch 3.28: Loss = 0.672058
Epoch 3.29: Loss = 0.67244
Epoch 3.30: Loss = 0.652191
Epoch 3.31: Loss = 0.749771
Epoch 3.32: Loss = 0.638748
Epoch 3.33: Loss = 0.579102
Epoch 3.34: Loss = 0.733597
Epoch 3.35: Loss = 0.714432
Epoch 3.36: Loss = 0.695679
Epoch 3.37: Loss = 0.723923
Epoch 3.38: Loss = 0.666931
Epoch 3.39: Loss = 0.727951
Epoch 3.40: Loss = 0.647476
Epoch 3.41: Loss = 0.720993
Epoch 3.42: Loss = 0.650589
Epoch 3.43: Loss = 0.674789
Epoch 3.44: Loss = 0.57225
Epoch 3.45: Loss = 0.689499
Epoch 3.46: Loss = 0.75856
Epoch 3.47: Loss = 0.610031
Epoch 3.48: Loss = 0.610809
Epoch 3.49: Loss = 0.694412
Epoch 3.50: Loss = 0.656204
Epoch 3.51: Loss = 0.555496
Epoch 3.52: Loss = 0.716476
Epoch 3.53: Loss = 0.765991
Epoch 3.54: Loss = 0.537399
Epoch 3.55: Loss = 0.68576
Epoch 3.56: Loss = 0.689804
Epoch 3.57: Loss = 0.734406
Epoch 3.58: Loss = 0.664612
Epoch 3.59: Loss = 0.724197
Epoch 3.60: Loss = 0.698029
Epoch 3.61: Loss = 0.623489
Epoch 3.62: Loss = 0.722122
Epoch 3.63: Loss = 0.57692
Epoch 3.64: Loss = 0.583954
Epoch 3.65: Loss = 0.670914
Epoch 3.66: Loss = 0.611542
Epoch 3.67: Loss = 0.644821
Epoch 3.68: Loss = 0.783157
Epoch 3.69: Loss = 0.68486
Epoch 3.70: Loss = 0.709412
Epoch 3.71: Loss = 0.55806
Epoch 3.72: Loss = 0.6642
Epoch 3.73: Loss = 0.761688
Epoch 3.74: Loss = 0.701294
Epoch 3.75: Loss = 0.599548
Epoch 3.76: Loss = 0.639923
Epoch 3.77: Loss = 0.619492
Epoch 3.78: Loss = 0.671677
Epoch 3.79: Loss = 0.650742
Epoch 3.80: Loss = 0.616043
Epoch 3.81: Loss = 0.634705
Epoch 3.82: Loss = 0.602768
Epoch 3.83: Loss = 0.710556
Epoch 3.84: Loss = 0.621063
Epoch 3.85: Loss = 0.651627
Epoch 3.86: Loss = 0.725357
Epoch 3.87: Loss = 0.671204
Epoch 3.88: Loss = 0.577209
Epoch 3.89: Loss = 0.756882
Epoch 3.90: Loss = 0.680542
Epoch 3.91: Loss = 0.731415
Epoch 3.92: Loss = 0.67453
Epoch 3.93: Loss = 0.694351
Epoch 3.94: Loss = 0.652267
Epoch 3.95: Loss = 0.658859
Epoch 3.96: Loss = 0.639633
Epoch 3.97: Loss = 0.558594
Epoch 3.98: Loss = 0.651642
Epoch 3.99: Loss = 0.660477
Epoch 3.100: Loss = 0.620026
Epoch 3.101: Loss = 0.732086
Epoch 3.102: Loss = 0.676712
Epoch 3.103: Loss = 0.652573
Epoch 3.104: Loss = 0.593262
Epoch 3.105: Loss = 0.599915
Epoch 3.106: Loss = 0.728287
Epoch 3.107: Loss = 0.731277
Epoch 3.108: Loss = 0.714005
Epoch 3.109: Loss = 0.698196
Epoch 3.110: Loss = 0.677979
Epoch 3.111: Loss = 0.605194
Epoch 3.112: Loss = 0.631027
Epoch 3.113: Loss = 0.655182
Epoch 3.114: Loss = 0.642517
Epoch 3.115: Loss = 0.657471
Epoch 3.116: Loss = 0.589722
Epoch 3.117: Loss = 0.708588
Epoch 3.118: Loss = 0.584167
Epoch 3.119: Loss = 0.649033
Epoch 3.120: Loss = 0.608597
TRAIN LOSS = 0.663727
TRAIN ACC = 78.6804 % (47210/60000)
Loss = 0.597504
Loss = 0.75206
Loss = 0.661789
Loss = 0.580093
Loss = 0.609451
Loss = 0.805542
Loss = 0.813324
Loss = 0.734879
Loss = 0.681595
Loss = 0.608109
Loss = 0.775284
Loss = 0.773331
Loss = 0.71077
Loss = 0.699646
Loss = 0.691299
Loss = 0.733749
Loss = 0.648224
Loss = 0.727142
Loss = 0.760025
Loss = 0.685684
TEST LOSS = 0.702475
TEST ACC = 472.099 % (7787/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.662231
Epoch 4.2: Loss = 0.658386
Epoch 4.3: Loss = 0.713425
Epoch 4.4: Loss = 0.577225
Epoch 4.5: Loss = 0.643082
Epoch 4.6: Loss = 0.736664
Epoch 4.7: Loss = 0.649719
Epoch 4.8: Loss = 0.758698
Epoch 4.9: Loss = 0.529816
Epoch 4.10: Loss = 0.457855
Epoch 4.11: Loss = 0.759903
Epoch 4.12: Loss = 0.683655
Epoch 4.13: Loss = 0.69165
Epoch 4.14: Loss = 0.655594
Epoch 4.15: Loss = 0.653641
Epoch 4.16: Loss = 0.71608
Epoch 4.17: Loss = 0.584045
Epoch 4.18: Loss = 0.670685
Epoch 4.19: Loss = 0.631744
Epoch 4.20: Loss = 0.749161
Epoch 4.21: Loss = 0.578659
Epoch 4.22: Loss = 0.504211
Epoch 4.23: Loss = 0.67627
Epoch 4.24: Loss = 0.733521
Epoch 4.25: Loss = 0.6297
Epoch 4.26: Loss = 0.554932
Epoch 4.27: Loss = 0.651489
Epoch 4.28: Loss = 0.666107
Epoch 4.29: Loss = 0.644806
Epoch 4.30: Loss = 0.64209
Epoch 4.31: Loss = 0.73909
Epoch 4.32: Loss = 0.629715
Epoch 4.33: Loss = 0.565033
Epoch 4.34: Loss = 0.727493
Epoch 4.35: Loss = 0.701431
Epoch 4.36: Loss = 0.689194
Epoch 4.37: Loss = 0.714905
Epoch 4.38: Loss = 0.66806
Epoch 4.39: Loss = 0.720734
Epoch 4.40: Loss = 0.628128
Epoch 4.41: Loss = 0.700638
Epoch 4.42: Loss = 0.641327
Epoch 4.43: Loss = 0.670349
Epoch 4.44: Loss = 0.560043
Epoch 4.45: Loss = 0.690674
Epoch 4.46: Loss = 0.752029
Epoch 4.47: Loss = 0.606995
Epoch 4.48: Loss = 0.595413
Epoch 4.49: Loss = 0.67511
Epoch 4.50: Loss = 0.639374
Epoch 4.51: Loss = 0.533615
Epoch 4.52: Loss = 0.711716
Epoch 4.53: Loss = 0.759018
Epoch 4.54: Loss = 0.51738
Epoch 4.55: Loss = 0.66304
Epoch 4.56: Loss = 0.6707
Epoch 4.57: Loss = 0.716629
Epoch 4.58: Loss = 0.643875
Epoch 4.59: Loss = 0.716705
Epoch 4.60: Loss = 0.681046
Epoch 4.61: Loss = 0.600266
Epoch 4.62: Loss = 0.695358
Epoch 4.63: Loss = 0.567291
Epoch 4.64: Loss = 0.552979
Epoch 4.65: Loss = 0.657013
Epoch 4.66: Loss = 0.586792
Epoch 4.67: Loss = 0.637238
Epoch 4.68: Loss = 0.786682
Epoch 4.69: Loss = 0.662247
Epoch 4.70: Loss = 0.678894
Epoch 4.71: Loss = 0.542252
Epoch 4.72: Loss = 0.646667
Epoch 4.73: Loss = 0.735809
Epoch 4.74: Loss = 0.678574
Epoch 4.75: Loss = 0.584259
Epoch 4.76: Loss = 0.634552
Epoch 4.77: Loss = 0.603882
Epoch 4.78: Loss = 0.666931
Epoch 4.79: Loss = 0.635452
Epoch 4.80: Loss = 0.601639
Epoch 4.81: Loss = 0.629807
Epoch 4.82: Loss = 0.598236
Epoch 4.83: Loss = 0.697647
Epoch 4.84: Loss = 0.61055
Epoch 4.85: Loss = 0.639343
Epoch 4.86: Loss = 0.708618
Epoch 4.87: Loss = 0.643677
Epoch 4.88: Loss = 0.571045
Epoch 4.89: Loss = 0.753952
Epoch 4.90: Loss = 0.677948
Epoch 4.91: Loss = 0.7211
Epoch 4.92: Loss = 0.670731
Epoch 4.93: Loss = 0.67955
Epoch 4.94: Loss = 0.646133
Epoch 4.95: Loss = 0.653671
Epoch 4.96: Loss = 0.616837
Epoch 4.97: Loss = 0.558517
Epoch 4.98: Loss = 0.628296
Epoch 4.99: Loss = 0.629639
Epoch 4.100: Loss = 0.611969
Epoch 4.101: Loss = 0.71727
Epoch 4.102: Loss = 0.686295
Epoch 4.103: Loss = 0.632202
Epoch 4.104: Loss = 0.575089
Epoch 4.105: Loss = 0.592621
Epoch 4.106: Loss = 0.709518
Epoch 4.107: Loss = 0.737488
Epoch 4.108: Loss = 0.719009
Epoch 4.109: Loss = 0.692612
Epoch 4.110: Loss = 0.663788
Epoch 4.111: Loss = 0.605865
Epoch 4.112: Loss = 0.620026
Epoch 4.113: Loss = 0.633621
Epoch 4.114: Loss = 0.641693
Epoch 4.115: Loss = 0.633926
Epoch 4.116: Loss = 0.592957
Epoch 4.117: Loss = 0.697861
Epoch 4.118: Loss = 0.575378
Epoch 4.119: Loss = 0.621841
Epoch 4.120: Loss = 0.602585
TRAIN LOSS = 0.650406
TRAIN ACC = 79.8096 % (47888/60000)
Loss = 0.592163
Loss = 0.724747
Loss = 0.654434
Loss = 0.57106
Loss = 0.607208
Loss = 0.78363
Loss = 0.805389
Loss = 0.725418
Loss = 0.665573
Loss = 0.596176
Loss = 0.788391
Loss = 0.784317
Loss = 0.700684
Loss = 0.693848
Loss = 0.688797
Loss = 0.714966
Loss = 0.655426
Loss = 0.711166
Loss = 0.747818
Loss = 0.672668
TEST LOSS = 0.694194
TEST ACC = 478.879 % (7904/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.638306
Epoch 5.2: Loss = 0.65654
Epoch 5.3: Loss = 0.696518
Epoch 5.4: Loss = 0.562622
Epoch 5.5: Loss = 0.646255
Epoch 5.6: Loss = 0.734604
Epoch 5.7: Loss = 0.635681
Epoch 5.8: Loss = 0.772095
Epoch 5.9: Loss = 0.527512
Epoch 5.10: Loss = 0.435837
Epoch 5.11: Loss = 0.758728
Epoch 5.12: Loss = 0.67569
Epoch 5.13: Loss = 0.680756
Epoch 5.14: Loss = 0.647644
Epoch 5.15: Loss = 0.642136
Epoch 5.16: Loss = 0.708496
Epoch 5.17: Loss = 0.572433
Epoch 5.18: Loss = 0.663391
Epoch 5.19: Loss = 0.61496
Epoch 5.20: Loss = 0.750549
Epoch 5.21: Loss = 0.572876
Epoch 5.22: Loss = 0.489517
Epoch 5.23: Loss = 0.638428
Epoch 5.24: Loss = 0.733063
Epoch 5.25: Loss = 0.629196
Epoch 5.26: Loss = 0.544754
Epoch 5.27: Loss = 0.638138
Epoch 5.28: Loss = 0.656265
Epoch 5.29: Loss = 0.648834
Epoch 5.30: Loss = 0.648849
Epoch 5.31: Loss = 0.735138
Epoch 5.32: Loss = 0.617218
Epoch 5.33: Loss = 0.567505
Epoch 5.34: Loss = 0.715424
Epoch 5.35: Loss = 0.690109
Epoch 5.36: Loss = 0.680466
Epoch 5.37: Loss = 0.712128
Epoch 5.38: Loss = 0.667709
Epoch 5.39: Loss = 0.707428
Epoch 5.40: Loss = 0.62355
Epoch 5.41: Loss = 0.700317
Epoch 5.42: Loss = 0.623215
Epoch 5.43: Loss = 0.649048
Epoch 5.44: Loss = 0.537109
Epoch 5.45: Loss = 0.690048
Epoch 5.46: Loss = 0.75972
Epoch 5.47: Loss = 0.600723
Epoch 5.48: Loss = 0.579819
Epoch 5.49: Loss = 0.674301
Epoch 5.50: Loss = 0.628525
Epoch 5.51: Loss = 0.520844
Epoch 5.52: Loss = 0.692856
Epoch 5.53: Loss = 0.731812
Epoch 5.54: Loss = 0.507492
Epoch 5.55: Loss = 0.655838
Epoch 5.56: Loss = 0.663559
Epoch 5.57: Loss = 0.717377
Epoch 5.58: Loss = 0.635544
Epoch 5.59: Loss = 0.726379
Epoch 5.60: Loss = 0.684601
Epoch 5.61: Loss = 0.587753
Epoch 5.62: Loss = 0.697479
Epoch 5.63: Loss = 0.553909
Epoch 5.64: Loss = 0.551239
Epoch 5.65: Loss = 0.662445
Epoch 5.66: Loss = 0.564407
Epoch 5.67: Loss = 0.641495
Epoch 5.68: Loss = 0.806732
Epoch 5.69: Loss = 0.651794
Epoch 5.70: Loss = 0.672516
Epoch 5.71: Loss = 0.536667
Epoch 5.72: Loss = 0.643341
Epoch 5.73: Loss = 0.736649
Epoch 5.74: Loss = 0.679184
Epoch 5.75: Loss = 0.584213
Epoch 5.76: Loss = 0.636749
Epoch 5.77: Loss = 0.60556
Epoch 5.78: Loss = 0.674545
Epoch 5.79: Loss = 0.628983
Epoch 5.80: Loss = 0.593765
Epoch 5.81: Loss = 0.618881
Epoch 5.82: Loss = 0.596497
Epoch 5.83: Loss = 0.689407
Epoch 5.84: Loss = 0.59581
Epoch 5.85: Loss = 0.638168
Epoch 5.86: Loss = 0.722794
Epoch 5.87: Loss = 0.632248
Epoch 5.88: Loss = 0.584763
Epoch 5.89: Loss = 0.76178
Epoch 5.90: Loss = 0.668137
Epoch 5.91: Loss = 0.705215
Epoch 5.92: Loss = 0.665268
Epoch 5.93: Loss = 0.679916
Epoch 5.94: Loss = 0.674744
Epoch 5.95: Loss = 0.66275
Epoch 5.96: Loss = 0.608475
Epoch 5.97: Loss = 0.544632
Epoch 5.98: Loss = 0.627808
Epoch 5.99: Loss = 0.638336
Epoch 5.100: Loss = 0.61998
Epoch 5.101: Loss = 0.699249
Epoch 5.102: Loss = 0.68486
Epoch 5.103: Loss = 0.623749
Epoch 5.104: Loss = 0.565262
Epoch 5.105: Loss = 0.586212
Epoch 5.106: Loss = 0.701263
Epoch 5.107: Loss = 0.712036
Epoch 5.108: Loss = 0.745041
Epoch 5.109: Loss = 0.697006
Epoch 5.110: Loss = 0.660599
Epoch 5.111: Loss = 0.610565
Epoch 5.112: Loss = 0.621063
Epoch 5.113: Loss = 0.636063
Epoch 5.114: Loss = 0.646637
Epoch 5.115: Loss = 0.632843
Epoch 5.116: Loss = 0.582275
Epoch 5.117: Loss = 0.68895
Epoch 5.118: Loss = 0.552826
Epoch 5.119: Loss = 0.620453
Epoch 5.120: Loss = 0.587631
TRAIN LOSS = 0.645142
TRAIN ACC = 80.4596 % (48278/60000)
Loss = 0.564804
Loss = 0.704254
Loss = 0.639038
Loss = 0.554916
Loss = 0.601349
Loss = 0.778198
Loss = 0.801575
Loss = 0.710129
Loss = 0.643387
Loss = 0.581802
Loss = 0.785095
Loss = 0.778625
Loss = 0.696167
Loss = 0.687393
Loss = 0.674545
Loss = 0.696808
Loss = 0.639481
Loss = 0.707565
Loss = 0.713287
Loss = 0.661072
TEST LOSS = 0.680974
TEST ACC = 482.779 % (7982/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.611725
Epoch 6.2: Loss = 0.656387
Epoch 6.3: Loss = 0.688675
Epoch 6.4: Loss = 0.565445
Epoch 6.5: Loss = 0.641006
Epoch 6.6: Loss = 0.723114
Epoch 6.7: Loss = 0.630219
Epoch 6.8: Loss = 0.744507
Epoch 6.9: Loss = 0.509247
Epoch 6.10: Loss = 0.425629
Epoch 6.11: Loss = 0.740891
Epoch 6.12: Loss = 0.657333
Epoch 6.13: Loss = 0.678406
Epoch 6.14: Loss = 0.641769
Epoch 6.15: Loss = 0.652573
Epoch 6.16: Loss = 0.705902
Epoch 6.17: Loss = 0.553696
Epoch 6.18: Loss = 0.648315
Epoch 6.19: Loss = 0.600983
Epoch 6.20: Loss = 0.746719
Epoch 6.21: Loss = 0.559113
Epoch 6.22: Loss = 0.467117
Epoch 6.23: Loss = 0.646835
Epoch 6.24: Loss = 0.734848
Epoch 6.25: Loss = 0.617645
Epoch 6.26: Loss = 0.534409
Epoch 6.27: Loss = 0.635712
Epoch 6.28: Loss = 0.655075
Epoch 6.29: Loss = 0.648697
Epoch 6.30: Loss = 0.636368
Epoch 6.31: Loss = 0.728027
Epoch 6.32: Loss = 0.607361
Epoch 6.33: Loss = 0.556717
Epoch 6.34: Loss = 0.698639
Epoch 6.35: Loss = 0.682999
Epoch 6.36: Loss = 0.671234
Epoch 6.37: Loss = 0.696747
Epoch 6.38: Loss = 0.659805
Epoch 6.39: Loss = 0.709534
Epoch 6.40: Loss = 0.617813
Epoch 6.41: Loss = 0.696701
Epoch 6.42: Loss = 0.629318
Epoch 6.43: Loss = 0.649261
Epoch 6.44: Loss = 0.516541
Epoch 6.45: Loss = 0.701523
Epoch 6.46: Loss = 0.764526
Epoch 6.47: Loss = 0.596054
Epoch 6.48: Loss = 0.589691
Epoch 6.49: Loss = 0.648315
Epoch 6.50: Loss = 0.62973
Epoch 6.51: Loss = 0.51506
Epoch 6.52: Loss = 0.68306
Epoch 6.53: Loss = 0.73703
Epoch 6.54: Loss = 0.492889
Epoch 6.55: Loss = 0.635315
Epoch 6.56: Loss = 0.65567
Epoch 6.57: Loss = 0.70845
Epoch 6.58: Loss = 0.624207
Epoch 6.59: Loss = 0.737579
Epoch 6.60: Loss = 0.658463
Epoch 6.61: Loss = 0.590317
Epoch 6.62: Loss = 0.698334
Epoch 6.63: Loss = 0.548203
Epoch 6.64: Loss = 0.547516
Epoch 6.65: Loss = 0.670486
Epoch 6.66: Loss = 0.54248
Epoch 6.67: Loss = 0.633835
Epoch 6.68: Loss = 0.811813
Epoch 6.69: Loss = 0.628235
Epoch 6.70: Loss = 0.665512
Epoch 6.71: Loss = 0.534927
Epoch 6.72: Loss = 0.637024
Epoch 6.73: Loss = 0.706711
Epoch 6.74: Loss = 0.666595
Epoch 6.75: Loss = 0.579498
Epoch 6.76: Loss = 0.624039
Epoch 6.77: Loss = 0.605911
Epoch 6.78: Loss = 0.655838
Epoch 6.79: Loss = 0.608627
Epoch 6.80: Loss = 0.588226
Epoch 6.81: Loss = 0.618988
Epoch 6.82: Loss = 0.609085
Epoch 6.83: Loss = 0.686874
Epoch 6.84: Loss = 0.580582
Epoch 6.85: Loss = 0.625931
Epoch 6.86: Loss = 0.710144
Epoch 6.87: Loss = 0.629013
Epoch 6.88: Loss = 0.566483
Epoch 6.89: Loss = 0.75119
Epoch 6.90: Loss = 0.661179
Epoch 6.91: Loss = 0.699142
Epoch 6.92: Loss = 0.659805
Epoch 6.93: Loss = 0.648972
Epoch 6.94: Loss = 0.661133
Epoch 6.95: Loss = 0.653122
Epoch 6.96: Loss = 0.608231
Epoch 6.97: Loss = 0.539047
Epoch 6.98: Loss = 0.623352
Epoch 6.99: Loss = 0.615051
Epoch 6.100: Loss = 0.618546
Epoch 6.101: Loss = 0.689758
Epoch 6.102: Loss = 0.673706
Epoch 6.103: Loss = 0.627853
Epoch 6.104: Loss = 0.569107
Epoch 6.105: Loss = 0.583557
Epoch 6.106: Loss = 0.709229
Epoch 6.107: Loss = 0.704834
Epoch 6.108: Loss = 0.745773
Epoch 6.109: Loss = 0.699783
Epoch 6.110: Loss = 0.664429
Epoch 6.111: Loss = 0.598236
Epoch 6.112: Loss = 0.626785
Epoch 6.113: Loss = 0.633026
Epoch 6.114: Loss = 0.652725
Epoch 6.115: Loss = 0.621399
Epoch 6.116: Loss = 0.566269
Epoch 6.117: Loss = 0.678864
Epoch 6.118: Loss = 0.563477
Epoch 6.119: Loss = 0.62175
Epoch 6.120: Loss = 0.57547
TRAIN LOSS = 0.638123
TRAIN ACC = 80.9631 % (48580/60000)
Loss = 0.567215
Loss = 0.702606
Loss = 0.630768
Loss = 0.547516
Loss = 0.604813
Loss = 0.764908
Loss = 0.807922
Loss = 0.700165
Loss = 0.654617
Loss = 0.585327
Loss = 0.80986
Loss = 0.800156
Loss = 0.710373
Loss = 0.690872
Loss = 0.677139
Loss = 0.691544
Loss = 0.629456
Loss = 0.718292
Loss = 0.707001
Loss = 0.659317
TEST LOSS = 0.682993
TEST ACC = 485.799 % (8029/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.603088
Epoch 7.2: Loss = 0.663086
Epoch 7.3: Loss = 0.689606
Epoch 7.4: Loss = 0.548782
Epoch 7.5: Loss = 0.658661
Epoch 7.6: Loss = 0.718781
Epoch 7.7: Loss = 0.618286
Epoch 7.8: Loss = 0.753799
Epoch 7.9: Loss = 0.522659
Epoch 7.10: Loss = 0.41626
Epoch 7.11: Loss = 0.742645
Epoch 7.12: Loss = 0.673843
Epoch 7.13: Loss = 0.672546
Epoch 7.14: Loss = 0.643433
Epoch 7.15: Loss = 0.661758
Epoch 7.16: Loss = 0.711227
Epoch 7.17: Loss = 0.547501
Epoch 7.18: Loss = 0.655594
Epoch 7.19: Loss = 0.593491
Epoch 7.20: Loss = 0.746765
Epoch 7.21: Loss = 0.550018
Epoch 7.22: Loss = 0.4758
Epoch 7.23: Loss = 0.637207
Epoch 7.24: Loss = 0.727676
Epoch 7.25: Loss = 0.622787
Epoch 7.26: Loss = 0.518631
Epoch 7.27: Loss = 0.621536
Epoch 7.28: Loss = 0.653427
Epoch 7.29: Loss = 0.630829
Epoch 7.30: Loss = 0.628754
Epoch 7.31: Loss = 0.710815
Epoch 7.32: Loss = 0.607788
Epoch 7.33: Loss = 0.565781
Epoch 7.34: Loss = 0.711334
Epoch 7.35: Loss = 0.680573
Epoch 7.36: Loss = 0.657028
Epoch 7.37: Loss = 0.708099
Epoch 7.38: Loss = 0.642578
Epoch 7.39: Loss = 0.705841
Epoch 7.40: Loss = 0.618118
Epoch 7.41: Loss = 0.678986
Epoch 7.42: Loss = 0.628342
Epoch 7.43: Loss = 0.652466
Epoch 7.44: Loss = 0.512421
Epoch 7.45: Loss = 0.695496
Epoch 7.46: Loss = 0.774368
Epoch 7.47: Loss = 0.59845
Epoch 7.48: Loss = 0.573669
Epoch 7.49: Loss = 0.630692
Epoch 7.50: Loss = 0.634842
Epoch 7.51: Loss = 0.498154
Epoch 7.52: Loss = 0.663864
Epoch 7.53: Loss = 0.734924
Epoch 7.54: Loss = 0.489471
Epoch 7.55: Loss = 0.624008
Epoch 7.56: Loss = 0.639557
Epoch 7.57: Loss = 0.712601
Epoch 7.58: Loss = 0.610992
Epoch 7.59: Loss = 0.734619
Epoch 7.60: Loss = 0.684814
Epoch 7.61: Loss = 0.588028
Epoch 7.62: Loss = 0.691956
Epoch 7.63: Loss = 0.543716
Epoch 7.64: Loss = 0.538727
Epoch 7.65: Loss = 0.674789
Epoch 7.66: Loss = 0.55307
Epoch 7.67: Loss = 0.617752
Epoch 7.68: Loss = 0.815002
Epoch 7.69: Loss = 0.619766
Epoch 7.70: Loss = 0.639984
Epoch 7.71: Loss = 0.52623
Epoch 7.72: Loss = 0.640381
Epoch 7.73: Loss = 0.710144
Epoch 7.74: Loss = 0.673386
Epoch 7.75: Loss = 0.579788
Epoch 7.76: Loss = 0.608978
Epoch 7.77: Loss = 0.601318
Epoch 7.78: Loss = 0.655869
Epoch 7.79: Loss = 0.623672
Epoch 7.80: Loss = 0.593948
Epoch 7.81: Loss = 0.640182
Epoch 7.82: Loss = 0.621826
Epoch 7.83: Loss = 0.694336
Epoch 7.84: Loss = 0.576599
Epoch 7.85: Loss = 0.622299
Epoch 7.86: Loss = 0.698929
Epoch 7.87: Loss = 0.618896
Epoch 7.88: Loss = 0.563171
Epoch 7.89: Loss = 0.741684
Epoch 7.90: Loss = 0.65538
Epoch 7.91: Loss = 0.677063
Epoch 7.92: Loss = 0.660965
Epoch 7.93: Loss = 0.646439
Epoch 7.94: Loss = 0.661621
Epoch 7.95: Loss = 0.660233
Epoch 7.96: Loss = 0.612289
Epoch 7.97: Loss = 0.537399
Epoch 7.98: Loss = 0.629852
Epoch 7.99: Loss = 0.624771
Epoch 7.100: Loss = 0.613937
Epoch 7.101: Loss = 0.680969
Epoch 7.102: Loss = 0.684357
Epoch 7.103: Loss = 0.625763
Epoch 7.104: Loss = 0.566956
Epoch 7.105: Loss = 0.584305
Epoch 7.106: Loss = 0.698135
Epoch 7.107: Loss = 0.694855
Epoch 7.108: Loss = 0.760849
Epoch 7.109: Loss = 0.690674
Epoch 7.110: Loss = 0.656204
Epoch 7.111: Loss = 0.601349
Epoch 7.112: Loss = 0.619003
Epoch 7.113: Loss = 0.640045
Epoch 7.114: Loss = 0.648514
Epoch 7.115: Loss = 0.616455
Epoch 7.116: Loss = 0.569061
Epoch 7.117: Loss = 0.67334
Epoch 7.118: Loss = 0.562439
Epoch 7.119: Loss = 0.624542
Epoch 7.120: Loss = 0.570435
TRAIN LOSS = 0.635941
TRAIN ACC = 81.4148 % (48851/60000)
Loss = 0.568375
Loss = 0.683411
Loss = 0.623566
Loss = 0.534622
Loss = 0.594513
Loss = 0.728668
Loss = 0.783127
Loss = 0.677002
Loss = 0.619598
Loss = 0.580627
Loss = 0.797516
Loss = 0.783218
Loss = 0.686554
Loss = 0.66861
Loss = 0.652893
Loss = 0.673157
Loss = 0.625366
Loss = 0.691254
Loss = 0.675125
Loss = 0.643768
TEST LOSS = 0.664548
TEST ACC = 488.509 % (8053/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.570953
Epoch 8.2: Loss = 0.658905
Epoch 8.3: Loss = 0.6866
Epoch 8.4: Loss = 0.54776
Epoch 8.5: Loss = 0.648666
Epoch 8.6: Loss = 0.711594
Epoch 8.7: Loss = 0.603455
Epoch 8.8: Loss = 0.74437
Epoch 8.9: Loss = 0.521362
Epoch 8.10: Loss = 0.40506
Epoch 8.11: Loss = 0.726868
Epoch 8.12: Loss = 0.655533
Epoch 8.13: Loss = 0.671783
Epoch 8.14: Loss = 0.63678
Epoch 8.15: Loss = 0.641739
Epoch 8.16: Loss = 0.700638
Epoch 8.17: Loss = 0.533539
Epoch 8.18: Loss = 0.638916
Epoch 8.19: Loss = 0.588547
Epoch 8.20: Loss = 0.738632
Epoch 8.21: Loss = 0.557281
Epoch 8.22: Loss = 0.476486
Epoch 8.23: Loss = 0.624435
Epoch 8.24: Loss = 0.715042
Epoch 8.25: Loss = 0.631577
Epoch 8.26: Loss = 0.514175
Epoch 8.27: Loss = 0.615067
Epoch 8.28: Loss = 0.650528
Epoch 8.29: Loss = 0.633667
Epoch 8.30: Loss = 0.639786
Epoch 8.31: Loss = 0.697968
Epoch 8.32: Loss = 0.590652
Epoch 8.33: Loss = 0.56781
Epoch 8.34: Loss = 0.708817
Epoch 8.35: Loss = 0.671631
Epoch 8.36: Loss = 0.664169
Epoch 8.37: Loss = 0.69101
Epoch 8.38: Loss = 0.657883
Epoch 8.39: Loss = 0.689178
Epoch 8.40: Loss = 0.610077
Epoch 8.41: Loss = 0.66481
Epoch 8.42: Loss = 0.631302
Epoch 8.43: Loss = 0.650146
Epoch 8.44: Loss = 0.497375
Epoch 8.45: Loss = 0.682541
Epoch 8.46: Loss = 0.767242
Epoch 8.47: Loss = 0.59816
Epoch 8.48: Loss = 0.571991
Epoch 8.49: Loss = 0.644028
Epoch 8.50: Loss = 0.619247
Epoch 8.51: Loss = 0.495636
Epoch 8.52: Loss = 0.651184
Epoch 8.53: Loss = 0.710571
Epoch 8.54: Loss = 0.488876
Epoch 8.55: Loss = 0.625397
Epoch 8.56: Loss = 0.623535
Epoch 8.57: Loss = 0.70845
Epoch 8.58: Loss = 0.592621
Epoch 8.59: Loss = 0.724472
Epoch 8.60: Loss = 0.687042
Epoch 8.61: Loss = 0.572418
Epoch 8.62: Loss = 0.68692
Epoch 8.63: Loss = 0.547455
Epoch 8.64: Loss = 0.529984
Epoch 8.65: Loss = 0.664063
Epoch 8.66: Loss = 0.535858
Epoch 8.67: Loss = 0.592926
Epoch 8.68: Loss = 0.802063
Epoch 8.69: Loss = 0.610931
Epoch 8.70: Loss = 0.626114
Epoch 8.71: Loss = 0.517212
Epoch 8.72: Loss = 0.644196
Epoch 8.73: Loss = 0.700668
Epoch 8.74: Loss = 0.664566
Epoch 8.75: Loss = 0.557205
Epoch 8.76: Loss = 0.591629
Epoch 8.77: Loss = 0.593658
Epoch 8.78: Loss = 0.632553
Epoch 8.79: Loss = 0.607132
Epoch 8.80: Loss = 0.58728
Epoch 8.81: Loss = 0.616013
Epoch 8.82: Loss = 0.603973
Epoch 8.83: Loss = 0.677307
Epoch 8.84: Loss = 0.57518
Epoch 8.85: Loss = 0.611511
Epoch 8.86: Loss = 0.692123
Epoch 8.87: Loss = 0.601456
Epoch 8.88: Loss = 0.571411
Epoch 8.89: Loss = 0.731308
Epoch 8.90: Loss = 0.658646
Epoch 8.91: Loss = 0.671921
Epoch 8.92: Loss = 0.661789
Epoch 8.93: Loss = 0.630722
Epoch 8.94: Loss = 0.649765
Epoch 8.95: Loss = 0.652084
Epoch 8.96: Loss = 0.61351
Epoch 8.97: Loss = 0.523132
Epoch 8.98: Loss = 0.631927
Epoch 8.99: Loss = 0.609879
Epoch 8.100: Loss = 0.610001
Epoch 8.101: Loss = 0.656555
Epoch 8.102: Loss = 0.665237
Epoch 8.103: Loss = 0.599487
Epoch 8.104: Loss = 0.556915
Epoch 8.105: Loss = 0.568619
Epoch 8.106: Loss = 0.684326
Epoch 8.107: Loss = 0.692001
Epoch 8.108: Loss = 0.757507
Epoch 8.109: Loss = 0.688919
Epoch 8.110: Loss = 0.654617
Epoch 8.111: Loss = 0.591156
Epoch 8.112: Loss = 0.609848
Epoch 8.113: Loss = 0.633987
Epoch 8.114: Loss = 0.666489
Epoch 8.115: Loss = 0.607285
Epoch 8.116: Loss = 0.55368
Epoch 8.117: Loss = 0.666473
Epoch 8.118: Loss = 0.562225
Epoch 8.119: Loss = 0.625717
Epoch 8.120: Loss = 0.577316
TRAIN LOSS = 0.62793
TRAIN ACC = 81.7276 % (49039/60000)
Loss = 0.559692
Loss = 0.692871
Loss = 0.632919
Loss = 0.538971
Loss = 0.60408
Loss = 0.745224
Loss = 0.802094
Loss = 0.693527
Loss = 0.624969
Loss = 0.57373
Loss = 0.827545
Loss = 0.794418
Loss = 0.704132
Loss = 0.685516
Loss = 0.684097
Loss = 0.681152
Loss = 0.64418
Loss = 0.705338
Loss = 0.70816
Loss = 0.668777
TEST LOSS = 0.67857
TEST ACC = 490.388 % (8075/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.568985
Epoch 9.2: Loss = 0.632294
Epoch 9.3: Loss = 0.685791
Epoch 9.4: Loss = 0.544357
Epoch 9.5: Loss = 0.661346
Epoch 9.6: Loss = 0.699982
Epoch 9.7: Loss = 0.588562
Epoch 9.8: Loss = 0.742111
Epoch 9.9: Loss = 0.500214
Epoch 9.10: Loss = 0.405685
Epoch 9.11: Loss = 0.731262
Epoch 9.12: Loss = 0.641708
Epoch 9.13: Loss = 0.660355
Epoch 9.14: Loss = 0.640457
Epoch 9.15: Loss = 0.654114
Epoch 9.16: Loss = 0.702209
Epoch 9.17: Loss = 0.531631
Epoch 9.18: Loss = 0.631836
Epoch 9.19: Loss = 0.588898
Epoch 9.20: Loss = 0.739716
Epoch 9.21: Loss = 0.547134
Epoch 9.22: Loss = 0.467972
Epoch 9.23: Loss = 0.624008
Epoch 9.24: Loss = 0.724426
Epoch 9.25: Loss = 0.634827
Epoch 9.26: Loss = 0.511414
Epoch 9.27: Loss = 0.611755
Epoch 9.28: Loss = 0.644135
Epoch 9.29: Loss = 0.63385
Epoch 9.30: Loss = 0.619949
Epoch 9.31: Loss = 0.697037
Epoch 9.32: Loss = 0.593094
Epoch 9.33: Loss = 0.572357
Epoch 9.34: Loss = 0.700043
Epoch 9.35: Loss = 0.658356
Epoch 9.36: Loss = 0.650314
Epoch 9.37: Loss = 0.700958
Epoch 9.38: Loss = 0.653885
Epoch 9.39: Loss = 0.685333
Epoch 9.40: Loss = 0.604004
Epoch 9.41: Loss = 0.660461
Epoch 9.42: Loss = 0.646301
Epoch 9.43: Loss = 0.659897
Epoch 9.44: Loss = 0.488617
Epoch 9.45: Loss = 0.685379
Epoch 9.46: Loss = 0.762314
Epoch 9.47: Loss = 0.577576
Epoch 9.48: Loss = 0.56749
Epoch 9.49: Loss = 0.648926
Epoch 9.50: Loss = 0.615723
Epoch 9.51: Loss = 0.502716
Epoch 9.52: Loss = 0.642105
Epoch 9.53: Loss = 0.703659
Epoch 9.54: Loss = 0.480789
Epoch 9.55: Loss = 0.634415
Epoch 9.56: Loss = 0.621399
Epoch 9.57: Loss = 0.697433
Epoch 9.58: Loss = 0.589203
Epoch 9.59: Loss = 0.72641
Epoch 9.60: Loss = 0.672043
Epoch 9.61: Loss = 0.57431
Epoch 9.62: Loss = 0.689346
Epoch 9.63: Loss = 0.545563
Epoch 9.64: Loss = 0.510727
Epoch 9.65: Loss = 0.665009
Epoch 9.66: Loss = 0.53093
Epoch 9.67: Loss = 0.58316
Epoch 9.68: Loss = 0.824097
Epoch 9.69: Loss = 0.605881
Epoch 9.70: Loss = 0.615128
Epoch 9.71: Loss = 0.520813
Epoch 9.72: Loss = 0.642548
Epoch 9.73: Loss = 0.698654
Epoch 9.74: Loss = 0.648453
Epoch 9.75: Loss = 0.567245
Epoch 9.76: Loss = 0.613266
Epoch 9.77: Loss = 0.600433
Epoch 9.78: Loss = 0.641205
Epoch 9.79: Loss = 0.600082
Epoch 9.80: Loss = 0.590622
Epoch 9.81: Loss = 0.616119
Epoch 9.82: Loss = 0.608902
Epoch 9.83: Loss = 0.65625
Epoch 9.84: Loss = 0.56897
Epoch 9.85: Loss = 0.599915
Epoch 9.86: Loss = 0.687393
Epoch 9.87: Loss = 0.598877
Epoch 9.88: Loss = 0.572952
Epoch 9.89: Loss = 0.717255
Epoch 9.90: Loss = 0.658676
Epoch 9.91: Loss = 0.655731
Epoch 9.92: Loss = 0.654739
Epoch 9.93: Loss = 0.623993
Epoch 9.94: Loss = 0.651611
Epoch 9.95: Loss = 0.646729
Epoch 9.96: Loss = 0.609726
Epoch 9.97: Loss = 0.51004
Epoch 9.98: Loss = 0.638306
Epoch 9.99: Loss = 0.621399
Epoch 9.100: Loss = 0.62059
Epoch 9.101: Loss = 0.653122
Epoch 9.102: Loss = 0.658508
Epoch 9.103: Loss = 0.593887
Epoch 9.104: Loss = 0.552032
Epoch 9.105: Loss = 0.58049
Epoch 9.106: Loss = 0.691269
Epoch 9.107: Loss = 0.686584
Epoch 9.108: Loss = 0.785446
Epoch 9.109: Loss = 0.686661
Epoch 9.110: Loss = 0.6651
Epoch 9.111: Loss = 0.604797
Epoch 9.112: Loss = 0.608215
Epoch 9.113: Loss = 0.619675
Epoch 9.114: Loss = 0.659409
Epoch 9.115: Loss = 0.586273
Epoch 9.116: Loss = 0.547394
Epoch 9.117: Loss = 0.68486
Epoch 9.118: Loss = 0.566681
Epoch 9.119: Loss = 0.585266
Epoch 9.120: Loss = 0.588318
TRAIN LOSS = 0.625504
TRAIN ACC = 82.0572 % (49237/60000)
Loss = 0.560043
Loss = 0.685303
Loss = 0.624893
Loss = 0.549759
Loss = 0.608551
Loss = 0.739365
Loss = 0.792526
Loss = 0.67244
Loss = 0.634125
Loss = 0.577652
Loss = 0.821228
Loss = 0.791641
Loss = 0.698654
Loss = 0.689499
Loss = 0.68634
Loss = 0.685654
Loss = 0.637283
Loss = 0.695419
Loss = 0.703995
Loss = 0.652435
TEST LOSS = 0.67534
TEST ACC = 492.369 % (8109/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.561462
Epoch 10.2: Loss = 0.639267
Epoch 10.3: Loss = 0.698914
Epoch 10.4: Loss = 0.547195
Epoch 10.5: Loss = 0.656662
Epoch 10.6: Loss = 0.704361
Epoch 10.7: Loss = 0.615753
Epoch 10.8: Loss = 0.727509
Epoch 10.9: Loss = 0.497513
Epoch 10.10: Loss = 0.398514
Epoch 10.11: Loss = 0.723099
Epoch 10.12: Loss = 0.628586
Epoch 10.13: Loss = 0.657578
Epoch 10.14: Loss = 0.620941
Epoch 10.15: Loss = 0.65213
Epoch 10.16: Loss = 0.704529
Epoch 10.17: Loss = 0.524277
Epoch 10.18: Loss = 0.629929
Epoch 10.19: Loss = 0.57608
Epoch 10.20: Loss = 0.736603
Epoch 10.21: Loss = 0.538742
Epoch 10.22: Loss = 0.487778
Epoch 10.23: Loss = 0.617706
Epoch 10.24: Loss = 0.724548
Epoch 10.25: Loss = 0.637466
Epoch 10.26: Loss = 0.510452
Epoch 10.27: Loss = 0.607773
Epoch 10.28: Loss = 0.649033
Epoch 10.29: Loss = 0.622894
Epoch 10.30: Loss = 0.613724
Epoch 10.31: Loss = 0.708511
Epoch 10.32: Loss = 0.590912
Epoch 10.33: Loss = 0.574036
Epoch 10.34: Loss = 0.697464
Epoch 10.35: Loss = 0.667068
Epoch 10.36: Loss = 0.669388
Epoch 10.37: Loss = 0.714859
Epoch 10.38: Loss = 0.657516
Epoch 10.39: Loss = 0.684647
Epoch 10.40: Loss = 0.59137
Epoch 10.41: Loss = 0.652634
Epoch 10.42: Loss = 0.626938
Epoch 10.43: Loss = 0.664841
Epoch 10.44: Loss = 0.491272
Epoch 10.45: Loss = 0.670776
Epoch 10.46: Loss = 0.771591
Epoch 10.47: Loss = 0.585846
Epoch 10.48: Loss = 0.554382
Epoch 10.49: Loss = 0.650253
Epoch 10.50: Loss = 0.616226
Epoch 10.51: Loss = 0.491455
Epoch 10.52: Loss = 0.632797
Epoch 10.53: Loss = 0.690781
Epoch 10.54: Loss = 0.49469
Epoch 10.55: Loss = 0.632858
Epoch 10.56: Loss = 0.612228
Epoch 10.57: Loss = 0.699814
Epoch 10.58: Loss = 0.579361
Epoch 10.59: Loss = 0.721848
Epoch 10.60: Loss = 0.681183
Epoch 10.61: Loss = 0.578659
Epoch 10.62: Loss = 0.690536
Epoch 10.63: Loss = 0.542282
Epoch 10.64: Loss = 0.504868
Epoch 10.65: Loss = 0.671082
Epoch 10.66: Loss = 0.530884
Epoch 10.67: Loss = 0.592407
Epoch 10.68: Loss = 0.84111
Epoch 10.69: Loss = 0.609604
Epoch 10.70: Loss = 0.620056
Epoch 10.71: Loss = 0.531754
Epoch 10.72: Loss = 0.65387
Epoch 10.73: Loss = 0.713287
Epoch 10.74: Loss = 0.679703
Epoch 10.75: Loss = 0.578033
Epoch 10.76: Loss = 0.606781
Epoch 10.77: Loss = 0.597763
Epoch 10.78: Loss = 0.632324
Epoch 10.79: Loss = 0.597855
Epoch 10.80: Loss = 0.600449
Epoch 10.81: Loss = 0.621613
Epoch 10.82: Loss = 0.62915
Epoch 10.83: Loss = 0.677002
Epoch 10.84: Loss = 0.57724
Epoch 10.85: Loss = 0.617401
Epoch 10.86: Loss = 0.686569
Epoch 10.87: Loss = 0.593277
Epoch 10.88: Loss = 0.568268
Epoch 10.89: Loss = 0.719879
Epoch 10.90: Loss = 0.674194
Epoch 10.91: Loss = 0.652374
Epoch 10.92: Loss = 0.642456
Epoch 10.93: Loss = 0.616791
Epoch 10.94: Loss = 0.662643
Epoch 10.95: Loss = 0.680786
Epoch 10.96: Loss = 0.620743
Epoch 10.97: Loss = 0.513275
Epoch 10.98: Loss = 0.647385
Epoch 10.99: Loss = 0.616653
Epoch 10.100: Loss = 0.62706
Epoch 10.101: Loss = 0.650513
Epoch 10.102: Loss = 0.665573
Epoch 10.103: Loss = 0.609818
Epoch 10.104: Loss = 0.547607
Epoch 10.105: Loss = 0.587189
Epoch 10.106: Loss = 0.708252
Epoch 10.107: Loss = 0.6931
Epoch 10.108: Loss = 0.811172
Epoch 10.109: Loss = 0.686905
Epoch 10.110: Loss = 0.659027
Epoch 10.111: Loss = 0.597733
Epoch 10.112: Loss = 0.604248
Epoch 10.113: Loss = 0.627518
Epoch 10.114: Loss = 0.673721
Epoch 10.115: Loss = 0.599152
Epoch 10.116: Loss = 0.550797
Epoch 10.117: Loss = 0.664047
Epoch 10.118: Loss = 0.553528
Epoch 10.119: Loss = 0.580215
Epoch 10.120: Loss = 0.583008
TRAIN LOSS = 0.627182
TRAIN ACC = 82.3105 % (49389/60000)
Loss = 0.555161
Loss = 0.656647
Loss = 0.632462
Loss = 0.535614
Loss = 0.593338
Loss = 0.727951
Loss = 0.813858
Loss = 0.667419
Loss = 0.631989
Loss = 0.574707
Loss = 0.834396
Loss = 0.796753
Loss = 0.706375
Loss = 0.668289
Loss = 0.66391
Loss = 0.671616
Loss = 0.623199
Loss = 0.706161
Loss = 0.70282
Loss = 0.659546
TEST LOSS = 0.67111
TEST ACC = 493.889 % (8141/10000)
