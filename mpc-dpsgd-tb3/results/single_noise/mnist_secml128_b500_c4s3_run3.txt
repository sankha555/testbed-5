Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.27917
Epoch 1.2: Loss = 2.25992
Epoch 1.3: Loss = 2.23991
Epoch 1.4: Loss = 2.23535
Epoch 1.5: Loss = 2.20183
Epoch 1.6: Loss = 2.17641
Epoch 1.7: Loss = 2.14133
Epoch 1.8: Loss = 2.12283
Epoch 1.9: Loss = 2.09181
Epoch 1.10: Loss = 2.07736
Epoch 1.11: Loss = 2.02176
Epoch 1.12: Loss = 2.0045
Epoch 1.13: Loss = 1.92981
Epoch 1.14: Loss = 1.95753
Epoch 1.15: Loss = 1.96014
Epoch 1.16: Loss = 1.9101
Epoch 1.17: Loss = 1.86186
Epoch 1.18: Loss = 1.85667
Epoch 1.19: Loss = 1.81677
Epoch 1.20: Loss = 1.77873
Epoch 1.21: Loss = 1.69777
Epoch 1.22: Loss = 1.68506
Epoch 1.23: Loss = 1.63733
Epoch 1.24: Loss = 1.69618
Epoch 1.25: Loss = 1.62979
Epoch 1.26: Loss = 1.64096
Epoch 1.27: Loss = 1.55788
Epoch 1.28: Loss = 1.56667
Epoch 1.29: Loss = 1.57751
Epoch 1.30: Loss = 1.60196
Epoch 1.31: Loss = 1.47078
Epoch 1.32: Loss = 1.45526
Epoch 1.33: Loss = 1.42163
Epoch 1.34: Loss = 1.46298
Epoch 1.35: Loss = 1.36746
Epoch 1.36: Loss = 1.46838
Epoch 1.37: Loss = 1.32497
Epoch 1.38: Loss = 1.30745
Epoch 1.39: Loss = 1.26401
Epoch 1.40: Loss = 1.17992
Epoch 1.41: Loss = 1.23375
Epoch 1.42: Loss = 1.22104
Epoch 1.43: Loss = 1.15749
Epoch 1.44: Loss = 1.07019
Epoch 1.45: Loss = 1.20532
Epoch 1.46: Loss = 1.1319
Epoch 1.47: Loss = 1.06985
Epoch 1.48: Loss = 1.13676
Epoch 1.49: Loss = 1.0565
Epoch 1.50: Loss = 1.12988
Epoch 1.51: Loss = 0.973404
Epoch 1.52: Loss = 0.983704
Epoch 1.53: Loss = 1.02782
Epoch 1.54: Loss = 1.0312
Epoch 1.55: Loss = 1.01825
Epoch 1.56: Loss = 0.930557
Epoch 1.57: Loss = 0.848557
Epoch 1.58: Loss = 0.916626
Epoch 1.59: Loss = 0.946075
Epoch 1.60: Loss = 1.02327
Epoch 1.61: Loss = 0.952652
Epoch 1.62: Loss = 0.984131
Epoch 1.63: Loss = 1.00478
Epoch 1.64: Loss = 0.955002
Epoch 1.65: Loss = 1.01625
Epoch 1.66: Loss = 0.860138
Epoch 1.67: Loss = 0.85788
Epoch 1.68: Loss = 0.721741
Epoch 1.69: Loss = 0.795334
Epoch 1.70: Loss = 0.860397
Epoch 1.71: Loss = 0.797699
Epoch 1.72: Loss = 0.792084
Epoch 1.73: Loss = 0.816635
Epoch 1.74: Loss = 0.687881
Epoch 1.75: Loss = 0.846313
Epoch 1.76: Loss = 0.789658
Epoch 1.77: Loss = 0.724991
Epoch 1.78: Loss = 0.702759
Epoch 1.79: Loss = 0.718384
Epoch 1.80: Loss = 0.799088
Epoch 1.81: Loss = 0.69043
Epoch 1.82: Loss = 0.67424
Epoch 1.83: Loss = 0.815704
Epoch 1.84: Loss = 0.734085
Epoch 1.85: Loss = 0.788071
Epoch 1.86: Loss = 0.721909
Epoch 1.87: Loss = 0.63443
Epoch 1.88: Loss = 0.67189
Epoch 1.89: Loss = 0.749268
Epoch 1.90: Loss = 0.63736
Epoch 1.91: Loss = 0.718399
Epoch 1.92: Loss = 0.688065
Epoch 1.93: Loss = 0.710739
Epoch 1.94: Loss = 0.579453
Epoch 1.95: Loss = 0.672424
Epoch 1.96: Loss = 0.6465
Epoch 1.97: Loss = 0.517822
Epoch 1.98: Loss = 0.633148
Epoch 1.99: Loss = 0.698944
Epoch 1.100: Loss = 0.779037
Epoch 1.101: Loss = 0.712006
Epoch 1.102: Loss = 0.652237
Epoch 1.103: Loss = 0.580734
Epoch 1.104: Loss = 0.591614
Epoch 1.105: Loss = 0.680588
Epoch 1.106: Loss = 0.663345
Epoch 1.107: Loss = 0.552719
Epoch 1.108: Loss = 0.619461
Epoch 1.109: Loss = 0.595795
Epoch 1.110: Loss = 0.589447
Epoch 1.111: Loss = 0.500656
Epoch 1.112: Loss = 0.479813
Epoch 1.113: Loss = 0.549973
Epoch 1.114: Loss = 0.503326
Epoch 1.115: Loss = 0.56102
Epoch 1.116: Loss = 0.565933
Epoch 1.117: Loss = 0.428696
Epoch 1.118: Loss = 0.392166
Epoch 1.119: Loss = 0.410446
Epoch 1.120: Loss = 0.417465
TRAIN LOSS = 1.11288
TRAIN ACC = 69.5816 % (41751/60000)
Loss = 0.585205
Loss = 0.604065
Loss = 0.698441
Loss = 0.695038
Loss = 0.732986
Loss = 0.603882
Loss = 0.565918
Loss = 0.74678
Loss = 0.695221
Loss = 0.632797
Loss = 0.321991
Loss = 0.498856
Loss = 0.355087
Loss = 0.540833
Loss = 0.396072
Loss = 0.410385
Loss = 0.402008
Loss = 0.230591
Loss = 0.383942
Loss = 0.636063
TEST LOSS = 0.536808
TEST ACC = 417.509 % (8434/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.498749
Epoch 2.2: Loss = 0.634277
Epoch 2.3: Loss = 0.610306
Epoch 2.4: Loss = 0.481155
Epoch 2.5: Loss = 0.499481
Epoch 2.6: Loss = 0.48259
Epoch 2.7: Loss = 0.540756
Epoch 2.8: Loss = 0.547485
Epoch 2.9: Loss = 0.516861
Epoch 2.10: Loss = 0.528549
Epoch 2.11: Loss = 0.504303
Epoch 2.12: Loss = 0.498352
Epoch 2.13: Loss = 0.417618
Epoch 2.14: Loss = 0.481491
Epoch 2.15: Loss = 0.614471
Epoch 2.16: Loss = 0.570145
Epoch 2.17: Loss = 0.575806
Epoch 2.18: Loss = 0.646347
Epoch 2.19: Loss = 0.512665
Epoch 2.20: Loss = 0.458389
Epoch 2.21: Loss = 0.44397
Epoch 2.22: Loss = 0.444931
Epoch 2.23: Loss = 0.43425
Epoch 2.24: Loss = 0.630249
Epoch 2.25: Loss = 0.52243
Epoch 2.26: Loss = 0.598969
Epoch 2.27: Loss = 0.572617
Epoch 2.28: Loss = 0.567413
Epoch 2.29: Loss = 0.617447
Epoch 2.30: Loss = 0.680389
Epoch 2.31: Loss = 0.4608
Epoch 2.32: Loss = 0.564804
Epoch 2.33: Loss = 0.506989
Epoch 2.34: Loss = 0.569504
Epoch 2.35: Loss = 0.515762
Epoch 2.36: Loss = 0.635269
Epoch 2.37: Loss = 0.431808
Epoch 2.38: Loss = 0.443298
Epoch 2.39: Loss = 0.481384
Epoch 2.40: Loss = 0.443359
Epoch 2.41: Loss = 0.485153
Epoch 2.42: Loss = 0.594315
Epoch 2.43: Loss = 0.432129
Epoch 2.44: Loss = 0.382568
Epoch 2.45: Loss = 0.502151
Epoch 2.46: Loss = 0.518173
Epoch 2.47: Loss = 0.438873
Epoch 2.48: Loss = 0.52446
Epoch 2.49: Loss = 0.479401
Epoch 2.50: Loss = 0.592285
Epoch 2.51: Loss = 0.416183
Epoch 2.52: Loss = 0.44577
Epoch 2.53: Loss = 0.469238
Epoch 2.54: Loss = 0.550476
Epoch 2.55: Loss = 0.496735
Epoch 2.56: Loss = 0.421448
Epoch 2.57: Loss = 0.402222
Epoch 2.58: Loss = 0.481216
Epoch 2.59: Loss = 0.541336
Epoch 2.60: Loss = 0.594406
Epoch 2.61: Loss = 0.556137
Epoch 2.62: Loss = 0.549652
Epoch 2.63: Loss = 0.625687
Epoch 2.64: Loss = 0.539673
Epoch 2.65: Loss = 0.689041
Epoch 2.66: Loss = 0.479965
Epoch 2.67: Loss = 0.505798
Epoch 2.68: Loss = 0.324997
Epoch 2.69: Loss = 0.42514
Epoch 2.70: Loss = 0.548157
Epoch 2.71: Loss = 0.421875
Epoch 2.72: Loss = 0.412415
Epoch 2.73: Loss = 0.481262
Epoch 2.74: Loss = 0.354218
Epoch 2.75: Loss = 0.590714
Epoch 2.76: Loss = 0.495697
Epoch 2.77: Loss = 0.404068
Epoch 2.78: Loss = 0.451645
Epoch 2.79: Loss = 0.49559
Epoch 2.80: Loss = 0.519882
Epoch 2.81: Loss = 0.411072
Epoch 2.82: Loss = 0.387329
Epoch 2.83: Loss = 0.563309
Epoch 2.84: Loss = 0.479507
Epoch 2.85: Loss = 0.578033
Epoch 2.86: Loss = 0.506012
Epoch 2.87: Loss = 0.380203
Epoch 2.88: Loss = 0.445313
Epoch 2.89: Loss = 0.521957
Epoch 2.90: Loss = 0.41037
Epoch 2.91: Loss = 0.509293
Epoch 2.92: Loss = 0.487747
Epoch 2.93: Loss = 0.520905
Epoch 2.94: Loss = 0.375015
Epoch 2.95: Loss = 0.450211
Epoch 2.96: Loss = 0.500137
Epoch 2.97: Loss = 0.36058
Epoch 2.98: Loss = 0.431229
Epoch 2.99: Loss = 0.542099
Epoch 2.100: Loss = 0.609589
Epoch 2.101: Loss = 0.574814
Epoch 2.102: Loss = 0.475006
Epoch 2.103: Loss = 0.407944
Epoch 2.104: Loss = 0.411133
Epoch 2.105: Loss = 0.55011
Epoch 2.106: Loss = 0.538681
Epoch 2.107: Loss = 0.392197
Epoch 2.108: Loss = 0.476654
Epoch 2.109: Loss = 0.440628
Epoch 2.110: Loss = 0.455582
Epoch 2.111: Loss = 0.359924
Epoch 2.112: Loss = 0.372604
Epoch 2.113: Loss = 0.418793
Epoch 2.114: Loss = 0.375336
Epoch 2.115: Loss = 0.393448
Epoch 2.116: Loss = 0.444839
Epoch 2.117: Loss = 0.287079
Epoch 2.118: Loss = 0.236572
Epoch 2.119: Loss = 0.299088
Epoch 2.120: Loss = 0.31012
TRAIN LOSS = 0.48764
TRAIN ACC = 85.1212 % (51075/60000)
Loss = 0.457611
Loss = 0.512207
Loss = 0.587448
Loss = 0.598083
Loss = 0.657974
Loss = 0.467987
Loss = 0.433243
Loss = 0.663116
Loss = 0.581177
Loss = 0.529434
Loss = 0.211777
Loss = 0.380722
Loss = 0.294022
Loss = 0.424088
Loss = 0.252441
Loss = 0.303955
Loss = 0.265182
Loss = 0.118591
Loss = 0.250046
Loss = 0.574951
TEST LOSS = 0.428203
TEST ACC = 510.75 % (8727/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.399826
Epoch 3.2: Loss = 0.517029
Epoch 3.3: Loss = 0.508438
Epoch 3.4: Loss = 0.366852
Epoch 3.5: Loss = 0.398972
Epoch 3.6: Loss = 0.379059
Epoch 3.7: Loss = 0.409134
Epoch 3.8: Loss = 0.433899
Epoch 3.9: Loss = 0.399811
Epoch 3.10: Loss = 0.460449
Epoch 3.11: Loss = 0.431854
Epoch 3.12: Loss = 0.406799
Epoch 3.13: Loss = 0.333618
Epoch 3.14: Loss = 0.394714
Epoch 3.15: Loss = 0.485794
Epoch 3.16: Loss = 0.480484
Epoch 3.17: Loss = 0.490173
Epoch 3.18: Loss = 0.607208
Epoch 3.19: Loss = 0.435974
Epoch 3.20: Loss = 0.382324
Epoch 3.21: Loss = 0.364075
Epoch 3.22: Loss = 0.343353
Epoch 3.23: Loss = 0.356415
Epoch 3.24: Loss = 0.550522
Epoch 3.25: Loss = 0.446976
Epoch 3.26: Loss = 0.566208
Epoch 3.27: Loss = 0.506958
Epoch 3.28: Loss = 0.502121
Epoch 3.29: Loss = 0.553146
Epoch 3.30: Loss = 0.622513
Epoch 3.31: Loss = 0.384247
Epoch 3.32: Loss = 0.517731
Epoch 3.33: Loss = 0.419113
Epoch 3.34: Loss = 0.517624
Epoch 3.35: Loss = 0.440796
Epoch 3.36: Loss = 0.556976
Epoch 3.37: Loss = 0.345459
Epoch 3.38: Loss = 0.391327
Epoch 3.39: Loss = 0.414322
Epoch 3.40: Loss = 0.401276
Epoch 3.41: Loss = 0.41452
Epoch 3.42: Loss = 0.585007
Epoch 3.43: Loss = 0.340469
Epoch 3.44: Loss = 0.333298
Epoch 3.45: Loss = 0.432053
Epoch 3.46: Loss = 0.455032
Epoch 3.47: Loss = 0.407074
Epoch 3.48: Loss = 0.470261
Epoch 3.49: Loss = 0.424591
Epoch 3.50: Loss = 0.531464
Epoch 3.51: Loss = 0.359787
Epoch 3.52: Loss = 0.389099
Epoch 3.53: Loss = 0.398499
Epoch 3.54: Loss = 0.527374
Epoch 3.55: Loss = 0.42392
Epoch 3.56: Loss = 0.37088
Epoch 3.57: Loss = 0.373322
Epoch 3.58: Loss = 0.443054
Epoch 3.59: Loss = 0.500412
Epoch 3.60: Loss = 0.520248
Epoch 3.61: Loss = 0.511429
Epoch 3.62: Loss = 0.51973
Epoch 3.63: Loss = 0.606796
Epoch 3.64: Loss = 0.530685
Epoch 3.65: Loss = 0.63768
Epoch 3.66: Loss = 0.428085
Epoch 3.67: Loss = 0.456772
Epoch 3.68: Loss = 0.270355
Epoch 3.69: Loss = 0.370941
Epoch 3.70: Loss = 0.50441
Epoch 3.71: Loss = 0.382385
Epoch 3.72: Loss = 0.367355
Epoch 3.73: Loss = 0.471588
Epoch 3.74: Loss = 0.317657
Epoch 3.75: Loss = 0.596115
Epoch 3.76: Loss = 0.439865
Epoch 3.77: Loss = 0.354538
Epoch 3.78: Loss = 0.426941
Epoch 3.79: Loss = 0.474945
Epoch 3.80: Loss = 0.463242
Epoch 3.81: Loss = 0.366989
Epoch 3.82: Loss = 0.349716
Epoch 3.83: Loss = 0.53476
Epoch 3.84: Loss = 0.437637
Epoch 3.85: Loss = 0.579208
Epoch 3.86: Loss = 0.507111
Epoch 3.87: Loss = 0.356033
Epoch 3.88: Loss = 0.416519
Epoch 3.89: Loss = 0.515869
Epoch 3.90: Loss = 0.373795
Epoch 3.91: Loss = 0.483154
Epoch 3.92: Loss = 0.477402
Epoch 3.93: Loss = 0.498184
Epoch 3.94: Loss = 0.353073
Epoch 3.95: Loss = 0.406876
Epoch 3.96: Loss = 0.491959
Epoch 3.97: Loss = 0.350571
Epoch 3.98: Loss = 0.406784
Epoch 3.99: Loss = 0.516403
Epoch 3.100: Loss = 0.60025
Epoch 3.101: Loss = 0.589401
Epoch 3.102: Loss = 0.464523
Epoch 3.103: Loss = 0.393051
Epoch 3.104: Loss = 0.375504
Epoch 3.105: Loss = 0.554123
Epoch 3.106: Loss = 0.530487
Epoch 3.107: Loss = 0.365601
Epoch 3.108: Loss = 0.46434
Epoch 3.109: Loss = 0.412903
Epoch 3.110: Loss = 0.436386
Epoch 3.111: Loss = 0.334518
Epoch 3.112: Loss = 0.350616
Epoch 3.113: Loss = 0.400452
Epoch 3.114: Loss = 0.352142
Epoch 3.115: Loss = 0.344879
Epoch 3.116: Loss = 0.401245
Epoch 3.117: Loss = 0.248688
Epoch 3.118: Loss = 0.206924
Epoch 3.119: Loss = 0.290482
Epoch 3.120: Loss = 0.309631
TRAIN LOSS = 0.43837
TRAIN ACC = 87.0102 % (52209/60000)
Loss = 0.422409
Loss = 0.481583
Loss = 0.551208
Loss = 0.589417
Loss = 0.625198
Loss = 0.442184
Loss = 0.398483
Loss = 0.648392
Loss = 0.548676
Loss = 0.488892
Loss = 0.193405
Loss = 0.329315
Loss = 0.302475
Loss = 0.419067
Loss = 0.218948
Loss = 0.309341
Loss = 0.240677
Loss = 0.0852509
Loss = 0.222
Loss = 0.565216
TEST LOSS = 0.404107
TEST ACC = 522.089 % (8843/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.382095
Epoch 4.2: Loss = 0.501755
Epoch 4.3: Loss = 0.503998
Epoch 4.4: Loss = 0.33609
Epoch 4.5: Loss = 0.36763
Epoch 4.6: Loss = 0.355179
Epoch 4.7: Loss = 0.382431
Epoch 4.8: Loss = 0.392365
Epoch 4.9: Loss = 0.353714
Epoch 4.10: Loss = 0.426422
Epoch 4.11: Loss = 0.413467
Epoch 4.12: Loss = 0.383865
Epoch 4.13: Loss = 0.298187
Epoch 4.14: Loss = 0.362717
Epoch 4.15: Loss = 0.457413
Epoch 4.16: Loss = 0.462967
Epoch 4.17: Loss = 0.480759
Epoch 4.18: Loss = 0.599014
Epoch 4.19: Loss = 0.433029
Epoch 4.20: Loss = 0.34697
Epoch 4.21: Loss = 0.355743
Epoch 4.22: Loss = 0.317688
Epoch 4.23: Loss = 0.335907
Epoch 4.24: Loss = 0.557434
Epoch 4.25: Loss = 0.440338
Epoch 4.26: Loss = 0.531403
Epoch 4.27: Loss = 0.473419
Epoch 4.28: Loss = 0.489273
Epoch 4.29: Loss = 0.530945
Epoch 4.30: Loss = 0.58725
Epoch 4.31: Loss = 0.374863
Epoch 4.32: Loss = 0.496933
Epoch 4.33: Loss = 0.383652
Epoch 4.34: Loss = 0.477997
Epoch 4.35: Loss = 0.422501
Epoch 4.36: Loss = 0.538605
Epoch 4.37: Loss = 0.33255
Epoch 4.38: Loss = 0.374664
Epoch 4.39: Loss = 0.390091
Epoch 4.40: Loss = 0.387314
Epoch 4.41: Loss = 0.370377
Epoch 4.42: Loss = 0.594589
Epoch 4.43: Loss = 0.309174
Epoch 4.44: Loss = 0.314468
Epoch 4.45: Loss = 0.401413
Epoch 4.46: Loss = 0.453217
Epoch 4.47: Loss = 0.391037
Epoch 4.48: Loss = 0.441833
Epoch 4.49: Loss = 0.399384
Epoch 4.50: Loss = 0.503815
Epoch 4.51: Loss = 0.353394
Epoch 4.52: Loss = 0.380875
Epoch 4.53: Loss = 0.382904
Epoch 4.54: Loss = 0.528641
Epoch 4.55: Loss = 0.396942
Epoch 4.56: Loss = 0.365631
Epoch 4.57: Loss = 0.371994
Epoch 4.58: Loss = 0.43248
Epoch 4.59: Loss = 0.490036
Epoch 4.60: Loss = 0.495422
Epoch 4.61: Loss = 0.490036
Epoch 4.62: Loss = 0.510193
Epoch 4.63: Loss = 0.586136
Epoch 4.64: Loss = 0.507584
Epoch 4.65: Loss = 0.633362
Epoch 4.66: Loss = 0.39006
Epoch 4.67: Loss = 0.423233
Epoch 4.68: Loss = 0.240906
Epoch 4.69: Loss = 0.362686
Epoch 4.70: Loss = 0.49765
Epoch 4.71: Loss = 0.353058
Epoch 4.72: Loss = 0.326752
Epoch 4.73: Loss = 0.45665
Epoch 4.74: Loss = 0.307861
Epoch 4.75: Loss = 0.632324
Epoch 4.76: Loss = 0.425262
Epoch 4.77: Loss = 0.328964
Epoch 4.78: Loss = 0.420624
Epoch 4.79: Loss = 0.470139
Epoch 4.80: Loss = 0.460861
Epoch 4.81: Loss = 0.349319
Epoch 4.82: Loss = 0.318863
Epoch 4.83: Loss = 0.505829
Epoch 4.84: Loss = 0.411194
Epoch 4.85: Loss = 0.575668
Epoch 4.86: Loss = 0.504242
Epoch 4.87: Loss = 0.320099
Epoch 4.88: Loss = 0.407471
Epoch 4.89: Loss = 0.486603
Epoch 4.90: Loss = 0.369186
Epoch 4.91: Loss = 0.516113
Epoch 4.92: Loss = 0.465424
Epoch 4.93: Loss = 0.514908
Epoch 4.94: Loss = 0.328903
Epoch 4.95: Loss = 0.380096
Epoch 4.96: Loss = 0.496231
Epoch 4.97: Loss = 0.339432
Epoch 4.98: Loss = 0.401855
Epoch 4.99: Loss = 0.499649
Epoch 4.100: Loss = 0.5914
Epoch 4.101: Loss = 0.592804
Epoch 4.102: Loss = 0.465851
Epoch 4.103: Loss = 0.378906
Epoch 4.104: Loss = 0.378082
Epoch 4.105: Loss = 0.542496
Epoch 4.106: Loss = 0.537247
Epoch 4.107: Loss = 0.347656
Epoch 4.108: Loss = 0.456467
Epoch 4.109: Loss = 0.410141
Epoch 4.110: Loss = 0.419266
Epoch 4.111: Loss = 0.338028
Epoch 4.112: Loss = 0.356201
Epoch 4.113: Loss = 0.381943
Epoch 4.114: Loss = 0.328629
Epoch 4.115: Loss = 0.320831
Epoch 4.116: Loss = 0.389923
Epoch 4.117: Loss = 0.233719
Epoch 4.118: Loss = 0.206604
Epoch 4.119: Loss = 0.296112
Epoch 4.120: Loss = 0.289474
TRAIN LOSS = 0.422455
TRAIN ACC = 87.9044 % (52745/60000)
Loss = 0.409561
Loss = 0.467377
Loss = 0.539749
Loss = 0.58696
Loss = 0.60704
Loss = 0.442429
Loss = 0.381439
Loss = 0.649399
Loss = 0.549377
Loss = 0.478806
Loss = 0.181183
Loss = 0.311111
Loss = 0.295609
Loss = 0.394394
Loss = 0.203018
Loss = 0.314148
Loss = 0.225723
Loss = 0.0641022
Loss = 0.224319
Loss = 0.568069
TEST LOSS = 0.394691
TEST ACC = 527.449 % (8921/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.402252
Epoch 5.2: Loss = 0.48407
Epoch 5.3: Loss = 0.512344
Epoch 5.4: Loss = 0.328247
Epoch 5.5: Loss = 0.35791
Epoch 5.6: Loss = 0.354187
Epoch 5.7: Loss = 0.362183
Epoch 5.8: Loss = 0.380798
Epoch 5.9: Loss = 0.35498
Epoch 5.10: Loss = 0.40274
Epoch 5.11: Loss = 0.408951
Epoch 5.12: Loss = 0.378677
Epoch 5.13: Loss = 0.289993
Epoch 5.14: Loss = 0.354401
Epoch 5.15: Loss = 0.444687
Epoch 5.16: Loss = 0.456467
Epoch 5.17: Loss = 0.51149
Epoch 5.18: Loss = 0.635803
Epoch 5.19: Loss = 0.421829
Epoch 5.20: Loss = 0.329605
Epoch 5.21: Loss = 0.350449
Epoch 5.22: Loss = 0.308594
Epoch 5.23: Loss = 0.329819
Epoch 5.24: Loss = 0.562958
Epoch 5.25: Loss = 0.442841
Epoch 5.26: Loss = 0.527832
Epoch 5.27: Loss = 0.487991
Epoch 5.28: Loss = 0.489685
Epoch 5.29: Loss = 0.525513
Epoch 5.30: Loss = 0.566589
Epoch 5.31: Loss = 0.380142
Epoch 5.32: Loss = 0.490936
Epoch 5.33: Loss = 0.373474
Epoch 5.34: Loss = 0.484787
Epoch 5.35: Loss = 0.401962
Epoch 5.36: Loss = 0.521835
Epoch 5.37: Loss = 0.301727
Epoch 5.38: Loss = 0.357178
Epoch 5.39: Loss = 0.378754
Epoch 5.40: Loss = 0.375793
Epoch 5.41: Loss = 0.386734
Epoch 5.42: Loss = 0.623215
Epoch 5.43: Loss = 0.321106
Epoch 5.44: Loss = 0.333893
Epoch 5.45: Loss = 0.403732
Epoch 5.46: Loss = 0.478836
Epoch 5.47: Loss = 0.393448
Epoch 5.48: Loss = 0.442703
Epoch 5.49: Loss = 0.396118
Epoch 5.50: Loss = 0.499268
Epoch 5.51: Loss = 0.337906
Epoch 5.52: Loss = 0.368591
Epoch 5.53: Loss = 0.37294
Epoch 5.54: Loss = 0.52861
Epoch 5.55: Loss = 0.401398
Epoch 5.56: Loss = 0.366776
Epoch 5.57: Loss = 0.364609
Epoch 5.58: Loss = 0.448868
Epoch 5.59: Loss = 0.488831
Epoch 5.60: Loss = 0.485275
Epoch 5.61: Loss = 0.464752
Epoch 5.62: Loss = 0.490921
Epoch 5.63: Loss = 0.591385
Epoch 5.64: Loss = 0.502853
Epoch 5.65: Loss = 0.630127
Epoch 5.66: Loss = 0.387772
Epoch 5.67: Loss = 0.411362
Epoch 5.68: Loss = 0.243866
Epoch 5.69: Loss = 0.349075
Epoch 5.70: Loss = 0.495728
Epoch 5.71: Loss = 0.347794
Epoch 5.72: Loss = 0.317734
Epoch 5.73: Loss = 0.447067
Epoch 5.74: Loss = 0.315842
Epoch 5.75: Loss = 0.648712
Epoch 5.76: Loss = 0.407562
Epoch 5.77: Loss = 0.310867
Epoch 5.78: Loss = 0.420746
Epoch 5.79: Loss = 0.485931
Epoch 5.80: Loss = 0.464859
Epoch 5.81: Loss = 0.310318
Epoch 5.82: Loss = 0.307083
Epoch 5.83: Loss = 0.498917
Epoch 5.84: Loss = 0.411407
Epoch 5.85: Loss = 0.573227
Epoch 5.86: Loss = 0.498291
Epoch 5.87: Loss = 0.316376
Epoch 5.88: Loss = 0.420517
Epoch 5.89: Loss = 0.498581
Epoch 5.90: Loss = 0.360123
Epoch 5.91: Loss = 0.507156
Epoch 5.92: Loss = 0.481903
Epoch 5.93: Loss = 0.530838
Epoch 5.94: Loss = 0.308807
Epoch 5.95: Loss = 0.387207
Epoch 5.96: Loss = 0.477966
Epoch 5.97: Loss = 0.33757
Epoch 5.98: Loss = 0.413925
Epoch 5.99: Loss = 0.484955
Epoch 5.100: Loss = 0.607361
Epoch 5.101: Loss = 0.592773
Epoch 5.102: Loss = 0.426453
Epoch 5.103: Loss = 0.370422
Epoch 5.104: Loss = 0.36795
Epoch 5.105: Loss = 0.532043
Epoch 5.106: Loss = 0.548065
Epoch 5.107: Loss = 0.335358
Epoch 5.108: Loss = 0.459396
Epoch 5.109: Loss = 0.403564
Epoch 5.110: Loss = 0.412735
Epoch 5.111: Loss = 0.33287
Epoch 5.112: Loss = 0.341782
Epoch 5.113: Loss = 0.367615
Epoch 5.114: Loss = 0.304977
Epoch 5.115: Loss = 0.314453
Epoch 5.116: Loss = 0.374664
Epoch 5.117: Loss = 0.225693
Epoch 5.118: Loss = 0.19873
Epoch 5.119: Loss = 0.280151
Epoch 5.120: Loss = 0.310959
TRAIN LOSS = 0.418686
TRAIN ACC = 88.5101 % (53109/60000)
Loss = 0.411301
Loss = 0.468674
Loss = 0.565018
Loss = 0.59993
Loss = 0.627747
Loss = 0.429672
Loss = 0.364273
Loss = 0.646774
Loss = 0.544998
Loss = 0.484772
Loss = 0.183182
Loss = 0.3013
Loss = 0.299408
Loss = 0.391769
Loss = 0.198578
Loss = 0.305252
Loss = 0.214996
Loss = 0.063446
Loss = 0.227219
Loss = 0.578659
TEST LOSS = 0.395348
TEST ACC = 531.09 % (8950/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.406372
Epoch 6.2: Loss = 0.484543
Epoch 6.3: Loss = 0.528824
Epoch 6.4: Loss = 0.32016
Epoch 6.5: Loss = 0.357132
Epoch 6.6: Loss = 0.334335
Epoch 6.7: Loss = 0.344589
Epoch 6.8: Loss = 0.387268
Epoch 6.9: Loss = 0.348648
Epoch 6.10: Loss = 0.412399
Epoch 6.11: Loss = 0.396103
Epoch 6.12: Loss = 0.367752
Epoch 6.13: Loss = 0.28479
Epoch 6.14: Loss = 0.338745
Epoch 6.15: Loss = 0.458679
Epoch 6.16: Loss = 0.455887
Epoch 6.17: Loss = 0.497452
Epoch 6.18: Loss = 0.615891
Epoch 6.19: Loss = 0.425476
Epoch 6.20: Loss = 0.323517
Epoch 6.21: Loss = 0.356186
Epoch 6.22: Loss = 0.30127
Epoch 6.23: Loss = 0.322037
Epoch 6.24: Loss = 0.543976
Epoch 6.25: Loss = 0.443024
Epoch 6.26: Loss = 0.54216
Epoch 6.27: Loss = 0.491058
Epoch 6.28: Loss = 0.476059
Epoch 6.29: Loss = 0.54512
Epoch 6.30: Loss = 0.548874
Epoch 6.31: Loss = 0.379715
Epoch 6.32: Loss = 0.492096
Epoch 6.33: Loss = 0.389709
Epoch 6.34: Loss = 0.482529
Epoch 6.35: Loss = 0.40065
Epoch 6.36: Loss = 0.490356
Epoch 6.37: Loss = 0.307449
Epoch 6.38: Loss = 0.364594
Epoch 6.39: Loss = 0.386551
Epoch 6.40: Loss = 0.380768
Epoch 6.41: Loss = 0.383667
Epoch 6.42: Loss = 0.615585
Epoch 6.43: Loss = 0.311523
Epoch 6.44: Loss = 0.332321
Epoch 6.45: Loss = 0.420868
Epoch 6.46: Loss = 0.46907
Epoch 6.47: Loss = 0.400436
Epoch 6.48: Loss = 0.450653
Epoch 6.49: Loss = 0.389374
Epoch 6.50: Loss = 0.500153
Epoch 6.51: Loss = 0.317749
Epoch 6.52: Loss = 0.366623
Epoch 6.53: Loss = 0.367798
Epoch 6.54: Loss = 0.548248
Epoch 6.55: Loss = 0.424698
Epoch 6.56: Loss = 0.368469
Epoch 6.57: Loss = 0.356323
Epoch 6.58: Loss = 0.449295
Epoch 6.59: Loss = 0.479004
Epoch 6.60: Loss = 0.489197
Epoch 6.61: Loss = 0.462112
Epoch 6.62: Loss = 0.491394
Epoch 6.63: Loss = 0.59491
Epoch 6.64: Loss = 0.505905
Epoch 6.65: Loss = 0.604553
Epoch 6.66: Loss = 0.376862
Epoch 6.67: Loss = 0.417847
Epoch 6.68: Loss = 0.240646
Epoch 6.69: Loss = 0.351852
Epoch 6.70: Loss = 0.491302
Epoch 6.71: Loss = 0.349457
Epoch 6.72: Loss = 0.296066
Epoch 6.73: Loss = 0.439178
Epoch 6.74: Loss = 0.317719
Epoch 6.75: Loss = 0.657852
Epoch 6.76: Loss = 0.426697
Epoch 6.77: Loss = 0.32309
Epoch 6.78: Loss = 0.438416
Epoch 6.79: Loss = 0.500031
Epoch 6.80: Loss = 0.481873
Epoch 6.81: Loss = 0.319153
Epoch 6.82: Loss = 0.314163
Epoch 6.83: Loss = 0.515503
Epoch 6.84: Loss = 0.39827
Epoch 6.85: Loss = 0.548615
Epoch 6.86: Loss = 0.507065
Epoch 6.87: Loss = 0.309845
Epoch 6.88: Loss = 0.407944
Epoch 6.89: Loss = 0.469376
Epoch 6.90: Loss = 0.343918
Epoch 6.91: Loss = 0.513443
Epoch 6.92: Loss = 0.473419
Epoch 6.93: Loss = 0.541275
Epoch 6.94: Loss = 0.309052
Epoch 6.95: Loss = 0.384903
Epoch 6.96: Loss = 0.466965
Epoch 6.97: Loss = 0.344742
Epoch 6.98: Loss = 0.416779
Epoch 6.99: Loss = 0.486191
Epoch 6.100: Loss = 0.617538
Epoch 6.101: Loss = 0.603333
Epoch 6.102: Loss = 0.432129
Epoch 6.103: Loss = 0.360748
Epoch 6.104: Loss = 0.34581
Epoch 6.105: Loss = 0.539948
Epoch 6.106: Loss = 0.525345
Epoch 6.107: Loss = 0.319427
Epoch 6.108: Loss = 0.443268
Epoch 6.109: Loss = 0.404846
Epoch 6.110: Loss = 0.403961
Epoch 6.111: Loss = 0.318161
Epoch 6.112: Loss = 0.325439
Epoch 6.113: Loss = 0.356186
Epoch 6.114: Loss = 0.293762
Epoch 6.115: Loss = 0.31665
Epoch 6.116: Loss = 0.371063
Epoch 6.117: Loss = 0.213486
Epoch 6.118: Loss = 0.192566
Epoch 6.119: Loss = 0.273544
Epoch 6.120: Loss = 0.307312
TRAIN LOSS = 0.416473
TRAIN ACC = 88.8367 % (53304/60000)
Loss = 0.390991
Loss = 0.462601
Loss = 0.537125
Loss = 0.57814
Loss = 0.600891
Loss = 0.410919
Loss = 0.338486
Loss = 0.638336
Loss = 0.525833
Loss = 0.473206
Loss = 0.16716
Loss = 0.296066
Loss = 0.323395
Loss = 0.374741
Loss = 0.176773
Loss = 0.294907
Loss = 0.209549
Loss = 0.0574036
Loss = 0.228271
Loss = 0.561676
TEST LOSS = 0.382323
TEST ACC = 533.04 % (8993/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.387299
Epoch 7.2: Loss = 0.451462
Epoch 7.3: Loss = 0.503128
Epoch 7.4: Loss = 0.308792
Epoch 7.5: Loss = 0.349503
Epoch 7.6: Loss = 0.333649
Epoch 7.7: Loss = 0.319061
Epoch 7.8: Loss = 0.355026
Epoch 7.9: Loss = 0.345337
Epoch 7.10: Loss = 0.393906
Epoch 7.11: Loss = 0.395676
Epoch 7.12: Loss = 0.351105
Epoch 7.13: Loss = 0.269653
Epoch 7.14: Loss = 0.339294
Epoch 7.15: Loss = 0.44194
Epoch 7.16: Loss = 0.456299
Epoch 7.17: Loss = 0.495026
Epoch 7.18: Loss = 0.615479
Epoch 7.19: Loss = 0.416992
Epoch 7.20: Loss = 0.305969
Epoch 7.21: Loss = 0.349197
Epoch 7.22: Loss = 0.27919
Epoch 7.23: Loss = 0.315872
Epoch 7.24: Loss = 0.515717
Epoch 7.25: Loss = 0.427948
Epoch 7.26: Loss = 0.536209
Epoch 7.27: Loss = 0.488937
Epoch 7.28: Loss = 0.478104
Epoch 7.29: Loss = 0.51622
Epoch 7.30: Loss = 0.551559
Epoch 7.31: Loss = 0.369415
Epoch 7.32: Loss = 0.46344
Epoch 7.33: Loss = 0.365356
Epoch 7.34: Loss = 0.462967
Epoch 7.35: Loss = 0.395462
Epoch 7.36: Loss = 0.476303
Epoch 7.37: Loss = 0.282135
Epoch 7.38: Loss = 0.343536
Epoch 7.39: Loss = 0.356461
Epoch 7.40: Loss = 0.35881
Epoch 7.41: Loss = 0.366135
Epoch 7.42: Loss = 0.620102
Epoch 7.43: Loss = 0.300949
Epoch 7.44: Loss = 0.30954
Epoch 7.45: Loss = 0.40918
Epoch 7.46: Loss = 0.456192
Epoch 7.47: Loss = 0.388748
Epoch 7.48: Loss = 0.445251
Epoch 7.49: Loss = 0.392395
Epoch 7.50: Loss = 0.498566
Epoch 7.51: Loss = 0.297272
Epoch 7.52: Loss = 0.345123
Epoch 7.53: Loss = 0.349915
Epoch 7.54: Loss = 0.530106
Epoch 7.55: Loss = 0.444382
Epoch 7.56: Loss = 0.350708
Epoch 7.57: Loss = 0.353424
Epoch 7.58: Loss = 0.423981
Epoch 7.59: Loss = 0.4534
Epoch 7.60: Loss = 0.476837
Epoch 7.61: Loss = 0.444687
Epoch 7.62: Loss = 0.484879
Epoch 7.63: Loss = 0.596634
Epoch 7.64: Loss = 0.503662
Epoch 7.65: Loss = 0.604828
Epoch 7.66: Loss = 0.363617
Epoch 7.67: Loss = 0.41391
Epoch 7.68: Loss = 0.230072
Epoch 7.69: Loss = 0.34671
Epoch 7.70: Loss = 0.481094
Epoch 7.71: Loss = 0.347427
Epoch 7.72: Loss = 0.29454
Epoch 7.73: Loss = 0.434647
Epoch 7.74: Loss = 0.327072
Epoch 7.75: Loss = 0.671951
Epoch 7.76: Loss = 0.434174
Epoch 7.77: Loss = 0.308578
Epoch 7.78: Loss = 0.416306
Epoch 7.79: Loss = 0.495728
Epoch 7.80: Loss = 0.457306
Epoch 7.81: Loss = 0.307541
Epoch 7.82: Loss = 0.299484
Epoch 7.83: Loss = 0.507935
Epoch 7.84: Loss = 0.384018
Epoch 7.85: Loss = 0.571457
Epoch 7.86: Loss = 0.495483
Epoch 7.87: Loss = 0.279495
Epoch 7.88: Loss = 0.403351
Epoch 7.89: Loss = 0.478317
Epoch 7.90: Loss = 0.327545
Epoch 7.91: Loss = 0.50383
Epoch 7.92: Loss = 0.471802
Epoch 7.93: Loss = 0.555634
Epoch 7.94: Loss = 0.301407
Epoch 7.95: Loss = 0.388412
Epoch 7.96: Loss = 0.435638
Epoch 7.97: Loss = 0.329742
Epoch 7.98: Loss = 0.401169
Epoch 7.99: Loss = 0.463867
Epoch 7.100: Loss = 0.581635
Epoch 7.101: Loss = 0.619797
Epoch 7.102: Loss = 0.415207
Epoch 7.103: Loss = 0.364883
Epoch 7.104: Loss = 0.357193
Epoch 7.105: Loss = 0.52655
Epoch 7.106: Loss = 0.519531
Epoch 7.107: Loss = 0.31163
Epoch 7.108: Loss = 0.448776
Epoch 7.109: Loss = 0.409042
Epoch 7.110: Loss = 0.397079
Epoch 7.111: Loss = 0.319794
Epoch 7.112: Loss = 0.318573
Epoch 7.113: Loss = 0.363068
Epoch 7.114: Loss = 0.305634
Epoch 7.115: Loss = 0.336456
Epoch 7.116: Loss = 0.37706
Epoch 7.117: Loss = 0.193787
Epoch 7.118: Loss = 0.186188
Epoch 7.119: Loss = 0.262314
Epoch 7.120: Loss = 0.317398
TRAIN LOSS = 0.407303
TRAIN ACC = 89.2639 % (53561/60000)
Loss = 0.389511
Loss = 0.484604
Loss = 0.533524
Loss = 0.598129
Loss = 0.620087
Loss = 0.39386
Loss = 0.336838
Loss = 0.664398
Loss = 0.540039
Loss = 0.497253
Loss = 0.166763
Loss = 0.29126
Loss = 0.347321
Loss = 0.391617
Loss = 0.186478
Loss = 0.296494
Loss = 0.229568
Loss = 0.0557556
Loss = 0.213577
Loss = 0.593307
TEST LOSS = 0.391519
TEST ACC = 535.609 % (8998/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.392807
Epoch 8.2: Loss = 0.455826
Epoch 8.3: Loss = 0.492218
Epoch 8.4: Loss = 0.28862
Epoch 8.5: Loss = 0.343384
Epoch 8.6: Loss = 0.346237
Epoch 8.7: Loss = 0.322266
Epoch 8.8: Loss = 0.373962
Epoch 8.9: Loss = 0.338379
Epoch 8.10: Loss = 0.38678
Epoch 8.11: Loss = 0.414688
Epoch 8.12: Loss = 0.365616
Epoch 8.13: Loss = 0.270477
Epoch 8.14: Loss = 0.334991
Epoch 8.15: Loss = 0.433838
Epoch 8.16: Loss = 0.434402
Epoch 8.17: Loss = 0.511337
Epoch 8.18: Loss = 0.650955
Epoch 8.19: Loss = 0.425552
Epoch 8.20: Loss = 0.326157
Epoch 8.21: Loss = 0.344971
Epoch 8.22: Loss = 0.277969
Epoch 8.23: Loss = 0.323578
Epoch 8.24: Loss = 0.498352
Epoch 8.25: Loss = 0.438721
Epoch 8.26: Loss = 0.555405
Epoch 8.27: Loss = 0.471924
Epoch 8.28: Loss = 0.471893
Epoch 8.29: Loss = 0.504868
Epoch 8.30: Loss = 0.57695
Epoch 8.31: Loss = 0.359192
Epoch 8.32: Loss = 0.467346
Epoch 8.33: Loss = 0.352997
Epoch 8.34: Loss = 0.478836
Epoch 8.35: Loss = 0.403229
Epoch 8.36: Loss = 0.478638
Epoch 8.37: Loss = 0.292526
Epoch 8.38: Loss = 0.339188
Epoch 8.39: Loss = 0.346252
Epoch 8.40: Loss = 0.369049
Epoch 8.41: Loss = 0.383499
Epoch 8.42: Loss = 0.646866
Epoch 8.43: Loss = 0.321045
Epoch 8.44: Loss = 0.324631
Epoch 8.45: Loss = 0.401001
Epoch 8.46: Loss = 0.428131
Epoch 8.47: Loss = 0.374207
Epoch 8.48: Loss = 0.459839
Epoch 8.49: Loss = 0.362991
Epoch 8.50: Loss = 0.538452
Epoch 8.51: Loss = 0.305298
Epoch 8.52: Loss = 0.345016
Epoch 8.53: Loss = 0.360657
Epoch 8.54: Loss = 0.540451
Epoch 8.55: Loss = 0.459366
Epoch 8.56: Loss = 0.363922
Epoch 8.57: Loss = 0.353027
Epoch 8.58: Loss = 0.417831
Epoch 8.59: Loss = 0.446365
Epoch 8.60: Loss = 0.483978
Epoch 8.61: Loss = 0.442795
Epoch 8.62: Loss = 0.473389
Epoch 8.63: Loss = 0.630112
Epoch 8.64: Loss = 0.52536
Epoch 8.65: Loss = 0.628784
Epoch 8.66: Loss = 0.371567
Epoch 8.67: Loss = 0.403351
Epoch 8.68: Loss = 0.235596
Epoch 8.69: Loss = 0.353699
Epoch 8.70: Loss = 0.476791
Epoch 8.71: Loss = 0.365463
Epoch 8.72: Loss = 0.298401
Epoch 8.73: Loss = 0.429474
Epoch 8.74: Loss = 0.316132
Epoch 8.75: Loss = 0.669708
Epoch 8.76: Loss = 0.441574
Epoch 8.77: Loss = 0.313477
Epoch 8.78: Loss = 0.42131
Epoch 8.79: Loss = 0.520386
Epoch 8.80: Loss = 0.4552
Epoch 8.81: Loss = 0.316147
Epoch 8.82: Loss = 0.308502
Epoch 8.83: Loss = 0.534058
Epoch 8.84: Loss = 0.367828
Epoch 8.85: Loss = 0.570251
Epoch 8.86: Loss = 0.525208
Epoch 8.87: Loss = 0.273483
Epoch 8.88: Loss = 0.376373
Epoch 8.89: Loss = 0.50621
Epoch 8.90: Loss = 0.338257
Epoch 8.91: Loss = 0.512924
Epoch 8.92: Loss = 0.46463
Epoch 8.93: Loss = 0.564789
Epoch 8.94: Loss = 0.299255
Epoch 8.95: Loss = 0.373398
Epoch 8.96: Loss = 0.439224
Epoch 8.97: Loss = 0.317169
Epoch 8.98: Loss = 0.397736
Epoch 8.99: Loss = 0.488052
Epoch 8.100: Loss = 0.6026
Epoch 8.101: Loss = 0.626038
Epoch 8.102: Loss = 0.417206
Epoch 8.103: Loss = 0.372162
Epoch 8.104: Loss = 0.364029
Epoch 8.105: Loss = 0.508743
Epoch 8.106: Loss = 0.524002
Epoch 8.107: Loss = 0.303818
Epoch 8.108: Loss = 0.456375
Epoch 8.109: Loss = 0.432236
Epoch 8.110: Loss = 0.403305
Epoch 8.111: Loss = 0.317184
Epoch 8.112: Loss = 0.319061
Epoch 8.113: Loss = 0.371979
Epoch 8.114: Loss = 0.300049
Epoch 8.115: Loss = 0.324265
Epoch 8.116: Loss = 0.389542
Epoch 8.117: Loss = 0.203308
Epoch 8.118: Loss = 0.191071
Epoch 8.119: Loss = 0.30011
Epoch 8.120: Loss = 0.310486
TRAIN LOSS = 0.411346
TRAIN ACC = 89.36 % (53619/60000)
Loss = 0.378235
Loss = 0.478348
Loss = 0.531982
Loss = 0.591949
Loss = 0.622894
Loss = 0.386856
Loss = 0.315872
Loss = 0.679504
Loss = 0.556976
Loss = 0.481888
Loss = 0.182358
Loss = 0.309952
Loss = 0.37851
Loss = 0.386215
Loss = 0.180466
Loss = 0.291519
Loss = 0.231216
Loss = 0.0545654
Loss = 0.218979
Loss = 0.586166
TEST LOSS = 0.392222
TEST ACC = 536.189 % (9010/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.376083
Epoch 9.2: Loss = 0.44606
Epoch 9.3: Loss = 0.506546
Epoch 9.4: Loss = 0.315308
Epoch 9.5: Loss = 0.34198
Epoch 9.6: Loss = 0.349228
Epoch 9.7: Loss = 0.330978
Epoch 9.8: Loss = 0.367584
Epoch 9.9: Loss = 0.329681
Epoch 9.10: Loss = 0.391785
Epoch 9.11: Loss = 0.41539
Epoch 9.12: Loss = 0.364212
Epoch 9.13: Loss = 0.257965
Epoch 9.14: Loss = 0.339157
Epoch 9.15: Loss = 0.430054
Epoch 9.16: Loss = 0.439636
Epoch 9.17: Loss = 0.514313
Epoch 9.18: Loss = 0.633682
Epoch 9.19: Loss = 0.42218
Epoch 9.20: Loss = 0.329376
Epoch 9.21: Loss = 0.366364
Epoch 9.22: Loss = 0.28508
Epoch 9.23: Loss = 0.31842
Epoch 9.24: Loss = 0.51181
Epoch 9.25: Loss = 0.421173
Epoch 9.26: Loss = 0.560532
Epoch 9.27: Loss = 0.470581
Epoch 9.28: Loss = 0.455917
Epoch 9.29: Loss = 0.509659
Epoch 9.30: Loss = 0.589493
Epoch 9.31: Loss = 0.351852
Epoch 9.32: Loss = 0.42189
Epoch 9.33: Loss = 0.355652
Epoch 9.34: Loss = 0.476608
Epoch 9.35: Loss = 0.377167
Epoch 9.36: Loss = 0.478577
Epoch 9.37: Loss = 0.270721
Epoch 9.38: Loss = 0.341812
Epoch 9.39: Loss = 0.319504
Epoch 9.40: Loss = 0.364899
Epoch 9.41: Loss = 0.36673
Epoch 9.42: Loss = 0.651062
Epoch 9.43: Loss = 0.306335
Epoch 9.44: Loss = 0.313354
Epoch 9.45: Loss = 0.362579
Epoch 9.46: Loss = 0.443665
Epoch 9.47: Loss = 0.376587
Epoch 9.48: Loss = 0.442078
Epoch 9.49: Loss = 0.337875
Epoch 9.50: Loss = 0.503082
Epoch 9.51: Loss = 0.293869
Epoch 9.52: Loss = 0.330444
Epoch 9.53: Loss = 0.349518
Epoch 9.54: Loss = 0.536438
Epoch 9.55: Loss = 0.458664
Epoch 9.56: Loss = 0.360962
Epoch 9.57: Loss = 0.346817
Epoch 9.58: Loss = 0.397675
Epoch 9.59: Loss = 0.444611
Epoch 9.60: Loss = 0.495544
Epoch 9.61: Loss = 0.431534
Epoch 9.62: Loss = 0.443192
Epoch 9.63: Loss = 0.600845
Epoch 9.64: Loss = 0.493607
Epoch 9.65: Loss = 0.622299
Epoch 9.66: Loss = 0.375214
Epoch 9.67: Loss = 0.39122
Epoch 9.68: Loss = 0.228973
Epoch 9.69: Loss = 0.342911
Epoch 9.70: Loss = 0.485016
Epoch 9.71: Loss = 0.350296
Epoch 9.72: Loss = 0.285461
Epoch 9.73: Loss = 0.405518
Epoch 9.74: Loss = 0.304443
Epoch 9.75: Loss = 0.672226
Epoch 9.76: Loss = 0.427231
Epoch 9.77: Loss = 0.334503
Epoch 9.78: Loss = 0.433853
Epoch 9.79: Loss = 0.535446
Epoch 9.80: Loss = 0.441437
Epoch 9.81: Loss = 0.333664
Epoch 9.82: Loss = 0.326752
Epoch 9.83: Loss = 0.551315
Epoch 9.84: Loss = 0.360046
Epoch 9.85: Loss = 0.572571
Epoch 9.86: Loss = 0.522598
Epoch 9.87: Loss = 0.265091
Epoch 9.88: Loss = 0.390289
Epoch 9.89: Loss = 0.516617
Epoch 9.90: Loss = 0.327728
Epoch 9.91: Loss = 0.499146
Epoch 9.92: Loss = 0.470734
Epoch 9.93: Loss = 0.579834
Epoch 9.94: Loss = 0.286697
Epoch 9.95: Loss = 0.367859
Epoch 9.96: Loss = 0.435562
Epoch 9.97: Loss = 0.312637
Epoch 9.98: Loss = 0.413055
Epoch 9.99: Loss = 0.497864
Epoch 9.100: Loss = 0.597977
Epoch 9.101: Loss = 0.617111
Epoch 9.102: Loss = 0.414688
Epoch 9.103: Loss = 0.354019
Epoch 9.104: Loss = 0.353851
Epoch 9.105: Loss = 0.48497
Epoch 9.106: Loss = 0.544983
Epoch 9.107: Loss = 0.30751
Epoch 9.108: Loss = 0.469772
Epoch 9.109: Loss = 0.434174
Epoch 9.110: Loss = 0.410416
Epoch 9.111: Loss = 0.320313
Epoch 9.112: Loss = 0.315277
Epoch 9.113: Loss = 0.375397
Epoch 9.114: Loss = 0.319794
Epoch 9.115: Loss = 0.316803
Epoch 9.116: Loss = 0.387634
Epoch 9.117: Loss = 0.215607
Epoch 9.118: Loss = 0.185272
Epoch 9.119: Loss = 0.312088
Epoch 9.120: Loss = 0.330719
TRAIN LOSS = 0.408096
TRAIN ACC = 89.6759 % (53808/60000)
Loss = 0.383041
Loss = 0.493973
Loss = 0.536606
Loss = 0.578278
Loss = 0.616516
Loss = 0.385651
Loss = 0.317734
Loss = 0.679657
Loss = 0.559341
Loss = 0.472366
Loss = 0.172653
Loss = 0.327133
Loss = 0.399536
Loss = 0.379517
Loss = 0.192413
Loss = 0.269836
Loss = 0.233307
Loss = 0.052475
Loss = 0.215439
Loss = 0.594833
TEST LOSS = 0.393015
TEST ACC = 538.08 % (9041/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.375015
Epoch 10.2: Loss = 0.447571
Epoch 10.3: Loss = 0.504028
Epoch 10.4: Loss = 0.307999
Epoch 10.5: Loss = 0.335861
Epoch 10.6: Loss = 0.341324
Epoch 10.7: Loss = 0.318878
Epoch 10.8: Loss = 0.393784
Epoch 10.9: Loss = 0.333908
Epoch 10.10: Loss = 0.382614
Epoch 10.11: Loss = 0.43129
Epoch 10.12: Loss = 0.358536
Epoch 10.13: Loss = 0.250366
Epoch 10.14: Loss = 0.323013
Epoch 10.15: Loss = 0.428314
Epoch 10.16: Loss = 0.409409
Epoch 10.17: Loss = 0.535599
Epoch 10.18: Loss = 0.627441
Epoch 10.19: Loss = 0.435898
Epoch 10.20: Loss = 0.343063
Epoch 10.21: Loss = 0.349762
Epoch 10.22: Loss = 0.297455
Epoch 10.23: Loss = 0.297195
Epoch 10.24: Loss = 0.505081
Epoch 10.25: Loss = 0.441269
Epoch 10.26: Loss = 0.572937
Epoch 10.27: Loss = 0.49324
Epoch 10.28: Loss = 0.457169
Epoch 10.29: Loss = 0.498459
Epoch 10.30: Loss = 0.571014
Epoch 10.31: Loss = 0.362
Epoch 10.32: Loss = 0.447647
Epoch 10.33: Loss = 0.3815
Epoch 10.34: Loss = 0.49469
Epoch 10.35: Loss = 0.356995
Epoch 10.36: Loss = 0.496658
Epoch 10.37: Loss = 0.274872
Epoch 10.38: Loss = 0.344299
Epoch 10.39: Loss = 0.321091
Epoch 10.40: Loss = 0.37674
Epoch 10.41: Loss = 0.374008
Epoch 10.42: Loss = 0.651871
Epoch 10.43: Loss = 0.331375
Epoch 10.44: Loss = 0.321365
Epoch 10.45: Loss = 0.368439
Epoch 10.46: Loss = 0.430832
Epoch 10.47: Loss = 0.387939
Epoch 10.48: Loss = 0.447464
Epoch 10.49: Loss = 0.359146
Epoch 10.50: Loss = 0.507629
Epoch 10.51: Loss = 0.30751
Epoch 10.52: Loss = 0.336807
Epoch 10.53: Loss = 0.370361
Epoch 10.54: Loss = 0.552292
Epoch 10.55: Loss = 0.488098
Epoch 10.56: Loss = 0.367905
Epoch 10.57: Loss = 0.350525
Epoch 10.58: Loss = 0.396118
Epoch 10.59: Loss = 0.444351
Epoch 10.60: Loss = 0.490295
Epoch 10.61: Loss = 0.438721
Epoch 10.62: Loss = 0.47522
Epoch 10.63: Loss = 0.622437
Epoch 10.64: Loss = 0.484894
Epoch 10.65: Loss = 0.624725
Epoch 10.66: Loss = 0.37999
Epoch 10.67: Loss = 0.393677
Epoch 10.68: Loss = 0.221588
Epoch 10.69: Loss = 0.342346
Epoch 10.70: Loss = 0.479355
Epoch 10.71: Loss = 0.390335
Epoch 10.72: Loss = 0.286224
Epoch 10.73: Loss = 0.422073
Epoch 10.74: Loss = 0.308578
Epoch 10.75: Loss = 0.691925
Epoch 10.76: Loss = 0.416412
Epoch 10.77: Loss = 0.337463
Epoch 10.78: Loss = 0.440826
Epoch 10.79: Loss = 0.557465
Epoch 10.80: Loss = 0.491806
Epoch 10.81: Loss = 0.338959
Epoch 10.82: Loss = 0.32225
Epoch 10.83: Loss = 0.553467
Epoch 10.84: Loss = 0.360565
Epoch 10.85: Loss = 0.542709
Epoch 10.86: Loss = 0.528961
Epoch 10.87: Loss = 0.272247
Epoch 10.88: Loss = 0.397751
Epoch 10.89: Loss = 0.47644
Epoch 10.90: Loss = 0.328171
Epoch 10.91: Loss = 0.524612
Epoch 10.92: Loss = 0.441727
Epoch 10.93: Loss = 0.565781
Epoch 10.94: Loss = 0.299774
Epoch 10.95: Loss = 0.376205
Epoch 10.96: Loss = 0.441467
Epoch 10.97: Loss = 0.309601
Epoch 10.98: Loss = 0.387863
Epoch 10.99: Loss = 0.502274
Epoch 10.100: Loss = 0.608231
Epoch 10.101: Loss = 0.622391
Epoch 10.102: Loss = 0.395889
Epoch 10.103: Loss = 0.363556
Epoch 10.104: Loss = 0.354782
Epoch 10.105: Loss = 0.52063
Epoch 10.106: Loss = 0.56163
Epoch 10.107: Loss = 0.308395
Epoch 10.108: Loss = 0.454193
Epoch 10.109: Loss = 0.427292
Epoch 10.110: Loss = 0.417282
Epoch 10.111: Loss = 0.328751
Epoch 10.112: Loss = 0.334625
Epoch 10.113: Loss = 0.366577
Epoch 10.114: Loss = 0.312515
Epoch 10.115: Loss = 0.325409
Epoch 10.116: Loss = 0.401199
Epoch 10.117: Loss = 0.220306
Epoch 10.118: Loss = 0.194534
Epoch 10.119: Loss = 0.322311
Epoch 10.120: Loss = 0.336685
TRAIN LOSS = 0.412186
TRAIN ACC = 89.8163 % (53893/60000)
Loss = 0.38707
Loss = 0.505447
Loss = 0.539917
Loss = 0.593536
Loss = 0.656235
Loss = 0.383789
Loss = 0.326523
Loss = 0.701248
Loss = 0.568176
Loss = 0.476257
Loss = 0.182465
Loss = 0.339142
Loss = 0.423615
Loss = 0.39093
Loss = 0.194839
Loss = 0.292282
Loss = 0.261093
Loss = 0.0570374
Loss = 0.205109
Loss = 0.61264
TEST LOSS = 0.404867
TEST ACC = 538.93 % (9046/10000)
