Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.34798
Epoch 1.2: Loss = 2.26324
Epoch 1.3: Loss = 2.22311
Epoch 1.4: Loss = 2.12698
Epoch 1.5: Loss = 2.08794
Epoch 1.6: Loss = 2.06253
Epoch 1.7: Loss = 1.98247
Epoch 1.8: Loss = 1.89757
Epoch 1.9: Loss = 1.9017
Epoch 1.10: Loss = 1.80136
Epoch 1.11: Loss = 1.8569
Epoch 1.12: Loss = 1.78178
Epoch 1.13: Loss = 1.73792
Epoch 1.14: Loss = 1.70967
Epoch 1.15: Loss = 1.6353
Epoch 1.16: Loss = 1.64001
Epoch 1.17: Loss = 1.60553
Epoch 1.18: Loss = 1.56628
Epoch 1.19: Loss = 1.50307
Epoch 1.20: Loss = 1.53569
Epoch 1.21: Loss = 1.47697
Epoch 1.22: Loss = 1.43082
Epoch 1.23: Loss = 1.40865
Epoch 1.24: Loss = 1.4534
Epoch 1.25: Loss = 1.38376
Epoch 1.26: Loss = 1.3078
Epoch 1.27: Loss = 1.27977
Epoch 1.28: Loss = 1.30072
Epoch 1.29: Loss = 1.27808
Epoch 1.30: Loss = 1.25215
Epoch 1.31: Loss = 1.26437
Epoch 1.32: Loss = 1.23039
Epoch 1.33: Loss = 1.13803
Epoch 1.34: Loss = 1.20555
Epoch 1.35: Loss = 1.22099
Epoch 1.36: Loss = 1.20715
Epoch 1.37: Loss = 1.17218
Epoch 1.38: Loss = 1.16087
Epoch 1.39: Loss = 1.11536
Epoch 1.40: Loss = 1.1171
Epoch 1.41: Loss = 1.15517
Epoch 1.42: Loss = 1.058
Epoch 1.43: Loss = 1.04477
Epoch 1.44: Loss = 1.01831
Epoch 1.45: Loss = 1.06053
Epoch 1.46: Loss = 1.05458
Epoch 1.47: Loss = 1.05005
Epoch 1.48: Loss = 0.991547
Epoch 1.49: Loss = 1.04308
Epoch 1.50: Loss = 0.990982
Epoch 1.51: Loss = 0.948883
Epoch 1.52: Loss = 1.01178
Epoch 1.53: Loss = 1.0081
Epoch 1.54: Loss = 0.890976
Epoch 1.55: Loss = 0.985107
Epoch 1.56: Loss = 0.997314
Epoch 1.57: Loss = 0.969299
Epoch 1.58: Loss = 0.966614
Epoch 1.59: Loss = 0.93042
Epoch 1.60: Loss = 0.967743
Epoch 1.61: Loss = 0.867645
Epoch 1.62: Loss = 0.980804
Epoch 1.63: Loss = 0.82225
Epoch 1.64: Loss = 0.866623
Epoch 1.65: Loss = 0.8862
Epoch 1.66: Loss = 0.900909
Epoch 1.67: Loss = 0.798553
Epoch 1.68: Loss = 0.953217
Epoch 1.69: Loss = 0.893005
Epoch 1.70: Loss = 0.879257
Epoch 1.71: Loss = 0.793701
Epoch 1.72: Loss = 0.854004
Epoch 1.73: Loss = 0.906769
Epoch 1.74: Loss = 0.892838
Epoch 1.75: Loss = 0.832718
Epoch 1.76: Loss = 0.843048
Epoch 1.77: Loss = 0.820679
Epoch 1.78: Loss = 0.848236
Epoch 1.79: Loss = 0.776871
Epoch 1.80: Loss = 0.807327
Epoch 1.81: Loss = 0.790268
Epoch 1.82: Loss = 0.822235
Epoch 1.83: Loss = 0.85231
Epoch 1.84: Loss = 0.829178
Epoch 1.85: Loss = 0.798431
Epoch 1.86: Loss = 0.83905
Epoch 1.87: Loss = 0.845184
Epoch 1.88: Loss = 0.719131
Epoch 1.89: Loss = 0.851746
Epoch 1.90: Loss = 0.789185
Epoch 1.91: Loss = 0.86554
Epoch 1.92: Loss = 0.814453
Epoch 1.93: Loss = 0.809723
Epoch 1.94: Loss = 0.790283
Epoch 1.95: Loss = 0.830017
Epoch 1.96: Loss = 0.754166
Epoch 1.97: Loss = 0.666962
Epoch 1.98: Loss = 0.781448
Epoch 1.99: Loss = 0.784439
Epoch 1.100: Loss = 0.745743
Epoch 1.101: Loss = 0.80365
Epoch 1.102: Loss = 0.80545
Epoch 1.103: Loss = 0.79834
Epoch 1.104: Loss = 0.752411
Epoch 1.105: Loss = 0.717743
Epoch 1.106: Loss = 0.876984
Epoch 1.107: Loss = 0.777267
Epoch 1.108: Loss = 0.780273
Epoch 1.109: Loss = 0.763229
Epoch 1.110: Loss = 0.776764
Epoch 1.111: Loss = 0.690872
Epoch 1.112: Loss = 0.689697
Epoch 1.113: Loss = 0.769821
Epoch 1.114: Loss = 0.781357
Epoch 1.115: Loss = 0.762497
Epoch 1.116: Loss = 0.675919
Epoch 1.117: Loss = 0.823761
Epoch 1.118: Loss = 0.678848
Epoch 1.119: Loss = 0.742798
Epoch 1.120: Loss = 0.710556
TRAIN LOSS = 1.09331
TRAIN ACC = 62.8952 % (37739/60000)
Loss = 0.699219
Loss = 0.808807
Loss = 0.788864
Loss = 0.704605
Loss = 0.713486
Loss = 0.850754
Loss = 0.874695
Loss = 0.822571
Loss = 0.748856
Loss = 0.693054
Loss = 0.825439
Loss = 0.800873
Loss = 0.780518
Loss = 0.788574
Loss = 0.744736
Loss = 0.80983
Loss = 0.732727
Loss = 0.769775
Loss = 0.806381
Loss = 0.750412
TEST LOSS = 0.775709
TEST ACC = 377.39 % (7344/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.750137
Epoch 2.2: Loss = 0.697296
Epoch 2.3: Loss = 0.800552
Epoch 2.4: Loss = 0.682846
Epoch 2.5: Loss = 0.751709
Epoch 2.6: Loss = 0.823563
Epoch 2.7: Loss = 0.7453
Epoch 2.8: Loss = 0.819534
Epoch 2.9: Loss = 0.654434
Epoch 2.10: Loss = 0.554276
Epoch 2.11: Loss = 0.841324
Epoch 2.12: Loss = 0.723633
Epoch 2.13: Loss = 0.732788
Epoch 2.14: Loss = 0.761993
Epoch 2.15: Loss = 0.721909
Epoch 2.16: Loss = 0.808945
Epoch 2.17: Loss = 0.699249
Epoch 2.18: Loss = 0.735596
Epoch 2.19: Loss = 0.713943
Epoch 2.20: Loss = 0.815796
Epoch 2.21: Loss = 0.682022
Epoch 2.22: Loss = 0.618942
Epoch 2.23: Loss = 0.721756
Epoch 2.24: Loss = 0.814514
Epoch 2.25: Loss = 0.717667
Epoch 2.26: Loss = 0.647781
Epoch 2.27: Loss = 0.70076
Epoch 2.28: Loss = 0.742081
Epoch 2.29: Loss = 0.742615
Epoch 2.30: Loss = 0.712906
Epoch 2.31: Loss = 0.811844
Epoch 2.32: Loss = 0.739136
Epoch 2.33: Loss = 0.615768
Epoch 2.34: Loss = 0.804489
Epoch 2.35: Loss = 0.792694
Epoch 2.36: Loss = 0.7966
Epoch 2.37: Loss = 0.775818
Epoch 2.38: Loss = 0.732742
Epoch 2.39: Loss = 0.780045
Epoch 2.40: Loss = 0.740173
Epoch 2.41: Loss = 0.768173
Epoch 2.42: Loss = 0.721817
Epoch 2.43: Loss = 0.732864
Epoch 2.44: Loss = 0.651031
Epoch 2.45: Loss = 0.755051
Epoch 2.46: Loss = 0.794998
Epoch 2.47: Loss = 0.69104
Epoch 2.48: Loss = 0.660294
Epoch 2.49: Loss = 0.772095
Epoch 2.50: Loss = 0.717026
Epoch 2.51: Loss = 0.643158
Epoch 2.52: Loss = 0.753952
Epoch 2.53: Loss = 0.842957
Epoch 2.54: Loss = 0.59613
Epoch 2.55: Loss = 0.761902
Epoch 2.56: Loss = 0.739899
Epoch 2.57: Loss = 0.753662
Epoch 2.58: Loss = 0.731628
Epoch 2.59: Loss = 0.754517
Epoch 2.60: Loss = 0.730804
Epoch 2.61: Loss = 0.682327
Epoch 2.62: Loss = 0.770386
Epoch 2.63: Loss = 0.640533
Epoch 2.64: Loss = 0.632507
Epoch 2.65: Loss = 0.726791
Epoch 2.66: Loss = 0.695404
Epoch 2.67: Loss = 0.641632
Epoch 2.68: Loss = 0.831268
Epoch 2.69: Loss = 0.751877
Epoch 2.70: Loss = 0.753967
Epoch 2.71: Loss = 0.596405
Epoch 2.72: Loss = 0.708862
Epoch 2.73: Loss = 0.780914
Epoch 2.74: Loss = 0.75206
Epoch 2.75: Loss = 0.671371
Epoch 2.76: Loss = 0.715652
Epoch 2.77: Loss = 0.701248
Epoch 2.78: Loss = 0.721069
Epoch 2.79: Loss = 0.673264
Epoch 2.80: Loss = 0.666199
Epoch 2.81: Loss = 0.658508
Epoch 2.82: Loss = 0.652664
Epoch 2.83: Loss = 0.746109
Epoch 2.84: Loss = 0.690369
Epoch 2.85: Loss = 0.712433
Epoch 2.86: Loss = 0.744843
Epoch 2.87: Loss = 0.714096
Epoch 2.88: Loss = 0.609543
Epoch 2.89: Loss = 0.780365
Epoch 2.90: Loss = 0.718414
Epoch 2.91: Loss = 0.789001
Epoch 2.92: Loss = 0.727386
Epoch 2.93: Loss = 0.749496
Epoch 2.94: Loss = 0.675201
Epoch 2.95: Loss = 0.718079
Epoch 2.96: Loss = 0.660538
Epoch 2.97: Loss = 0.594788
Epoch 2.98: Loss = 0.686447
Epoch 2.99: Loss = 0.723801
Epoch 2.100: Loss = 0.667236
Epoch 2.101: Loss = 0.771744
Epoch 2.102: Loss = 0.754898
Epoch 2.103: Loss = 0.720703
Epoch 2.104: Loss = 0.659332
Epoch 2.105: Loss = 0.64679
Epoch 2.106: Loss = 0.807053
Epoch 2.107: Loss = 0.719177
Epoch 2.108: Loss = 0.751099
Epoch 2.109: Loss = 0.737305
Epoch 2.110: Loss = 0.752213
Epoch 2.111: Loss = 0.644897
Epoch 2.112: Loss = 0.648819
Epoch 2.113: Loss = 0.717316
Epoch 2.114: Loss = 0.731369
Epoch 2.115: Loss = 0.694122
Epoch 2.116: Loss = 0.642715
Epoch 2.117: Loss = 0.773926
Epoch 2.118: Loss = 0.621735
Epoch 2.119: Loss = 0.699509
Epoch 2.120: Loss = 0.663239
TRAIN LOSS = 0.719925
TRAIN ACC = 76.0559 % (45636/60000)
Loss = 0.649185
Loss = 0.808456
Loss = 0.726654
Loss = 0.644791
Loss = 0.69191
Loss = 0.848328
Loss = 0.863022
Loss = 0.830841
Loss = 0.725189
Loss = 0.643311
Loss = 0.838913
Loss = 0.793427
Loss = 0.74115
Loss = 0.732544
Loss = 0.703369
Loss = 0.788666
Loss = 0.695221
Loss = 0.767471
Loss = 0.808533
Loss = 0.723328
TEST LOSS = 0.751215
TEST ACC = 456.36 % (7597/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.745209
Epoch 3.2: Loss = 0.669083
Epoch 3.3: Loss = 0.757767
Epoch 3.4: Loss = 0.633636
Epoch 3.5: Loss = 0.707825
Epoch 3.6: Loss = 0.838593
Epoch 3.7: Loss = 0.772339
Epoch 3.8: Loss = 0.796524
Epoch 3.9: Loss = 0.618576
Epoch 3.10: Loss = 0.533813
Epoch 3.11: Loss = 0.865005
Epoch 3.12: Loss = 0.726166
Epoch 3.13: Loss = 0.756897
Epoch 3.14: Loss = 0.729813
Epoch 3.15: Loss = 0.701645
Epoch 3.16: Loss = 0.817413
Epoch 3.17: Loss = 0.668686
Epoch 3.18: Loss = 0.714737
Epoch 3.19: Loss = 0.677307
Epoch 3.20: Loss = 0.835297
Epoch 3.21: Loss = 0.643738
Epoch 3.22: Loss = 0.564896
Epoch 3.23: Loss = 0.73703
Epoch 3.24: Loss = 0.823792
Epoch 3.25: Loss = 0.71843
Epoch 3.26: Loss = 0.600647
Epoch 3.27: Loss = 0.736069
Epoch 3.28: Loss = 0.730453
Epoch 3.29: Loss = 0.758575
Epoch 3.30: Loss = 0.704117
Epoch 3.31: Loss = 0.771362
Epoch 3.32: Loss = 0.696396
Epoch 3.33: Loss = 0.585281
Epoch 3.34: Loss = 0.767334
Epoch 3.35: Loss = 0.772461
Epoch 3.36: Loss = 0.793701
Epoch 3.37: Loss = 0.775024
Epoch 3.38: Loss = 0.735535
Epoch 3.39: Loss = 0.782242
Epoch 3.40: Loss = 0.713211
Epoch 3.41: Loss = 0.76767
Epoch 3.42: Loss = 0.684784
Epoch 3.43: Loss = 0.733841
Epoch 3.44: Loss = 0.609741
Epoch 3.45: Loss = 0.718109
Epoch 3.46: Loss = 0.848267
Epoch 3.47: Loss = 0.67392
Epoch 3.48: Loss = 0.641174
Epoch 3.49: Loss = 0.77594
Epoch 3.50: Loss = 0.731262
Epoch 3.51: Loss = 0.601944
Epoch 3.52: Loss = 0.745377
Epoch 3.53: Loss = 0.830475
Epoch 3.54: Loss = 0.562973
Epoch 3.55: Loss = 0.739044
Epoch 3.56: Loss = 0.726089
Epoch 3.57: Loss = 0.756699
Epoch 3.58: Loss = 0.728973
Epoch 3.59: Loss = 0.764297
Epoch 3.60: Loss = 0.70166
Epoch 3.61: Loss = 0.631775
Epoch 3.62: Loss = 0.748184
Epoch 3.63: Loss = 0.593002
Epoch 3.64: Loss = 0.613968
Epoch 3.65: Loss = 0.705109
Epoch 3.66: Loss = 0.662338
Epoch 3.67: Loss = 0.643188
Epoch 3.68: Loss = 0.847366
Epoch 3.69: Loss = 0.713776
Epoch 3.70: Loss = 0.749588
Epoch 3.71: Loss = 0.592407
Epoch 3.72: Loss = 0.70195
Epoch 3.73: Loss = 0.808212
Epoch 3.74: Loss = 0.716583
Epoch 3.75: Loss = 0.653732
Epoch 3.76: Loss = 0.729416
Epoch 3.77: Loss = 0.697388
Epoch 3.78: Loss = 0.716843
Epoch 3.79: Loss = 0.684601
Epoch 3.80: Loss = 0.617416
Epoch 3.81: Loss = 0.63208
Epoch 3.82: Loss = 0.615356
Epoch 3.83: Loss = 0.747726
Epoch 3.84: Loss = 0.655548
Epoch 3.85: Loss = 0.698898
Epoch 3.86: Loss = 0.731903
Epoch 3.87: Loss = 0.68158
Epoch 3.88: Loss = 0.596054
Epoch 3.89: Loss = 0.765366
Epoch 3.90: Loss = 0.720734
Epoch 3.91: Loss = 0.771759
Epoch 3.92: Loss = 0.717239
Epoch 3.93: Loss = 0.755493
Epoch 3.94: Loss = 0.670105
Epoch 3.95: Loss = 0.715256
Epoch 3.96: Loss = 0.665268
Epoch 3.97: Loss = 0.576004
Epoch 3.98: Loss = 0.676147
Epoch 3.99: Loss = 0.718094
Epoch 3.100: Loss = 0.677322
Epoch 3.101: Loss = 0.766251
Epoch 3.102: Loss = 0.754745
Epoch 3.103: Loss = 0.691162
Epoch 3.104: Loss = 0.624832
Epoch 3.105: Loss = 0.634171
Epoch 3.106: Loss = 0.802078
Epoch 3.107: Loss = 0.706909
Epoch 3.108: Loss = 0.764191
Epoch 3.109: Loss = 0.742111
Epoch 3.110: Loss = 0.724289
Epoch 3.111: Loss = 0.614212
Epoch 3.112: Loss = 0.651779
Epoch 3.113: Loss = 0.707886
Epoch 3.114: Loss = 0.745514
Epoch 3.115: Loss = 0.697708
Epoch 3.116: Loss = 0.607422
Epoch 3.117: Loss = 0.782608
Epoch 3.118: Loss = 0.601242
Epoch 3.119: Loss = 0.699005
Epoch 3.120: Loss = 0.664154
TRAIN LOSS = 0.707642
TRAIN ACC = 78.096 % (46860/60000)
Loss = 0.649796
Loss = 0.772812
Loss = 0.694275
Loss = 0.606461
Loss = 0.666351
Loss = 0.796097
Loss = 0.868149
Loss = 0.824692
Loss = 0.721191
Loss = 0.616684
Loss = 0.82814
Loss = 0.810562
Loss = 0.746048
Loss = 0.740601
Loss = 0.699753
Loss = 0.733948
Loss = 0.700195
Loss = 0.743408
Loss = 0.785538
Loss = 0.701004
TEST LOSS = 0.735285
TEST ACC = 468.599 % (7801/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.735062
Epoch 4.2: Loss = 0.655426
Epoch 4.3: Loss = 0.738617
Epoch 4.4: Loss = 0.626236
Epoch 4.5: Loss = 0.710403
Epoch 4.6: Loss = 0.818253
Epoch 4.7: Loss = 0.724854
Epoch 4.8: Loss = 0.777344
Epoch 4.9: Loss = 0.588776
Epoch 4.10: Loss = 0.557251
Epoch 4.11: Loss = 0.853821
Epoch 4.12: Loss = 0.685028
Epoch 4.13: Loss = 0.747986
Epoch 4.14: Loss = 0.725586
Epoch 4.15: Loss = 0.692352
Epoch 4.16: Loss = 0.79364
Epoch 4.17: Loss = 0.636444
Epoch 4.18: Loss = 0.688034
Epoch 4.19: Loss = 0.669266
Epoch 4.20: Loss = 0.786804
Epoch 4.21: Loss = 0.62323
Epoch 4.22: Loss = 0.525894
Epoch 4.23: Loss = 0.682587
Epoch 4.24: Loss = 0.80896
Epoch 4.25: Loss = 0.653091
Epoch 4.26: Loss = 0.594528
Epoch 4.27: Loss = 0.694702
Epoch 4.28: Loss = 0.714874
Epoch 4.29: Loss = 0.757706
Epoch 4.30: Loss = 0.709335
Epoch 4.31: Loss = 0.809875
Epoch 4.32: Loss = 0.709549
Epoch 4.33: Loss = 0.624435
Epoch 4.34: Loss = 0.784836
Epoch 4.35: Loss = 0.787537
Epoch 4.36: Loss = 0.776031
Epoch 4.37: Loss = 0.784515
Epoch 4.38: Loss = 0.799057
Epoch 4.39: Loss = 0.808609
Epoch 4.40: Loss = 0.734726
Epoch 4.41: Loss = 0.783112
Epoch 4.42: Loss = 0.667648
Epoch 4.43: Loss = 0.76239
Epoch 4.44: Loss = 0.606583
Epoch 4.45: Loss = 0.730637
Epoch 4.46: Loss = 0.84024
Epoch 4.47: Loss = 0.724808
Epoch 4.48: Loss = 0.614014
Epoch 4.49: Loss = 0.769333
Epoch 4.50: Loss = 0.752121
Epoch 4.51: Loss = 0.590561
Epoch 4.52: Loss = 0.747131
Epoch 4.53: Loss = 0.817917
Epoch 4.54: Loss = 0.52504
Epoch 4.55: Loss = 0.735779
Epoch 4.56: Loss = 0.740616
Epoch 4.57: Loss = 0.800095
Epoch 4.58: Loss = 0.711746
Epoch 4.59: Loss = 0.757233
Epoch 4.60: Loss = 0.734634
Epoch 4.61: Loss = 0.657089
Epoch 4.62: Loss = 0.769028
Epoch 4.63: Loss = 0.593796
Epoch 4.64: Loss = 0.575211
Epoch 4.65: Loss = 0.770355
Epoch 4.66: Loss = 0.665344
Epoch 4.67: Loss = 0.687531
Epoch 4.68: Loss = 0.903183
Epoch 4.69: Loss = 0.756454
Epoch 4.70: Loss = 0.781509
Epoch 4.71: Loss = 0.60025
Epoch 4.72: Loss = 0.684097
Epoch 4.73: Loss = 0.802322
Epoch 4.74: Loss = 0.708511
Epoch 4.75: Loss = 0.642517
Epoch 4.76: Loss = 0.693359
Epoch 4.77: Loss = 0.689545
Epoch 4.78: Loss = 0.703415
Epoch 4.79: Loss = 0.691757
Epoch 4.80: Loss = 0.633286
Epoch 4.81: Loss = 0.611938
Epoch 4.82: Loss = 0.623917
Epoch 4.83: Loss = 0.769653
Epoch 4.84: Loss = 0.65802
Epoch 4.85: Loss = 0.703812
Epoch 4.86: Loss = 0.775024
Epoch 4.87: Loss = 0.688568
Epoch 4.88: Loss = 0.598267
Epoch 4.89: Loss = 0.831543
Epoch 4.90: Loss = 0.737198
Epoch 4.91: Loss = 0.797806
Epoch 4.92: Loss = 0.720428
Epoch 4.93: Loss = 0.76326
Epoch 4.94: Loss = 0.676453
Epoch 4.95: Loss = 0.739014
Epoch 4.96: Loss = 0.664703
Epoch 4.97: Loss = 0.616837
Epoch 4.98: Loss = 0.714096
Epoch 4.99: Loss = 0.760864
Epoch 4.100: Loss = 0.685776
Epoch 4.101: Loss = 0.767303
Epoch 4.102: Loss = 0.743591
Epoch 4.103: Loss = 0.689316
Epoch 4.104: Loss = 0.611206
Epoch 4.105: Loss = 0.642944
Epoch 4.106: Loss = 0.813446
Epoch 4.107: Loss = 0.726273
Epoch 4.108: Loss = 0.803619
Epoch 4.109: Loss = 0.747757
Epoch 4.110: Loss = 0.727783
Epoch 4.111: Loss = 0.647751
Epoch 4.112: Loss = 0.725113
Epoch 4.113: Loss = 0.740707
Epoch 4.114: Loss = 0.798721
Epoch 4.115: Loss = 0.726532
Epoch 4.116: Loss = 0.641541
Epoch 4.117: Loss = 0.788803
Epoch 4.118: Loss = 0.605392
Epoch 4.119: Loss = 0.735352
Epoch 4.120: Loss = 0.710983
TRAIN LOSS = 0.712906
TRAIN ACC = 79.0833 % (47453/60000)
Loss = 0.645401
Loss = 0.782883
Loss = 0.69986
Loss = 0.619125
Loss = 0.716309
Loss = 0.829819
Loss = 0.90506
Loss = 0.84761
Loss = 0.740723
Loss = 0.631393
Loss = 0.860535
Loss = 0.851852
Loss = 0.725815
Loss = 0.783035
Loss = 0.745483
Loss = 0.72789
Loss = 0.728592
Loss = 0.740646
Loss = 0.801376
Loss = 0.718475
TEST LOSS = 0.755094
TEST ACC = 474.529 % (7852/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.753983
Epoch 5.2: Loss = 0.676514
Epoch 5.3: Loss = 0.73053
Epoch 5.4: Loss = 0.610413
Epoch 5.5: Loss = 0.745178
Epoch 5.6: Loss = 0.821808
Epoch 5.7: Loss = 0.731369
Epoch 5.8: Loss = 0.819656
Epoch 5.9: Loss = 0.604645
Epoch 5.10: Loss = 0.551422
Epoch 5.11: Loss = 0.87207
Epoch 5.12: Loss = 0.711716
Epoch 5.13: Loss = 0.756927
Epoch 5.14: Loss = 0.750946
Epoch 5.15: Loss = 0.732315
Epoch 5.16: Loss = 0.821121
Epoch 5.17: Loss = 0.61882
Epoch 5.18: Loss = 0.735275
Epoch 5.19: Loss = 0.640793
Epoch 5.20: Loss = 0.816086
Epoch 5.21: Loss = 0.643799
Epoch 5.22: Loss = 0.539307
Epoch 5.23: Loss = 0.695984
Epoch 5.24: Loss = 0.807281
Epoch 5.25: Loss = 0.675476
Epoch 5.26: Loss = 0.588837
Epoch 5.27: Loss = 0.707489
Epoch 5.28: Loss = 0.730942
Epoch 5.29: Loss = 0.805542
Epoch 5.30: Loss = 0.733505
Epoch 5.31: Loss = 0.826477
Epoch 5.32: Loss = 0.668793
Epoch 5.33: Loss = 0.611694
Epoch 5.34: Loss = 0.773132
Epoch 5.35: Loss = 0.761887
Epoch 5.36: Loss = 0.785431
Epoch 5.37: Loss = 0.76442
Epoch 5.38: Loss = 0.742462
Epoch 5.39: Loss = 0.7948
Epoch 5.40: Loss = 0.720367
Epoch 5.41: Loss = 0.721619
Epoch 5.42: Loss = 0.664139
Epoch 5.43: Loss = 0.74939
Epoch 5.44: Loss = 0.570602
Epoch 5.45: Loss = 0.698013
Epoch 5.46: Loss = 0.834457
Epoch 5.47: Loss = 0.721603
Epoch 5.48: Loss = 0.600616
Epoch 5.49: Loss = 0.749481
Epoch 5.50: Loss = 0.734589
Epoch 5.51: Loss = 0.560059
Epoch 5.52: Loss = 0.711319
Epoch 5.53: Loss = 0.778122
Epoch 5.54: Loss = 0.533417
Epoch 5.55: Loss = 0.711334
Epoch 5.56: Loss = 0.746338
Epoch 5.57: Loss = 0.73201
Epoch 5.58: Loss = 0.662643
Epoch 5.59: Loss = 0.759872
Epoch 5.60: Loss = 0.693802
Epoch 5.61: Loss = 0.643341
Epoch 5.62: Loss = 0.737183
Epoch 5.63: Loss = 0.593079
Epoch 5.64: Loss = 0.549683
Epoch 5.65: Loss = 0.793182
Epoch 5.66: Loss = 0.647522
Epoch 5.67: Loss = 0.668228
Epoch 5.68: Loss = 0.924286
Epoch 5.69: Loss = 0.739258
Epoch 5.70: Loss = 0.777603
Epoch 5.71: Loss = 0.613464
Epoch 5.72: Loss = 0.696777
Epoch 5.73: Loss = 0.7883
Epoch 5.74: Loss = 0.723831
Epoch 5.75: Loss = 0.654739
Epoch 5.76: Loss = 0.712784
Epoch 5.77: Loss = 0.697281
Epoch 5.78: Loss = 0.729782
Epoch 5.79: Loss = 0.686508
Epoch 5.80: Loss = 0.632492
Epoch 5.81: Loss = 0.620071
Epoch 5.82: Loss = 0.636475
Epoch 5.83: Loss = 0.725418
Epoch 5.84: Loss = 0.666229
Epoch 5.85: Loss = 0.707565
Epoch 5.86: Loss = 0.765244
Epoch 5.87: Loss = 0.660828
Epoch 5.88: Loss = 0.625702
Epoch 5.89: Loss = 0.782211
Epoch 5.90: Loss = 0.74205
Epoch 5.91: Loss = 0.78006
Epoch 5.92: Loss = 0.722885
Epoch 5.93: Loss = 0.768402
Epoch 5.94: Loss = 0.680557
Epoch 5.95: Loss = 0.740173
Epoch 5.96: Loss = 0.681976
Epoch 5.97: Loss = 0.610733
Epoch 5.98: Loss = 0.712189
Epoch 5.99: Loss = 0.781998
Epoch 5.100: Loss = 0.695572
Epoch 5.101: Loss = 0.752243
Epoch 5.102: Loss = 0.688705
Epoch 5.103: Loss = 0.661087
Epoch 5.104: Loss = 0.612671
Epoch 5.105: Loss = 0.627243
Epoch 5.106: Loss = 0.824753
Epoch 5.107: Loss = 0.737152
Epoch 5.108: Loss = 0.792435
Epoch 5.109: Loss = 0.733551
Epoch 5.110: Loss = 0.730942
Epoch 5.111: Loss = 0.636169
Epoch 5.112: Loss = 0.718109
Epoch 5.113: Loss = 0.714325
Epoch 5.114: Loss = 0.806946
Epoch 5.115: Loss = 0.707001
Epoch 5.116: Loss = 0.660629
Epoch 5.117: Loss = 0.805023
Epoch 5.118: Loss = 0.611191
Epoch 5.119: Loss = 0.742996
Epoch 5.120: Loss = 0.689484
TRAIN LOSS = 0.709854
TRAIN ACC = 79.6829 % (47812/60000)
Loss = 0.644028
Loss = 0.769455
Loss = 0.713867
Loss = 0.646515
Loss = 0.697723
Loss = 0.854187
Loss = 0.894623
Loss = 0.828354
Loss = 0.748459
Loss = 0.628418
Loss = 0.878876
Loss = 0.849136
Loss = 0.70845
Loss = 0.766953
Loss = 0.737915
Loss = 0.717255
Loss = 0.721313
Loss = 0.737701
Loss = 0.799728
Loss = 0.727386
TEST LOSS = 0.753517
TEST ACC = 478.119 % (7887/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.770264
Epoch 6.2: Loss = 0.661636
Epoch 6.3: Loss = 0.745697
Epoch 6.4: Loss = 0.61438
Epoch 6.5: Loss = 0.737091
Epoch 6.6: Loss = 0.828339
Epoch 6.7: Loss = 0.714203
Epoch 6.8: Loss = 0.76004
Epoch 6.9: Loss = 0.559616
Epoch 6.10: Loss = 0.511887
Epoch 6.11: Loss = 0.842346
Epoch 6.12: Loss = 0.700363
Epoch 6.13: Loss = 0.772751
Epoch 6.14: Loss = 0.717514
Epoch 6.15: Loss = 0.766022
Epoch 6.16: Loss = 0.819092
Epoch 6.17: Loss = 0.622559
Epoch 6.18: Loss = 0.764557
Epoch 6.19: Loss = 0.632324
Epoch 6.20: Loss = 0.846069
Epoch 6.21: Loss = 0.610825
Epoch 6.22: Loss = 0.52002
Epoch 6.23: Loss = 0.658157
Epoch 6.24: Loss = 0.80069
Epoch 6.25: Loss = 0.738388
Epoch 6.26: Loss = 0.611252
Epoch 6.27: Loss = 0.679367
Epoch 6.28: Loss = 0.746323
Epoch 6.29: Loss = 0.765137
Epoch 6.30: Loss = 0.747849
Epoch 6.31: Loss = 0.80864
Epoch 6.32: Loss = 0.671036
Epoch 6.33: Loss = 0.608795
Epoch 6.34: Loss = 0.763641
Epoch 6.35: Loss = 0.780289
Epoch 6.36: Loss = 0.759567
Epoch 6.37: Loss = 0.791687
Epoch 6.38: Loss = 0.750519
Epoch 6.39: Loss = 0.815903
Epoch 6.40: Loss = 0.705246
Epoch 6.41: Loss = 0.775269
Epoch 6.42: Loss = 0.674026
Epoch 6.43: Loss = 0.734573
Epoch 6.44: Loss = 0.583405
Epoch 6.45: Loss = 0.690125
Epoch 6.46: Loss = 0.807968
Epoch 6.47: Loss = 0.704712
Epoch 6.48: Loss = 0.592331
Epoch 6.49: Loss = 0.71521
Epoch 6.50: Loss = 0.752426
Epoch 6.51: Loss = 0.56395
Epoch 6.52: Loss = 0.727783
Epoch 6.53: Loss = 0.773743
Epoch 6.54: Loss = 0.544922
Epoch 6.55: Loss = 0.716568
Epoch 6.56: Loss = 0.785797
Epoch 6.57: Loss = 0.792282
Epoch 6.58: Loss = 0.674362
Epoch 6.59: Loss = 0.796799
Epoch 6.60: Loss = 0.688583
Epoch 6.61: Loss = 0.618347
Epoch 6.62: Loss = 0.716522
Epoch 6.63: Loss = 0.595123
Epoch 6.64: Loss = 0.554749
Epoch 6.65: Loss = 0.764847
Epoch 6.66: Loss = 0.632767
Epoch 6.67: Loss = 0.645355
Epoch 6.68: Loss = 0.945801
Epoch 6.69: Loss = 0.746384
Epoch 6.70: Loss = 0.784668
Epoch 6.71: Loss = 0.571655
Epoch 6.72: Loss = 0.702301
Epoch 6.73: Loss = 0.815598
Epoch 6.74: Loss = 0.674042
Epoch 6.75: Loss = 0.671722
Epoch 6.76: Loss = 0.711807
Epoch 6.77: Loss = 0.707184
Epoch 6.78: Loss = 0.693863
Epoch 6.79: Loss = 0.67601
Epoch 6.80: Loss = 0.64801
Epoch 6.81: Loss = 0.634644
Epoch 6.82: Loss = 0.685471
Epoch 6.83: Loss = 0.743561
Epoch 6.84: Loss = 0.68045
Epoch 6.85: Loss = 0.744308
Epoch 6.86: Loss = 0.72757
Epoch 6.87: Loss = 0.683807
Epoch 6.88: Loss = 0.635086
Epoch 6.89: Loss = 0.795181
Epoch 6.90: Loss = 0.766815
Epoch 6.91: Loss = 0.751068
Epoch 6.92: Loss = 0.684723
Epoch 6.93: Loss = 0.763977
Epoch 6.94: Loss = 0.666885
Epoch 6.95: Loss = 0.759521
Epoch 6.96: Loss = 0.665848
Epoch 6.97: Loss = 0.610062
Epoch 6.98: Loss = 0.740356
Epoch 6.99: Loss = 0.719299
Epoch 6.100: Loss = 0.69899
Epoch 6.101: Loss = 0.732193
Epoch 6.102: Loss = 0.730545
Epoch 6.103: Loss = 0.681122
Epoch 6.104: Loss = 0.646545
Epoch 6.105: Loss = 0.606598
Epoch 6.106: Loss = 0.767761
Epoch 6.107: Loss = 0.756821
Epoch 6.108: Loss = 0.80191
Epoch 6.109: Loss = 0.720291
Epoch 6.110: Loss = 0.71582
Epoch 6.111: Loss = 0.595627
Epoch 6.112: Loss = 0.707397
Epoch 6.113: Loss = 0.758286
Epoch 6.114: Loss = 0.832535
Epoch 6.115: Loss = 0.708084
Epoch 6.116: Loss = 0.642792
Epoch 6.117: Loss = 0.793091
Epoch 6.118: Loss = 0.582001
Epoch 6.119: Loss = 0.710693
Epoch 6.120: Loss = 0.684662
TRAIN LOSS = 0.708542
TRAIN ACC = 79.9408 % (47967/60000)
Loss = 0.643448
Loss = 0.797562
Loss = 0.740479
Loss = 0.632584
Loss = 0.71138
Loss = 0.837402
Loss = 0.894226
Loss = 0.835297
Loss = 0.749847
Loss = 0.666626
Loss = 0.900833
Loss = 0.894119
Loss = 0.774536
Loss = 0.78421
Loss = 0.768707
Loss = 0.720688
Loss = 0.728561
Loss = 0.777924
Loss = 0.821304
Loss = 0.720871
TEST LOSS = 0.77003
TEST ACC = 479.669 % (7903/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.753021
Epoch 7.2: Loss = 0.664948
Epoch 7.3: Loss = 0.748032
Epoch 7.4: Loss = 0.620132
Epoch 7.5: Loss = 0.725983
Epoch 7.6: Loss = 0.798843
Epoch 7.7: Loss = 0.73912
Epoch 7.8: Loss = 0.797348
Epoch 7.9: Loss = 0.590134
Epoch 7.10: Loss = 0.523605
Epoch 7.11: Loss = 0.827637
Epoch 7.12: Loss = 0.686661
Epoch 7.13: Loss = 0.765778
Epoch 7.14: Loss = 0.702255
Epoch 7.15: Loss = 0.772522
Epoch 7.16: Loss = 0.781403
Epoch 7.17: Loss = 0.620605
Epoch 7.18: Loss = 0.752335
Epoch 7.19: Loss = 0.621368
Epoch 7.20: Loss = 0.845367
Epoch 7.21: Loss = 0.654785
Epoch 7.22: Loss = 0.513474
Epoch 7.23: Loss = 0.705566
Epoch 7.24: Loss = 0.807678
Epoch 7.25: Loss = 0.689301
Epoch 7.26: Loss = 0.591568
Epoch 7.27: Loss = 0.655228
Epoch 7.28: Loss = 0.709167
Epoch 7.29: Loss = 0.720352
Epoch 7.30: Loss = 0.749557
Epoch 7.31: Loss = 0.806915
Epoch 7.32: Loss = 0.657959
Epoch 7.33: Loss = 0.593903
Epoch 7.34: Loss = 0.790756
Epoch 7.35: Loss = 0.774994
Epoch 7.36: Loss = 0.756287
Epoch 7.37: Loss = 0.780838
Epoch 7.38: Loss = 0.73793
Epoch 7.39: Loss = 0.797928
Epoch 7.40: Loss = 0.712845
Epoch 7.41: Loss = 0.802078
Epoch 7.42: Loss = 0.681305
Epoch 7.43: Loss = 0.760529
Epoch 7.44: Loss = 0.570206
Epoch 7.45: Loss = 0.728104
Epoch 7.46: Loss = 0.800476
Epoch 7.47: Loss = 0.684525
Epoch 7.48: Loss = 0.601059
Epoch 7.49: Loss = 0.718964
Epoch 7.50: Loss = 0.731461
Epoch 7.51: Loss = 0.569244
Epoch 7.52: Loss = 0.724564
Epoch 7.53: Loss = 0.794342
Epoch 7.54: Loss = 0.542862
Epoch 7.55: Loss = 0.702118
Epoch 7.56: Loss = 0.787521
Epoch 7.57: Loss = 0.775726
Epoch 7.58: Loss = 0.682922
Epoch 7.59: Loss = 0.852692
Epoch 7.60: Loss = 0.753952
Epoch 7.61: Loss = 0.631531
Epoch 7.62: Loss = 0.75116
Epoch 7.63: Loss = 0.626099
Epoch 7.64: Loss = 0.588135
Epoch 7.65: Loss = 0.791275
Epoch 7.66: Loss = 0.621399
Epoch 7.67: Loss = 0.654297
Epoch 7.68: Loss = 0.984467
Epoch 7.69: Loss = 0.74588
Epoch 7.70: Loss = 0.753616
Epoch 7.71: Loss = 0.574326
Epoch 7.72: Loss = 0.715591
Epoch 7.73: Loss = 0.825455
Epoch 7.74: Loss = 0.704071
Epoch 7.75: Loss = 0.685303
Epoch 7.76: Loss = 0.703171
Epoch 7.77: Loss = 0.71553
Epoch 7.78: Loss = 0.668182
Epoch 7.79: Loss = 0.661331
Epoch 7.80: Loss = 0.694275
Epoch 7.81: Loss = 0.651337
Epoch 7.82: Loss = 0.684326
Epoch 7.83: Loss = 0.771637
Epoch 7.84: Loss = 0.659714
Epoch 7.85: Loss = 0.714233
Epoch 7.86: Loss = 0.737976
Epoch 7.87: Loss = 0.665924
Epoch 7.88: Loss = 0.671738
Epoch 7.89: Loss = 0.867279
Epoch 7.90: Loss = 0.750656
Epoch 7.91: Loss = 0.757416
Epoch 7.92: Loss = 0.706421
Epoch 7.93: Loss = 0.72226
Epoch 7.94: Loss = 0.690643
Epoch 7.95: Loss = 0.713974
Epoch 7.96: Loss = 0.686935
Epoch 7.97: Loss = 0.58429
Epoch 7.98: Loss = 0.712646
Epoch 7.99: Loss = 0.769791
Epoch 7.100: Loss = 0.721451
Epoch 7.101: Loss = 0.748306
Epoch 7.102: Loss = 0.732025
Epoch 7.103: Loss = 0.684006
Epoch 7.104: Loss = 0.64241
Epoch 7.105: Loss = 0.63501
Epoch 7.106: Loss = 0.813812
Epoch 7.107: Loss = 0.740646
Epoch 7.108: Loss = 0.833847
Epoch 7.109: Loss = 0.719818
Epoch 7.110: Loss = 0.758316
Epoch 7.111: Loss = 0.626205
Epoch 7.112: Loss = 0.696732
Epoch 7.113: Loss = 0.757233
Epoch 7.114: Loss = 0.826721
Epoch 7.115: Loss = 0.685043
Epoch 7.116: Loss = 0.655121
Epoch 7.117: Loss = 0.800415
Epoch 7.118: Loss = 0.628113
Epoch 7.119: Loss = 0.730972
Epoch 7.120: Loss = 0.700104
TRAIN LOSS = 0.712997
TRAIN ACC = 80.278 % (48169/60000)
Loss = 0.687149
Loss = 0.783829
Loss = 0.769974
Loss = 0.669907
Loss = 0.728195
Loss = 0.845932
Loss = 0.924698
Loss = 0.848663
Loss = 0.787918
Loss = 0.737045
Loss = 0.921494
Loss = 0.940536
Loss = 0.769424
Loss = 0.784409
Loss = 0.768478
Loss = 0.736435
Loss = 0.753693
Loss = 0.776672
Loss = 0.833786
Loss = 0.750031
TEST LOSS = 0.790913
TEST ACC = 481.689 % (7918/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.772308
Epoch 8.2: Loss = 0.676254
Epoch 8.3: Loss = 0.774307
Epoch 8.4: Loss = 0.629135
Epoch 8.5: Loss = 0.743576
Epoch 8.6: Loss = 0.860519
Epoch 8.7: Loss = 0.735138
Epoch 8.8: Loss = 0.797424
Epoch 8.9: Loss = 0.572433
Epoch 8.10: Loss = 0.535889
Epoch 8.11: Loss = 0.807953
Epoch 8.12: Loss = 0.667526
Epoch 8.13: Loss = 0.757278
Epoch 8.14: Loss = 0.701508
Epoch 8.15: Loss = 0.757019
Epoch 8.16: Loss = 0.77272
Epoch 8.17: Loss = 0.671005
Epoch 8.18: Loss = 0.747818
Epoch 8.19: Loss = 0.617157
Epoch 8.20: Loss = 0.858719
Epoch 8.21: Loss = 0.626709
Epoch 8.22: Loss = 0.557709
Epoch 8.23: Loss = 0.707794
Epoch 8.24: Loss = 0.785599
Epoch 8.25: Loss = 0.740112
Epoch 8.26: Loss = 0.60759
Epoch 8.27: Loss = 0.71051
Epoch 8.28: Loss = 0.742432
Epoch 8.29: Loss = 0.762619
Epoch 8.30: Loss = 0.771683
Epoch 8.31: Loss = 0.853775
Epoch 8.32: Loss = 0.69165
Epoch 8.33: Loss = 0.620361
Epoch 8.34: Loss = 0.808334
Epoch 8.35: Loss = 0.778748
Epoch 8.36: Loss = 0.764221
Epoch 8.37: Loss = 0.85495
Epoch 8.38: Loss = 0.72551
Epoch 8.39: Loss = 0.819946
Epoch 8.40: Loss = 0.704926
Epoch 8.41: Loss = 0.748245
Epoch 8.42: Loss = 0.693344
Epoch 8.43: Loss = 0.773376
Epoch 8.44: Loss = 0.564575
Epoch 8.45: Loss = 0.736877
Epoch 8.46: Loss = 0.833603
Epoch 8.47: Loss = 0.674072
Epoch 8.48: Loss = 0.642914
Epoch 8.49: Loss = 0.689407
Epoch 8.50: Loss = 0.721939
Epoch 8.51: Loss = 0.551926
Epoch 8.52: Loss = 0.730087
Epoch 8.53: Loss = 0.808746
Epoch 8.54: Loss = 0.54332
Epoch 8.55: Loss = 0.74527
Epoch 8.56: Loss = 0.78006
Epoch 8.57: Loss = 0.790268
Epoch 8.58: Loss = 0.709198
Epoch 8.59: Loss = 0.86647
Epoch 8.60: Loss = 0.733719
Epoch 8.61: Loss = 0.66037
Epoch 8.62: Loss = 0.744629
Epoch 8.63: Loss = 0.634247
Epoch 8.64: Loss = 0.615311
Epoch 8.65: Loss = 0.786865
Epoch 8.66: Loss = 0.661484
Epoch 8.67: Loss = 0.629227
Epoch 8.68: Loss = 0.977097
Epoch 8.69: Loss = 0.738434
Epoch 8.70: Loss = 0.773468
Epoch 8.71: Loss = 0.598907
Epoch 8.72: Loss = 0.771729
Epoch 8.73: Loss = 0.81987
Epoch 8.74: Loss = 0.697754
Epoch 8.75: Loss = 0.66806
Epoch 8.76: Loss = 0.704239
Epoch 8.77: Loss = 0.680969
Epoch 8.78: Loss = 0.679138
Epoch 8.79: Loss = 0.661575
Epoch 8.80: Loss = 0.671951
Epoch 8.81: Loss = 0.657181
Epoch 8.82: Loss = 0.745239
Epoch 8.83: Loss = 0.753662
Epoch 8.84: Loss = 0.656464
Epoch 8.85: Loss = 0.700974
Epoch 8.86: Loss = 0.719421
Epoch 8.87: Loss = 0.69136
Epoch 8.88: Loss = 0.656982
Epoch 8.89: Loss = 0.814758
Epoch 8.90: Loss = 0.785065
Epoch 8.91: Loss = 0.774506
Epoch 8.92: Loss = 0.718018
Epoch 8.93: Loss = 0.70639
Epoch 8.94: Loss = 0.69516
Epoch 8.95: Loss = 0.739639
Epoch 8.96: Loss = 0.682877
Epoch 8.97: Loss = 0.574707
Epoch 8.98: Loss = 0.779358
Epoch 8.99: Loss = 0.803513
Epoch 8.100: Loss = 0.707092
Epoch 8.101: Loss = 0.740051
Epoch 8.102: Loss = 0.722458
Epoch 8.103: Loss = 0.69252
Epoch 8.104: Loss = 0.645294
Epoch 8.105: Loss = 0.675705
Epoch 8.106: Loss = 0.80719
Epoch 8.107: Loss = 0.771622
Epoch 8.108: Loss = 0.815475
Epoch 8.109: Loss = 0.744843
Epoch 8.110: Loss = 0.727615
Epoch 8.111: Loss = 0.606583
Epoch 8.112: Loss = 0.701309
Epoch 8.113: Loss = 0.741577
Epoch 8.114: Loss = 0.81572
Epoch 8.115: Loss = 0.693832
Epoch 8.116: Loss = 0.646301
Epoch 8.117: Loss = 0.829895
Epoch 8.118: Loss = 0.623093
Epoch 8.119: Loss = 0.739166
Epoch 8.120: Loss = 0.650757
TRAIN LOSS = 0.719666
TRAIN ACC = 80.3101 % (48188/60000)
Loss = 0.665619
Loss = 0.772476
Loss = 0.788147
Loss = 0.636734
Loss = 0.696442
Loss = 0.924088
Loss = 0.982819
Loss = 0.84079
Loss = 0.78212
Loss = 0.719101
Loss = 0.984756
Loss = 0.875397
Loss = 0.766785
Loss = 0.811874
Loss = 0.725983
Loss = 0.741302
Loss = 0.78331
Loss = 0.790329
Loss = 0.843292
Loss = 0.755661
TEST LOSS = 0.794351
TEST ACC = 481.879 % (7896/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.777451
Epoch 9.2: Loss = 0.664597
Epoch 9.3: Loss = 0.737061
Epoch 9.4: Loss = 0.587494
Epoch 9.5: Loss = 0.727509
Epoch 9.6: Loss = 0.841919
Epoch 9.7: Loss = 0.749344
Epoch 9.8: Loss = 0.807114
Epoch 9.9: Loss = 0.573151
Epoch 9.10: Loss = 0.53096
Epoch 9.11: Loss = 0.866272
Epoch 9.12: Loss = 0.701462
Epoch 9.13: Loss = 0.772812
Epoch 9.14: Loss = 0.738586
Epoch 9.15: Loss = 0.81839
Epoch 9.16: Loss = 0.77977
Epoch 9.17: Loss = 0.687454
Epoch 9.18: Loss = 0.744354
Epoch 9.19: Loss = 0.648941
Epoch 9.20: Loss = 0.888779
Epoch 9.21: Loss = 0.622894
Epoch 9.22: Loss = 0.535858
Epoch 9.23: Loss = 0.76326
Epoch 9.24: Loss = 0.870178
Epoch 9.25: Loss = 0.734604
Epoch 9.26: Loss = 0.608658
Epoch 9.27: Loss = 0.694702
Epoch 9.28: Loss = 0.740784
Epoch 9.29: Loss = 0.751282
Epoch 9.30: Loss = 0.791611
Epoch 9.31: Loss = 0.879578
Epoch 9.32: Loss = 0.728195
Epoch 9.33: Loss = 0.609497
Epoch 9.34: Loss = 0.820343
Epoch 9.35: Loss = 0.829712
Epoch 9.36: Loss = 0.771652
Epoch 9.37: Loss = 0.854126
Epoch 9.38: Loss = 0.860657
Epoch 9.39: Loss = 0.864655
Epoch 9.40: Loss = 0.880173
Epoch 9.41: Loss = 0.805939
Epoch 9.42: Loss = 0.728455
Epoch 9.43: Loss = 0.780548
Epoch 9.44: Loss = 0.573273
Epoch 9.45: Loss = 0.749084
Epoch 9.46: Loss = 0.820374
Epoch 9.47: Loss = 0.750931
Epoch 9.48: Loss = 0.639282
Epoch 9.49: Loss = 0.770889
Epoch 9.50: Loss = 0.751434
Epoch 9.51: Loss = 0.615753
Epoch 9.52: Loss = 0.732834
Epoch 9.53: Loss = 0.791794
Epoch 9.54: Loss = 0.552124
Epoch 9.55: Loss = 2213.87
Epoch 9.56: Loss = 917.109
Epoch 9.57: Loss = 328.374
Epoch 9.58: Loss = 524.89
Epoch 9.59: Loss = -220.095
Epoch 9.60: Loss = -150.526
Epoch 9.61: Loss = 963.231
Epoch 9.62: Loss = 743.905
Epoch 9.63: Loss = 1799.11
Epoch 9.64: Loss = 124.986
Epoch 9.65: Loss = -466.698
Epoch 9.66: Loss = -644.101
Epoch 9.67: Loss = -2088.8
Epoch 9.68: Loss = 814.033
Epoch 9.69: Loss = 1598.46
Epoch 9.70: Loss = -267.914
Epoch 9.71: Loss = -1373.13
Epoch 9.72: Loss = -1148.98
Epoch 9.73: Loss = -1112.69
Epoch 9.74: Loss = -664.263
Epoch 9.75: Loss = -1449.77
Epoch 9.76: Loss = -295.972
Epoch 9.77: Loss = 395.68
Epoch 9.78: Loss = 248.047
Epoch 9.79: Loss = -2002.59
Epoch 9.80: Loss = 1120.02
Epoch 9.81: Loss = -288.915
Epoch 9.82: Loss = 56.5821
Epoch 9.83: Loss = 368.355
Epoch 9.84: Loss = -823.14
Epoch 9.85: Loss = 439.699
Epoch 9.86: Loss = -1750.01
Epoch 9.87: Loss = -2353.28
Epoch 9.88: Loss = 2009.82
Epoch 9.89: Loss = 638.603
Epoch 9.90: Loss = 1163.99
Epoch 9.91: Loss = 444.546
Epoch 9.92: Loss = 297.223
Epoch 9.93: Loss = 326.554
Epoch 9.94: Loss = -64.2997
Epoch 9.95: Loss = 710.098
Epoch 9.96: Loss = 2038.98
Epoch 9.97: Loss = -3310.78
Epoch 9.98: Loss = 1393.19
Epoch 9.99: Loss = -332.228
Epoch 9.100: Loss = -614.789
Epoch 9.101: Loss = -327.38
Epoch 9.102: Loss = 974.497
Epoch 9.103: Loss = -563.595
Epoch 9.104: Loss = -215.3
Epoch 9.105: Loss = 124.035
Epoch 9.106: Loss = -251.326
Epoch 9.107: Loss = -415.83
Epoch 9.108: Loss = 1176.96
Epoch 9.109: Loss = -695.801
Epoch 9.110: Loss = -1530.03
Epoch 9.111: Loss = -2534.29
Epoch 9.112: Loss = -1589.41
Epoch 9.113: Loss = -1412.37
Epoch 9.114: Loss = -644.132
Epoch 9.115: Loss = 342.197
Epoch 9.116: Loss = -1147.79
Epoch 9.117: Loss = 992.259
Epoch 9.118: Loss = -464.034
Epoch 9.119: Loss = 628.46
Epoch 9.120: Loss = -466.688
TRAIN LOSS = -64.3615
TRAIN ACC = 41.5955 % (24959/60000)
Loss = -861.427
Loss = -1628.47
Loss = 935.484
Loss = -1136.05
Loss = -135.579
Loss = 1180.88
Loss = -778.301
Loss = -503.076
Loss = 820.351
Loss = -1053.14
Loss = -182.232
Loss = -527.731
Loss = 762.727
Loss = 2990.28
Loss = -1016.01
Loss = 462.594
Loss = -1441.2
Loss = -275.634
Loss = -342.393
Loss = 2456.31
TEST LOSS = -0.523337
TEST ACC = 249.59 % (940/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = -750.426
Epoch 10.2: Loss = -883.322
Epoch 10.3: Loss = -1894.62
Epoch 10.4: Loss = 652.285
Epoch 10.5: Loss = -638.862
Epoch 10.6: Loss = 198.703
Epoch 10.7: Loss = -1565.69
Epoch 10.8: Loss = 565.352
Epoch 10.9: Loss = -1316.54
Epoch 10.10: Loss = -37.6783
Epoch 10.11: Loss = 1195.92
Epoch 10.12: Loss = -228.408
Epoch 10.13: Loss = -316.999
Epoch 10.14: Loss = 135.675
Epoch 10.15: Loss = 898.343
Epoch 10.16: Loss = 196.304
Epoch 10.17: Loss = -716.373
Epoch 10.18: Loss = 1087.88
Epoch 10.19: Loss = -738.518
Epoch 10.20: Loss = 19.5914
Epoch 10.21: Loss = 445.974
Epoch 10.22: Loss = 456.756
Epoch 10.23: Loss = -394.571
Epoch 10.24: Loss = 886.838
Epoch 10.25: Loss = 1033.23
Epoch 10.26: Loss = -421.978
Epoch 10.27: Loss = 848.015
Epoch 10.28: Loss = 209.672
Epoch 10.29: Loss = 1034
Epoch 10.30: Loss = 2002.61
Epoch 10.31: Loss = 1992.92
Epoch 10.32: Loss = -556.195
Epoch 10.33: Loss = -207.855
Epoch 10.34: Loss = -1768.99
Epoch 10.35: Loss = -1911.91
Epoch 10.36: Loss = 2352.96
Epoch 10.37: Loss = 2095.27
Epoch 10.38: Loss = 64.7356
Epoch 10.39: Loss = 1163.28
Epoch 10.40: Loss = -1970.87
Epoch 10.41: Loss = -1833.86
Epoch 10.42: Loss = -2235.01
Epoch 10.43: Loss = -551.024
Epoch 10.44: Loss = -2030.23
Epoch 10.45: Loss = -94.0788
Epoch 10.46: Loss = -8.89491
Epoch 10.47: Loss = -886.075
Epoch 10.48: Loss = -1476.01
Epoch 10.49: Loss = -255.06
Epoch 10.50: Loss = 1007.41
Epoch 10.51: Loss = 472.878
Epoch 10.52: Loss = -191.057
Epoch 10.53: Loss = -947.723
Epoch 10.54: Loss = 1458.2
Epoch 10.55: Loss = -2332.98
Epoch 10.56: Loss = 2413.93
Epoch 10.57: Loss = -213.706
Epoch 10.58: Loss = 907.21
Epoch 10.59: Loss = -76.5675
Epoch 10.60: Loss = 1736.95
Epoch 10.61: Loss = 422.099
Epoch 10.62: Loss = 824.418
Epoch 10.63: Loss = -2473.12
Epoch 10.64: Loss = 577.145
Epoch 10.65: Loss = -275.629
Epoch 10.66: Loss = 333.323
Epoch 10.67: Loss = -417.553
Epoch 10.68: Loss = -1248.48
Epoch 10.69: Loss = 1272.96
Epoch 10.70: Loss = -445.866
Epoch 10.71: Loss = 865.06
Epoch 10.72: Loss = 611.137
Epoch 10.73: Loss = -1454.55
Epoch 10.74: Loss = -538.449
Epoch 10.75: Loss = 2198.25
Epoch 10.76: Loss = -1094.71
Epoch 10.77: Loss = -328.941
Epoch 10.78: Loss = -412.268
Epoch 10.79: Loss = -966.543
Epoch 10.80: Loss = 612.872
Epoch 10.81: Loss = 410.333
Epoch 10.82: Loss = 745.273
Epoch 10.83: Loss = -1928.45
Epoch 10.84: Loss = -234.377
Epoch 10.85: Loss = 545.621
Epoch 10.86: Loss = -2468.87
Epoch 10.87: Loss = 1432.18
Epoch 10.88: Loss = 87.9039
Epoch 10.89: Loss = 1363.84
Epoch 10.90: Loss = -54.0755
Epoch 10.91: Loss = -91.3864
Epoch 10.92: Loss = 383.756
Epoch 10.93: Loss = 1501.49
Epoch 10.94: Loss = -903.453
Epoch 10.95: Loss = 1763.2
Epoch 10.96: Loss = 1760.15
Epoch 10.97: Loss = -840.94
Epoch 10.98: Loss = -1052.83
Epoch 10.99: Loss = -665.191
Epoch 10.100: Loss = -307.993
Epoch 10.101: Loss = -884.903
Epoch 10.102: Loss = -42.5419
Epoch 10.103: Loss = 64.071
Epoch 10.104: Loss = -2526.99
Epoch 10.105: Loss = -881.041
Epoch 10.106: Loss = 1408.17
Epoch 10.107: Loss = 1777.66
Epoch 10.108: Loss = -2758.54
Epoch 10.109: Loss = 302.635
Epoch 10.110: Loss = -85.2825
Epoch 10.111: Loss = 2078.86
Epoch 10.112: Loss = 891.676
Epoch 10.113: Loss = 815.071
Epoch 10.114: Loss = 2612.11
Epoch 10.115: Loss = 244.817
Epoch 10.116: Loss = 1923.12
Epoch 10.117: Loss = -1190.04
Epoch 10.118: Loss = -3227.01
Epoch 10.119: Loss = -528.73
Epoch 10.120: Loss = 971.808
TRAIN LOSS = -12.058
TRAIN ACC = 10.0296 % (6018/60000)
Loss = 1001.89
Loss = 1954.74
Loss = -565.049
Loss = -619.502
Loss = -969.521
Loss = -17.3812
Loss = 555.695
Loss = 241.483
Loss = -2570.95
Loss = 1684.37
Loss = 981.619
Loss = 488.292
Loss = 1710.23
Loss = -635.473
Loss = 183.816
Loss = 730.526
Loss = 1283.65
Loss = 196.72
Loss = 1043.41
Loss = 213.407
TEST LOSS = 3.81198
TEST ACC = 60.1791 % (990/10000)
