Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.32393
Epoch 1.2: Loss = 2.2632
Epoch 1.3: Loss = 2.21014
Epoch 1.4: Loss = 2.14545
Epoch 1.5: Loss = 2.08386
Epoch 1.6: Loss = 2.05023
Epoch 1.7: Loss = 1.99847
Epoch 1.8: Loss = 1.95288
Epoch 1.9: Loss = 1.89833
Epoch 1.10: Loss = 1.83496
Epoch 1.11: Loss = 1.85332
Epoch 1.12: Loss = 1.79109
Epoch 1.13: Loss = 1.75148
Epoch 1.14: Loss = 1.70628
Epoch 1.15: Loss = 1.63974
Epoch 1.16: Loss = 1.63205
Epoch 1.17: Loss = 1.60635
Epoch 1.18: Loss = 1.5589
Epoch 1.19: Loss = 1.50839
Epoch 1.20: Loss = 1.52599
Epoch 1.21: Loss = 1.45027
Epoch 1.22: Loss = 1.42361
Epoch 1.23: Loss = 1.38342
Epoch 1.24: Loss = 1.43596
Epoch 1.25: Loss = 1.38525
Epoch 1.26: Loss = 1.29953
Epoch 1.27: Loss = 1.29021
Epoch 1.28: Loss = 1.28044
Epoch 1.29: Loss = 1.25974
Epoch 1.30: Loss = 1.2412
Epoch 1.31: Loss = 1.24933
Epoch 1.32: Loss = 1.22165
Epoch 1.33: Loss = 1.14561
Epoch 1.34: Loss = 1.19461
Epoch 1.35: Loss = 1.22765
Epoch 1.36: Loss = 1.19147
Epoch 1.37: Loss = 1.15059
Epoch 1.38: Loss = 1.12215
Epoch 1.39: Loss = 1.10703
Epoch 1.40: Loss = 1.11339
Epoch 1.41: Loss = 1.12846
Epoch 1.42: Loss = 1.04932
Epoch 1.43: Loss = 1.04652
Epoch 1.44: Loss = 1.00607
Epoch 1.45: Loss = 1.05641
Epoch 1.46: Loss = 1.05371
Epoch 1.47: Loss = 1.01727
Epoch 1.48: Loss = 0.994278
Epoch 1.49: Loss = 1.0276
Epoch 1.50: Loss = 0.955704
Epoch 1.51: Loss = 0.934418
Epoch 1.52: Loss = 1.0118
Epoch 1.53: Loss = 1.01653
Epoch 1.54: Loss = 0.883987
Epoch 1.55: Loss = 0.980804
Epoch 1.56: Loss = 0.972778
Epoch 1.57: Loss = 0.994263
Epoch 1.58: Loss = 0.937561
Epoch 1.59: Loss = 0.958023
Epoch 1.60: Loss = 0.955917
Epoch 1.61: Loss = 0.868393
Epoch 1.62: Loss = 0.954514
Epoch 1.63: Loss = 0.817566
Epoch 1.64: Loss = 0.865158
Epoch 1.65: Loss = 0.884567
Epoch 1.66: Loss = 0.887985
Epoch 1.67: Loss = 0.826263
Epoch 1.68: Loss = 0.93483
Epoch 1.69: Loss = 0.899078
Epoch 1.70: Loss = 0.871124
Epoch 1.71: Loss = 0.811203
Epoch 1.72: Loss = 0.829117
Epoch 1.73: Loss = 0.917694
Epoch 1.74: Loss = 0.893234
Epoch 1.75: Loss = 0.839188
Epoch 1.76: Loss = 0.852371
Epoch 1.77: Loss = 0.81601
Epoch 1.78: Loss = 0.846191
Epoch 1.79: Loss = 0.76091
Epoch 1.80: Loss = 0.828735
Epoch 1.81: Loss = 0.796921
Epoch 1.82: Loss = 0.82254
Epoch 1.83: Loss = 0.842743
Epoch 1.84: Loss = 0.832504
Epoch 1.85: Loss = 0.798538
Epoch 1.86: Loss = 0.875397
Epoch 1.87: Loss = 0.846481
Epoch 1.88: Loss = 0.718658
Epoch 1.89: Loss = 0.88739
Epoch 1.90: Loss = 0.772156
Epoch 1.91: Loss = 0.861969
Epoch 1.92: Loss = 0.825897
Epoch 1.93: Loss = 0.841156
Epoch 1.94: Loss = 0.804886
Epoch 1.95: Loss = 0.839127
Epoch 1.96: Loss = 0.779053
Epoch 1.97: Loss = 0.665314
Epoch 1.98: Loss = 0.795975
Epoch 1.99: Loss = 0.788025
Epoch 1.100: Loss = 0.774139
Epoch 1.101: Loss = 0.847015
Epoch 1.102: Loss = 0.840302
Epoch 1.103: Loss = 0.78804
Epoch 1.104: Loss = 0.767441
Epoch 1.105: Loss = 0.737045
Epoch 1.106: Loss = 0.891418
Epoch 1.107: Loss = 0.790237
Epoch 1.108: Loss = 0.792709
Epoch 1.109: Loss = 0.78067
Epoch 1.110: Loss = 0.805435
Epoch 1.111: Loss = 0.712463
Epoch 1.112: Loss = 0.714096
Epoch 1.113: Loss = 0.771133
Epoch 1.114: Loss = 0.761276
Epoch 1.115: Loss = 0.779343
Epoch 1.116: Loss = 0.686096
Epoch 1.117: Loss = 0.83931
Epoch 1.118: Loss = 0.702652
Epoch 1.119: Loss = 0.742691
Epoch 1.120: Loss = 0.714828
TRAIN LOSS = 1.09441
TRAIN ACC = 64.1998 % (38522/60000)
Loss = 0.70488
Loss = 0.818466
Loss = 0.804703
Loss = 0.710281
Loss = 0.721298
Loss = 0.876236
Loss = 0.88739
Loss = 0.832748
Loss = 0.754944
Loss = 0.710907
Loss = 0.858063
Loss = 0.824722
Loss = 0.787369
Loss = 0.796204
Loss = 0.767853
Loss = 0.832886
Loss = 0.738068
Loss = 0.788208
Loss = 0.857651
Loss = 0.765854
TEST LOSS = 0.791936
TEST ACC = 385.219 % (7156/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.791306
Epoch 2.2: Loss = 0.722351
Epoch 2.3: Loss = 0.827438
Epoch 2.4: Loss = 0.68251
Epoch 2.5: Loss = 0.750702
Epoch 2.6: Loss = 0.841797
Epoch 2.7: Loss = 0.762848
Epoch 2.8: Loss = 0.791458
Epoch 2.9: Loss = 0.66748
Epoch 2.10: Loss = 0.594635
Epoch 2.11: Loss = 0.819305
Epoch 2.12: Loss = 0.761887
Epoch 2.13: Loss = 0.756851
Epoch 2.14: Loss = 0.758102
Epoch 2.15: Loss = 0.7435
Epoch 2.16: Loss = 0.79303
Epoch 2.17: Loss = 0.721558
Epoch 2.18: Loss = 0.766815
Epoch 2.19: Loss = 0.71489
Epoch 2.20: Loss = 0.837616
Epoch 2.21: Loss = 0.703506
Epoch 2.22: Loss = 0.657867
Epoch 2.23: Loss = 0.728195
Epoch 2.24: Loss = 0.836838
Epoch 2.25: Loss = 0.744263
Epoch 2.26: Loss = 0.667435
Epoch 2.27: Loss = 0.727631
Epoch 2.28: Loss = 0.732864
Epoch 2.29: Loss = 0.738785
Epoch 2.30: Loss = 0.73085
Epoch 2.31: Loss = 0.80278
Epoch 2.32: Loss = 0.695847
Epoch 2.33: Loss = 0.637329
Epoch 2.34: Loss = 0.793243
Epoch 2.35: Loss = 0.813034
Epoch 2.36: Loss = 0.774261
Epoch 2.37: Loss = 0.755051
Epoch 2.38: Loss = 0.733215
Epoch 2.39: Loss = 0.771622
Epoch 2.40: Loss = 0.749649
Epoch 2.41: Loss = 0.759903
Epoch 2.42: Loss = 0.726181
Epoch 2.43: Loss = 0.73085
Epoch 2.44: Loss = 0.636551
Epoch 2.45: Loss = 0.74054
Epoch 2.46: Loss = 0.82933
Epoch 2.47: Loss = 0.656387
Epoch 2.48: Loss = 0.660629
Epoch 2.49: Loss = 0.772095
Epoch 2.50: Loss = 0.70076
Epoch 2.51: Loss = 0.617538
Epoch 2.52: Loss = 0.766937
Epoch 2.53: Loss = 0.808807
Epoch 2.54: Loss = 0.606781
Epoch 2.55: Loss = 0.731598
Epoch 2.56: Loss = 0.752548
Epoch 2.57: Loss = 0.78685
Epoch 2.58: Loss = 0.728302
Epoch 2.59: Loss = 0.735931
Epoch 2.60: Loss = 0.724487
Epoch 2.61: Loss = 0.674255
Epoch 2.62: Loss = 0.772583
Epoch 2.63: Loss = 0.611435
Epoch 2.64: Loss = 0.651123
Epoch 2.65: Loss = 0.697998
Epoch 2.66: Loss = 0.666534
Epoch 2.67: Loss = 0.65802
Epoch 2.68: Loss = 0.772903
Epoch 2.69: Loss = 0.689819
Epoch 2.70: Loss = 0.734741
Epoch 2.71: Loss = 0.627914
Epoch 2.72: Loss = 0.683762
Epoch 2.73: Loss = 0.785934
Epoch 2.74: Loss = 0.757401
Epoch 2.75: Loss = 0.676682
Epoch 2.76: Loss = 0.683777
Epoch 2.77: Loss = 0.67305
Epoch 2.78: Loss = 0.708878
Epoch 2.79: Loss = 0.656052
Epoch 2.80: Loss = 0.66626
Epoch 2.81: Loss = 0.677383
Epoch 2.82: Loss = 0.660217
Epoch 2.83: Loss = 0.744705
Epoch 2.84: Loss = 0.675598
Epoch 2.85: Loss = 0.692535
Epoch 2.86: Loss = 0.755127
Epoch 2.87: Loss = 0.71843
Epoch 2.88: Loss = 0.608398
Epoch 2.89: Loss = 0.783096
Epoch 2.90: Loss = 0.675446
Epoch 2.91: Loss = 0.784424
Epoch 2.92: Loss = 0.720535
Epoch 2.93: Loss = 0.744766
Epoch 2.94: Loss = 0.6922
Epoch 2.95: Loss = 0.716034
Epoch 2.96: Loss = 0.686737
Epoch 2.97: Loss = 0.583847
Epoch 2.98: Loss = 0.67807
Epoch 2.99: Loss = 0.698105
Epoch 2.100: Loss = 0.692184
Epoch 2.101: Loss = 0.758179
Epoch 2.102: Loss = 0.740585
Epoch 2.103: Loss = 0.670731
Epoch 2.104: Loss = 0.646225
Epoch 2.105: Loss = 0.636429
Epoch 2.106: Loss = 0.772659
Epoch 2.107: Loss = 0.738098
Epoch 2.108: Loss = 0.723358
Epoch 2.109: Loss = 0.728058
Epoch 2.110: Loss = 0.724976
Epoch 2.111: Loss = 0.639801
Epoch 2.112: Loss = 0.655472
Epoch 2.113: Loss = 0.667923
Epoch 2.114: Loss = 0.669128
Epoch 2.115: Loss = 0.686646
Epoch 2.116: Loss = 0.614243
Epoch 2.117: Loss = 0.7556
Epoch 2.118: Loss = 0.610382
Epoch 2.119: Loss = 0.682114
Epoch 2.120: Loss = 0.645203
TRAIN LOSS = 0.716476
TRAIN ACC = 75.4898 % (45296/60000)
Loss = 0.637238
Loss = 0.760941
Loss = 0.721893
Loss = 0.640778
Loss = 0.646408
Loss = 0.837387
Loss = 0.853729
Loss = 0.777298
Loss = 0.700455
Loss = 0.626999
Loss = 0.810669
Loss = 0.800781
Loss = 0.725998
Loss = 0.743179
Loss = 0.724915
Loss = 0.768127
Loss = 0.691971
Loss = 0.752167
Loss = 0.795258
Loss = 0.706116
TEST LOSS = 0.736115
TEST ACC = 452.959 % (7595/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.713974
Epoch 3.2: Loss = 0.674942
Epoch 3.3: Loss = 0.738922
Epoch 3.4: Loss = 0.620804
Epoch 3.5: Loss = 0.683868
Epoch 3.6: Loss = 0.789139
Epoch 3.7: Loss = 0.727554
Epoch 3.8: Loss = 0.760742
Epoch 3.9: Loss = 0.583328
Epoch 3.10: Loss = 0.516159
Epoch 3.11: Loss = 0.784363
Epoch 3.12: Loss = 0.696548
Epoch 3.13: Loss = 0.721924
Epoch 3.14: Loss = 0.684708
Epoch 3.15: Loss = 0.680756
Epoch 3.16: Loss = 0.747681
Epoch 3.17: Loss = 0.637894
Epoch 3.18: Loss = 0.709442
Epoch 3.19: Loss = 0.655746
Epoch 3.20: Loss = 0.787994
Epoch 3.21: Loss = 0.641739
Epoch 3.22: Loss = 0.564835
Epoch 3.23: Loss = 0.685165
Epoch 3.24: Loss = 0.790161
Epoch 3.25: Loss = 0.687561
Epoch 3.26: Loss = 0.617615
Epoch 3.27: Loss = 0.690048
Epoch 3.28: Loss = 0.689621
Epoch 3.29: Loss = 0.697617
Epoch 3.30: Loss = 0.677948
Epoch 3.31: Loss = 0.7612
Epoch 3.32: Loss = 0.65358
Epoch 3.33: Loss = 0.60936
Epoch 3.34: Loss = 0.763596
Epoch 3.35: Loss = 0.773926
Epoch 3.36: Loss = 0.74939
Epoch 3.37: Loss = 0.706207
Epoch 3.38: Loss = 0.694077
Epoch 3.39: Loss = 0.744827
Epoch 3.40: Loss = 0.696487
Epoch 3.41: Loss = 0.721573
Epoch 3.42: Loss = 0.664673
Epoch 3.43: Loss = 0.693466
Epoch 3.44: Loss = 0.598953
Epoch 3.45: Loss = 0.706268
Epoch 3.46: Loss = 0.795135
Epoch 3.47: Loss = 0.628342
Epoch 3.48: Loss = 0.611572
Epoch 3.49: Loss = 0.733154
Epoch 3.50: Loss = 0.681793
Epoch 3.51: Loss = 0.576141
Epoch 3.52: Loss = 0.733566
Epoch 3.53: Loss = 0.804947
Epoch 3.54: Loss = 0.558838
Epoch 3.55: Loss = 0.697174
Epoch 3.56: Loss = 0.731506
Epoch 3.57: Loss = 0.763748
Epoch 3.58: Loss = 0.681946
Epoch 3.59: Loss = 0.756104
Epoch 3.60: Loss = 0.709686
Epoch 3.61: Loss = 0.648102
Epoch 3.62: Loss = 0.72554
Epoch 3.63: Loss = 0.579544
Epoch 3.64: Loss = 0.592773
Epoch 3.65: Loss = 0.693665
Epoch 3.66: Loss = 0.623703
Epoch 3.67: Loss = 0.658035
Epoch 3.68: Loss = 0.77301
Epoch 3.69: Loss = 0.688324
Epoch 3.70: Loss = 0.716003
Epoch 3.71: Loss = 0.594055
Epoch 3.72: Loss = 0.668228
Epoch 3.73: Loss = 0.773163
Epoch 3.74: Loss = 0.719101
Epoch 3.75: Loss = 0.627335
Epoch 3.76: Loss = 0.666763
Epoch 3.77: Loss = 0.626251
Epoch 3.78: Loss = 0.694519
Epoch 3.79: Loss = 0.655182
Epoch 3.80: Loss = 0.638535
Epoch 3.81: Loss = 0.635468
Epoch 3.82: Loss = 0.6241
Epoch 3.83: Loss = 0.71312
Epoch 3.84: Loss = 0.648132
Epoch 3.85: Loss = 0.66539
Epoch 3.86: Loss = 0.741623
Epoch 3.87: Loss = 0.692459
Epoch 3.88: Loss = 0.596191
Epoch 3.89: Loss = 0.779785
Epoch 3.90: Loss = 0.679138
Epoch 3.91: Loss = 0.773834
Epoch 3.92: Loss = 0.689102
Epoch 3.93: Loss = 0.731247
Epoch 3.94: Loss = 0.675064
Epoch 3.95: Loss = 0.687378
Epoch 3.96: Loss = 0.67662
Epoch 3.97: Loss = 0.561707
Epoch 3.98: Loss = 0.657516
Epoch 3.99: Loss = 0.674469
Epoch 3.100: Loss = 0.676041
Epoch 3.101: Loss = 0.747543
Epoch 3.102: Loss = 0.742661
Epoch 3.103: Loss = 0.648453
Epoch 3.104: Loss = 0.611008
Epoch 3.105: Loss = 0.622849
Epoch 3.106: Loss = 0.782135
Epoch 3.107: Loss = 0.733017
Epoch 3.108: Loss = 0.742203
Epoch 3.109: Loss = 0.720978
Epoch 3.110: Loss = 0.698624
Epoch 3.111: Loss = 0.635254
Epoch 3.112: Loss = 0.649429
Epoch 3.113: Loss = 0.640305
Epoch 3.114: Loss = 0.659561
Epoch 3.115: Loss = 0.66922
Epoch 3.116: Loss = 0.594833
Epoch 3.117: Loss = 0.737015
Epoch 3.118: Loss = 0.600266
Epoch 3.119: Loss = 0.664597
Epoch 3.120: Loss = 0.638733
TRAIN LOSS = 0.684509
TRAIN ACC = 77.9541 % (46775/60000)
Loss = 0.59996
Loss = 0.753311
Loss = 0.670044
Loss = 0.604874
Loss = 0.622223
Loss = 0.801193
Loss = 0.83577
Loss = 0.764282
Loss = 0.682739
Loss = 0.618973
Loss = 0.813644
Loss = 0.785645
Loss = 0.708908
Loss = 0.718262
Loss = 0.700302
Loss = 0.755173
Loss = 0.665222
Loss = 0.72554
Loss = 0.761078
Loss = 0.679626
TEST LOSS = 0.713338
TEST ACC = 467.749 % (7759/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.683731
Epoch 4.2: Loss = 0.648209
Epoch 4.3: Loss = 0.739929
Epoch 4.4: Loss = 0.609222
Epoch 4.5: Loss = 0.700287
Epoch 4.6: Loss = 0.766769
Epoch 4.7: Loss = 0.715454
Epoch 4.8: Loss = 0.782379
Epoch 4.9: Loss = 0.556229
Epoch 4.10: Loss = 0.48465
Epoch 4.11: Loss = 0.796692
Epoch 4.12: Loss = 0.697983
Epoch 4.13: Loss = 0.734467
Epoch 4.14: Loss = 0.685806
Epoch 4.15: Loss = 0.689255
Epoch 4.16: Loss = 0.750122
Epoch 4.17: Loss = 0.644745
Epoch 4.18: Loss = 0.693466
Epoch 4.19: Loss = 0.634827
Epoch 4.20: Loss = 0.783539
Epoch 4.21: Loss = 0.622009
Epoch 4.22: Loss = 0.538315
Epoch 4.23: Loss = 0.682907
Epoch 4.24: Loss = 0.79306
Epoch 4.25: Loss = 0.662735
Epoch 4.26: Loss = 0.603775
Epoch 4.27: Loss = 0.694336
Epoch 4.28: Loss = 0.696945
Epoch 4.29: Loss = 0.692383
Epoch 4.30: Loss = 0.700638
Epoch 4.31: Loss = 0.781036
Epoch 4.32: Loss = 0.637314
Epoch 4.33: Loss = 0.57991
Epoch 4.34: Loss = 0.756165
Epoch 4.35: Loss = 0.774048
Epoch 4.36: Loss = 0.767975
Epoch 4.37: Loss = 0.73764
Epoch 4.38: Loss = 0.696259
Epoch 4.39: Loss = 0.743881
Epoch 4.40: Loss = 0.693832
Epoch 4.41: Loss = 0.727158
Epoch 4.42: Loss = 0.666473
Epoch 4.43: Loss = 0.695099
Epoch 4.44: Loss = 0.587296
Epoch 4.45: Loss = 0.699051
Epoch 4.46: Loss = 0.805206
Epoch 4.47: Loss = 0.620743
Epoch 4.48: Loss = 0.603958
Epoch 4.49: Loss = 0.712524
Epoch 4.50: Loss = 0.666687
Epoch 4.51: Loss = 0.552505
Epoch 4.52: Loss = 0.723007
Epoch 4.53: Loss = 0.792007
Epoch 4.54: Loss = 0.535995
Epoch 4.55: Loss = 0.698639
Epoch 4.56: Loss = 0.748428
Epoch 4.57: Loss = 0.767639
Epoch 4.58: Loss = 0.667633
Epoch 4.59: Loss = 0.751389
Epoch 4.60: Loss = 0.685822
Epoch 4.61: Loss = 0.647186
Epoch 4.62: Loss = 0.725266
Epoch 4.63: Loss = 0.579803
Epoch 4.64: Loss = 0.571442
Epoch 4.65: Loss = 0.693832
Epoch 4.66: Loss = 0.622742
Epoch 4.67: Loss = 0.661316
Epoch 4.68: Loss = 0.79834
Epoch 4.69: Loss = 0.669861
Epoch 4.70: Loss = 0.702728
Epoch 4.71: Loss = 0.585739
Epoch 4.72: Loss = 0.676102
Epoch 4.73: Loss = 0.771927
Epoch 4.74: Loss = 0.735474
Epoch 4.75: Loss = 0.630554
Epoch 4.76: Loss = 0.649994
Epoch 4.77: Loss = 0.636505
Epoch 4.78: Loss = 0.694336
Epoch 4.79: Loss = 0.660477
Epoch 4.80: Loss = 0.643433
Epoch 4.81: Loss = 0.646637
Epoch 4.82: Loss = 0.637817
Epoch 4.83: Loss = 0.735855
Epoch 4.84: Loss = 0.628632
Epoch 4.85: Loss = 0.675781
Epoch 4.86: Loss = 0.742554
Epoch 4.87: Loss = 0.694611
Epoch 4.88: Loss = 0.609161
Epoch 4.89: Loss = 0.782761
Epoch 4.90: Loss = 0.69104
Epoch 4.91: Loss = 0.76387
Epoch 4.92: Loss = 0.685974
Epoch 4.93: Loss = 0.729691
Epoch 4.94: Loss = 0.678299
Epoch 4.95: Loss = 0.668289
Epoch 4.96: Loss = 0.66832
Epoch 4.97: Loss = 0.552063
Epoch 4.98: Loss = 0.657547
Epoch 4.99: Loss = 0.672409
Epoch 4.100: Loss = 0.687042
Epoch 4.101: Loss = 0.744507
Epoch 4.102: Loss = 0.744308
Epoch 4.103: Loss = 0.643509
Epoch 4.104: Loss = 0.615005
Epoch 4.105: Loss = 0.615585
Epoch 4.106: Loss = 0.799286
Epoch 4.107: Loss = 0.718948
Epoch 4.108: Loss = 0.751099
Epoch 4.109: Loss = 0.720779
Epoch 4.110: Loss = 0.698578
Epoch 4.111: Loss = 0.641251
Epoch 4.112: Loss = 0.664825
Epoch 4.113: Loss = 0.652252
Epoch 4.114: Loss = 0.691452
Epoch 4.115: Loss = 0.672623
Epoch 4.116: Loss = 0.600098
Epoch 4.117: Loss = 0.750656
Epoch 4.118: Loss = 0.60289
Epoch 4.119: Loss = 0.693359
Epoch 4.120: Loss = 0.651428
TRAIN LOSS = 0.683334
TRAIN ACC = 78.9963 % (47400/60000)
Loss = 0.610855
Loss = 0.775192
Loss = 0.683929
Loss = 0.610062
Loss = 0.638046
Loss = 0.814636
Loss = 0.855972
Loss = 0.791733
Loss = 0.702469
Loss = 0.651093
Loss = 0.853882
Loss = 0.82196
Loss = 0.720032
Loss = 0.72464
Loss = 0.737305
Loss = 0.771881
Loss = 0.698669
Loss = 0.755585
Loss = 0.778763
Loss = 0.703033
TEST LOSS = 0.734987
TEST ACC = 473.999 % (7832/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.694733
Epoch 5.2: Loss = 0.676468
Epoch 5.3: Loss = 0.74025
Epoch 5.4: Loss = 0.595352
Epoch 5.5: Loss = 0.70578
Epoch 5.6: Loss = 0.781525
Epoch 5.7: Loss = 0.731583
Epoch 5.8: Loss = 0.805496
Epoch 5.9: Loss = 0.554672
Epoch 5.10: Loss = 0.481369
Epoch 5.11: Loss = 0.826645
Epoch 5.12: Loss = 0.68988
Epoch 5.13: Loss = 0.744812
Epoch 5.14: Loss = 0.689056
Epoch 5.15: Loss = 0.717545
Epoch 5.16: Loss = 0.760147
Epoch 5.17: Loss = 0.641266
Epoch 5.18: Loss = 0.698288
Epoch 5.19: Loss = 0.649323
Epoch 5.20: Loss = 0.805023
Epoch 5.21: Loss = 0.626099
Epoch 5.22: Loss = 0.532227
Epoch 5.23: Loss = 0.709579
Epoch 5.24: Loss = 0.805435
Epoch 5.25: Loss = 0.666718
Epoch 5.26: Loss = 0.613632
Epoch 5.27: Loss = 0.70871
Epoch 5.28: Loss = 0.700394
Epoch 5.29: Loss = 0.696991
Epoch 5.30: Loss = 0.696426
Epoch 5.31: Loss = 0.776932
Epoch 5.32: Loss = 0.642441
Epoch 5.33: Loss = 0.58049
Epoch 5.34: Loss = 0.753265
Epoch 5.35: Loss = 0.774643
Epoch 5.36: Loss = 0.773819
Epoch 5.37: Loss = 0.715866
Epoch 5.38: Loss = 0.700424
Epoch 5.39: Loss = 0.751038
Epoch 5.40: Loss = 0.688461
Epoch 5.41: Loss = 0.727081
Epoch 5.42: Loss = 0.675079
Epoch 5.43: Loss = 0.701691
Epoch 5.44: Loss = 0.581848
Epoch 5.45: Loss = 0.715424
Epoch 5.46: Loss = 0.856018
Epoch 5.47: Loss = 0.638351
Epoch 5.48: Loss = 0.617447
Epoch 5.49: Loss = 0.708572
Epoch 5.50: Loss = 0.669647
Epoch 5.51: Loss = 0.544479
Epoch 5.52: Loss = 0.7323
Epoch 5.53: Loss = 0.800644
Epoch 5.54: Loss = 0.535629
Epoch 5.55: Loss = 0.679886
Epoch 5.56: Loss = 0.746902
Epoch 5.57: Loss = 0.77034
Epoch 5.58: Loss = 0.664032
Epoch 5.59: Loss = 0.768997
Epoch 5.60: Loss = 0.712555
Epoch 5.61: Loss = 0.665466
Epoch 5.62: Loss = 0.729828
Epoch 5.63: Loss = 0.582031
Epoch 5.64: Loss = 0.570251
Epoch 5.65: Loss = 0.690247
Epoch 5.66: Loss = 0.605789
Epoch 5.67: Loss = 0.672028
Epoch 5.68: Loss = 0.835648
Epoch 5.69: Loss = 0.674744
Epoch 5.70: Loss = 0.695282
Epoch 5.71: Loss = 0.570343
Epoch 5.72: Loss = 0.684784
Epoch 5.73: Loss = 0.740494
Epoch 5.74: Loss = 0.716019
Epoch 5.75: Loss = 0.634583
Epoch 5.76: Loss = 0.66539
Epoch 5.77: Loss = 0.635284
Epoch 5.78: Loss = 0.683395
Epoch 5.79: Loss = 0.661026
Epoch 5.80: Loss = 0.645554
Epoch 5.81: Loss = 0.652954
Epoch 5.82: Loss = 0.650223
Epoch 5.83: Loss = 0.732956
Epoch 5.84: Loss = 0.607788
Epoch 5.85: Loss = 0.666016
Epoch 5.86: Loss = 0.743439
Epoch 5.87: Loss = 0.662216
Epoch 5.88: Loss = 0.582687
Epoch 5.89: Loss = 0.776062
Epoch 5.90: Loss = 0.688614
Epoch 5.91: Loss = 0.765839
Epoch 5.92: Loss = 0.707413
Epoch 5.93: Loss = 0.712463
Epoch 5.94: Loss = 0.671234
Epoch 5.95: Loss = 0.659393
Epoch 5.96: Loss = 0.685272
Epoch 5.97: Loss = 0.550186
Epoch 5.98: Loss = 0.647339
Epoch 5.99: Loss = 0.667511
Epoch 5.100: Loss = 0.676773
Epoch 5.101: Loss = 0.729782
Epoch 5.102: Loss = 0.715958
Epoch 5.103: Loss = 0.631271
Epoch 5.104: Loss = 0.6082
Epoch 5.105: Loss = 0.614471
Epoch 5.106: Loss = 0.78598
Epoch 5.107: Loss = 0.732147
Epoch 5.108: Loss = 0.792801
Epoch 5.109: Loss = 0.714554
Epoch 5.110: Loss = 0.687622
Epoch 5.111: Loss = 0.638809
Epoch 5.112: Loss = 0.690414
Epoch 5.113: Loss = 0.665802
Epoch 5.114: Loss = 0.711502
Epoch 5.115: Loss = 0.656937
Epoch 5.116: Loss = 0.624146
Epoch 5.117: Loss = 0.747238
Epoch 5.118: Loss = 0.616089
Epoch 5.119: Loss = 0.709991
Epoch 5.120: Loss = 0.654114
TRAIN LOSS = 0.686203
TRAIN ACC = 79.6494 % (47792/60000)
Loss = 0.591324
Loss = 0.770386
Loss = 0.671326
Loss = 0.614578
Loss = 0.645081
Loss = 0.812897
Loss = 0.854889
Loss = 0.767624
Loss = 0.712952
Loss = 0.649231
Loss = 0.870224
Loss = 0.822723
Loss = 0.737457
Loss = 0.73317
Loss = 0.738846
Loss = 0.749649
Loss = 0.679123
Loss = 0.780289
Loss = 0.772018
Loss = 0.699799
TEST LOSS = 0.733679
TEST ACC = 477.919 % (7909/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.681076
Epoch 6.2: Loss = 0.666611
Epoch 6.3: Loss = 0.738556
Epoch 6.4: Loss = 0.591446
Epoch 6.5: Loss = 0.699417
Epoch 6.6: Loss = 0.767914
Epoch 6.7: Loss = 0.687424
Epoch 6.8: Loss = 0.788513
Epoch 6.9: Loss = 0.55246
Epoch 6.10: Loss = 0.463135
Epoch 6.11: Loss = 0.828033
Epoch 6.12: Loss = 0.682434
Epoch 6.13: Loss = 0.744446
Epoch 6.14: Loss = 0.681046
Epoch 6.15: Loss = 0.700516
Epoch 6.16: Loss = 0.738632
Epoch 6.17: Loss = 0.630814
Epoch 6.18: Loss = 0.702423
Epoch 6.19: Loss = 0.641739
Epoch 6.20: Loss = 0.774582
Epoch 6.21: Loss = 0.618607
Epoch 6.22: Loss = 0.537018
Epoch 6.23: Loss = 0.699707
Epoch 6.24: Loss = 0.79866
Epoch 6.25: Loss = 0.670502
Epoch 6.26: Loss = 0.593628
Epoch 6.27: Loss = 0.696793
Epoch 6.28: Loss = 0.695923
Epoch 6.29: Loss = 0.683685
Epoch 6.30: Loss = 0.703049
Epoch 6.31: Loss = 0.775436
Epoch 6.32: Loss = 0.658005
Epoch 6.33: Loss = 0.569077
Epoch 6.34: Loss = 0.757507
Epoch 6.35: Loss = 0.765732
Epoch 6.36: Loss = 0.778305
Epoch 6.37: Loss = 0.715897
Epoch 6.38: Loss = 0.707016
Epoch 6.39: Loss = 0.756638
Epoch 6.40: Loss = 0.685516
Epoch 6.41: Loss = 0.719788
Epoch 6.42: Loss = 0.664078
Epoch 6.43: Loss = 0.710831
Epoch 6.44: Loss = 0.583344
Epoch 6.45: Loss = 0.702301
Epoch 6.46: Loss = 0.839035
Epoch 6.47: Loss = 0.643051
Epoch 6.48: Loss = 0.614716
Epoch 6.49: Loss = 0.694733
Epoch 6.50: Loss = 0.65863
Epoch 6.51: Loss = 0.534714
Epoch 6.52: Loss = 0.738022
Epoch 6.53: Loss = 0.786697
Epoch 6.54: Loss = 0.528687
Epoch 6.55: Loss = 0.689194
Epoch 6.56: Loss = 0.77092
Epoch 6.57: Loss = 0.758484
Epoch 6.58: Loss = 0.659912
Epoch 6.59: Loss = 0.791855
Epoch 6.60: Loss = 0.683838
Epoch 6.61: Loss = 0.683685
Epoch 6.62: Loss = 0.720886
Epoch 6.63: Loss = 0.565186
Epoch 6.64: Loss = 0.560699
Epoch 6.65: Loss = 0.679504
Epoch 6.66: Loss = 0.605972
Epoch 6.67: Loss = 0.663864
Epoch 6.68: Loss = 0.825073
Epoch 6.69: Loss = 0.65683
Epoch 6.70: Loss = 0.693085
Epoch 6.71: Loss = 0.554642
Epoch 6.72: Loss = 0.684189
Epoch 6.73: Loss = 0.77449
Epoch 6.74: Loss = 0.737946
Epoch 6.75: Loss = 0.626587
Epoch 6.76: Loss = 0.680801
Epoch 6.77: Loss = 0.646988
Epoch 6.78: Loss = 0.68364
Epoch 6.79: Loss = 0.660278
Epoch 6.80: Loss = 0.641815
Epoch 6.81: Loss = 0.62883
Epoch 6.82: Loss = 0.647278
Epoch 6.83: Loss = 0.746887
Epoch 6.84: Loss = 0.621857
Epoch 6.85: Loss = 0.64444
Epoch 6.86: Loss = 0.73439
Epoch 6.87: Loss = 0.663513
Epoch 6.88: Loss = 0.572052
Epoch 6.89: Loss = 0.775742
Epoch 6.90: Loss = 0.691528
Epoch 6.91: Loss = 0.766068
Epoch 6.92: Loss = 0.686951
Epoch 6.93: Loss = 0.715164
Epoch 6.94: Loss = 0.685699
Epoch 6.95: Loss = 0.681625
Epoch 6.96: Loss = 0.678009
Epoch 6.97: Loss = 0.551682
Epoch 6.98: Loss = 0.651276
Epoch 6.99: Loss = 0.658371
Epoch 6.100: Loss = 0.693054
Epoch 6.101: Loss = 0.700119
Epoch 6.102: Loss = 0.723648
Epoch 6.103: Loss = 0.640518
Epoch 6.104: Loss = 0.615509
Epoch 6.105: Loss = 0.587906
Epoch 6.106: Loss = 0.766022
Epoch 6.107: Loss = 0.711609
Epoch 6.108: Loss = 0.792221
Epoch 6.109: Loss = 0.697784
Epoch 6.110: Loss = 0.672424
Epoch 6.111: Loss = 0.622711
Epoch 6.112: Loss = 0.680298
Epoch 6.113: Loss = 0.638641
Epoch 6.114: Loss = 0.705765
Epoch 6.115: Loss = 0.636703
Epoch 6.116: Loss = 0.617432
Epoch 6.117: Loss = 0.749298
Epoch 6.118: Loss = 0.594421
Epoch 6.119: Loss = 0.682724
Epoch 6.120: Loss = 0.665253
TRAIN LOSS = 0.681198
TRAIN ACC = 80.4031 % (48244/60000)
Loss = 0.595963
Loss = 0.744537
Loss = 0.652817
Loss = 0.608887
Loss = 0.639374
Loss = 0.774048
Loss = 0.874313
Loss = 0.756958
Loss = 0.693878
Loss = 0.654602
Loss = 0.881683
Loss = 0.831345
Loss = 0.720795
Loss = 0.73027
Loss = 0.747681
Loss = 0.721283
Loss = 0.673706
Loss = 0.757431
Loss = 0.757217
Loss = 0.701553
TEST LOSS = 0.725917
TEST ACC = 482.439 % (7972/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.649567
Epoch 7.2: Loss = 0.661621
Epoch 7.3: Loss = 0.73822
Epoch 7.4: Loss = 0.567276
Epoch 7.5: Loss = 0.695328
Epoch 7.6: Loss = 0.775909
Epoch 7.7: Loss = 0.71109
Epoch 7.8: Loss = 0.787811
Epoch 7.9: Loss = 0.54509
Epoch 7.10: Loss = 0.461044
Epoch 7.11: Loss = 0.826385
Epoch 7.12: Loss = 0.663345
Epoch 7.13: Loss = 0.734787
Epoch 7.14: Loss = 0.684265
Epoch 7.15: Loss = 0.711823
Epoch 7.16: Loss = 0.746658
Epoch 7.17: Loss = 0.626602
Epoch 7.18: Loss = 0.703674
Epoch 7.19: Loss = 0.63356
Epoch 7.20: Loss = 0.781067
Epoch 7.21: Loss = 0.622147
Epoch 7.22: Loss = 0.530685
Epoch 7.23: Loss = 0.690948
Epoch 7.24: Loss = 0.799667
Epoch 7.25: Loss = 0.688507
Epoch 7.26: Loss = 0.589966
Epoch 7.27: Loss = 0.691086
Epoch 7.28: Loss = 0.707962
Epoch 7.29: Loss = 0.679596
Epoch 7.30: Loss = 0.733246
Epoch 7.31: Loss = 0.759262
Epoch 7.32: Loss = 0.644958
Epoch 7.33: Loss = 0.56749
Epoch 7.34: Loss = 0.735077
Epoch 7.35: Loss = 0.781219
Epoch 7.36: Loss = 0.803741
Epoch 7.37: Loss = 0.752243
Epoch 7.38: Loss = 0.710953
Epoch 7.39: Loss = 0.764267
Epoch 7.40: Loss = 0.666397
Epoch 7.41: Loss = 0.707565
Epoch 7.42: Loss = 0.652863
Epoch 7.43: Loss = 0.744659
Epoch 7.44: Loss = 0.578369
Epoch 7.45: Loss = 0.726288
Epoch 7.46: Loss = 0.842697
Epoch 7.47: Loss = 0.638397
Epoch 7.48: Loss = 0.613007
Epoch 7.49: Loss = 0.705368
Epoch 7.50: Loss = 0.691254
Epoch 7.51: Loss = 0.530411
Epoch 7.52: Loss = 0.745956
Epoch 7.53: Loss = 0.797913
Epoch 7.54: Loss = 0.511017
Epoch 7.55: Loss = 0.683929
Epoch 7.56: Loss = 0.790634
Epoch 7.57: Loss = 0.795837
Epoch 7.58: Loss = 0.658844
Epoch 7.59: Loss = 0.809341
Epoch 7.60: Loss = 0.701401
Epoch 7.61: Loss = 0.711258
Epoch 7.62: Loss = 0.731293
Epoch 7.63: Loss = 0.605988
Epoch 7.64: Loss = 0.561005
Epoch 7.65: Loss = 0.674789
Epoch 7.66: Loss = 0.569016
Epoch 7.67: Loss = 0.649658
Epoch 7.68: Loss = 0.850113
Epoch 7.69: Loss = 0.664993
Epoch 7.70: Loss = 0.707596
Epoch 7.71: Loss = 0.564316
Epoch 7.72: Loss = 0.703384
Epoch 7.73: Loss = 0.801422
Epoch 7.74: Loss = 0.7388
Epoch 7.75: Loss = 0.617142
Epoch 7.76: Loss = 0.661285
Epoch 7.77: Loss = 0.652603
Epoch 7.78: Loss = 0.672836
Epoch 7.79: Loss = 0.669083
Epoch 7.80: Loss = 0.662231
Epoch 7.81: Loss = 0.663818
Epoch 7.82: Loss = 0.676544
Epoch 7.83: Loss = 0.806366
Epoch 7.84: Loss = 0.635361
Epoch 7.85: Loss = 0.657242
Epoch 7.86: Loss = 0.773438
Epoch 7.87: Loss = 0.694916
Epoch 7.88: Loss = 0.59845
Epoch 7.89: Loss = 0.760315
Epoch 7.90: Loss = 0.722092
Epoch 7.91: Loss = 0.764923
Epoch 7.92: Loss = 0.691666
Epoch 7.93: Loss = 0.733109
Epoch 7.94: Loss = 0.709244
Epoch 7.95: Loss = 0.684174
Epoch 7.96: Loss = 0.694336
Epoch 7.97: Loss = 0.561905
Epoch 7.98: Loss = 0.687134
Epoch 7.99: Loss = 0.669449
Epoch 7.100: Loss = 0.690308
Epoch 7.101: Loss = 0.727783
Epoch 7.102: Loss = 0.750137
Epoch 7.103: Loss = 0.63121
Epoch 7.104: Loss = 0.633347
Epoch 7.105: Loss = 0.622711
Epoch 7.106: Loss = 0.790466
Epoch 7.107: Loss = 0.738281
Epoch 7.108: Loss = 0.82515
Epoch 7.109: Loss = 0.735519
Epoch 7.110: Loss = 0.703384
Epoch 7.111: Loss = 0.646057
Epoch 7.112: Loss = 0.688431
Epoch 7.113: Loss = 0.672577
Epoch 7.114: Loss = 0.732162
Epoch 7.115: Loss = 0.667297
Epoch 7.116: Loss = 0.630234
Epoch 7.117: Loss = 0.780777
Epoch 7.118: Loss = 0.62027
Epoch 7.119: Loss = 0.684494
Epoch 7.120: Loss = 0.682571
TRAIN LOSS = 0.690491
TRAIN ACC = 80.7922 % (48478/60000)
Loss = 0.611191
Loss = 0.767746
Loss = 0.683929
Loss = 0.607666
Loss = 0.658859
Loss = 0.791168
Loss = 0.899841
Loss = 0.787643
Loss = 0.725937
Loss = 0.682022
Loss = 0.897308
Loss = 0.875061
Loss = 0.74617
Loss = 0.743851
Loss = 0.759399
Loss = 0.737976
Loss = 0.740585
Loss = 0.765152
Loss = 0.785812
Loss = 0.748001
TEST LOSS = 0.750766
TEST ACC = 484.779 % (7984/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.643295
Epoch 8.2: Loss = 0.677979
Epoch 8.3: Loss = 0.739807
Epoch 8.4: Loss = 0.566986
Epoch 8.5: Loss = 0.719894
Epoch 8.6: Loss = 0.781815
Epoch 8.7: Loss = 0.70665
Epoch 8.8: Loss = 0.781006
Epoch 8.9: Loss = 0.558319
Epoch 8.10: Loss = 0.462204
Epoch 8.11: Loss = 0.825195
Epoch 8.12: Loss = 0.67392
Epoch 8.13: Loss = 0.743195
Epoch 8.14: Loss = 0.675552
Epoch 8.15: Loss = 0.733414
Epoch 8.16: Loss = 0.748291
Epoch 8.17: Loss = 0.64299
Epoch 8.18: Loss = 0.747513
Epoch 8.19: Loss = 0.624603
Epoch 8.20: Loss = 0.7901
Epoch 8.21: Loss = 0.620773
Epoch 8.22: Loss = 0.521667
Epoch 8.23: Loss = 0.692108
Epoch 8.24: Loss = 0.789444
Epoch 8.25: Loss = 0.695511
Epoch 8.26: Loss = 0.583328
Epoch 8.27: Loss = 0.702454
Epoch 8.28: Loss = 0.713226
Epoch 8.29: Loss = 0.681381
Epoch 8.30: Loss = 0.713379
Epoch 8.31: Loss = 0.778137
Epoch 8.32: Loss = 0.65596
Epoch 8.33: Loss = 0.573502
Epoch 8.34: Loss = 0.750992
Epoch 8.35: Loss = 0.794983
Epoch 8.36: Loss = 0.816818
Epoch 8.37: Loss = 0.773285
Epoch 8.38: Loss = 0.694992
Epoch 8.39: Loss = 0.786774
Epoch 8.40: Loss = 0.669861
Epoch 8.41: Loss = 0.691147
Epoch 8.42: Loss = 0.658707
Epoch 8.43: Loss = 0.7341
Epoch 8.44: Loss = 0.57959
Epoch 8.45: Loss = 0.733185
Epoch 8.46: Loss = 0.81012
Epoch 8.47: Loss = 0.640823
Epoch 8.48: Loss = 0.613846
Epoch 8.49: Loss = 0.716965
Epoch 8.50: Loss = 0.699371
Epoch 8.51: Loss = 0.531464
Epoch 8.52: Loss = 0.764603
Epoch 8.53: Loss = 0.771469
Epoch 8.54: Loss = 0.500732
Epoch 8.55: Loss = 0.708328
Epoch 8.56: Loss = 0.789444
Epoch 8.57: Loss = 0.771835
Epoch 8.58: Loss = 0.658356
Epoch 8.59: Loss = 0.812057
Epoch 8.60: Loss = 0.692566
Epoch 8.61: Loss = 0.69426
Epoch 8.62: Loss = 0.72995
Epoch 8.63: Loss = 0.581604
Epoch 8.64: Loss = 0.565659
Epoch 8.65: Loss = 0.664108
Epoch 8.66: Loss = 0.591629
Epoch 8.67: Loss = 0.639511
Epoch 8.68: Loss = 0.876038
Epoch 8.69: Loss = 0.667297
Epoch 8.70: Loss = 0.679565
Epoch 8.71: Loss = 0.571579
Epoch 8.72: Loss = 0.699722
Epoch 8.73: Loss = 0.800507
Epoch 8.74: Loss = 0.718735
Epoch 8.75: Loss = 0.615646
Epoch 8.76: Loss = 0.676453
Epoch 8.77: Loss = 0.669708
Epoch 8.78: Loss = 0.673279
Epoch 8.79: Loss = 0.642487
Epoch 8.80: Loss = 0.667084
Epoch 8.81: Loss = 0.667526
Epoch 8.82: Loss = 0.67157
Epoch 8.83: Loss = 0.815872
Epoch 8.84: Loss = 0.642242
Epoch 8.85: Loss = 0.666046
Epoch 8.86: Loss = 0.738907
Epoch 8.87: Loss = 0.685471
Epoch 8.88: Loss = 0.606842
Epoch 8.89: Loss = 0.772842
Epoch 8.90: Loss = 0.736389
Epoch 8.91: Loss = 0.785004
Epoch 8.92: Loss = 0.672577
Epoch 8.93: Loss = 0.71637
Epoch 8.94: Loss = 0.689102
Epoch 8.95: Loss = 0.681259
Epoch 8.96: Loss = 0.671143
Epoch 8.97: Loss = 0.564499
Epoch 8.98: Loss = 0.644882
Epoch 8.99: Loss = 0.664688
Epoch 8.100: Loss = 0.682022
Epoch 8.101: Loss = 0.724335
Epoch 8.102: Loss = 0.745651
Epoch 8.103: Loss = 0.62265
Epoch 8.104: Loss = 0.598114
Epoch 8.105: Loss = 0.615402
Epoch 8.106: Loss = 0.7836
Epoch 8.107: Loss = 0.726639
Epoch 8.108: Loss = 0.8349
Epoch 8.109: Loss = 0.729233
Epoch 8.110: Loss = 0.69455
Epoch 8.111: Loss = 0.62381
Epoch 8.112: Loss = 0.678406
Epoch 8.113: Loss = 0.637589
Epoch 8.114: Loss = 0.728851
Epoch 8.115: Loss = 0.646149
Epoch 8.116: Loss = 0.612823
Epoch 8.117: Loss = 0.767181
Epoch 8.118: Loss = 0.6241
Epoch 8.119: Loss = 0.695526
Epoch 8.120: Loss = 0.684494
TRAIN LOSS = 0.689209
TRAIN ACC = 81.2866 % (48774/60000)
Loss = 0.615128
Loss = 0.738968
Loss = 0.673615
Loss = 0.609955
Loss = 0.659241
Loss = 0.775543
Loss = 0.925064
Loss = 0.809235
Loss = 0.729919
Loss = 0.69693
Loss = 0.906113
Loss = 0.869934
Loss = 0.736679
Loss = 0.744598
Loss = 0.769806
Loss = 0.735275
Loss = 0.716202
Loss = 0.767914
Loss = 0.779114
Loss = 0.732117
TEST LOSS = 0.749567
TEST ACC = 487.74 % (8043/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.634949
Epoch 9.2: Loss = 0.675201
Epoch 9.3: Loss = 0.735748
Epoch 9.4: Loss = 0.564209
Epoch 9.5: Loss = 0.735641
Epoch 9.6: Loss = 0.786575
Epoch 9.7: Loss = 0.692215
Epoch 9.8: Loss = 0.800766
Epoch 9.9: Loss = 0.561981
Epoch 9.10: Loss = 0.452103
Epoch 9.11: Loss = 0.828171
Epoch 9.12: Loss = 0.676926
Epoch 9.13: Loss = 0.736389
Epoch 9.14: Loss = 0.689285
Epoch 9.15: Loss = 0.731247
Epoch 9.16: Loss = 0.754196
Epoch 9.17: Loss = 0.643204
Epoch 9.18: Loss = 0.771667
Epoch 9.19: Loss = 0.637268
Epoch 9.20: Loss = 0.786163
Epoch 9.21: Loss = 0.610428
Epoch 9.22: Loss = 0.510651
Epoch 9.23: Loss = 0.713242
Epoch 9.24: Loss = 0.794434
Epoch 9.25: Loss = 0.694
Epoch 9.26: Loss = 0.578537
Epoch 9.27: Loss = 0.703934
Epoch 9.28: Loss = 0.711197
Epoch 9.29: Loss = 0.678635
Epoch 9.30: Loss = 0.749237
Epoch 9.31: Loss = 0.759888
Epoch 9.32: Loss = 0.64006
Epoch 9.33: Loss = 0.571442
Epoch 9.34: Loss = 0.755127
Epoch 9.35: Loss = 0.794952
Epoch 9.36: Loss = 0.802734
Epoch 9.37: Loss = 0.789597
Epoch 9.38: Loss = 0.677811
Epoch 9.39: Loss = 0.801468
Epoch 9.40: Loss = 0.673584
Epoch 9.41: Loss = 0.685486
Epoch 9.42: Loss = 0.667709
Epoch 9.43: Loss = 0.748474
Epoch 9.44: Loss = 0.591934
Epoch 9.45: Loss = 0.740265
Epoch 9.46: Loss = 0.841644
Epoch 9.47: Loss = 0.644684
Epoch 9.48: Loss = 0.621719
Epoch 9.49: Loss = 0.714249
Epoch 9.50: Loss = 0.705994
Epoch 9.51: Loss = 0.547821
Epoch 9.52: Loss = 0.769119
Epoch 9.53: Loss = 0.772369
Epoch 9.54: Loss = 0.506012
Epoch 9.55: Loss = 0.707321
Epoch 9.56: Loss = 0.764404
Epoch 9.57: Loss = 0.783432
Epoch 9.58: Loss = 0.671005
Epoch 9.59: Loss = 0.799347
Epoch 9.60: Loss = 0.691833
Epoch 9.61: Loss = 0.676514
Epoch 9.62: Loss = 0.691498
Epoch 9.63: Loss = 0.590012
Epoch 9.64: Loss = 0.579147
Epoch 9.65: Loss = 0.684128
Epoch 9.66: Loss = 0.588974
Epoch 9.67: Loss = 0.657806
Epoch 9.68: Loss = 0.890411
Epoch 9.69: Loss = 0.670639
Epoch 9.70: Loss = 0.678787
Epoch 9.71: Loss = 0.57135
Epoch 9.72: Loss = 0.729584
Epoch 9.73: Loss = 0.79744
Epoch 9.74: Loss = 0.725266
Epoch 9.75: Loss = 0.618057
Epoch 9.76: Loss = 0.666687
Epoch 9.77: Loss = 0.674713
Epoch 9.78: Loss = 0.677231
Epoch 9.79: Loss = 0.660416
Epoch 9.80: Loss = 0.662842
Epoch 9.81: Loss = 0.652084
Epoch 9.82: Loss = 0.676758
Epoch 9.83: Loss = 0.795807
Epoch 9.84: Loss = 0.626694
Epoch 9.85: Loss = 0.677826
Epoch 9.86: Loss = 0.721817
Epoch 9.87: Loss = 0.695099
Epoch 9.88: Loss = 0.61116
Epoch 9.89: Loss = 0.751251
Epoch 9.90: Loss = 0.732971
Epoch 9.91: Loss = 0.804626
Epoch 9.92: Loss = 0.670227
Epoch 9.93: Loss = 0.720688
Epoch 9.94: Loss = 0.697723
Epoch 9.95: Loss = 0.675201
Epoch 9.96: Loss = 0.638855
Epoch 9.97: Loss = 0.561707
Epoch 9.98: Loss = 0.652313
Epoch 9.99: Loss = 0.683884
Epoch 9.100: Loss = 0.684814
Epoch 9.101: Loss = 0.698486
Epoch 9.102: Loss = 0.751099
Epoch 9.103: Loss = 0.626007
Epoch 9.104: Loss = 0.59816
Epoch 9.105: Loss = 0.626495
Epoch 9.106: Loss = 0.789825
Epoch 9.107: Loss = 0.698273
Epoch 9.108: Loss = 0.854172
Epoch 9.109: Loss = 0.743439
Epoch 9.110: Loss = 0.718506
Epoch 9.111: Loss = 0.634125
Epoch 9.112: Loss = 0.676468
Epoch 9.113: Loss = 0.627045
Epoch 9.114: Loss = 0.756577
Epoch 9.115: Loss = 0.654694
Epoch 9.116: Loss = 0.611603
Epoch 9.117: Loss = 0.763687
Epoch 9.118: Loss = 0.620071
Epoch 9.119: Loss = 0.678696
Epoch 9.120: Loss = 0.682892
TRAIN LOSS = 0.690964
TRAIN ACC = 81.6284 % (48980/60000)
Loss = 0.605713
Loss = 0.735199
Loss = 0.671677
Loss = 0.601227
Loss = 0.667755
Loss = 0.78334
Loss = 0.928818
Loss = 0.797409
Loss = 0.726303
Loss = 0.711975
Loss = 0.922607
Loss = 0.883133
Loss = 0.76123
Loss = 0.738983
Loss = 0.781372
Loss = 0.740341
Loss = 0.744736
Loss = 0.750641
Loss = 0.791412
Loss = 0.736862
TEST LOSS = 0.754037
TEST ACC = 489.799 % (8063/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.647705
Epoch 10.2: Loss = 0.692474
Epoch 10.3: Loss = 0.764069
Epoch 10.4: Loss = 0.587769
Epoch 10.5: Loss = 0.715988
Epoch 10.6: Loss = 0.79422
Epoch 10.7: Loss = 0.709702
Epoch 10.8: Loss = 0.782486
Epoch 10.9: Loss = 0.547562
Epoch 10.10: Loss = 0.468506
Epoch 10.11: Loss = 0.811249
Epoch 10.12: Loss = 0.681
Epoch 10.13: Loss = 0.745087
Epoch 10.14: Loss = 0.684448
Epoch 10.15: Loss = 0.738144
Epoch 10.16: Loss = 0.755341
Epoch 10.17: Loss = 0.654968
Epoch 10.18: Loss = 0.769287
Epoch 10.19: Loss = 0.626526
Epoch 10.20: Loss = 0.805237
Epoch 10.21: Loss = 0.630188
Epoch 10.22: Loss = 0.526596
Epoch 10.23: Loss = 0.690491
Epoch 10.24: Loss = 0.798233
Epoch 10.25: Loss = 0.711639
Epoch 10.26: Loss = 0.583481
Epoch 10.27: Loss = 0.691833
Epoch 10.28: Loss = 0.709946
Epoch 10.29: Loss = 0.681961
Epoch 10.30: Loss = 0.738052
Epoch 10.31: Loss = 0.742203
Epoch 10.32: Loss = 0.639282
Epoch 10.33: Loss = 0.582138
Epoch 10.34: Loss = 0.771179
Epoch 10.35: Loss = 0.806381
Epoch 10.36: Loss = 0.816803
Epoch 10.37: Loss = 0.805649
Epoch 10.38: Loss = 0.712311
Epoch 10.39: Loss = 0.80806
Epoch 10.40: Loss = 0.664627
Epoch 10.41: Loss = 0.692642
Epoch 10.42: Loss = 0.672623
Epoch 10.43: Loss = 0.758057
Epoch 10.44: Loss = 0.601929
Epoch 10.45: Loss = 0.745697
Epoch 10.46: Loss = 0.857651
Epoch 10.47: Loss = 0.656967
Epoch 10.48: Loss = 0.615036
Epoch 10.49: Loss = 0.746582
Epoch 10.50: Loss = 0.733566
Epoch 10.51: Loss = 0.533844
Epoch 10.52: Loss = 0.798386
Epoch 10.53: Loss = 0.780807
Epoch 10.54: Loss = 0.532211
Epoch 10.55: Loss = 0.742935
Epoch 10.56: Loss = 0.766968
Epoch 10.57: Loss = 0.780548
Epoch 10.58: Loss = 0.677429
Epoch 10.59: Loss = 0.796555
Epoch 10.60: Loss = 0.726486
Epoch 10.61: Loss = 0.690491
Epoch 10.62: Loss = 0.708542
Epoch 10.63: Loss = 0.596176
Epoch 10.64: Loss = 0.596008
Epoch 10.65: Loss = 0.708267
Epoch 10.66: Loss = 0.583267
Epoch 10.67: Loss = 0.652084
Epoch 10.68: Loss = 0.917053
Epoch 10.69: Loss = 0.690323
Epoch 10.70: Loss = 0.669296
Epoch 10.71: Loss = 0.5616
Epoch 10.72: Loss = 0.716827
Epoch 10.73: Loss = 0.783646
Epoch 10.74: Loss = 0.740021
Epoch 10.75: Loss = 0.604553
Epoch 10.76: Loss = 0.679871
Epoch 10.77: Loss = 0.685547
Epoch 10.78: Loss = 0.693726
Epoch 10.79: Loss = 0.667099
Epoch 10.80: Loss = 0.676941
Epoch 10.81: Loss = 0.663437
Epoch 10.82: Loss = 0.682602
Epoch 10.83: Loss = 0.81572
Epoch 10.84: Loss = 0.632217
Epoch 10.85: Loss = 0.679916
Epoch 10.86: Loss = 0.699478
Epoch 10.87: Loss = 0.673782
Epoch 10.88: Loss = 0.621704
Epoch 10.89: Loss = 0.761551
Epoch 10.90: Loss = 0.753479
Epoch 10.91: Loss = 0.796143
Epoch 10.92: Loss = 0.684219
Epoch 10.93: Loss = 0.707062
Epoch 10.94: Loss = 0.727982
Epoch 10.95: Loss = 0.686722
Epoch 10.96: Loss = 0.656128
Epoch 10.97: Loss = 0.568756
Epoch 10.98: Loss = 0.639435
Epoch 10.99: Loss = 0.65918
Epoch 10.100: Loss = 0.683929
Epoch 10.101: Loss = 0.704086
Epoch 10.102: Loss = 0.742477
Epoch 10.103: Loss = 0.630493
Epoch 10.104: Loss = 0.61348
Epoch 10.105: Loss = 0.613403
Epoch 10.106: Loss = 0.795044
Epoch 10.107: Loss = 0.697678
Epoch 10.108: Loss = 0.850708
Epoch 10.109: Loss = 0.744614
Epoch 10.110: Loss = 0.716339
Epoch 10.111: Loss = 0.638702
Epoch 10.112: Loss = 0.674042
Epoch 10.113: Loss = 0.625671
Epoch 10.114: Loss = 0.726166
Epoch 10.115: Loss = 0.646301
Epoch 10.116: Loss = 0.633011
Epoch 10.117: Loss = 0.771545
Epoch 10.118: Loss = 0.607666
Epoch 10.119: Loss = 0.663452
Epoch 10.120: Loss = 0.670517
TRAIN LOSS = 0.695755
TRAIN ACC = 81.6879 % (49015/60000)
Loss = 0.610794
Loss = 0.749771
Loss = 0.671555
Loss = 0.597961
Loss = 0.682327
Loss = 0.801453
Loss = 0.93335
Loss = 0.797424
Loss = 0.710052
Loss = 0.704926
Loss = 0.941727
Loss = 0.883926
Loss = 0.754822
Loss = 0.762741
Loss = 0.758041
Loss = 0.738449
Loss = 0.724152
Loss = 0.771637
Loss = 0.783676
Loss = 0.733597
TEST LOSS = 0.755619
TEST ACC = 490.149 % (8065/10000)
