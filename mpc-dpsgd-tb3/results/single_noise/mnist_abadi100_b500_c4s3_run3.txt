Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.3913
Epoch 1.2: Loss = 2.35992
Epoch 1.3: Loss = 2.32188
Epoch 1.4: Loss = 2.26907
Epoch 1.5: Loss = 2.18816
Epoch 1.6: Loss = 2.16574
Epoch 1.7: Loss = 2.15135
Epoch 1.8: Loss = 2.12097
Epoch 1.9: Loss = 2.05614
Epoch 1.10: Loss = 1.98943
Epoch 1.11: Loss = 1.96625
Epoch 1.12: Loss = 1.95619
Epoch 1.13: Loss = 1.89047
Epoch 1.14: Loss = 1.87683
Epoch 1.15: Loss = 1.93095
Epoch 1.16: Loss = 1.8622
Epoch 1.17: Loss = 1.804
Epoch 1.18: Loss = 1.78677
Epoch 1.19: Loss = 1.72073
Epoch 1.20: Loss = 1.69611
Epoch 1.21: Loss = 1.64388
Epoch 1.22: Loss = 1.63142
Epoch 1.23: Loss = 1.59512
Epoch 1.24: Loss = 1.63643
Epoch 1.25: Loss = 1.57887
Epoch 1.26: Loss = 1.59245
Epoch 1.27: Loss = 1.53822
Epoch 1.28: Loss = 1.5211
Epoch 1.29: Loss = 1.51591
Epoch 1.30: Loss = 1.58463
Epoch 1.31: Loss = 1.43712
Epoch 1.32: Loss = 1.4579
Epoch 1.33: Loss = 1.37949
Epoch 1.34: Loss = 1.41428
Epoch 1.35: Loss = 1.34033
Epoch 1.36: Loss = 1.43521
Epoch 1.37: Loss = 1.2827
Epoch 1.38: Loss = 1.23132
Epoch 1.39: Loss = 1.24869
Epoch 1.40: Loss = 1.14914
Epoch 1.41: Loss = 1.25444
Epoch 1.42: Loss = 1.19984
Epoch 1.43: Loss = 1.16284
Epoch 1.44: Loss = 1.07295
Epoch 1.45: Loss = 1.20142
Epoch 1.46: Loss = 1.14934
Epoch 1.47: Loss = 1.06075
Epoch 1.48: Loss = 1.1179
Epoch 1.49: Loss = 1.05389
Epoch 1.50: Loss = 1.13037
Epoch 1.51: Loss = 0.991043
Epoch 1.52: Loss = 0.975876
Epoch 1.53: Loss = 1.0293
Epoch 1.54: Loss = 1.04056
Epoch 1.55: Loss = 1.03203
Epoch 1.56: Loss = 0.938446
Epoch 1.57: Loss = 0.886124
Epoch 1.58: Loss = 0.954407
Epoch 1.59: Loss = 0.973938
Epoch 1.60: Loss = 1.04507
Epoch 1.61: Loss = 0.986893
Epoch 1.62: Loss = 0.989716
Epoch 1.63: Loss = 1.03761
Epoch 1.64: Loss = 0.989532
Epoch 1.65: Loss = 1.01372
Epoch 1.66: Loss = 0.893097
Epoch 1.67: Loss = 0.908875
Epoch 1.68: Loss = 0.734116
Epoch 1.69: Loss = 0.80957
Epoch 1.70: Loss = 0.923828
Epoch 1.71: Loss = 0.83931
Epoch 1.72: Loss = 0.83168
Epoch 1.73: Loss = 0.879501
Epoch 1.74: Loss = 0.718933
Epoch 1.75: Loss = 0.867523
Epoch 1.76: Loss = 0.812531
Epoch 1.77: Loss = 0.791916
Epoch 1.78: Loss = 0.755203
Epoch 1.79: Loss = 0.754608
Epoch 1.80: Loss = 0.868134
Epoch 1.81: Loss = 0.735535
Epoch 1.82: Loss = 0.682251
Epoch 1.83: Loss = 0.886475
Epoch 1.84: Loss = 0.784348
Epoch 1.85: Loss = 0.856308
Epoch 1.86: Loss = 0.75032
Epoch 1.87: Loss = 0.695145
Epoch 1.88: Loss = 0.704117
Epoch 1.89: Loss = 0.83252
Epoch 1.90: Loss = 0.656113
Epoch 1.91: Loss = 0.753204
Epoch 1.92: Loss = 0.746719
Epoch 1.93: Loss = 0.755646
Epoch 1.94: Loss = 0.602859
Epoch 1.95: Loss = 0.732376
Epoch 1.96: Loss = 0.695023
Epoch 1.97: Loss = 0.521927
Epoch 1.98: Loss = 0.672424
Epoch 1.99: Loss = 0.749359
Epoch 1.100: Loss = 0.842438
Epoch 1.101: Loss = 0.733658
Epoch 1.102: Loss = 0.673599
Epoch 1.103: Loss = 0.627716
Epoch 1.104: Loss = 0.588852
Epoch 1.105: Loss = 0.70459
Epoch 1.106: Loss = 0.690842
Epoch 1.107: Loss = 0.591309
Epoch 1.108: Loss = 0.640427
Epoch 1.109: Loss = 0.623444
Epoch 1.110: Loss = 0.648926
Epoch 1.111: Loss = 0.53569
Epoch 1.112: Loss = 0.516937
Epoch 1.113: Loss = 0.606781
Epoch 1.114: Loss = 0.537201
Epoch 1.115: Loss = 0.614136
Epoch 1.116: Loss = 0.589661
Epoch 1.117: Loss = 0.49498
Epoch 1.118: Loss = 0.430298
Epoch 1.119: Loss = 0.446793
Epoch 1.120: Loss = 0.493073
TRAIN LOSS = 1.12367
TRAIN ACC = 68.6905 % (41216/60000)
Loss = 0.627029
Loss = 0.66098
Loss = 0.735947
Loss = 0.717468
Loss = 0.734192
Loss = 0.630859
Loss = 0.610794
Loss = 0.764221
Loss = 0.725357
Loss = 0.694794
Loss = 0.357498
Loss = 0.527969
Loss = 0.382492
Loss = 0.558533
Loss = 0.465683
Loss = 0.467758
Loss = 0.421829
Loss = 0.238388
Loss = 0.436081
Loss = 0.72731
TEST LOSS = 0.574259
TEST ACC = 412.16 % (8359/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.55423
Epoch 2.2: Loss = 0.677597
Epoch 2.3: Loss = 0.675522
Epoch 2.4: Loss = 0.538239
Epoch 2.5: Loss = 0.527145
Epoch 2.6: Loss = 0.519791
Epoch 2.7: Loss = 0.603851
Epoch 2.8: Loss = 0.566223
Epoch 2.9: Loss = 0.55777
Epoch 2.10: Loss = 0.5513
Epoch 2.11: Loss = 0.549133
Epoch 2.12: Loss = 0.541016
Epoch 2.13: Loss = 0.46759
Epoch 2.14: Loss = 0.497681
Epoch 2.15: Loss = 0.646225
Epoch 2.16: Loss = 0.610504
Epoch 2.17: Loss = 0.622772
Epoch 2.18: Loss = 0.686539
Epoch 2.19: Loss = 0.514069
Epoch 2.20: Loss = 0.495605
Epoch 2.21: Loss = 0.480087
Epoch 2.22: Loss = 0.477524
Epoch 2.23: Loss = 0.475143
Epoch 2.24: Loss = 0.669556
Epoch 2.25: Loss = 0.559753
Epoch 2.26: Loss = 0.635376
Epoch 2.27: Loss = 0.612259
Epoch 2.28: Loss = 0.609589
Epoch 2.29: Loss = 0.65535
Epoch 2.30: Loss = 0.749084
Epoch 2.31: Loss = 0.495682
Epoch 2.32: Loss = 0.645599
Epoch 2.33: Loss = 0.516769
Epoch 2.34: Loss = 0.60643
Epoch 2.35: Loss = 0.570999
Epoch 2.36: Loss = 0.635284
Epoch 2.37: Loss = 0.468811
Epoch 2.38: Loss = 0.442825
Epoch 2.39: Loss = 0.526672
Epoch 2.40: Loss = 0.473419
Epoch 2.41: Loss = 0.556152
Epoch 2.42: Loss = 0.608139
Epoch 2.43: Loss = 0.470474
Epoch 2.44: Loss = 0.406784
Epoch 2.45: Loss = 0.548019
Epoch 2.46: Loss = 0.586929
Epoch 2.47: Loss = 0.47934
Epoch 2.48: Loss = 0.536484
Epoch 2.49: Loss = 0.501953
Epoch 2.50: Loss = 0.585144
Epoch 2.51: Loss = 0.465164
Epoch 2.52: Loss = 0.425201
Epoch 2.53: Loss = 0.480743
Epoch 2.54: Loss = 0.569305
Epoch 2.55: Loss = 0.510727
Epoch 2.56: Loss = 0.455017
Epoch 2.57: Loss = 0.444275
Epoch 2.58: Loss = 0.508957
Epoch 2.59: Loss = 0.555328
Epoch 2.60: Loss = 0.610443
Epoch 2.61: Loss = 0.586777
Epoch 2.62: Loss = 0.583984
Epoch 2.63: Loss = 0.650864
Epoch 2.64: Loss = 0.604584
Epoch 2.65: Loss = 0.649811
Epoch 2.66: Loss = 0.504578
Epoch 2.67: Loss = 0.549377
Epoch 2.68: Loss = 0.350067
Epoch 2.69: Loss = 0.430618
Epoch 2.70: Loss = 0.596863
Epoch 2.71: Loss = 0.458435
Epoch 2.72: Loss = 0.475769
Epoch 2.73: Loss = 0.533661
Epoch 2.74: Loss = 0.396149
Epoch 2.75: Loss = 0.626236
Epoch 2.76: Loss = 0.516861
Epoch 2.77: Loss = 0.4599
Epoch 2.78: Loss = 0.474045
Epoch 2.79: Loss = 0.510788
Epoch 2.80: Loss = 0.558685
Epoch 2.81: Loss = 0.437439
Epoch 2.82: Loss = 0.394104
Epoch 2.83: Loss = 0.612778
Epoch 2.84: Loss = 0.494034
Epoch 2.85: Loss = 0.62233
Epoch 2.86: Loss = 0.518616
Epoch 2.87: Loss = 0.391266
Epoch 2.88: Loss = 0.461258
Epoch 2.89: Loss = 0.596603
Epoch 2.90: Loss = 0.389175
Epoch 2.91: Loss = 0.512558
Epoch 2.92: Loss = 0.525024
Epoch 2.93: Loss = 0.541489
Epoch 2.94: Loss = 0.390533
Epoch 2.95: Loss = 0.509903
Epoch 2.96: Loss = 0.523651
Epoch 2.97: Loss = 0.342682
Epoch 2.98: Loss = 0.452194
Epoch 2.99: Loss = 0.552124
Epoch 2.100: Loss = 0.60997
Epoch 2.101: Loss = 0.558334
Epoch 2.102: Loss = 0.468353
Epoch 2.103: Loss = 0.438431
Epoch 2.104: Loss = 0.382401
Epoch 2.105: Loss = 0.539719
Epoch 2.106: Loss = 0.5224
Epoch 2.107: Loss = 0.398941
Epoch 2.108: Loss = 0.47197
Epoch 2.109: Loss = 0.443359
Epoch 2.110: Loss = 0.492004
Epoch 2.111: Loss = 0.372574
Epoch 2.112: Loss = 0.369583
Epoch 2.113: Loss = 0.437759
Epoch 2.114: Loss = 0.373764
Epoch 2.115: Loss = 0.410217
Epoch 2.116: Loss = 0.427399
Epoch 2.117: Loss = 0.311081
Epoch 2.118: Loss = 0.260895
Epoch 2.119: Loss = 0.306793
Epoch 2.120: Loss = 0.362732
TRAIN LOSS = 0.514664
TRAIN ACC = 84.8831 % (50933/60000)
Loss = 0.455643
Loss = 0.50946
Loss = 0.576675
Loss = 0.575073
Loss = 0.596222
Loss = 0.467682
Loss = 0.446945
Loss = 0.626389
Loss = 0.57515
Loss = 0.552002
Loss = 0.238998
Loss = 0.36409
Loss = 0.2836
Loss = 0.420319
Loss = 0.304138
Loss = 0.351212
Loss = 0.270004
Loss = 0.121765
Loss = 0.297409
Loss = 0.597382
TEST LOSS = 0.431508
TEST ACC = 509.329 % (8695/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.416779
Epoch 3.2: Loss = 0.540237
Epoch 3.3: Loss = 0.538406
Epoch 3.4: Loss = 0.380432
Epoch 3.5: Loss = 0.392319
Epoch 3.6: Loss = 0.387619
Epoch 3.7: Loss = 0.435654
Epoch 3.8: Loss = 0.421051
Epoch 3.9: Loss = 0.415237
Epoch 3.10: Loss = 0.422226
Epoch 3.11: Loss = 0.433014
Epoch 3.12: Loss = 0.406982
Epoch 3.13: Loss = 0.346329
Epoch 3.14: Loss = 0.385941
Epoch 3.15: Loss = 0.489548
Epoch 3.16: Loss = 0.493423
Epoch 3.17: Loss = 0.519669
Epoch 3.18: Loss = 0.597168
Epoch 3.19: Loss = 0.415222
Epoch 3.20: Loss = 0.384552
Epoch 3.21: Loss = 0.363419
Epoch 3.22: Loss = 0.359924
Epoch 3.23: Loss = 0.372055
Epoch 3.24: Loss = 0.573685
Epoch 3.25: Loss = 0.449661
Epoch 3.26: Loss = 0.543686
Epoch 3.27: Loss = 0.520782
Epoch 3.28: Loss = 0.50206
Epoch 3.29: Loss = 0.553345
Epoch 3.30: Loss = 0.637894
Epoch 3.31: Loss = 0.393234
Epoch 3.32: Loss = 0.514832
Epoch 3.33: Loss = 0.393875
Epoch 3.34: Loss = 0.508453
Epoch 3.35: Loss = 0.459152
Epoch 3.36: Loss = 0.520737
Epoch 3.37: Loss = 0.358826
Epoch 3.38: Loss = 0.350922
Epoch 3.39: Loss = 0.422165
Epoch 3.40: Loss = 0.370605
Epoch 3.41: Loss = 0.442184
Epoch 3.42: Loss = 0.538696
Epoch 3.43: Loss = 0.369873
Epoch 3.44: Loss = 0.323135
Epoch 3.45: Loss = 0.446182
Epoch 3.46: Loss = 0.494736
Epoch 3.47: Loss = 0.396622
Epoch 3.48: Loss = 0.443848
Epoch 3.49: Loss = 0.413574
Epoch 3.50: Loss = 0.498886
Epoch 3.51: Loss = 0.373306
Epoch 3.52: Loss = 0.34111
Epoch 3.53: Loss = 0.394745
Epoch 3.54: Loss = 0.507858
Epoch 3.55: Loss = 0.429916
Epoch 3.56: Loss = 0.375244
Epoch 3.57: Loss = 0.378082
Epoch 3.58: Loss = 0.427261
Epoch 3.59: Loss = 0.485931
Epoch 3.60: Loss = 0.535934
Epoch 3.61: Loss = 0.489944
Epoch 3.62: Loss = 0.508179
Epoch 3.63: Loss = 0.57283
Epoch 3.64: Loss = 0.530289
Epoch 3.65: Loss = 0.563873
Epoch 3.66: Loss = 0.428101
Epoch 3.67: Loss = 0.454971
Epoch 3.68: Loss = 0.276596
Epoch 3.69: Loss = 0.362732
Epoch 3.70: Loss = 0.53125
Epoch 3.71: Loss = 0.372849
Epoch 3.72: Loss = 0.377853
Epoch 3.73: Loss = 0.460831
Epoch 3.74: Loss = 0.327042
Epoch 3.75: Loss = 0.590958
Epoch 3.76: Loss = 0.449707
Epoch 3.77: Loss = 0.379562
Epoch 3.78: Loss = 0.413818
Epoch 3.79: Loss = 0.472229
Epoch 3.80: Loss = 0.463745
Epoch 3.81: Loss = 0.374252
Epoch 3.82: Loss = 0.330399
Epoch 3.83: Loss = 0.528091
Epoch 3.84: Loss = 0.420349
Epoch 3.85: Loss = 0.567429
Epoch 3.86: Loss = 0.481323
Epoch 3.87: Loss = 0.333069
Epoch 3.88: Loss = 0.403091
Epoch 3.89: Loss = 0.525528
Epoch 3.90: Loss = 0.331604
Epoch 3.91: Loss = 0.461929
Epoch 3.92: Loss = 0.471313
Epoch 3.93: Loss = 0.501266
Epoch 3.94: Loss = 0.346313
Epoch 3.95: Loss = 0.445877
Epoch 3.96: Loss = 0.471008
Epoch 3.97: Loss = 0.309067
Epoch 3.98: Loss = 0.393433
Epoch 3.99: Loss = 0.481354
Epoch 3.100: Loss = 0.559509
Epoch 3.101: Loss = 0.524796
Epoch 3.102: Loss = 0.415161
Epoch 3.103: Loss = 0.378738
Epoch 3.104: Loss = 0.340927
Epoch 3.105: Loss = 0.512039
Epoch 3.106: Loss = 0.494125
Epoch 3.107: Loss = 0.349518
Epoch 3.108: Loss = 0.432846
Epoch 3.109: Loss = 0.391754
Epoch 3.110: Loss = 0.442764
Epoch 3.111: Loss = 0.332565
Epoch 3.112: Loss = 0.322281
Epoch 3.113: Loss = 0.382019
Epoch 3.114: Loss = 0.326309
Epoch 3.115: Loss = 0.337799
Epoch 3.116: Loss = 0.378082
Epoch 3.117: Loss = 0.239395
Epoch 3.118: Loss = 0.223724
Epoch 3.119: Loss = 0.283875
Epoch 3.120: Loss = 0.325775
TRAIN LOSS = 0.431717
TRAIN ACC = 87.114 % (52271/60000)
Loss = 0.404846
Loss = 0.463364
Loss = 0.519516
Loss = 0.536331
Loss = 0.537445
Loss = 0.41893
Loss = 0.388306
Loss = 0.583527
Loss = 0.518082
Loss = 0.495316
Loss = 0.201996
Loss = 0.322159
Loss = 0.263474
Loss = 0.373734
Loss = 0.252716
Loss = 0.31102
Loss = 0.222733
Loss = 0.0875702
Loss = 0.258545
Loss = 0.540558
TEST LOSS = 0.385008
TEST ACC = 522.71 % (8856/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.370056
Epoch 4.2: Loss = 0.486694
Epoch 4.3: Loss = 0.492203
Epoch 4.4: Loss = 0.324036
Epoch 4.5: Loss = 0.356552
Epoch 4.6: Loss = 0.350845
Epoch 4.7: Loss = 0.382645
Epoch 4.8: Loss = 0.36499
Epoch 4.9: Loss = 0.3638
Epoch 4.10: Loss = 0.373871
Epoch 4.11: Loss = 0.38414
Epoch 4.12: Loss = 0.358978
Epoch 4.13: Loss = 0.293381
Epoch 4.14: Loss = 0.355331
Epoch 4.15: Loss = 0.426987
Epoch 4.16: Loss = 0.441071
Epoch 4.17: Loss = 0.47998
Epoch 4.18: Loss = 0.575806
Epoch 4.19: Loss = 0.379898
Epoch 4.20: Loss = 0.363266
Epoch 4.21: Loss = 0.317245
Epoch 4.22: Loss = 0.305374
Epoch 4.23: Loss = 0.324326
Epoch 4.24: Loss = 0.546021
Epoch 4.25: Loss = 0.436157
Epoch 4.26: Loss = 0.509048
Epoch 4.27: Loss = 0.46019
Epoch 4.28: Loss = 0.471359
Epoch 4.29: Loss = 0.528702
Epoch 4.30: Loss = 0.60112
Epoch 4.31: Loss = 0.348816
Epoch 4.32: Loss = 0.46666
Epoch 4.33: Loss = 0.35849
Epoch 4.34: Loss = 0.457733
Epoch 4.35: Loss = 0.417847
Epoch 4.36: Loss = 0.478485
Epoch 4.37: Loss = 0.320023
Epoch 4.38: Loss = 0.324799
Epoch 4.39: Loss = 0.378632
Epoch 4.40: Loss = 0.341492
Epoch 4.41: Loss = 0.399216
Epoch 4.42: Loss = 0.514221
Epoch 4.43: Loss = 0.324341
Epoch 4.44: Loss = 0.291779
Epoch 4.45: Loss = 0.416138
Epoch 4.46: Loss = 0.453262
Epoch 4.47: Loss = 0.359589
Epoch 4.48: Loss = 0.399597
Epoch 4.49: Loss = 0.37851
Epoch 4.50: Loss = 0.473282
Epoch 4.51: Loss = 0.340363
Epoch 4.52: Loss = 0.314468
Epoch 4.53: Loss = 0.372925
Epoch 4.54: Loss = 0.47435
Epoch 4.55: Loss = 0.386139
Epoch 4.56: Loss = 0.338394
Epoch 4.57: Loss = 0.353836
Epoch 4.58: Loss = 0.38443
Epoch 4.59: Loss = 0.44606
Epoch 4.60: Loss = 0.495377
Epoch 4.61: Loss = 0.439911
Epoch 4.62: Loss = 0.473846
Epoch 4.63: Loss = 0.543213
Epoch 4.64: Loss = 0.489532
Epoch 4.65: Loss = 0.539856
Epoch 4.66: Loss = 0.394608
Epoch 4.67: Loss = 0.425629
Epoch 4.68: Loss = 0.249268
Epoch 4.69: Loss = 0.322189
Epoch 4.70: Loss = 0.498276
Epoch 4.71: Loss = 0.345062
Epoch 4.72: Loss = 0.336349
Epoch 4.73: Loss = 0.423187
Epoch 4.74: Loss = 0.303818
Epoch 4.75: Loss = 0.592194
Epoch 4.76: Loss = 0.435242
Epoch 4.77: Loss = 0.338211
Epoch 4.78: Loss = 0.384628
Epoch 4.79: Loss = 0.460236
Epoch 4.80: Loss = 0.43692
Epoch 4.81: Loss = 0.340805
Epoch 4.82: Loss = 0.304428
Epoch 4.83: Loss = 0.495239
Epoch 4.84: Loss = 0.390793
Epoch 4.85: Loss = 0.560287
Epoch 4.86: Loss = 0.469772
Epoch 4.87: Loss = 0.303574
Epoch 4.88: Loss = 0.382233
Epoch 4.89: Loss = 0.487381
Epoch 4.90: Loss = 0.312531
Epoch 4.91: Loss = 0.458221
Epoch 4.92: Loss = 0.465317
Epoch 4.93: Loss = 0.484283
Epoch 4.94: Loss = 0.311783
Epoch 4.95: Loss = 0.422379
Epoch 4.96: Loss = 0.448929
Epoch 4.97: Loss = 0.291443
Epoch 4.98: Loss = 0.37384
Epoch 4.99: Loss = 0.4664
Epoch 4.100: Loss = 0.533005
Epoch 4.101: Loss = 0.515701
Epoch 4.102: Loss = 0.375732
Epoch 4.103: Loss = 0.351654
Epoch 4.104: Loss = 0.33429
Epoch 4.105: Loss = 0.486404
Epoch 4.106: Loss = 0.493561
Epoch 4.107: Loss = 0.325134
Epoch 4.108: Loss = 0.420547
Epoch 4.109: Loss = 0.364685
Epoch 4.110: Loss = 0.429306
Epoch 4.111: Loss = 0.32312
Epoch 4.112: Loss = 0.320175
Epoch 4.113: Loss = 0.35498
Epoch 4.114: Loss = 0.306656
Epoch 4.115: Loss = 0.30246
Epoch 4.116: Loss = 0.367722
Epoch 4.117: Loss = 0.205383
Epoch 4.118: Loss = 0.20993
Epoch 4.119: Loss = 0.266785
Epoch 4.120: Loss = 0.310684
TRAIN LOSS = 0.400269
TRAIN ACC = 88.205 % (52925/60000)
Loss = 0.379684
Loss = 0.462158
Loss = 0.512146
Loss = 0.524307
Loss = 0.526016
Loss = 0.400635
Loss = 0.369934
Loss = 0.581711
Loss = 0.514847
Loss = 0.47287
Loss = 0.185318
Loss = 0.30162
Loss = 0.242249
Loss = 0.349945
Loss = 0.225479
Loss = 0.291229
Loss = 0.203049
Loss = 0.0695801
Loss = 0.235153
Loss = 0.542587
TEST LOSS = 0.369526
TEST ACC = 529.25 % (8920/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.360672
Epoch 5.2: Loss = 0.473511
Epoch 5.3: Loss = 0.479218
Epoch 5.4: Loss = 0.303329
Epoch 5.5: Loss = 0.32634
Epoch 5.6: Loss = 0.330124
Epoch 5.7: Loss = 0.360794
Epoch 5.8: Loss = 0.350769
Epoch 5.9: Loss = 0.348129
Epoch 5.10: Loss = 0.345535
Epoch 5.11: Loss = 0.367508
Epoch 5.12: Loss = 0.35318
Epoch 5.13: Loss = 0.287033
Epoch 5.14: Loss = 0.348038
Epoch 5.15: Loss = 0.407471
Epoch 5.16: Loss = 0.407196
Epoch 5.17: Loss = 0.462692
Epoch 5.18: Loss = 0.596802
Epoch 5.19: Loss = 0.372513
Epoch 5.20: Loss = 0.338928
Epoch 5.21: Loss = 0.307892
Epoch 5.22: Loss = 0.287399
Epoch 5.23: Loss = 0.302063
Epoch 5.24: Loss = 0.534485
Epoch 5.25: Loss = 0.445831
Epoch 5.26: Loss = 0.493378
Epoch 5.27: Loss = 0.447357
Epoch 5.28: Loss = 0.462784
Epoch 5.29: Loss = 0.522949
Epoch 5.30: Loss = 0.570358
Epoch 5.31: Loss = 0.340988
Epoch 5.32: Loss = 0.44754
Epoch 5.33: Loss = 0.341492
Epoch 5.34: Loss = 0.433289
Epoch 5.35: Loss = 0.405411
Epoch 5.36: Loss = 0.453476
Epoch 5.37: Loss = 0.290955
Epoch 5.38: Loss = 0.314194
Epoch 5.39: Loss = 0.348434
Epoch 5.40: Loss = 0.335266
Epoch 5.41: Loss = 0.385483
Epoch 5.42: Loss = 0.524826
Epoch 5.43: Loss = 0.308975
Epoch 5.44: Loss = 0.283279
Epoch 5.45: Loss = 0.392975
Epoch 5.46: Loss = 0.43956
Epoch 5.47: Loss = 0.356003
Epoch 5.48: Loss = 0.392853
Epoch 5.49: Loss = 0.365326
Epoch 5.50: Loss = 0.468491
Epoch 5.51: Loss = 0.329422
Epoch 5.52: Loss = 0.289688
Epoch 5.53: Loss = 0.354584
Epoch 5.54: Loss = 0.474442
Epoch 5.55: Loss = 0.375748
Epoch 5.56: Loss = 0.33522
Epoch 5.57: Loss = 0.35437
Epoch 5.58: Loss = 0.386551
Epoch 5.59: Loss = 0.444473
Epoch 5.60: Loss = 0.474655
Epoch 5.61: Loss = 0.42305
Epoch 5.62: Loss = 0.465439
Epoch 5.63: Loss = 0.540131
Epoch 5.64: Loss = 0.485016
Epoch 5.65: Loss = 0.548752
Epoch 5.66: Loss = 0.386032
Epoch 5.67: Loss = 0.423691
Epoch 5.68: Loss = 0.235168
Epoch 5.69: Loss = 0.317917
Epoch 5.70: Loss = 0.490997
Epoch 5.71: Loss = 0.33815
Epoch 5.72: Loss = 0.327438
Epoch 5.73: Loss = 0.403931
Epoch 5.74: Loss = 0.278854
Epoch 5.75: Loss = 0.602951
Epoch 5.76: Loss = 0.406281
Epoch 5.77: Loss = 0.329651
Epoch 5.78: Loss = 0.370453
Epoch 5.79: Loss = 0.465286
Epoch 5.80: Loss = 0.400635
Epoch 5.81: Loss = 0.328537
Epoch 5.82: Loss = 0.291519
Epoch 5.83: Loss = 0.479858
Epoch 5.84: Loss = 0.37915
Epoch 5.85: Loss = 0.558578
Epoch 5.86: Loss = 0.46492
Epoch 5.87: Loss = 0.286911
Epoch 5.88: Loss = 0.376968
Epoch 5.89: Loss = 0.459274
Epoch 5.90: Loss = 0.297913
Epoch 5.91: Loss = 0.433792
Epoch 5.92: Loss = 0.448669
Epoch 5.93: Loss = 0.479187
Epoch 5.94: Loss = 0.28775
Epoch 5.95: Loss = 0.403778
Epoch 5.96: Loss = 0.440231
Epoch 5.97: Loss = 0.279617
Epoch 5.98: Loss = 0.351456
Epoch 5.99: Loss = 0.445068
Epoch 5.100: Loss = 0.526703
Epoch 5.101: Loss = 0.500732
Epoch 5.102: Loss = 0.360794
Epoch 5.103: Loss = 0.327988
Epoch 5.104: Loss = 0.309418
Epoch 5.105: Loss = 0.473114
Epoch 5.106: Loss = 0.489868
Epoch 5.107: Loss = 0.306732
Epoch 5.108: Loss = 0.414124
Epoch 5.109: Loss = 0.345612
Epoch 5.110: Loss = 0.410614
Epoch 5.111: Loss = 0.307297
Epoch 5.112: Loss = 0.31398
Epoch 5.113: Loss = 0.336761
Epoch 5.114: Loss = 0.293762
Epoch 5.115: Loss = 0.282135
Epoch 5.116: Loss = 0.344376
Epoch 5.117: Loss = 0.206879
Epoch 5.118: Loss = 0.19986
Epoch 5.119: Loss = 0.253708
Epoch 5.120: Loss = 0.299942
TRAIN LOSS = 0.387329
TRAIN ACC = 88.7558 % (53256/60000)
Loss = 0.366348
Loss = 0.443008
Loss = 0.482712
Loss = 0.522049
Loss = 0.521362
Loss = 0.387726
Loss = 0.354599
Loss = 0.586685
Loss = 0.510178
Loss = 0.470795
Loss = 0.166992
Loss = 0.281937
Loss = 0.235321
Loss = 0.352768
Loss = 0.204514
Loss = 0.286758
Loss = 0.192886
Loss = 0.0612793
Loss = 0.217041
Loss = 0.543976
TEST LOSS = 0.359447
TEST ACC = 532.559 % (8959/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.346344
Epoch 6.2: Loss = 0.460587
Epoch 6.3: Loss = 0.467285
Epoch 6.4: Loss = 0.278992
Epoch 6.5: Loss = 0.308411
Epoch 6.6: Loss = 0.311264
Epoch 6.7: Loss = 0.335297
Epoch 6.8: Loss = 0.339737
Epoch 6.9: Loss = 0.356552
Epoch 6.10: Loss = 0.326828
Epoch 6.11: Loss = 0.36116
Epoch 6.12: Loss = 0.332489
Epoch 6.13: Loss = 0.27774
Epoch 6.14: Loss = 0.340851
Epoch 6.15: Loss = 0.394424
Epoch 6.16: Loss = 0.396774
Epoch 6.17: Loss = 0.445328
Epoch 6.18: Loss = 0.59787
Epoch 6.19: Loss = 0.379242
Epoch 6.20: Loss = 0.321243
Epoch 6.21: Loss = 0.302078
Epoch 6.22: Loss = 0.282532
Epoch 6.23: Loss = 0.292725
Epoch 6.24: Loss = 0.492279
Epoch 6.25: Loss = 0.428909
Epoch 6.26: Loss = 0.501877
Epoch 6.27: Loss = 0.432114
Epoch 6.28: Loss = 0.452576
Epoch 6.29: Loss = 0.507141
Epoch 6.30: Loss = 0.56546
Epoch 6.31: Loss = 0.320816
Epoch 6.32: Loss = 0.414719
Epoch 6.33: Loss = 0.326492
Epoch 6.34: Loss = 0.41272
Epoch 6.35: Loss = 0.382172
Epoch 6.36: Loss = 0.434845
Epoch 6.37: Loss = 0.271332
Epoch 6.38: Loss = 0.302628
Epoch 6.39: Loss = 0.325043
Epoch 6.40: Loss = 0.32724
Epoch 6.41: Loss = 0.364243
Epoch 6.42: Loss = 0.520432
Epoch 6.43: Loss = 0.306076
Epoch 6.44: Loss = 0.284424
Epoch 6.45: Loss = 0.371796
Epoch 6.46: Loss = 0.412491
Epoch 6.47: Loss = 0.341125
Epoch 6.48: Loss = 0.382034
Epoch 6.49: Loss = 0.348328
Epoch 6.50: Loss = 0.461685
Epoch 6.51: Loss = 0.311722
Epoch 6.52: Loss = 0.281326
Epoch 6.53: Loss = 0.355484
