Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.33981
Epoch 1.2: Loss = 2.30627
Epoch 1.3: Loss = 2.27646
Epoch 1.4: Loss = 2.26636
Epoch 1.5: Loss = 2.2316
Epoch 1.6: Loss = 2.17987
Epoch 1.7: Loss = 2.18341
Epoch 1.8: Loss = 2.14642
Epoch 1.9: Loss = 2.09207
Epoch 1.10: Loss = 2.05292
Epoch 1.11: Loss = 1.9884
Epoch 1.12: Loss = 1.99869
Epoch 1.13: Loss = 1.92015
Epoch 1.14: Loss = 1.9272
Epoch 1.15: Loss = 1.96948
Epoch 1.16: Loss = 1.89439
Epoch 1.17: Loss = 1.83875
Epoch 1.18: Loss = 1.81119
Epoch 1.19: Loss = 1.77486
Epoch 1.20: Loss = 1.71654
Epoch 1.21: Loss = 1.65564
Epoch 1.22: Loss = 1.66425
Epoch 1.23: Loss = 1.58606
Epoch 1.24: Loss = 1.67386
Epoch 1.25: Loss = 1.62434
Epoch 1.26: Loss = 1.6105
Epoch 1.27: Loss = 1.53914
Epoch 1.28: Loss = 1.55005
Epoch 1.29: Loss = 1.53711
Epoch 1.30: Loss = 1.58827
Epoch 1.31: Loss = 1.4326
Epoch 1.32: Loss = 1.45401
Epoch 1.33: Loss = 1.38574
Epoch 1.34: Loss = 1.43134
Epoch 1.35: Loss = 1.34639
Epoch 1.36: Loss = 1.44118
Epoch 1.37: Loss = 1.30705
Epoch 1.38: Loss = 1.26563
Epoch 1.39: Loss = 1.24312
Epoch 1.40: Loss = 1.16676
Epoch 1.41: Loss = 1.22099
Epoch 1.42: Loss = 1.21144
Epoch 1.43: Loss = 1.13058
Epoch 1.44: Loss = 1.07704
Epoch 1.45: Loss = 1.20267
Epoch 1.46: Loss = 1.12398
Epoch 1.47: Loss = 1.07433
Epoch 1.48: Loss = 1.13362
Epoch 1.49: Loss = 1.05701
Epoch 1.50: Loss = 1.10791
Epoch 1.51: Loss = 0.961548
Epoch 1.52: Loss = 0.958359
Epoch 1.53: Loss = 1.02298
Epoch 1.54: Loss = 1.0554
Epoch 1.55: Loss = 1.03586
Epoch 1.56: Loss = 0.946503
Epoch 1.57: Loss = 0.879288
Epoch 1.58: Loss = 0.936066
Epoch 1.59: Loss = 0.931015
Epoch 1.60: Loss = 1.04153
Epoch 1.61: Loss = 0.977737
Epoch 1.62: Loss = 1.02869
Epoch 1.63: Loss = 0.986969
Epoch 1.64: Loss = 0.971909
Epoch 1.65: Loss = 1.01799
Epoch 1.66: Loss = 0.864716
Epoch 1.67: Loss = 0.879395
Epoch 1.68: Loss = 0.714493
Epoch 1.69: Loss = 0.782089
Epoch 1.70: Loss = 0.89502
Epoch 1.71: Loss = 0.811523
Epoch 1.72: Loss = 0.811111
Epoch 1.73: Loss = 0.810974
Epoch 1.74: Loss = 0.679413
Epoch 1.75: Loss = 0.824921
Epoch 1.76: Loss = 0.801041
Epoch 1.77: Loss = 0.753571
Epoch 1.78: Loss = 0.706833
Epoch 1.79: Loss = 0.733826
Epoch 1.80: Loss = 0.83551
Epoch 1.81: Loss = 0.696091
Epoch 1.82: Loss = 0.670013
Epoch 1.83: Loss = 0.835327
Epoch 1.84: Loss = 0.710724
Epoch 1.85: Loss = 0.827347
Epoch 1.86: Loss = 0.730682
Epoch 1.87: Loss = 0.635849
Epoch 1.88: Loss = 0.708588
Epoch 1.89: Loss = 0.755325
Epoch 1.90: Loss = 0.646759
Epoch 1.91: Loss = 0.706329
Epoch 1.92: Loss = 0.672699
Epoch 1.93: Loss = 0.715912
Epoch 1.94: Loss = 0.585632
Epoch 1.95: Loss = 0.686111
Epoch 1.96: Loss = 0.655899
Epoch 1.97: Loss = 0.513626
Epoch 1.98: Loss = 0.601654
Epoch 1.99: Loss = 0.703033
Epoch 1.100: Loss = 0.84761
Epoch 1.101: Loss = 0.716949
Epoch 1.102: Loss = 0.680084
Epoch 1.103: Loss = 0.59108
Epoch 1.104: Loss = 0.565445
Epoch 1.105: Loss = 0.646088
Epoch 1.106: Loss = 0.680496
Epoch 1.107: Loss = 0.546051
Epoch 1.108: Loss = 0.617813
Epoch 1.109: Loss = 0.578262
Epoch 1.110: Loss = 0.604568
Epoch 1.111: Loss = 0.497147
Epoch 1.112: Loss = 0.478928
Epoch 1.113: Loss = 0.5522
Epoch 1.114: Loss = 0.462891
Epoch 1.115: Loss = 0.585785
Epoch 1.116: Loss = 0.578964
Epoch 1.117: Loss = 0.437546
Epoch 1.118: Loss = 0.407211
Epoch 1.119: Loss = 0.397797
Epoch 1.120: Loss = 0.420395
TRAIN LOSS = 1.11139
TRAIN ACC = 67.5354 % (40523/60000)
Loss = 0.598709
Loss = 0.634262
Loss = 0.737305
Loss = 0.698608
Loss = 0.716232
Loss = 0.607422
Loss = 0.616455
Loss = 0.758087
Loss = 0.707855
Loss = 0.672867
Loss = 0.324936
Loss = 0.457367
Loss = 0.34465
Loss = 0.497986
Loss = 0.386353
Loss = 0.420044
Loss = 0.39296
Loss = 0.20932
Loss = 0.365616
Loss = 0.670914
TEST LOSS = 0.540897
TEST ACC = 405.229 % (8357/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.522049
Epoch 2.2: Loss = 0.67308
Epoch 2.3: Loss = 0.619797
Epoch 2.4: Loss = 0.500961
Epoch 2.5: Loss = 0.511154
Epoch 2.6: Loss = 0.490494
Epoch 2.7: Loss = 0.562347
Epoch 2.8: Loss = 0.529068
Epoch 2.9: Loss = 0.519836
Epoch 2.10: Loss = 0.496124
Epoch 2.11: Loss = 0.545303
Epoch 2.12: Loss = 0.538727
Epoch 2.13: Loss = 0.439087
Epoch 2.14: Loss = 0.475693
Epoch 2.15: Loss = 0.635468
Epoch 2.16: Loss = 0.579712
Epoch 2.17: Loss = 0.583176
Epoch 2.18: Loss = 0.647446
Epoch 2.19: Loss = 0.498184
Epoch 2.20: Loss = 0.431625
Epoch 2.21: Loss = 0.427063
Epoch 2.22: Loss = 0.442245
Epoch 2.23: Loss = 0.423126
Epoch 2.24: Loss = 0.657349
Epoch 2.25: Loss = 0.515549
Epoch 2.26: Loss = 0.614166
Epoch 2.27: Loss = 0.579178
Epoch 2.28: Loss = 0.565826
Epoch 2.29: Loss = 0.603485
Epoch 2.30: Loss = 0.708359
Epoch 2.31: Loss = 0.443924
Epoch 2.32: Loss = 0.590225
Epoch 2.33: Loss = 0.485184
Epoch 2.34: Loss = 0.53656
Epoch 2.35: Loss = 0.526199
Epoch 2.36: Loss = 0.595978
Epoch 2.37: Loss = 0.43924
Epoch 2.38: Loss = 0.422974
Epoch 2.39: Loss = 0.503876
Epoch 2.40: Loss = 0.443787
Epoch 2.41: Loss = 0.512177
Epoch 2.42: Loss = 0.613403
Epoch 2.43: Loss = 0.442825
Epoch 2.44: Loss = 0.392746
Epoch 2.45: Loss = 0.511261
Epoch 2.46: Loss = 0.528091
Epoch 2.47: Loss = 0.46463
Epoch 2.48: Loss = 0.523941
Epoch 2.49: Loss = 0.506622
Epoch 2.50: Loss = 0.560654
Epoch 2.51: Loss = 0.43222
Epoch 2.52: Loss = 0.430878
Epoch 2.53: Loss = 0.470764
Epoch 2.54: Loss = 0.592407
Epoch 2.55: Loss = 0.502167
Epoch 2.56: Loss = 0.456192
Epoch 2.57: Loss = 0.440979
Epoch 2.58: Loss = 0.49176
Epoch 2.59: Loss = 0.530472
Epoch 2.60: Loss = 0.605698
Epoch 2.61: Loss = 0.545898
Epoch 2.62: Loss = 0.602234
Epoch 2.63: Loss = 0.62146
Epoch 2.64: Loss = 0.562744
Epoch 2.65: Loss = 0.644974
Epoch 2.66: Loss = 0.491943
Epoch 2.67: Loss = 0.527069
Epoch 2.68: Loss = 0.32666
Epoch 2.69: Loss = 0.408936
Epoch 2.70: Loss = 0.584
Epoch 2.71: Loss = 0.422012
Epoch 2.72: Loss = 0.451645
Epoch 2.73: Loss = 0.466599
Epoch 2.74: Loss = 0.353714
Epoch 2.75: Loss = 0.600464
Epoch 2.76: Loss = 0.507431
Epoch 2.77: Loss = 0.463089
Epoch 2.78: Loss = 0.456192
Epoch 2.79: Loss = 0.501205
Epoch 2.80: Loss = 0.575363
Epoch 2.81: Loss = 0.421082
Epoch 2.82: Loss = 0.375824
Epoch 2.83: Loss = 0.583603
Epoch 2.84: Loss = 0.442383
Epoch 2.85: Loss = 0.641403
Epoch 2.86: Loss = 0.519928
Epoch 2.87: Loss = 0.387283
Epoch 2.88: Loss = 0.43428
Epoch 2.89: Loss = 0.534912
Epoch 2.90: Loss = 0.405609
Epoch 2.91: Loss = 0.517838
Epoch 2.92: Loss = 0.517731
Epoch 2.93: Loss = 0.537155
Epoch 2.94: Loss = 0.404922
Epoch 2.95: Loss = 0.491791
Epoch 2.96: Loss = 0.51355
Epoch 2.97: Loss = 0.36821
Epoch 2.98: Loss = 0.42659
Epoch 2.99: Loss = 0.51976
Epoch 2.100: Loss = 0.674957
Epoch 2.101: Loss = 0.591873
Epoch 2.102: Loss = 0.523941
Epoch 2.103: Loss = 0.436874
Epoch 2.104: Loss = 0.407455
Epoch 2.105: Loss = 0.526627
Epoch 2.106: Loss = 0.540176
Epoch 2.107: Loss = 0.397705
Epoch 2.108: Loss = 0.492905
Epoch 2.109: Loss = 0.430862
Epoch 2.110: Loss = 0.472549
Epoch 2.111: Loss = 0.357864
Epoch 2.112: Loss = 0.344742
Epoch 2.113: Loss = 0.432129
Epoch 2.114: Loss = 0.351181
Epoch 2.115: Loss = 0.433899
Epoch 2.116: Loss = 0.444107
Epoch 2.117: Loss = 0.28569
Epoch 2.118: Loss = 0.273117
Epoch 2.119: Loss = 0.334564
Epoch 2.120: Loss = 0.335266
TRAIN LOSS = 0.49649
TRAIN ACC = 84.7549 % (50855/60000)
Loss = 0.449066
Loss = 0.536621
Loss = 0.633514
Loss = 0.579498
Loss = 0.62413
Loss = 0.482605
Loss = 0.482758
Loss = 0.643463
Loss = 0.585022
Loss = 0.581696
Loss = 0.227585
Loss = 0.363007
Loss = 0.34491
Loss = 0.410248
Loss = 0.250824
Loss = 0.328735
Loss = 0.280273
Loss = 0.115128
Loss = 0.261215
Loss = 0.603821
TEST LOSS = 0.439206
TEST ACC = 508.549 % (8643/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.430435
Epoch 3.2: Loss = 0.527267
Epoch 3.3: Loss = 0.531387
Epoch 3.4: Loss = 0.391068
Epoch 3.5: Loss = 0.401596
Epoch 3.6: Loss = 0.407318
Epoch 3.7: Loss = 0.416656
Epoch 3.8: Loss = 0.422684
Epoch 3.9: Loss = 0.409821
Epoch 3.10: Loss = 0.424881
Epoch 3.11: Loss = 0.463837
Epoch 3.12: Loss = 0.406433
Epoch 3.13: Loss = 0.350327
Epoch 3.14: Loss = 0.391296
Epoch 3.15: Loss = 0.522171
Epoch 3.16: Loss = 0.480057
Epoch 3.17: Loss = 0.496399
Epoch 3.18: Loss = 0.591705
Epoch 3.19: Loss = 0.442947
Epoch 3.20: Loss = 0.380539
Epoch 3.21: Loss = 0.355209
Epoch 3.22: Loss = 0.344635
Epoch 3.23: Loss = 0.334717
Epoch 3.24: Loss = 0.610474
Epoch 3.25: Loss = 0.430252
Epoch 3.26: Loss = 0.563644
Epoch 3.27: Loss = 0.491623
Epoch 3.28: Loss = 0.499527
Epoch 3.29: Loss = 0.515564
Epoch 3.30: Loss = 0.64888
Epoch 3.31: Loss = 0.396805
Epoch 3.32: Loss = 0.515411
Epoch 3.33: Loss = 0.424866
Epoch 3.34: Loss = 0.474747
Epoch 3.35: Loss = 0.442581
Epoch 3.36: Loss = 0.524933
Epoch 3.37: Loss = 0.36145
Epoch 3.38: Loss = 0.36824
Epoch 3.39: Loss = 0.441711
Epoch 3.40: Loss = 0.415787
Epoch 3.41: Loss = 0.462128
Epoch 3.42: Loss = 0.597076
Epoch 3.43: Loss = 0.385468
Epoch 3.44: Loss = 0.33815
Epoch 3.45: Loss = 0.462326
Epoch 3.46: Loss = 0.438705
Epoch 3.47: Loss = 0.410736
Epoch 3.48: Loss = 0.472305
Epoch 3.49: Loss = 0.459869
Epoch 3.50: Loss = 0.53273
Epoch 3.51: Loss = 0.391541
Epoch 3.52: Loss = 0.374939
Epoch 3.53: Loss = 0.422974
Epoch 3.54: Loss = 0.551376
Epoch 3.55: Loss = 0.477676
Epoch 3.56: Loss = 0.415848
Epoch 3.57: Loss = 0.420441
Epoch 3.58: Loss = 0.445694
Epoch 3.59: Loss = 0.506424
Epoch 3.60: Loss = 0.546768
Epoch 3.61: Loss = 0.517349
Epoch 3.62: Loss = 0.552383
Epoch 3.63: Loss = 0.572266
Epoch 3.64: Loss = 0.516434
Epoch 3.65: Loss = 0.580429
Epoch 3.66: Loss = 0.440887
Epoch 3.67: Loss = 0.4785
Epoch 3.68: Loss = 0.278625
Epoch 3.69: Loss = 0.364975
Epoch 3.70: Loss = 0.55542
Epoch 3.71: Loss = 0.363342
Epoch 3.72: Loss = 0.370514
Epoch 3.73: Loss = 0.400192
Epoch 3.74: Loss = 0.329666
Epoch 3.75: Loss = 0.593704
Epoch 3.76: Loss = 0.488464
Epoch 3.77: Loss = 0.414352
Epoch 3.78: Loss = 0.42041
Epoch 3.79: Loss = 0.462204
Epoch 3.80: Loss = 0.56543
Epoch 3.81: Loss = 0.400543
Epoch 3.82: Loss = 0.337891
Epoch 3.83: Loss = 0.52211
Epoch 3.84: Loss = 0.407913
Epoch 3.85: Loss = 0.602859
Epoch 3.86: Loss = 0.505234
Epoch 3.87: Loss = 0.322998
Epoch 3.88: Loss = 0.421204
Epoch 3.89: Loss = 0.498047
Epoch 3.90: Loss = 0.384811
Epoch 3.91: Loss = 0.48674
Epoch 3.92: Loss = 0.488251
Epoch 3.93: Loss = 0.509003
Epoch 3.94: Loss = 0.344742
Epoch 3.95: Loss = 0.455872
Epoch 3.96: Loss = 0.485977
Epoch 3.97: Loss = 0.357697
Epoch 3.98: Loss = 0.388138
Epoch 3.99: Loss = 0.479034
Epoch 3.100: Loss = 0.619446
Epoch 3.101: Loss = 0.600464
Epoch 3.102: Loss = 0.475342
Epoch 3.103: Loss = 0.412247
Epoch 3.104: Loss = 0.362091
Epoch 3.105: Loss = 0.533646
Epoch 3.106: Loss = 0.531601
Epoch 3.107: Loss = 0.377197
Epoch 3.108: Loss = 0.479324
Epoch 3.109: Loss = 0.416458
Epoch 3.110: Loss = 0.44313
Epoch 3.111: Loss = 0.340363
Epoch 3.112: Loss = 0.324173
Epoch 3.113: Loss = 0.423477
Epoch 3.114: Loss = 0.324417
Epoch 3.115: Loss = 0.407364
Epoch 3.116: Loss = 0.418762
Epoch 3.117: Loss = 0.274353
Epoch 3.118: Loss = 0.258408
Epoch 3.119: Loss = 0.295975
Epoch 3.120: Loss = 0.329376
TRAIN LOSS = 0.445633
TRAIN ACC = 86.792 % (52078/60000)
Loss = 0.385132
Loss = 0.504211
Loss = 0.584579
Loss = 0.564072
Loss = 0.601913
Loss = 0.45549
Loss = 0.43306
Loss = 0.612549
Loss = 0.580276
Loss = 0.559586
Loss = 0.202881
Loss = 0.319489
Loss = 0.350891
Loss = 0.398514
Loss = 0.229553
Loss = 0.323883
Loss = 0.258026
Loss = 0.0719299
Loss = 0.22525
Loss = 0.557373
TEST LOSS = 0.410933
TEST ACC = 520.779 % (8802/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.395874
Epoch 4.2: Loss = 0.506973
Epoch 4.3: Loss = 0.530106
Epoch 4.4: Loss = 0.37764
Epoch 4.5: Loss = 0.369125
Epoch 4.6: Loss = 0.399673
Epoch 4.7: Loss = 0.3582
Epoch 4.8: Loss = 0.403351
Epoch 4.9: Loss = 0.391953
Epoch 4.10: Loss = 0.376831
Epoch 4.11: Loss = 0.448486
Epoch 4.12: Loss = 0.409653
Epoch 4.13: Loss = 0.332596
Epoch 4.14: Loss = 0.366364
Epoch 4.15: Loss = 0.475098
Epoch 4.16: Loss = 0.443054
Epoch 4.17: Loss = 0.507935
Epoch 4.18: Loss = 0.585892
Epoch 4.19: Loss = 0.452835
Epoch 4.20: Loss = 0.343613
Epoch 4.21: Loss = 0.370956
Epoch 4.22: Loss = 0.30954
Epoch 4.23: Loss = 0.323593
Epoch 4.24: Loss = 0.612396
Epoch 4.25: Loss = 0.438904
Epoch 4.26: Loss = 0.55632
Epoch 4.27: Loss = 0.491669
Epoch 4.28: Loss = 0.498764
Epoch 4.29: Loss = 0.502853
Epoch 4.30: Loss = 0.634094
Epoch 4.31: Loss = 0.379974
Epoch 4.32: Loss = 0.486801
Epoch 4.33: Loss = 0.369064
Epoch 4.34: Loss = 0.461334
Epoch 4.35: Loss = 0.418503
Epoch 4.36: Loss = 0.527191
Epoch 4.37: Loss = 0.348358
Epoch 4.38: Loss = 0.329529
Epoch 4.39: Loss = 0.39209
Epoch 4.40: Loss = 0.397247
Epoch 4.41: Loss = 0.449371
Epoch 4.42: Loss = 0.636139
Epoch 4.43: Loss = 0.351089
Epoch 4.44: Loss = 0.314148
Epoch 4.45: Loss = 0.433746
Epoch 4.46: Loss = 0.441513
Epoch 4.47: Loss = 0.386917
Epoch 4.48: Loss = 0.457291
Epoch 4.49: Loss = 0.441513
Epoch 4.50: Loss = 0.514938
Epoch 4.51: Loss = 0.36203
Epoch 4.52: Loss = 0.37175
Epoch 4.53: Loss = 0.416733
Epoch 4.54: Loss = 0.529709
Epoch 4.55: Loss = 0.423065
Epoch 4.56: Loss = 0.418182
Epoch 4.57: Loss = 0.413605
Epoch 4.58: Loss = 0.418152
Epoch 4.59: Loss = 0.515564
Epoch 4.60: Loss = 0.498444
Epoch 4.61: Loss = 0.464478
Epoch 4.62: Loss = 0.549728
Epoch 4.63: Loss = 0.551331
Epoch 4.64: Loss = 0.518723
Epoch 4.65: Loss = 0.593842
Epoch 4.66: Loss = 0.411041
Epoch 4.67: Loss = 0.455933
Epoch 4.68: Loss = 0.26149
Epoch 4.69: Loss = 0.361191
Epoch 4.70: Loss = 0.580444
Epoch 4.71: Loss = 0.381287
Epoch 4.72: Loss = 0.344727
Epoch 4.73: Loss = 0.437302
Epoch 4.74: Loss = 0.290878
Epoch 4.75: Loss = 0.64064
Epoch 4.76: Loss = 0.465485
Epoch 4.77: Loss = 0.370544
Epoch 4.78: Loss = 0.408539
Epoch 4.79: Loss = 0.481018
Epoch 4.80: Loss = 0.525864
Epoch 4.81: Loss = 0.374207
Epoch 4.82: Loss = 0.323044
Epoch 4.83: Loss = 0.525894
Epoch 4.84: Loss = 0.42897
Epoch 4.85: Loss = 0.605042
Epoch 4.86: Loss = 0.514984
Epoch 4.87: Loss = 0.320572
Epoch 4.88: Loss = 0.394791
Epoch 4.89: Loss = 0.488739
Epoch 4.90: Loss = 0.36235
Epoch 4.91: Loss = 0.492294
Epoch 4.92: Loss = 0.47612
Epoch 4.93: Loss = 0.54277
Epoch 4.94: Loss = 0.317825
Epoch 4.95: Loss = 0.457504
Epoch 4.96: Loss = 0.474731
Epoch 4.97: Loss = 0.337601
Epoch 4.98: Loss = 0.374405
Epoch 4.99: Loss = 0.466171
Epoch 4.100: Loss = 0.613037
Epoch 4.101: Loss = 0.617722
Epoch 4.102: Loss = 0.424667
Epoch 4.103: Loss = 0.391296
Epoch 4.104: Loss = 0.376984
Epoch 4.105: Loss = 0.512146
Epoch 4.106: Loss = 0.508011
Epoch 4.107: Loss = 0.376816
Epoch 4.108: Loss = 0.467529
Epoch 4.109: Loss = 0.384766
Epoch 4.110: Loss = 0.429047
Epoch 4.111: Loss = 0.33815
Epoch 4.112: Loss = 0.31694
Epoch 4.113: Loss = 0.39502
Epoch 4.114: Loss = 0.303162
Epoch 4.115: Loss = 0.403778
Epoch 4.116: Loss = 0.403885
Epoch 4.117: Loss = 0.272751
Epoch 4.118: Loss = 0.24794
Epoch 4.119: Loss = 0.285629
Epoch 4.120: Loss = 0.357834
TRAIN LOSS = 0.432648
TRAIN ACC = 87.7167 % (52632/60000)
Loss = 0.385513
Loss = 0.510803
Loss = 0.578476
Loss = 0.56221
Loss = 0.582565
Loss = 0.453079
Loss = 0.411438
Loss = 0.619751
Loss = 0.580704
Loss = 0.542587
Loss = 0.222885
Loss = 0.314468
Loss = 0.381729
Loss = 0.386475
Loss = 0.207626
Loss = 0.315582
Loss = 0.234787
Loss = 0.06987
Loss = 0.249557
Loss = 0.570206
TEST LOSS = 0.409016
TEST ACC = 526.318 % (8840/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.394852
Epoch 5.2: Loss = 0.528122
Epoch 5.3: Loss = 0.556061
Epoch 5.4: Loss = 0.333252
Epoch 5.5: Loss = 0.366943
Epoch 5.6: Loss = 0.371292
Epoch 5.7: Loss = 0.355927
Epoch 5.8: Loss = 0.402161
Epoch 5.9: Loss = 0.368378
Epoch 5.10: Loss = 0.365799
Epoch 5.11: Loss = 0.455414
Epoch 5.12: Loss = 0.432327
Epoch 5.13: Loss = 0.313889
Epoch 5.14: Loss = 0.395493
Epoch 5.15: Loss = 0.472092
Epoch 5.16: Loss = 0.438889
Epoch 5.17: Loss = 0.543915
Epoch 5.18: Loss = 0.611588
Epoch 5.19: Loss = 0.442123
Epoch 5.20: Loss = 0.352173
Epoch 5.21: Loss = 0.367538
Epoch 5.22: Loss = 0.312164
Epoch 5.23: Loss = 0.313263
Epoch 5.24: Loss = 0.588226
Epoch 5.25: Loss = 0.463898
Epoch 5.26: Loss = 0.552902
Epoch 5.27: Loss = 0.506592
Epoch 5.28: Loss = 0.513596
Epoch 5.29: Loss = 0.530045
Epoch 5.30: Loss = 0.668259
Epoch 5.31: Loss = 0.382355
Epoch 5.32: Loss = 0.503204
Epoch 5.33: Loss = 0.386215
Epoch 5.34: Loss = 0.44841
Epoch 5.35: Loss = 0.432663
Epoch 5.36: Loss = 0.547546
Epoch 5.37: Loss = 0.34317
Epoch 5.38: Loss = 0.323273
Epoch 5.39: Loss = 0.405212
Epoch 5.40: Loss = 0.411972
Epoch 5.41: Loss = 0.44191
Epoch 5.42: Loss = 0.630188
Epoch 5.43: Loss = 0.350281
Epoch 5.44: Loss = 0.311768
Epoch 5.45: Loss = 0.449921
Epoch 5.46: Loss = 0.436508
Epoch 5.47: Loss = 0.39389
Epoch 5.48: Loss = 0.444778
Epoch 5.49: Loss = 0.443329
Epoch 5.50: Loss = 0.469925
Epoch 5.51: Loss = 0.307388
Epoch 5.52: Loss = 0.382339
Epoch 5.53: Loss = 0.407639
Epoch 5.54: Loss = 0.543289
Epoch 5.55: Loss = 0.442993
Epoch 5.56: Loss = 0.427368
Epoch 5.57: Loss = 0.406219
Epoch 5.58: Loss = 0.428253
Epoch 5.59: Loss = 0.487198
Epoch 5.60: Loss = 0.488342
Epoch 5.61: Loss = 0.469635
Epoch 5.62: Loss = 0.566055
Epoch 5.63: Loss = 0.587021
Epoch 5.64: Loss = 0.550293
Epoch 5.65: Loss = 0.648621
Epoch 5.66: Loss = 0.425644
Epoch 5.67: Loss = 0.444839
Epoch 5.68: Loss = 0.26915
Epoch 5.69: Loss = 0.368347
Epoch 5.70: Loss = 0.55304
Epoch 5.71: Loss = 0.369461
Epoch 5.72: Loss = 0.322357
Epoch 5.73: Loss = 0.442245
Epoch 5.74: Loss = 0.297104
Epoch 5.75: Loss = 0.586487
Epoch 5.76: Loss = 0.499313
Epoch 5.77: Loss = 0.360535
Epoch 5.78: Loss = 0.419037
Epoch 5.79: Loss = 0.45845
Epoch 5.80: Loss = 0.52597
Epoch 5.81: Loss = 0.367447
Epoch 5.82: Loss = 0.30986
Epoch 5.83: Loss = 0.512238
Epoch 5.84: Loss = 0.435486
Epoch 5.85: Loss = 0.614105
Epoch 5.86: Loss = 0.534332
Epoch 5.87: Loss = 0.32869
Epoch 5.88: Loss = 0.408539
Epoch 5.89: Loss = 0.532837
Epoch 5.90: Loss = 0.362549
Epoch 5.91: Loss = 0.493515
Epoch 5.92: Loss = 0.528641
Epoch 5.93: Loss = 0.541962
Epoch 5.94: Loss = 0.314606
Epoch 5.95: Loss = 0.467422
Epoch 5.96: Loss = 0.482788
Epoch 5.97: Loss = 0.36615
Epoch 5.98: Loss = 0.363571
Epoch 5.99: Loss = 0.471695
Epoch 5.100: Loss = 0.5802
Epoch 5.101: Loss = 0.59816
Epoch 5.102: Loss = 0.433716
Epoch 5.103: Loss = 0.419464
Epoch 5.104: Loss = 0.368134
Epoch 5.105: Loss = 0.544937
Epoch 5.106: Loss = 0.530243
Epoch 5.107: Loss = 0.383835
Epoch 5.108: Loss = 0.487106
Epoch 5.109: Loss = 0.348206
Epoch 5.110: Loss = 0.449646
Epoch 5.111: Loss = 0.35408
Epoch 5.112: Loss = 0.357986
Epoch 5.113: Loss = 0.406662
Epoch 5.114: Loss = 0.345535
Epoch 5.115: Loss = 0.384857
Epoch 5.116: Loss = 0.425888
Epoch 5.117: Loss = 0.279343
Epoch 5.118: Loss = 0.237762
Epoch 5.119: Loss = 0.304947
Epoch 5.120: Loss = 0.348816
TRAIN LOSS = 0.436539
TRAIN ACC = 88.0081 % (52807/60000)
Loss = 0.405777
Loss = 0.504562
Loss = 0.598877
Loss = 0.599899
Loss = 0.625092
Loss = 0.426987
Loss = 0.421753
Loss = 0.637466
Loss = 0.579102
Loss = 0.589584
Loss = 0.201599
Loss = 0.340286
Loss = 0.370407
Loss = 0.37291
Loss = 0.184906
Loss = 0.307037
Loss = 0.218781
Loss = 0.060257
Loss = 0.21701
Loss = 0.55278
TEST LOSS = 0.410754
TEST ACC = 528.069 % (8888/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.437439
Epoch 6.2: Loss = 0.535675
Epoch 6.3: Loss = 0.541397
Epoch 6.4: Loss = 0.36142
Epoch 6.5: Loss = 0.371887
Epoch 6.6: Loss = 0.409027
Epoch 6.7: Loss = 0.35495
Epoch 6.8: Loss = 0.410431
Epoch 6.9: Loss = 0.337204
Epoch 6.10: Loss = 0.399994
Epoch 6.11: Loss = 0.461823
Epoch 6.12: Loss = 0.447464
Epoch 6.13: Loss = 0.330872
Epoch 6.14: Loss = 0.391937
Epoch 6.15: Loss = 0.471741
Epoch 6.16: Loss = 0.460831
Epoch 6.17: Loss = 0.539017
Epoch 6.18: Loss = 0.67218
Epoch 6.19: Loss = 0.453156
Epoch 6.20: Loss = 0.367523
Epoch 6.21: Loss = 0.384201
Epoch 6.22: Loss = 0.329514
Epoch 6.23: Loss = 0.30687
Epoch 6.24: Loss = 0.610428
Epoch 6.25: Loss = 0.454758
Epoch 6.26: Loss = 0.596451
Epoch 6.27: Loss = 0.537552
Epoch 6.28: Loss = 0.546814
Epoch 6.29: Loss = 0.53923
Epoch 6.30: Loss = 0.708542
Epoch 6.31: Loss = 0.394302
Epoch 6.32: Loss = 0.540207
Epoch 6.33: Loss = 0.387207
Epoch 6.34: Loss = 0.470016
Epoch 6.35: Loss = 0.434601
Epoch 6.36: Loss = 0.557648
Epoch 6.37: Loss = 0.346573
Epoch 6.38: Loss = 0.326614
Epoch 6.39: Loss = 0.391449
Epoch 6.40: Loss = 0.427551
Epoch 6.41: Loss = 0.435211
Epoch 6.42: Loss = 0.629089
Epoch 6.43: Loss = 0.360565
Epoch 6.44: Loss = 0.344025
Epoch 6.45: Loss = 0.44339
Epoch 6.46: Loss = 0.476883
Epoch 6.47: Loss = 0.407089
Epoch 6.48: Loss = 0.476379
Epoch 6.49: Loss = 0.467468
Epoch 6.50: Loss = 0.495758
Epoch 6.51: Loss = 0.344147
Epoch 6.52: Loss = 0.398361
Epoch 6.53: Loss = 0.405731
Epoch 6.54: Loss = 0.55423
Epoch 6.55: Loss = 0.502808
Epoch 6.56: Loss = 0.459702
Epoch 6.57: Loss = 0.44957
Epoch 6.58: Loss = 0.467728
Epoch 6.59: Loss = 0.49913
Epoch 6.60: Loss = 0.486603
Epoch 6.61: Loss = 0.474426
Epoch 6.62: Loss = 0.574387
Epoch 6.63: Loss = 0.665115
Epoch 6.64: Loss = 0.580887
Epoch 6.65: Loss = 0.56572
Epoch 6.66: Loss = 0.458725
Epoch 6.67: Loss = 0.437775
Epoch 6.68: Loss = 0.277451
Epoch 6.69: Loss = 0.37204
Epoch 6.70: Loss = 0.548019
Epoch 6.71: Loss = 0.382095
Epoch 6.72: Loss = 0.319382
Epoch 6.73: Loss = 0.413528
Epoch 6.74: Loss = 0.31424
Epoch 6.75: Loss = 0.607269
Epoch 6.76: Loss = 0.511841
Epoch 6.77: Loss = 0.352158
Epoch 6.78: Loss = 0.441025
Epoch 6.79: Loss = 0.505814
Epoch 6.80: Loss = 0.548904
Epoch 6.81: Loss = 0.366379
Epoch 6.82: Loss = 0.326324
Epoch 6.83: Loss = 0.531647
Epoch 6.84: Loss = 0.439011
Epoch 6.85: Loss = 0.673752
Epoch 6.86: Loss = 0.524002
Epoch 6.87: Loss = 0.331406
Epoch 6.88: Loss = 0.414124
Epoch 6.89: Loss = 0.54158
Epoch 6.90: Loss = 0.361618
Epoch 6.91: Loss = 0.525436
Epoch 6.92: Loss = 0.512634
Epoch 6.93: Loss = 0.557877
Epoch 6.94: Loss = 0.312607
Epoch 6.95: Loss = 0.465469
Epoch 6.96: Loss = 0.495407
Epoch 6.97: Loss = 0.366699
Epoch 6.98: Loss = 0.395325
Epoch 6.99: Loss = 0.483475
Epoch 6.100: Loss = 0.544937
Epoch 6.101: Loss = 0.605377
Epoch 6.102: Loss = 0.443893
Epoch 6.103: Loss = 0.446533
Epoch 6.104: Loss = 0.403046
Epoch 6.105: Loss = 0.53511
Epoch 6.106: Loss = 0.573471
Epoch 6.107: Loss = 0.372726
Epoch 6.108: Loss = 0.501816
Epoch 6.109: Loss = 0.37912
Epoch 6.110: Loss = 0.470047
Epoch 6.111: Loss = 0.338791
Epoch 6.112: Loss = 0.377594
Epoch 6.113: Loss = 0.446289
Epoch 6.114: Loss = 0.326584
Epoch 6.115: Loss = 0.39917
Epoch 6.116: Loss = 0.432831
Epoch 6.117: Loss = 0.256912
Epoch 6.118: Loss = 0.272095
Epoch 6.119: Loss = 0.333878
Epoch 6.120: Loss = 0.377991
TRAIN LOSS = 0.449692
TRAIN ACC = 88.0112 % (52809/60000)
Loss = 0.399933
Loss = 0.539627
Loss = 0.645264
Loss = 0.633102
Loss = 0.623917
Loss = 0.478348
Loss = 0.451782
Loss = 0.646469
Loss = 0.602173
Loss = 0.601563
Loss = 0.20488
Loss = 0.344757
Loss = 0.327087
Loss = 0.378784
Loss = 0.19754
Loss = 0.317993
Loss = 0.232483
Loss = 0.0605164
Loss = 0.218689
Loss = 0.599716
TEST LOSS = 0.425231
TEST ACC = 528.088 % (8899/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.437561
Epoch 7.2: Loss = 0.567673
Epoch 7.3: Loss = 0.601532
Epoch 7.4: Loss = 0.316864
Epoch 7.5: Loss = 0.383896
Epoch 7.6: Loss = 0.399826
Epoch 7.7: Loss = 0.354446
Epoch 7.8: Loss = 0.407608
Epoch 7.9: Loss = 0.36261
Epoch 7.10: Loss = 0.393906
Epoch 7.11: Loss = 0.434402
Epoch 7.12: Loss = 0.434357
Epoch 7.13: Loss = 0.324234
Epoch 7.14: Loss = 0.412979
Epoch 7.15: Loss = 0.485626
Epoch 7.16: Loss = 0.465942
Epoch 7.17: Loss = 0.519836
Epoch 7.18: Loss = 0.668198
Epoch 7.19: Loss = 0.467056
Epoch 7.20: Loss = 0.35675
Epoch 7.21: Loss = 0.414246
Epoch 7.22: Loss = 0.326172
Epoch 7.23: Loss = 0.308792
Epoch 7.24: Loss = 0.644058
Epoch 7.25: Loss = 0.509781
Epoch 7.26: Loss = 0.582062
Epoch 7.27: Loss = 0.523529
Epoch 7.28: Loss = 0.503769
Epoch 7.29: Loss = 0.597595
Epoch 7.30: Loss = 0.697601
Epoch 7.31: Loss = 0.395111
Epoch 7.32: Loss = 0.549347
Epoch 7.33: Loss = 0.387161
Epoch 7.34: Loss = 0.471756
Epoch 7.35: Loss = 0.450958
Epoch 7.36: Loss = 0.537735
Epoch 7.37: Loss = 0.354431
Epoch 7.38: Loss = 0.365738
Epoch 7.39: Loss = 0.379288
Epoch 7.40: Loss = 0.437653
Epoch 7.41: Loss = 0.417542
Epoch 7.42: Loss = 0.656616
Epoch 7.43: Loss = 0.363907
Epoch 7.44: Loss = 0.357864
Epoch 7.45: Loss = 0.440353
Epoch 7.46: Loss = 0.494339
Epoch 7.47: Loss = 0.443527
Epoch 7.48: Loss = 0.466125
Epoch 7.49: Loss = 0.464523
Epoch 7.50: Loss = 0.558716
Epoch 7.51: Loss = 0.361862
Epoch 7.52: Loss = 0.37323
Epoch 7.53: Loss = 0.445618
Epoch 7.54: Loss = 0.579819
Epoch 7.55: Loss = 0.475052
Epoch 7.56: Loss = 0.436188
Epoch 7.57: Loss = 0.472092
Epoch 7.58: Loss = 0.457474
Epoch 7.59: Loss = 0.532166
Epoch 7.60: Loss = 0.506012
Epoch 7.61: Loss = 0.47522
Epoch 7.62: Loss = 0.609329
Epoch 7.63: Loss = 0.605392
Epoch 7.64: Loss = 0.578323
Epoch 7.65: Loss = 0.638123
Epoch 7.66: Loss = 0.45163
Epoch 7.67: Loss = 0.422516
Epoch 7.68: Loss = 0.297165
Epoch 7.69: Loss = 0.373825
Epoch 7.70: Loss = 0.556915
Epoch 7.71: Loss = 0.422943
Epoch 7.72: Loss = 0.312134
Epoch 7.73: Loss = 0.452271
Epoch 7.74: Loss = 0.323547
Epoch 7.75: Loss = 0.625809
Epoch 7.76: Loss = 0.518066
Epoch 7.77: Loss = 0.360825
Epoch 7.78: Loss = 0.491653
Epoch 7.79: Loss = 0.540939
Epoch 7.80: Loss = 0.521561
Epoch 7.81: Loss = 0.395081
Epoch 7.82: Loss = 0.361038
Epoch 7.83: Loss = 0.603058
Epoch 7.84: Loss = 0.499115
Epoch 7.85: Loss = 0.653595
Epoch 7.86: Loss = 0.548523
Epoch 7.87: Loss = 0.34903
Epoch 7.88: Loss = 0.435516
Epoch 7.89: Loss = 0.530975
Epoch 7.90: Loss = 0.377029
Epoch 7.91: Loss = 0.506409
Epoch 7.92: Loss = 0.54895
Epoch 7.93: Loss = 0.584946
Epoch 7.94: Loss = 0.337585
Epoch 7.95: Loss = 0.5233
Epoch 7.96: Loss = 0.476074
Epoch 7.97: Loss = 0.362854
Epoch 7.98: Loss = 0.368317
Epoch 7.99: Loss = 0.489639
Epoch 7.100: Loss = 0.54071
Epoch 7.101: Loss = 0.62738
Epoch 7.102: Loss = 0.405029
Epoch 7.103: Loss = 0.426605
Epoch 7.104: Loss = 0.40831
Epoch 7.105: Loss = 0.566483
Epoch 7.106: Loss = 0.612701
Epoch 7.107: Loss = 0.378052
Epoch 7.108: Loss = 0.534149
Epoch 7.109: Loss = 0.365067
Epoch 7.110: Loss = 0.476639
Epoch 7.111: Loss = 0.350967
Epoch 7.112: Loss = 0.345917
Epoch 7.113: Loss = 0.436081
Epoch 7.114: Loss = 0.337006
Epoch 7.115: Loss = 0.449371
Epoch 7.116: Loss = 0.467453
Epoch 7.117: Loss = 0.290817
Epoch 7.118: Loss = 0.266418
Epoch 7.119: Loss = 0.314407
Epoch 7.120: Loss = 0.389282
TRAIN LOSS = 0.458771
TRAIN ACC = 88.3957 % (53040/60000)
Loss = 0.424362
Loss = 0.563614
Loss = 0.635727
Loss = 0.677216
Loss = 0.66481
Loss = 0.460449
Loss = 0.464645
Loss = 0.677612
Loss = 0.604065
Loss = 0.62265
Loss = 0.259537
Loss = 0.328262
Loss = 0.383255
Loss = 0.394669
Loss = 0.178787
Loss = 0.299545
Loss = 0.223572
Loss = 0.0544739
Loss = 0.218216
Loss = 0.624405
TEST LOSS = 0.437993
TEST ACC = 530.399 % (8907/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.442276
Epoch 8.2: Loss = 0.605392
Epoch 8.3: Loss = 0.571701
Epoch 8.4: Loss = 0.312164
Epoch 8.5: Loss = 0.395264
Epoch 8.6: Loss = 0.432037
Epoch 8.7: Loss = 0.330688
Epoch 8.8: Loss = 0.404617
Epoch 8.9: Loss = 0.400589
Epoch 8.10: Loss = 0.407455
Epoch 8.11: Loss = 0.479202
Epoch 8.12: Loss = 0.475204
Epoch 8.13: Loss = 0.31311
Epoch 8.14: Loss = 0.401215
Epoch 8.15: Loss = 0.535034
Epoch 8.16: Loss = 0.443649
Epoch 8.17: Loss = 0.522263
Epoch 8.18: Loss = 0.678268
Epoch 8.19: Loss = 0.510422
Epoch 8.20: Loss = 0.302643
Epoch 8.21: Loss = 0.427856
Epoch 8.22: Loss = 0.343109
Epoch 8.23: Loss = 0.352982
Epoch 8.24: Loss = 0.648804
Epoch 8.25: Loss = 0.517105
Epoch 8.26: Loss = 0.621109
Epoch 8.27: Loss = 0.511169
Epoch 8.28: Loss = 0.500275
Epoch 8.29: Loss = 0.584335
Epoch 8.30: Loss = 0.677505
Epoch 8.31: Loss = 0.391113
Epoch 8.32: Loss = 0.569077
Epoch 8.33: Loss = 0.393524
Epoch 8.34: Loss = 0.452866
Epoch 8.35: Loss = 0.48497
Epoch 8.36: Loss = 0.553436
Epoch 8.37: Loss = 0.362869
Epoch 8.38: Loss = 0.372009
Epoch 8.39: Loss = 0.363464
Epoch 8.40: Loss = 0.455612
Epoch 8.41: Loss = 0.416275
Epoch 8.42: Loss = 0.664398
Epoch 8.43: Loss = 0.360672
Epoch 8.44: Loss = 0.359268
Epoch 8.45: Loss = 0.434982
Epoch 8.46: Loss = 0.548355
Epoch 8.47: Loss = 0.465652
Epoch 8.48: Loss = 0.493439
Epoch 8.49: Loss = 0.44519
Epoch 8.50: Loss = 0.556213
Epoch 8.51: Loss = 0.349319
Epoch 8.52: Loss = 0.386337
Epoch 8.53: Loss = 0.461182
Epoch 8.54: Loss = 0.577484
Epoch 8.55: Loss = 0.477921
Epoch 8.56: Loss = 0.452377
Epoch 8.57: Loss = 0.473923
Epoch 8.58: Loss = 0.451324
Epoch 8.59: Loss = 0.546768
Epoch 8.60: Loss = 0.517136
Epoch 8.61: Loss = 0.474808
Epoch 8.62: Loss = 0.643158
Epoch 8.63: Loss = 0.58551
Epoch 8.64: Loss = 0.536606
Epoch 8.65: Loss = 0.637405
Epoch 8.66: Loss = 0.454681
Epoch 8.67: Loss = 0.443512
Epoch 8.68: Loss = 0.321899
Epoch 8.69: Loss = 0.424026
Epoch 8.70: Loss = 0.557999
Epoch 8.71: Loss = 0.404953
Epoch 8.72: Loss = 0.335495
Epoch 8.73: Loss = 0.506104
Epoch 8.74: Loss = 0.331116
Epoch 8.75: Loss = 0.609558
Epoch 8.76: Loss = 0.500351
Epoch 8.77: Loss = 0.376022
Epoch 8.78: Loss = 0.481537
Epoch 8.79: Loss = 0.58197
Epoch 8.80: Loss = 0.554596
Epoch 8.81: Loss = 0.428513
Epoch 8.82: Loss = 0.360367
Epoch 8.83: Loss = 0.610641
Epoch 8.84: Loss = 0.525208
Epoch 8.85: Loss = 0.664398
Epoch 8.86: Loss = 0.561554
Epoch 8.87: Loss = 0.339325
Epoch 8.88: Loss = 0.469315
Epoch 8.89: Loss = 0.571808
Epoch 8.90: Loss = 0.433868
Epoch 8.91: Loss = 0.522186
Epoch 8.92: Loss = 0.530014
Epoch 8.93: Loss = 0.665894
Epoch 8.94: Loss = 0.349228
Epoch 8.95: Loss = 0.513992
Epoch 8.96: Loss = 0.508774
Epoch 8.97: Loss = 0.417023
Epoch 8.98: Loss = 0.395615
Epoch 8.99: Loss = 0.5224
Epoch 8.100: Loss = 0.613495
Epoch 8.101: Loss = 0.65564
Epoch 8.102: Loss = 0.425262
Epoch 8.103: Loss = 0.4048
Epoch 8.104: Loss = 0.403915
Epoch 8.105: Loss = 0.572678
Epoch 8.106: Loss = 0.67334
Epoch 8.107: Loss = 0.427322
Epoch 8.108: Loss = 0.556
Epoch 8.109: Loss = 0.350586
Epoch 8.110: Loss = 0.520035
Epoch 8.111: Loss = 0.379196
Epoch 8.112: Loss = 0.38829
Epoch 8.113: Loss = 0.420059
Epoch 8.114: Loss = 0.324982
Epoch 8.115: Loss = 0.43483
Epoch 8.116: Loss = 0.503418
Epoch 8.117: Loss = 0.283157
Epoch 8.118: Loss = 0.267303
Epoch 8.119: Loss = 0.31694
Epoch 8.120: Loss = 0.423172
TRAIN LOSS = 0.470673
TRAIN ACC = 88.4933 % (53099/60000)
Loss = 0.436111
Loss = 0.56694
Loss = 0.647476
Loss = 0.682587
Loss = 0.667603
Loss = 0.488068
Loss = 0.497482
Loss = 0.676208
Loss = 0.621506
Loss = 0.602417
Loss = 0.228226
Loss = 0.334564
Loss = 0.305313
Loss = 0.393768
Loss = 0.201309
Loss = 0.251068
Loss = 0.210663
Loss = 0.0673676
Loss = 0.235107
Loss = 0.639206
TEST LOSS = 0.437649
TEST ACC = 530.989 % (8918/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.434998
Epoch 9.2: Loss = 0.600113
Epoch 9.3: Loss = 0.563599
Epoch 9.4: Loss = 0.305771
Epoch 9.5: Loss = 0.421753
Epoch 9.6: Loss = 0.423492
Epoch 9.7: Loss = 0.345688
Epoch 9.8: Loss = 0.417755
Epoch 9.9: Loss = 0.381363
Epoch 9.10: Loss = 0.455688
Epoch 9.11: Loss = 0.492813
Epoch 9.12: Loss = 0.450287
Epoch 9.13: Loss = 0.343948
Epoch 9.14: Loss = 0.400757
Epoch 9.15: Loss = 0.518906
Epoch 9.16: Loss = 0.470184
Epoch 9.17: Loss = 0.559052
Epoch 9.18: Loss = 0.731232
Epoch 9.19: Loss = 0.489014
Epoch 9.20: Loss = 0.33345
Epoch 9.21: Loss = 0.436478
Epoch 9.22: Loss = 0.34317
Epoch 9.23: Loss = 0.368866
Epoch 9.24: Loss = 0.681107
Epoch 9.25: Loss = 0.600296
Epoch 9.26: Loss = 0.635422
Epoch 9.27: Loss = 0.543533
Epoch 9.28: Loss = 0.541168
Epoch 9.29: Loss = 0.620392
Epoch 9.30: Loss = 0.672897
Epoch 9.31: Loss = 0.407104
Epoch 9.32: Loss = 0.579498
Epoch 9.33: Loss = 0.393768
Epoch 9.34: Loss = 0.480621
Epoch 9.35: Loss = 0.500778
Epoch 9.36: Loss = 0.578278
Epoch 9.37: Loss = 0.391418
Epoch 9.38: Loss = 0.410324
Epoch 9.39: Loss = 0.350922
Epoch 9.40: Loss = 0.471115
Epoch 9.41: Loss = 0.454727
Epoch 9.42: Loss = 0.728256
Epoch 9.43: Loss = 0.391296
Epoch 9.44: Loss = 0.375488
Epoch 9.45: Loss = 0.467285
Epoch 9.46: Loss = 0.560913
Epoch 9.47: Loss = 0.500946
Epoch 9.48: Loss = 0.499924
Epoch 9.49: Loss = 0.463043
Epoch 9.50: Loss = 0.572296
Epoch 9.51: Loss = 0.345932
Epoch 9.52: Loss = 0.411224
Epoch 9.53: Loss = 0.481705
Epoch 9.54: Loss = 0.595108
Epoch 9.55: Loss = 0.524887
Epoch 9.56: Loss = 0.47493
Epoch 9.57: Loss = 0.459076
Epoch 9.58: Loss = 0.506119
Epoch 9.59: Loss = 0.530685
Epoch 9.60: Loss = 0.560852
Epoch 9.61: Loss = 0.501938
Epoch 9.62: Loss = 0.655777
Epoch 9.63: Loss = 0.635559
Epoch 9.64: Loss = 0.543411
Epoch 9.65: Loss = 0.672668
Epoch 9.66: Loss = 0.483017
Epoch 9.67: Loss = 0.437317
Epoch 9.68: Loss = 0.292694
Epoch 9.69: Loss = 0.416992
Epoch 9.70: Loss = 0.55835
Epoch 9.71: Loss = 0.398727
Epoch 9.72: Loss = 0.328888
Epoch 9.73: Loss = 0.502411
Epoch 9.74: Loss = 0.375031
Epoch 9.75: Loss = 0.635452
Epoch 9.76: Loss = 0.493027
Epoch 9.77: Loss = 0.362457
Epoch 9.78: Loss = 0.470078
Epoch 9.79: Loss = 0.560303
Epoch 9.80: Loss = 0.53421
Epoch 9.81: Loss = 0.44136
Epoch 9.82: Loss = 0.385315
Epoch 9.83: Loss = 0.582855
Epoch 9.84: Loss = 0.508438
Epoch 9.85: Loss = 0.629791
Epoch 9.86: Loss = 0.6064
Epoch 9.87: Loss = 0.357941
Epoch 9.88: Loss = 0.472626
Epoch 9.89: Loss = 0.59375
Epoch 9.90: Loss = 0.445313
Epoch 9.91: Loss = 0.539764
Epoch 9.92: Loss = 0.591705
Epoch 9.93: Loss = 0.689316
Epoch 9.94: Loss = 0.32515
Epoch 9.95: Loss = 0.480591
Epoch 9.96: Loss = 0.513458
Epoch 9.97: Loss = 0.408554
Epoch 9.98: Loss = 0.436066
Epoch 9.99: Loss = 0.475693
Epoch 9.100: Loss = 0.634781
Epoch 9.101: Loss = 0.683167
Epoch 9.102: Loss = 0.442841
Epoch 9.103: Loss = 0.413086
Epoch 9.104: Loss = 0.443756
Epoch 9.105: Loss = 0.628647
Epoch 9.106: Loss = 0.651413
Epoch 9.107: Loss = 0.428482
Epoch 9.108: Loss = 0.608139
Epoch 9.109: Loss = 0.397018
Epoch 9.110: Loss = 0.5215
Epoch 9.111: Loss = 0.404694
Epoch 9.112: Loss = 0.405029
Epoch 9.113: Loss = 0.456512
Epoch 9.114: Loss = 0.368378
Epoch 9.115: Loss = 0.397858
Epoch 9.116: Loss = 0.508896
Epoch 9.117: Loss = 0.286804
Epoch 9.118: Loss = 0.264801
Epoch 9.119: Loss = 0.322266
Epoch 9.120: Loss = 0.421524
TRAIN LOSS = 0.484039
TRAIN ACC = 88.4338 % (53063/60000)
Loss = 0.42543
Loss = 0.580688
Loss = 0.681488
Loss = 0.719711
Loss = 0.680405
Loss = 0.469162
Loss = 0.473221
Loss = 0.728821
Loss = 0.645218
Loss = 0.612961
Loss = 0.188721
Loss = 0.404175
Loss = 0.382782
Loss = 0.417282
Loss = 0.173431
Loss = 0.330429
Loss = 0.234924
Loss = 0.0693359
Loss = 0.262421
Loss = 0.659637
TEST LOSS = 0.457012
TEST ACC = 530.629 % (8927/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.433441
Epoch 10.2: Loss = 0.546402
Epoch 10.3: Loss = 0.572708
Epoch 10.4: Loss = 0.323547
Epoch 10.5: Loss = 0.412369
Epoch 10.6: Loss = 0.428558
Epoch 10.7: Loss = 0.371429
Epoch 10.8: Loss = 0.437454
Epoch 10.9: Loss = 0.391785
Epoch 10.10: Loss = 0.442215
Epoch 10.11: Loss = 0.468781
Epoch 10.12: Loss = 0.461273
Epoch 10.13: Loss = 0.298004
Epoch 10.14: Loss = 0.415298
Epoch 10.15: Loss = 0.48439
Epoch 10.16: Loss = 0.53717
Epoch 10.17: Loss = 0.552307
Epoch 10.18: Loss = 0.742722
Epoch 10.19: Loss = 0.485123
Epoch 10.20: Loss = 0.378052
Epoch 10.21: Loss = 0.393097
Epoch 10.22: Loss = 0.380997
Epoch 10.23: Loss = 0.385071
Epoch 10.24: Loss = 0.689545
Epoch 10.25: Loss = 0.577133
Epoch 10.26: Loss = 0.607071
Epoch 10.27: Loss = 0.494598
Epoch 10.28: Loss = 0.542938
Epoch 10.29: Loss = 0.633163
Epoch 10.30: Loss = 0.71875
Epoch 10.31: Loss = 0.416016
Epoch 10.32: Loss = 0.541687
Epoch 10.33: Loss = 0.375015
Epoch 10.34: Loss = 0.490738
Epoch 10.35: Loss = 0.50383
Epoch 10.36: Loss = 0.579636
Epoch 10.37: Loss = 0.382233
Epoch 10.38: Loss = 0.381516
Epoch 10.39: Loss = 0.364594
Epoch 10.40: Loss = 0.445404
Epoch 10.41: Loss = 0.446518
Epoch 10.42: Loss = 0.704681
Epoch 10.43: Loss = 0.385101
Epoch 10.44: Loss = 0.359024
Epoch 10.45: Loss = 0.469437
Epoch 10.46: Loss = 0.517868
Epoch 10.47: Loss = 0.491455
Epoch 10.48: Loss = 0.469055
Epoch 10.49: Loss = 0.466629
Epoch 10.50: Loss = 0.604401
Epoch 10.51: Loss = 0.340515
Epoch 10.52: Loss = 0.440918
Epoch 10.53: Loss = 0.47789
Epoch 10.54: Loss = 0.615768
Epoch 10.55: Loss = 0.566177
Epoch 10.56: Loss = 0.449539
Epoch 10.57: Loss = 0.458954
Epoch 10.58: Loss = 0.462479
Epoch 10.59: Loss = 0.509827
Epoch 10.60: Loss = 0.53067
Epoch 10.61: Loss = 0.458313
Epoch 10.62: Loss = 0.591797
Epoch 10.63: Loss = 0.659988
Epoch 10.64: Loss = 0.535278
Epoch 10.65: Loss = 0.597366
Epoch 10.66: Loss = 0.47998
Epoch 10.67: Loss = 0.461426
Epoch 10.68: Loss = 0.284775
Epoch 10.69: Loss = 0.414093
Epoch 10.70: Loss = 0.532318
Epoch 10.71: Loss = 0.395081
Epoch 10.72: Loss = 0.346359
Epoch 10.73: Loss = 0.486526
Epoch 10.74: Loss = 0.344025
Epoch 10.75: Loss = 0.661194
Epoch 10.76: Loss = 0.550247
Epoch 10.77: Loss = 0.328049
Epoch 10.78: Loss = 0.482605
Epoch 10.79: Loss = 0.558533
Epoch 10.80: Loss = 0.540482
Epoch 10.81: Loss = 0.448563
Epoch 10.82: Loss = 0.367157
Epoch 10.83: Loss = 0.616089
Epoch 10.84: Loss = 0.505142
Epoch 10.85: Loss = 0.701965
Epoch 10.86: Loss = 0.578278
Epoch 10.87: Loss = 0.383041
Epoch 10.88: Loss = 0.467667
Epoch 10.89: Loss = 0.615005
Epoch 10.90: Loss = 0.414703
Epoch 10.91: Loss = 0.594864
Epoch 10.92: Loss = 0.598038
Epoch 10.93: Loss = 0.64444
Epoch 10.94: Loss = 0.336578
Epoch 10.95: Loss = 0.482758
Epoch 10.96: Loss = 0.525513
Epoch 10.97: Loss = 0.454666
Epoch 10.98: Loss = 0.434616
Epoch 10.99: Loss = 0.533844
Epoch 10.100: Loss = 0.644043
Epoch 10.101: Loss = 0.684235
Epoch 10.102: Loss = 0.454391
Epoch 10.103: Loss = 0.413483
Epoch 10.104: Loss = 0.390259
Epoch 10.105: Loss = 0.679459
Epoch 10.106: Loss = 0.643341
Epoch 10.107: Loss = 0.434647
Epoch 10.108: Loss = 0.563751
Epoch 10.109: Loss = 0.412476
Epoch 10.110: Loss = 0.527863
Epoch 10.111: Loss = 0.430771
Epoch 10.112: Loss = 0.380676
Epoch 10.113: Loss = 0.464783
Epoch 10.114: Loss = 0.315674
Epoch 10.115: Loss = 0.37767
Epoch 10.116: Loss = 0.494507
Epoch 10.117: Loss = 0.283813
Epoch 10.118: Loss = 0.294144
Epoch 10.119: Loss = 0.36113
Epoch 10.120: Loss = 0.427826
TRAIN LOSS = 0.482864
TRAIN ACC = 88.7482 % (53252/60000)
Loss = 0.444153
Loss = 0.593658
Loss = 0.691299
Loss = 0.747543
Loss = 0.706833
Loss = 0.494461
Loss = 0.466156
Loss = 0.770416
Loss = 0.72052
Loss = 0.678772
Loss = 0.17128
Loss = 0.396454
Loss = 0.36174
Loss = 0.467407
Loss = 0.180222
Loss = 0.372223
Loss = 0.24411
Loss = 0.0723572
Loss = 0.293396
Loss = 0.678253
TEST LOSS = 0.477563
TEST ACC = 532.52 % (8932/10000)
