Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.332
Epoch 1.2: Loss = 2.33102
Epoch 1.3: Loss = 2.26459
Epoch 1.4: Loss = 2.2439
Epoch 1.5: Loss = 2.20058
Epoch 1.6: Loss = 2.18919
Epoch 1.7: Loss = 2.16588
Epoch 1.8: Loss = 2.11945
Epoch 1.9: Loss = 2.1113
Epoch 1.10: Loss = 2.05344
Epoch 1.11: Loss = 2.04527
Epoch 1.12: Loss = 2.02562
Epoch 1.13: Loss = 1.95692
Epoch 1.14: Loss = 1.9465
Epoch 1.15: Loss = 1.98369
Epoch 1.16: Loss = 1.92415
Epoch 1.17: Loss = 1.89595
Epoch 1.18: Loss = 1.83704
Epoch 1.19: Loss = 1.79626
Epoch 1.20: Loss = 1.77695
Epoch 1.21: Loss = 1.70094
Epoch 1.22: Loss = 1.6991
Epoch 1.23: Loss = 1.64694
Epoch 1.24: Loss = 1.74232
Epoch 1.25: Loss = 1.62819
Epoch 1.26: Loss = 1.64821
Epoch 1.27: Loss = 1.61455
Epoch 1.28: Loss = 1.56796
Epoch 1.29: Loss = 1.54083
Epoch 1.30: Loss = 1.59393
Epoch 1.31: Loss = 1.50215
Epoch 1.32: Loss = 1.50601
Epoch 1.33: Loss = 1.41902
Epoch 1.34: Loss = 1.45308
Epoch 1.35: Loss = 1.41513
Epoch 1.36: Loss = 1.5191
Epoch 1.37: Loss = 1.33386
Epoch 1.38: Loss = 1.29147
Epoch 1.39: Loss = 1.29967
Epoch 1.40: Loss = 1.18904
Epoch 1.41: Loss = 1.26802
Epoch 1.42: Loss = 1.2336
Epoch 1.43: Loss = 1.18436
Epoch 1.44: Loss = 1.09258
Epoch 1.45: Loss = 1.22789
Epoch 1.46: Loss = 1.18111
Epoch 1.47: Loss = 1.08664
Epoch 1.48: Loss = 1.14488
Epoch 1.49: Loss = 1.08939
Epoch 1.50: Loss = 1.13806
Epoch 1.51: Loss = 0.965439
Epoch 1.52: Loss = 0.993408
Epoch 1.53: Loss = 1.03662
Epoch 1.54: Loss = 1.03203
Epoch 1.55: Loss = 1.03938
Epoch 1.56: Loss = 0.952942
Epoch 1.57: Loss = 0.867615
Epoch 1.58: Loss = 0.902863
Epoch 1.59: Loss = 0.925095
Epoch 1.60: Loss = 1.04904
Epoch 1.61: Loss = 0.966766
Epoch 1.62: Loss = 1.01176
Epoch 1.63: Loss = 1.00745
Epoch 1.64: Loss = 0.949631
Epoch 1.65: Loss = 1.03777
Epoch 1.66: Loss = 0.875305
Epoch 1.67: Loss = 0.863007
Epoch 1.68: Loss = 0.709106
Epoch 1.69: Loss = 0.802551
Epoch 1.70: Loss = 0.877197
Epoch 1.71: Loss = 0.78627
Epoch 1.72: Loss = 0.779694
Epoch 1.73: Loss = 0.813461
Epoch 1.74: Loss = 0.66861
Epoch 1.75: Loss = 0.830673
Epoch 1.76: Loss = 0.772827
Epoch 1.77: Loss = 0.731232
Epoch 1.78: Loss = 0.726837
Epoch 1.79: Loss = 0.72171
Epoch 1.80: Loss = 0.834564
Epoch 1.81: Loss = 0.675171
Epoch 1.82: Loss = 0.645508
Epoch 1.83: Loss = 0.834549
Epoch 1.84: Loss = 0.730042
Epoch 1.85: Loss = 0.780945
Epoch 1.86: Loss = 0.718948
Epoch 1.87: Loss = 0.65239
Epoch 1.88: Loss = 0.67746
Epoch 1.89: Loss = 0.760376
Epoch 1.90: Loss = 0.643539
Epoch 1.91: Loss = 0.709549
Epoch 1.92: Loss = 0.669846
Epoch 1.93: Loss = 0.703445
Epoch 1.94: Loss = 0.582642
Epoch 1.95: Loss = 0.706924
Epoch 1.96: Loss = 0.637299
Epoch 1.97: Loss = 0.511398
Epoch 1.98: Loss = 0.604675
Epoch 1.99: Loss = 0.704071
Epoch 1.100: Loss = 0.808167
Epoch 1.101: Loss = 0.706406
Epoch 1.102: Loss = 0.600739
Epoch 1.103: Loss = 0.573624
Epoch 1.104: Loss = 0.544662
Epoch 1.105: Loss = 0.666855
Epoch 1.106: Loss = 0.646698
Epoch 1.107: Loss = 0.551529
Epoch 1.108: Loss = 0.615662
Epoch 1.109: Loss = 0.557129
Epoch 1.110: Loss = 0.60527
Epoch 1.111: Loss = 0.497192
Epoch 1.112: Loss = 0.464066
Epoch 1.113: Loss = 0.582993
Epoch 1.114: Loss = 0.480576
Epoch 1.115: Loss = 0.559799
Epoch 1.116: Loss = 0.55394
Epoch 1.117: Loss = 0.471191
Epoch 1.118: Loss = 0.407547
Epoch 1.119: Loss = 0.395355
Epoch 1.120: Loss = 0.431046
TRAIN LOSS = 1.11966
TRAIN ACC = 70.1904 % (42116/60000)
Loss = 0.59375
Loss = 0.603928
Loss = 0.702515
Loss = 0.671249
Loss = 0.732376
Loss = 0.6064
Loss = 0.545593
Loss = 0.726089
Loss = 0.663116
Loss = 0.640335
Loss = 0.323624
Loss = 0.494049
Loss = 0.345215
Loss = 0.543716
Loss = 0.417465
Loss = 0.436111
Loss = 0.3815
Loss = 0.220428
Loss = 0.396149
Loss = 0.67276
TEST LOSS = 0.535818
TEST ACC = 421.159 % (8421/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.522598
Epoch 2.2: Loss = 0.65416
Epoch 2.3: Loss = 0.596466
Epoch 2.4: Loss = 0.463882
Epoch 2.5: Loss = 0.504623
Epoch 2.6: Loss = 0.506149
Epoch 2.7: Loss = 0.521469
Epoch 2.8: Loss = 0.500214
Epoch 2.9: Loss = 0.481918
Epoch 2.10: Loss = 0.50528
Epoch 2.11: Loss = 0.527084
Epoch 2.12: Loss = 0.516281
Epoch 2.13: Loss = 0.436035
Epoch 2.14: Loss = 0.490952
Epoch 2.15: Loss = 0.604919
Epoch 2.16: Loss = 0.57518
Epoch 2.17: Loss = 0.582596
Epoch 2.18: Loss = 0.640854
Epoch 2.19: Loss = 0.485153
Epoch 2.20: Loss = 0.453506
Epoch 2.21: Loss = 0.417557
Epoch 2.22: Loss = 0.424011
Epoch 2.23: Loss = 0.438156
Epoch 2.24: Loss = 0.600449
Epoch 2.25: Loss = 0.484772
Epoch 2.26: Loss = 0.606033
Epoch 2.27: Loss = 0.590851
Epoch 2.28: Loss = 0.541046
Epoch 2.29: Loss = 0.585052
Epoch 2.30: Loss = 0.685211
Epoch 2.31: Loss = 0.451355
Epoch 2.32: Loss = 0.600861
Epoch 2.33: Loss = 0.470779
Epoch 2.34: Loss = 0.56813
Epoch 2.35: Loss = 0.534729
Epoch 2.36: Loss = 0.618378
Epoch 2.37: Loss = 0.427261
Epoch 2.38: Loss = 0.407227
Epoch 2.39: Loss = 0.509293
Epoch 2.40: Loss = 0.440216
Epoch 2.41: Loss = 0.497375
Epoch 2.42: Loss = 0.558121
Epoch 2.43: Loss = 0.443344
Epoch 2.44: Loss = 0.383072
Epoch 2.45: Loss = 0.50502
Epoch 2.46: Loss = 0.514252
Epoch 2.47: Loss = 0.433762
Epoch 2.48: Loss = 0.509216
Epoch 2.49: Loss = 0.454163
Epoch 2.50: Loss = 0.57518
Epoch 2.51: Loss = 0.408737
Epoch 2.52: Loss = 0.423798
Epoch 2.53: Loss = 0.471115
Epoch 2.54: Loss = 0.551865
Epoch 2.55: Loss = 0.500565
Epoch 2.56: Loss = 0.436249
Epoch 2.57: Loss = 0.419388
Epoch 2.58: Loss = 0.46666
Epoch 2.59: Loss = 0.503082
Epoch 2.60: Loss = 0.57756
Epoch 2.61: Loss = 0.528961
Epoch 2.62: Loss = 0.564346
Epoch 2.63: Loss = 0.616867
Epoch 2.64: Loss = 0.536057
Epoch 2.65: Loss = 0.665298
Epoch 2.66: Loss = 0.4655
Epoch 2.67: Loss = 0.504486
Epoch 2.68: Loss = 0.315277
Epoch 2.69: Loss = 0.409256
Epoch 2.70: Loss = 0.561172
Epoch 2.71: Loss = 0.404633
Epoch 2.72: Loss = 0.40509
Epoch 2.73: Loss = 0.467392
Epoch 2.74: Loss = 0.345108
Epoch 2.75: Loss = 0.598648
Epoch 2.76: Loss = 0.474167
Epoch 2.77: Loss = 0.396515
Epoch 2.78: Loss = 0.44664
Epoch 2.79: Loss = 0.506958
Epoch 2.80: Loss = 0.514389
Epoch 2.81: Loss = 0.394653
Epoch 2.82: Loss = 0.372345
Epoch 2.83: Loss = 0.540009
Epoch 2.84: Loss = 0.446671
Epoch 2.85: Loss = 0.572083
Epoch 2.86: Loss = 0.52179
Epoch 2.87: Loss = 0.393509
Epoch 2.88: Loss = 0.447968
Epoch 2.89: Loss = 0.518799
Epoch 2.90: Loss = 0.394897
Epoch 2.91: Loss = 0.50412
Epoch 2.92: Loss = 0.47464
Epoch 2.93: Loss = 0.527756
Epoch 2.94: Loss = 0.371704
Epoch 2.95: Loss = 0.470062
Epoch 2.96: Loss = 0.499725
Epoch 2.97: Loss = 0.35849
Epoch 2.98: Loss = 0.401688
Epoch 2.99: Loss = 0.528595
Epoch 2.100: Loss = 0.595978
Epoch 2.101: Loss = 0.565933
Epoch 2.102: Loss = 0.429871
Epoch 2.103: Loss = 0.415726
Epoch 2.104: Loss = 0.377151
Epoch 2.105: Loss = 0.547531
Epoch 2.106: Loss = 0.528931
Epoch 2.107: Loss = 0.395981
Epoch 2.108: Loss = 0.477951
Epoch 2.109: Loss = 0.396347
Epoch 2.110: Loss = 0.455536
Epoch 2.111: Loss = 0.352524
Epoch 2.112: Loss = 0.34201
Epoch 2.113: Loss = 0.425583
Epoch 2.114: Loss = 0.340225
Epoch 2.115: Loss = 0.37384
Epoch 2.116: Loss = 0.411987
Epoch 2.117: Loss = 0.300949
Epoch 2.118: Loss = 0.241364
Epoch 2.119: Loss = 0.299973
Epoch 2.120: Loss = 0.328217
TRAIN LOSS = 0.480621
TRAIN ACC = 85.4935 % (51299/60000)
Loss = 0.445084
Loss = 0.482819
Loss = 0.581711
Loss = 0.567184
Loss = 0.610779
Loss = 0.46904
Loss = 0.416397
Loss = 0.635956
Loss = 0.546463
Loss = 0.533173
Loss = 0.205902
Loss = 0.369675
Loss = 0.27153
Loss = 0.406464
Loss = 0.269806
Loss = 0.329697
Loss = 0.263275
Loss = 0.108536
Loss = 0.272842
Loss = 0.56604
TEST LOSS = 0.417618
TEST ACC = 512.99 % (8718/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.404388
Epoch 3.2: Loss = 0.538391
Epoch 3.3: Loss = 0.47641
Epoch 3.4: Loss = 0.342819
Epoch 3.5: Loss = 0.374283
Epoch 3.6: Loss = 0.400467
Epoch 3.7: Loss = 0.377701
Epoch 3.8: Loss = 0.402496
Epoch 3.9: Loss = 0.358002
Epoch 3.10: Loss = 0.41629
Epoch 3.11: Loss = 0.427567
Epoch 3.12: Loss = 0.413193
Epoch 3.13: Loss = 0.330536
Epoch 3.14: Loss = 0.403259
Epoch 3.15: Loss = 0.481064
Epoch 3.16: Loss = 0.470795
Epoch 3.17: Loss = 0.506866
Epoch 3.18: Loss = 0.591675
Epoch 3.19: Loss = 0.422394
Epoch 3.20: Loss = 0.374451
Epoch 3.21: Loss = 0.334229
Epoch 3.22: Loss = 0.316879
Epoch 3.23: Loss = 0.361801
Epoch 3.24: Loss = 0.522629
Epoch 3.25: Loss = 0.417068
Epoch 3.26: Loss = 0.538559
Epoch 3.27: Loss = 0.516937
Epoch 3.28: Loss = 0.476944
Epoch 3.29: Loss = 0.533981
Epoch 3.30: Loss = 0.587616
Epoch 3.31: Loss = 0.369934
Epoch 3.32: Loss = 0.500992
Epoch 3.33: Loss = 0.378448
Epoch 3.34: Loss = 0.467834
Epoch 3.35: Loss = 0.453918
Epoch 3.36: Loss = 0.527466
Epoch 3.37: Loss = 0.351257
Epoch 3.38: Loss = 0.349091
Epoch 3.39: Loss = 0.429672
Epoch 3.40: Loss = 0.374252
Epoch 3.41: Loss = 0.411606
Epoch 3.42: Loss = 0.530258
Epoch 3.43: Loss = 0.367798
Epoch 3.44: Loss = 0.32782
Epoch 3.45: Loss = 0.431976
Epoch 3.46: Loss = 0.451218
Epoch 3.47: Loss = 0.383163
Epoch 3.48: Loss = 0.445541
Epoch 3.49: Loss = 0.389694
Epoch 3.50: Loss = 0.52742
Epoch 3.51: Loss = 0.346344
Epoch 3.52: Loss = 0.352325
Epoch 3.53: Loss = 0.403641
Epoch 3.54: Loss = 0.489624
Epoch 3.55: Loss = 0.4272
Epoch 3.56: Loss = 0.384918
Epoch 3.57: Loss = 0.390015
Epoch 3.58: Loss = 0.409393
Epoch 3.59: Loss = 0.486176
Epoch 3.60: Loss = 0.513245
Epoch 3.61: Loss = 0.449448
Epoch 3.62: Loss = 0.498734
Epoch 3.63: Loss = 0.590546
Epoch 3.64: Loss = 0.509064
Epoch 3.65: Loss = 0.610199
Epoch 3.66: Loss = 0.41214
Epoch 3.67: Loss = 0.453644
Epoch 3.68: Loss = 0.274124
Epoch 3.69: Loss = 0.35408
Epoch 3.70: Loss = 0.523102
Epoch 3.71: Loss = 0.344193
Epoch 3.72: Loss = 0.340607
Epoch 3.73: Loss = 0.432343
Epoch 3.74: Loss = 0.325684
Epoch 3.75: Loss = 0.616318
Epoch 3.76: Loss = 0.438995
Epoch 3.77: Loss = 0.347549
Epoch 3.78: Loss = 0.41185
Epoch 3.79: Loss = 0.473785
Epoch 3.80: Loss = 0.472107
Epoch 3.81: Loss = 0.327728
Epoch 3.82: Loss = 0.329285
Epoch 3.83: Loss = 0.480286
Epoch 3.84: Loss = 0.385223
Epoch 3.85: Loss = 0.540833
Epoch 3.86: Loss = 0.484818
Epoch 3.87: Loss = 0.327911
Epoch 3.88: Loss = 0.418442
Epoch 3.89: Loss = 0.444473
Epoch 3.90: Loss = 0.352524
Epoch 3.91: Loss = 0.465988
Epoch 3.92: Loss = 0.450546
Epoch 3.93: Loss = 0.500671
Epoch 3.94: Loss = 0.336548
Epoch 3.95: Loss = 0.417099
Epoch 3.96: Loss = 0.498322
Epoch 3.97: Loss = 0.337784
Epoch 3.98: Loss = 0.362411
Epoch 3.99: Loss = 0.502441
Epoch 3.100: Loss = 0.577499
Epoch 3.101: Loss = 0.5345
Epoch 3.102: Loss = 0.398468
Epoch 3.103: Loss = 0.366241
Epoch 3.104: Loss = 0.343719
Epoch 3.105: Loss = 0.525574
Epoch 3.106: Loss = 0.523682
Epoch 3.107: Loss = 0.370468
Epoch 3.108: Loss = 0.464661
Epoch 3.109: Loss = 0.376114
Epoch 3.110: Loss = 0.430267
Epoch 3.111: Loss = 0.323639
Epoch 3.112: Loss = 0.329071
Epoch 3.113: Loss = 0.39325
Epoch 3.114: Loss = 0.322662
Epoch 3.115: Loss = 0.328079
Epoch 3.116: Loss = 0.369629
Epoch 3.117: Loss = 0.260925
Epoch 3.118: Loss = 0.196762
Epoch 3.119: Loss = 0.28093
Epoch 3.120: Loss = 0.301514
TRAIN LOSS = 0.422104
TRAIN ACC = 87.471 % (52485/60000)
Loss = 0.421967
Loss = 0.475647
Loss = 0.526947
Loss = 0.564484
Loss = 0.594086
Loss = 0.438309
Loss = 0.384613
Loss = 0.621231
Loss = 0.529327
Loss = 0.509995
Loss = 0.176544
Loss = 0.372345
Loss = 0.284836
Loss = 0.383026
Loss = 0.217697
Loss = 0.272858
Loss = 0.238312
Loss = 0.0810547
Loss = 0.222046
Loss = 0.566238
TEST LOSS = 0.394078
TEST ACC = 524.849 % (8834/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.381912
Epoch 4.2: Loss = 0.525681
Epoch 4.3: Loss = 0.453293
Epoch 4.4: Loss = 0.324905
Epoch 4.5: Loss = 0.347702
Epoch 4.6: Loss = 0.391907
Epoch 4.7: Loss = 0.323807
Epoch 4.8: Loss = 0.374634
Epoch 4.9: Loss = 0.331757
Epoch 4.10: Loss = 0.384262
Epoch 4.11: Loss = 0.414047
Epoch 4.12: Loss = 0.401016
Epoch 4.13: Loss = 0.319504
Epoch 4.14: Loss = 0.380814
Epoch 4.15: Loss = 0.421967
Epoch 4.16: Loss = 0.456467
Epoch 4.17: Loss = 0.470016
Epoch 4.18: Loss = 0.614624
Epoch 4.19: Loss = 0.420563
Epoch 4.20: Loss = 0.365295
Epoch 4.21: Loss = 0.315796
Epoch 4.22: Loss = 0.287979
Epoch 4.23: Loss = 0.344803
Epoch 4.24: Loss = 0.497589
Epoch 4.25: Loss = 0.409836
Epoch 4.26: Loss = 0.5569
Epoch 4.27: Loss = 0.49324
Epoch 4.28: Loss = 0.465393
Epoch 4.29: Loss = 0.540726
Epoch 4.30: Loss = 0.558578
Epoch 4.31: Loss = 0.352264
Epoch 4.32: Loss = 0.492981
Epoch 4.33: Loss = 0.367752
Epoch 4.34: Loss = 0.452362
Epoch 4.35: Loss = 0.429413
Epoch 4.36: Loss = 0.483612
Epoch 4.37: Loss = 0.315384
Epoch 4.38: Loss = 0.354797
Epoch 4.39: Loss = 0.394821
Epoch 4.40: Loss = 0.358261
Epoch 4.41: Loss = 0.379776
Epoch 4.42: Loss = 0.566391
Epoch 4.43: Loss = 0.339798
Epoch 4.44: Loss = 0.338562
Epoch 4.45: Loss = 0.415558
Epoch 4.46: Loss = 0.431244
Epoch 4.47: Loss = 0.406525
Epoch 4.48: Loss = 0.441467
Epoch 4.49: Loss = 0.381271
Epoch 4.50: Loss = 0.532761
Epoch 4.51: Loss = 0.336761
Epoch 4.52: Loss = 0.32283
Epoch 4.53: Loss = 0.40239
Epoch 4.54: Loss = 0.492203
Epoch 4.55: Loss = 0.406464
Epoch 4.56: Loss = 0.374786
Epoch 4.57: Loss = 0.384491
Epoch 4.58: Loss = 0.417099
Epoch 4.59: Loss = 0.488495
Epoch 4.60: Loss = 0.508896
Epoch 4.61: Loss = 0.416626
Epoch 4.62: Loss = 0.486984
Epoch 4.63: Loss = 0.594055
Epoch 4.64: Loss = 0.511032
Epoch 4.65: Loss = 0.606812
Epoch 4.66: Loss = 0.399612
Epoch 4.67: Loss = 0.437897
Epoch 4.68: Loss = 0.272141
Epoch 4.69: Loss = 0.361786
Epoch 4.70: Loss = 0.509277
Epoch 4.71: Loss = 0.327698
Epoch 4.72: Loss = 0.312546
Epoch 4.73: Loss = 0.433929
Epoch 4.74: Loss = 0.315216
Epoch 4.75: Loss = 0.626404
Epoch 4.76: Loss = 0.419662
Epoch 4.77: Loss = 0.324799
Epoch 4.78: Loss = 0.408508
Epoch 4.79: Loss = 0.49263
Epoch 4.80: Loss = 0.438797
Epoch 4.81: Loss = 0.325317
Epoch 4.82: Loss = 0.321838
Epoch 4.83: Loss = 0.468933
Epoch 4.84: Loss = 0.384689
Epoch 4.85: Loss = 0.516891
Epoch 4.86: Loss = 0.488052
Epoch 4.87: Loss = 0.307632
Epoch 4.88: Loss = 0.408813
Epoch 4.89: Loss = 0.403687
Epoch 4.90: Loss = 0.345016
Epoch 4.91: Loss = 0.462097
Epoch 4.92: Loss = 0.448776
Epoch 4.93: Loss = 0.514984
Epoch 4.94: Loss = 0.337662
Epoch 4.95: Loss = 0.396851
Epoch 4.96: Loss = 0.475952
Epoch 4.97: Loss = 0.343002
Epoch 4.98: Loss = 0.351089
Epoch 4.99: Loss = 0.481873
Epoch 4.100: Loss = 0.5513
Epoch 4.101: Loss = 0.547363
Epoch 4.102: Loss = 0.408447
Epoch 4.103: Loss = 0.340652
Epoch 4.104: Loss = 0.32933
Epoch 4.105: Loss = 0.519516
Epoch 4.106: Loss = 0.527664
Epoch 4.107: Loss = 0.347763
Epoch 4.108: Loss = 0.464462
Epoch 4.109: Loss = 0.35025
Epoch 4.110: Loss = 0.419281
Epoch 4.111: Loss = 0.327667
Epoch 4.112: Loss = 0.310349
Epoch 4.113: Loss = 0.37793
Epoch 4.114: Loss = 0.311005
Epoch 4.115: Loss = 0.316437
Epoch 4.116: Loss = 0.378372
Epoch 4.117: Loss = 0.237244
Epoch 4.118: Loss = 0.197052
Epoch 4.119: Loss = 0.251785
Epoch 4.120: Loss = 0.32222
TRAIN LOSS = 0.410507
TRAIN ACC = 88.327 % (52999/60000)
Loss = 0.398148
Loss = 0.467209
Loss = 0.509491
Loss = 0.545319
Loss = 0.576752
Loss = 0.413162
Loss = 0.37439
Loss = 0.586365
Loss = 0.529892
Loss = 0.471329
Loss = 0.152283
Loss = 0.343918
Loss = 0.278137
Loss = 0.360321
Loss = 0.208405
Loss = 0.280457
Loss = 0.21637
Loss = 0.0655212
Loss = 0.22168
Loss = 0.551941
TEST LOSS = 0.377554
TEST ACC = 529.99 % (8912/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.380203
Epoch 5.2: Loss = 0.506683
Epoch 5.3: Loss = 0.459824
Epoch 5.4: Loss = 0.310318
Epoch 5.5: Loss = 0.327469
Epoch 5.6: Loss = 0.378876
Epoch 5.7: Loss = 0.309662
Epoch 5.8: Loss = 0.367874
Epoch 5.9: Loss = 0.316574
Epoch 5.10: Loss = 0.365448
Epoch 5.11: Loss = 0.412659
Epoch 5.12: Loss = 0.387024
Epoch 5.13: Loss = 0.304871
Epoch 5.14: Loss = 0.361099
Epoch 5.15: Loss = 0.410126
Epoch 5.16: Loss = 0.443634
Epoch 5.17: Loss = 0.482269
Epoch 5.18: Loss = 0.59697
Epoch 5.19: Loss = 0.423553
Epoch 5.20: Loss = 0.369141
Epoch 5.21: Loss = 0.319717
Epoch 5.22: Loss = 0.281342
Epoch 5.23: Loss = 0.322144
Epoch 5.24: Loss = 0.512131
Epoch 5.25: Loss = 0.417099
Epoch 5.26: Loss = 0.564392
Epoch 5.27: Loss = 0.49852
Epoch 5.28: Loss = 0.459732
Epoch 5.29: Loss = 0.533737
Epoch 5.30: Loss = 0.538666
Epoch 5.31: Loss = 0.362228
Epoch 5.32: Loss = 0.488876
Epoch 5.33: Loss = 0.367508
Epoch 5.34: Loss = 0.403198
Epoch 5.35: Loss = 0.395691
Epoch 5.36: Loss = 0.441086
Epoch 5.37: Loss = 0.30217
Epoch 5.38: Loss = 0.342545
Epoch 5.39: Loss = 0.378922
Epoch 5.40: Loss = 0.36673
Epoch 5.41: Loss = 0.373993
Epoch 5.42: Loss = 0.562943
Epoch 5.43: Loss = 0.335327
Epoch 5.44: Loss = 0.315491
Epoch 5.45: Loss = 0.399582
Epoch 5.46: Loss = 0.410553
Epoch 5.47: Loss = 0.375473
Epoch 5.48: Loss = 0.412659
Epoch 5.49: Loss = 0.386642
Epoch 5.50: Loss = 0.539001
Epoch 5.51: Loss = 0.317154
Epoch 5.52: Loss = 0.315125
Epoch 5.53: Loss = 0.400635
Epoch 5.54: Loss = 0.503342
Epoch 5.55: Loss = 0.415329
Epoch 5.56: Loss = 0.366821
Epoch 5.57: Loss = 0.378601
Epoch 5.58: Loss = 0.399429
Epoch 5.59: Loss = 0.468735
Epoch 5.60: Loss = 0.520279
Epoch 5.61: Loss = 0.408554
Epoch 5.62: Loss = 0.491791
Epoch 5.63: Loss = 0.582626
Epoch 5.64: Loss = 0.473862
Epoch 5.65: Loss = 0.560135
Epoch 5.66: Loss = 0.388351
Epoch 5.67: Loss = 0.411102
Epoch 5.68: Loss = 0.279755
Epoch 5.69: Loss = 0.342316
Epoch 5.70: Loss = 0.513245
Epoch 5.71: Loss = 0.314636
Epoch 5.72: Loss = 0.315964
Epoch 5.73: Loss = 0.410583
Epoch 5.74: Loss = 0.320572
Epoch 5.75: Loss = 0.645798
Epoch 5.76: Loss = 0.414505
Epoch 5.77: Loss = 0.311661
Epoch 5.78: Loss = 0.408707
Epoch 5.79: Loss = 0.49173
Epoch 5.80: Loss = 0.433807
Epoch 5.81: Loss = 0.337112
Epoch 5.82: Loss = 0.314346
Epoch 5.83: Loss = 0.462875
Epoch 5.84: Loss = 0.364426
Epoch 5.85: Loss = 0.542297
Epoch 5.86: Loss = 0.486786
Epoch 5.87: Loss = 0.283142
Epoch 5.88: Loss = 0.419769
Epoch 5.89: Loss = 0.417755
Epoch 5.90: Loss = 0.358353
Epoch 5.91: Loss = 0.455292
Epoch 5.92: Loss = 0.447647
Epoch 5.93: Loss = 0.535248
Epoch 5.94: Loss = 0.327255
Epoch 5.95: Loss = 0.389877
Epoch 5.96: Loss = 0.469879
Epoch 5.97: Loss = 0.333252
Epoch 5.98: Loss = 0.335678
Epoch 5.99: Loss = 0.460159
Epoch 5.100: Loss = 0.542679
Epoch 5.101: Loss = 0.559311
Epoch 5.102: Loss = 0.384506
Epoch 5.103: Loss = 0.342346
Epoch 5.104: Loss = 0.326599
Epoch 5.105: Loss = 0.510086
Epoch 5.106: Loss = 0.526367
Epoch 5.107: Loss = 0.333755
Epoch 5.108: Loss = 0.451019
Epoch 5.109: Loss = 0.347092
Epoch 5.110: Loss = 0.405457
Epoch 5.111: Loss = 0.326935
Epoch 5.112: Loss = 0.31842
Epoch 5.113: Loss = 0.362411
Epoch 5.114: Loss = 0.317795
Epoch 5.115: Loss = 0.284164
Epoch 5.116: Loss = 0.364227
Epoch 5.117: Loss = 0.216904
Epoch 5.118: Loss = 0.195587
Epoch 5.119: Loss = 0.277664
Epoch 5.120: Loss = 0.337799
TRAIN LOSS = 0.40332
TRAIN ACC = 88.9175 % (53353/60000)
Loss = 0.388214
Loss = 0.477173
Loss = 0.524109
Loss = 0.57106
Loss = 0.611755
Loss = 0.410767
Loss = 0.373184
Loss = 0.575867
Loss = 0.524872
Loss = 0.470718
Loss = 0.15625
Loss = 0.348892
Loss = 0.292892
Loss = 0.356506
Loss = 0.187042
Loss = 0.283798
Loss = 0.210587
Loss = 0.060257
Loss = 0.214874
Loss = 0.547714
TEST LOSS = 0.379327
TEST ACC = 533.53 % (8930/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.346802
Epoch 6.2: Loss = 0.478882
Epoch 6.3: Loss = 0.47052
Epoch 6.4: Loss = 0.307816
Epoch 6.5: Loss = 0.32486
Epoch 6.6: Loss = 0.354904
Epoch 6.7: Loss = 0.296097
Epoch 6.8: Loss = 0.36969
Epoch 6.9: Loss = 0.317261
Epoch 6.10: Loss = 0.369476
Epoch 6.11: Loss = 0.394196
Epoch 6.12: Loss = 0.402191
Epoch 6.13: Loss = 0.292435
Epoch 6.14: Loss = 0.36322
Epoch 6.15: Loss = 0.38855
Epoch 6.16: Loss = 0.443222
Epoch 6.17: Loss = 0.48793
Epoch 6.18: Loss = 0.626999
Epoch 6.19: Loss = 0.428726
Epoch 6.20: Loss = 0.3573
Epoch 6.21: Loss = 0.306839
Epoch 6.22: Loss = 0.273468
Epoch 6.23: Loss = 0.314102
Epoch 6.24: Loss = 0.549545
Epoch 6.25: Loss = 0.424728
Epoch 6.26: Loss = 0.578156
Epoch 6.27: Loss = 0.505173
Epoch 6.28: Loss = 0.450882
Epoch 6.29: Loss = 0.560135
Epoch 6.30: Loss = 0.556152
Epoch 6.31: Loss = 0.367142
Epoch 6.32: Loss = 0.445786
Epoch 6.33: Loss = 0.361557
Epoch 6.34: Loss = 0.418167
Epoch 6.35: Loss = 0.395111
Epoch 6.36: Loss = 0.45285
Epoch 6.37: Loss = 0.296661
Epoch 6.38: Loss = 0.362167
Epoch 6.39: Loss = 0.386871
Epoch 6.40: Loss = 0.388245
Epoch 6.41: Loss = 0.360199
Epoch 6.42: Loss = 0.59259
Epoch 6.43: Loss = 0.353928
Epoch 6.44: Loss = 0.327789
Epoch 6.45: Loss = 0.392624
Epoch 6.46: Loss = 0.419205
Epoch 6.47: Loss = 0.382202
Epoch 6.48: Loss = 0.423553
Epoch 6.49: Loss = 0.396774
Epoch 6.50: Loss = 0.524811
Epoch 6.51: Loss = 0.324829
Epoch 6.52: Loss = 0.308395
Epoch 6.53: Loss = 0.38147
Epoch 6.54: Loss = 0.513535
Epoch 6.55: Loss = 0.404861
Epoch 6.56: Loss = 0.403717
Epoch 6.57: Loss = 0.38591
Epoch 6.58: Loss = 0.422424
Epoch 6.59: Loss = 0.46727
Epoch 6.60: Loss = 0.53038
Epoch 6.61: Loss = 0.3965
Epoch 6.62: Loss = 0.498062
Epoch 6.63: Loss = 0.576752
Epoch 6.64: Loss = 0.477737
Epoch 6.65: Loss = 0.57489
Epoch 6.66: Loss = 0.410599
Epoch 6.67: Loss = 0.39798
Epoch 6.68: Loss = 0.286118
Epoch 6.69: Loss = 0.340103
Epoch 6.70: Loss = 0.510391
Epoch 6.71: Loss = 0.315018
Epoch 6.72: Loss = 0.29068
Epoch 6.73: Loss = 0.433624
Epoch 6.74: Loss = 0.32103
Epoch 6.75: Loss = 0.662491
Epoch 6.76: Loss = 0.41832
Epoch 6.77: Loss = 0.308121
Epoch 6.78: Loss = 0.39975
Epoch 6.79: Loss = 0.475922
Epoch 6.80: Loss = 0.425125
Epoch 6.81: Loss = 0.335999
Epoch 6.82: Loss = 0.316177
Epoch 6.83: Loss = 0.474121
Epoch 6.84: Loss = 0.364944
Epoch 6.85: Loss = 0.555603
Epoch 6.86: Loss = 0.513443
Epoch 6.87: Loss = 0.277344
Epoch 6.88: Loss = 0.409882
Epoch 6.89: Loss = 0.406815
Epoch 6.90: Loss = 0.350342
Epoch 6.91: Loss = 0.467941
Epoch 6.92: Loss = 0.458237
Epoch 6.93: Loss = 0.526505
Epoch 6.94: Loss = 0.336075
Epoch 6.95: Loss = 0.361252
Epoch 6.96: Loss = 0.462753
Epoch 6.97: Loss = 0.321747
Epoch 6.98: Loss = 0.321518
Epoch 6.99: Loss = 0.474548
Epoch 6.100: Loss = 0.547638
Epoch 6.101: Loss = 0.556107
Epoch 6.102: Loss = 0.412979
Epoch 6.103: Loss = 0.342026
Epoch 6.104: Loss = 0.337784
Epoch 6.105: Loss = 0.546005
Epoch 6.106: Loss = 0.52597
Epoch 6.107: Loss = 0.334946
Epoch 6.108: Loss = 0.472412
Epoch 6.109: Loss = 0.343414
Epoch 6.110: Loss = 0.419373
Epoch 6.111: Loss = 0.330032
Epoch 6.112: Loss = 0.320541
Epoch 6.113: Loss = 0.365997
Epoch 6.114: Loss = 0.329163
Epoch 6.115: Loss = 0.281174
Epoch 6.116: Loss = 0.371964
Epoch 6.117: Loss = 0.226563
Epoch 6.118: Loss = 0.201996
Epoch 6.119: Loss = 0.269089
Epoch 6.120: Loss = 0.34111
TRAIN LOSS = 0.405502
TRAIN ACC = 89.1891 % (53516/60000)
Loss = 0.393906
Loss = 0.484436
Loss = 0.537491
Loss = 0.566696
Loss = 0.604492
Loss = 0.402237
Loss = 0.365158
Loss = 0.629974
Loss = 0.528778
Loss = 0.46077
Loss = 0.161484
Loss = 0.316391
Loss = 0.313919
Loss = 0.365677
Loss = 0.167297
Loss = 0.289505
Loss = 0.211334
Loss = 0.0592804
Loss = 0.232254
Loss = 0.60611
TEST LOSS = 0.384859
TEST ACC = 535.159 % (8962/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.373505
Epoch 7.2: Loss = 0.475403
Epoch 7.3: Loss = 0.485367
Epoch 7.4: Loss = 0.312424
Epoch 7.5: Loss = 0.326782
Epoch 7.6: Loss = 0.364456
Epoch 7.7: Loss = 0.283234
Epoch 7.8: Loss = 0.373123
Epoch 7.9: Loss = 0.316711
Epoch 7.10: Loss = 0.381439
Epoch 7.11: Loss = 0.399887
Epoch 7.12: Loss = 0.422714
Epoch 7.13: Loss = 0.297333
Epoch 7.14: Loss = 0.370728
Epoch 7.15: Loss = 0.370346
Epoch 7.16: Loss = 0.432236
Epoch 7.17: Loss = 0.48819
Epoch 7.18: Loss = 0.635132
Epoch 7.19: Loss = 0.442947
Epoch 7.20: Loss = 0.345825
Epoch 7.21: Loss = 0.318222
Epoch 7.22: Loss = 0.279465
Epoch 7.23: Loss = 0.309891
Epoch 7.24: Loss = 0.553726
Epoch 7.25: Loss = 0.415466
Epoch 7.26: Loss = 0.585358
Epoch 7.27: Loss = 0.505188
Epoch 7.28: Loss = 0.445679
Epoch 7.29: Loss = 0.551453
Epoch 7.30: Loss = 0.548599
Epoch 7.31: Loss = 0.36879
Epoch 7.32: Loss = 0.468201
Epoch 7.33: Loss = 0.352646
Epoch 7.34: Loss = 0.444305
Epoch 7.35: Loss = 0.391983
Epoch 7.36: Loss = 0.466232
Epoch 7.37: Loss = 0.2948
Epoch 7.38: Loss = 0.372757
Epoch 7.39: Loss = 0.379486
Epoch 7.40: Loss = 0.376236
Epoch 7.41: Loss = 0.339203
Epoch 7.42: Loss = 0.58934
Epoch 7.43: Loss = 0.334167
Epoch 7.44: Loss = 0.324417
Epoch 7.45: Loss = 0.399429
Epoch 7.46: Loss = 0.424362
Epoch 7.47: Loss = 0.389664
Epoch 7.48: Loss = 0.439713
Epoch 7.49: Loss = 0.426834
Epoch 7.50: Loss = 0.541794
Epoch 7.51: Loss = 0.329102
Epoch 7.52: Loss = 0.355026
Epoch 7.53: Loss = 0.397064
Epoch 7.54: Loss = 0.521408
Epoch 7.55: Loss = 0.418945
Epoch 7.56: Loss = 0.398499
Epoch 7.57: Loss = 0.393005
Epoch 7.58: Loss = 0.458755
Epoch 7.59: Loss = 0.484818
Epoch 7.60: Loss = 0.556122
Epoch 7.61: Loss = 0.422226
Epoch 7.62: Loss = 0.502731
Epoch 7.63: Loss = 0.585159
Epoch 7.64: Loss = 0.49498
Epoch 7.65: Loss = 0.600769
Epoch 7.66: Loss = 0.397308
Epoch 7.67: Loss = 0.399811
Epoch 7.68: Loss = 0.289658
Epoch 7.69: Loss = 0.334183
Epoch 7.70: Loss = 0.506744
Epoch 7.71: Loss = 0.339584
Epoch 7.72: Loss = 0.304993
Epoch 7.73: Loss = 0.420395
Epoch 7.74: Loss = 0.331223
Epoch 7.75: Loss = 0.70369
Epoch 7.76: Loss = 0.438065
Epoch 7.77: Loss = 0.298843
Epoch 7.78: Loss = 0.400955
Epoch 7.79: Loss = 0.50264
Epoch 7.80: Loss = 0.419113
Epoch 7.81: Loss = 0.337646
Epoch 7.82: Loss = 0.312973
Epoch 7.83: Loss = 0.481888
Epoch 7.84: Loss = 0.369934
Epoch 7.85: Loss = 0.579559
Epoch 7.86: Loss = 0.518051
Epoch 7.87: Loss = 0.295135
Epoch 7.88: Loss = 0.438034
Epoch 7.89: Loss = 0.42099
Epoch 7.90: Loss = 0.357742
Epoch 7.91: Loss = 0.476044
Epoch 7.92: Loss = 0.476089
Epoch 7.93: Loss = 0.529541
Epoch 7.94: Loss = 0.327026
Epoch 7.95: Loss = 0.392242
Epoch 7.96: Loss = 0.465179
Epoch 7.97: Loss = 0.330826
Epoch 7.98: Loss = 0.345581
Epoch 7.99: Loss = 0.467209
Epoch 7.100: Loss = 0.567688
Epoch 7.101: Loss = 0.572144
Epoch 7.102: Loss = 0.400406
Epoch 7.103: Loss = 0.340088
Epoch 7.104: Loss = 0.351273
Epoch 7.105: Loss = 0.546432
Epoch 7.106: Loss = 0.5336
Epoch 7.107: Loss = 0.321838
Epoch 7.108: Loss = 0.461578
Epoch 7.109: Loss = 0.347687
Epoch 7.110: Loss = 0.416031
Epoch 7.111: Loss = 0.321289
Epoch 7.112: Loss = 0.307587
Epoch 7.113: Loss = 0.374664
Epoch 7.114: Loss = 0.31221
Epoch 7.115: Loss = 0.265381
Epoch 7.116: Loss = 0.354675
Epoch 7.117: Loss = 0.216354
Epoch 7.118: Loss = 0.191116
Epoch 7.119: Loss = 0.263596
Epoch 7.120: Loss = 0.350052
TRAIN LOSS = 0.41066
TRAIN ACC = 89.3204 % (53595/60000)
Loss = 0.396881
Loss = 0.46727
Loss = 0.528259
Loss = 0.591476
Loss = 0.580734
Loss = 0.417892
Loss = 0.353989
Loss = 0.656342
Loss = 0.533173
Loss = 0.448135
Loss = 0.157028
Loss = 0.326736
Loss = 0.260635
Loss = 0.371231
Loss = 0.163986
Loss = 0.257187
Loss = 0.205643
Loss = 0.0559845
Loss = 0.260956
Loss = 0.592819
TEST LOSS = 0.381318
TEST ACC = 535.95 % (8990/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.386078
Epoch 8.2: Loss = 0.475571
Epoch 8.3: Loss = 0.480545
Epoch 8.4: Loss = 0.326492
Epoch 8.5: Loss = 0.322784
Epoch 8.6: Loss = 0.341248
Epoch 8.7: Loss = 0.272858
Epoch 8.8: Loss = 0.36467
Epoch 8.9: Loss = 0.333878
Epoch 8.10: Loss = 0.383484
Epoch 8.11: Loss = 0.401184
Epoch 8.12: Loss = 0.398148
Epoch 8.13: Loss = 0.268631
Epoch 8.14: Loss = 0.367966
Epoch 8.15: Loss = 0.393433
Epoch 8.16: Loss = 0.439682
Epoch 8.17: Loss = 0.501175
Epoch 8.18: Loss = 0.631378
Epoch 8.19: Loss = 0.438492
Epoch 8.20: Loss = 0.325653
Epoch 8.21: Loss = 0.329849
Epoch 8.22: Loss = 0.266541
Epoch 8.23: Loss = 0.293854
Epoch 8.24: Loss = 0.5383
Epoch 8.25: Loss = 0.457657
Epoch 8.26: Loss = 0.555679
Epoch 8.27: Loss = 0.485931
Epoch 8.28: Loss = 0.420105
Epoch 8.29: Loss = 0.557785
Epoch 8.30: Loss = 0.583267
Epoch 8.31: Loss = 0.375183
Epoch 8.32: Loss = 0.478516
Epoch 8.33: Loss = 0.359589
Epoch 8.34: Loss = 0.422531
Epoch 8.35: Loss = 0.380966
Epoch 8.36: Loss = 0.453934
Epoch 8.37: Loss = 0.291519
Epoch 8.38: Loss = 0.341858
Epoch 8.39: Loss = 0.386154
Epoch 8.40: Loss = 0.364594
Epoch 8.41: Loss = 0.337616
Epoch 8.42: Loss = 0.57608
Epoch 8.43: Loss = 0.32666
Epoch 8.44: Loss = 0.314682
Epoch 8.45: Loss = 0.378433
Epoch 8.46: Loss = 0.419174
Epoch 8.47: Loss = 0.392181
Epoch 8.48: Loss = 0.404221
Epoch 8.49: Loss = 0.420776
Epoch 8.50: Loss = 0.5103
Epoch 8.51: Loss = 0.324203
Epoch 8.52: Loss = 0.353165
Epoch 8.53: Loss = 0.395721
Epoch 8.54: Loss = 0.513306
Epoch 8.55: Loss = 0.435257
Epoch 8.56: Loss = 0.404755
Epoch 8.57: Loss = 0.398285
Epoch 8.58: Loss = 0.455399
Epoch 8.59: Loss = 0.466553
Epoch 8.60: Loss = 0.536179
Epoch 8.61: Loss = 0.395309
Epoch 8.62: Loss = 0.481247
Epoch 8.63: Loss = 0.588806
Epoch 8.64: Loss = 0.493973
Epoch 8.65: Loss = 0.580841
Epoch 8.66: Loss = 0.38443
Epoch 8.67: Loss = 0.379486
Epoch 8.68: Loss = 0.275284
Epoch 8.69: Loss = 0.335968
Epoch 8.70: Loss = 0.511963
Epoch 8.71: Loss = 0.349045
Epoch 8.72: Loss = 0.295792
Epoch 8.73: Loss = 0.418869
Epoch 8.74: Loss = 0.312592
Epoch 8.75: Loss = 0.709106
Epoch 8.76: Loss = 0.425385
Epoch 8.77: Loss = 0.301208
Epoch 8.78: Loss = 0.408356
Epoch 8.79: Loss = 0.525574
Epoch 8.80: Loss = 0.399582
Epoch 8.81: Loss = 0.325378
Epoch 8.82: Loss = 0.289963
Epoch 8.83: Loss = 0.512695
Epoch 8.84: Loss = 0.369522
Epoch 8.85: Loss = 0.557266
Epoch 8.86: Loss = 0.522095
Epoch 8.87: Loss = 0.281036
Epoch 8.88: Loss = 0.459045
Epoch 8.89: Loss = 0.413956
Epoch 8.90: Loss = 0.372177
Epoch 8.91: Loss = 0.486343
Epoch 8.92: Loss = 0.460175
Epoch 8.93: Loss = 0.539993
Epoch 8.94: Loss = 0.338913
Epoch 8.95: Loss = 0.371704
Epoch 8.96: Loss = 0.445068
Epoch 8.97: Loss = 0.336288
Epoch 8.98: Loss = 0.347946
Epoch 8.99: Loss = 0.460007
Epoch 8.100: Loss = 0.600739
Epoch 8.101: Loss = 0.572983
Epoch 8.102: Loss = 0.364532
Epoch 8.103: Loss = 0.339828
Epoch 8.104: Loss = 0.338226
Epoch 8.105: Loss = 0.537048
Epoch 8.106: Loss = 0.52771
Epoch 8.107: Loss = 0.34021
Epoch 8.108: Loss = 0.471756
Epoch 8.109: Loss = 0.345276
Epoch 8.110: Loss = 0.392288
Epoch 8.111: Loss = 0.314682
Epoch 8.112: Loss = 0.297592
Epoch 8.113: Loss = 0.363754
Epoch 8.114: Loss = 0.277267
Epoch 8.115: Loss = 0.28009
Epoch 8.116: Loss = 0.360916
Epoch 8.117: Loss = 0.225388
Epoch 8.118: Loss = 0.194382
Epoch 8.119: Loss = 0.257385
Epoch 8.120: Loss = 0.375076
TRAIN LOSS = 0.406677
TRAIN ACC = 89.6057 % (53766/60000)
Loss = 0.398758
Loss = 0.440491
Loss = 0.514389
Loss = 0.588974
Loss = 0.560074
Loss = 0.396637
Loss = 0.335327
Loss = 0.655228
Loss = 0.542816
Loss = 0.453644
Loss = 0.157608
Loss = 0.279251
Loss = 0.302933
Loss = 0.399979
Loss = 0.18631
Loss = 0.263687
Loss = 0.213867
Loss = 0.0548096
Loss = 0.25058
Loss = 0.591431
TEST LOSS = 0.379339
TEST ACC = 537.659 % (8997/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.376556
Epoch 9.2: Loss = 0.486954
Epoch 9.3: Loss = 0.515869
Epoch 9.4: Loss = 0.325424
Epoch 9.5: Loss = 0.318848
Epoch 9.6: Loss = 0.341003
Epoch 9.7: Loss = 0.259583
Epoch 9.8: Loss = 0.34494
Epoch 9.9: Loss = 0.345245
Epoch 9.10: Loss = 0.400299
Epoch 9.11: Loss = 0.423019
Epoch 9.12: Loss = 0.396133
Epoch 9.13: Loss = 0.29184
Epoch 9.14: Loss = 0.361511
Epoch 9.15: Loss = 0.381866
Epoch 9.16: Loss = 0.419662
Epoch 9.17: Loss = 0.506668
Epoch 9.18: Loss = 0.632477
Epoch 9.19: Loss = 0.444168
Epoch 9.20: Loss = 0.31723
Epoch 9.21: Loss = 0.341034
Epoch 9.22: Loss = 0.264938
Epoch 9.23: Loss = 0.293869
Epoch 9.24: Loss = 0.566711
Epoch 9.25: Loss = 0.421631
Epoch 9.26: Loss = 0.580521
Epoch 9.27: Loss = 0.510544
Epoch 9.28: Loss = 0.424454
Epoch 9.29: Loss = 0.523743
Epoch 9.30: Loss = 0.559708
Epoch 9.31: Loss = 0.373184
Epoch 9.32: Loss = 0.449844
Epoch 9.33: Loss = 0.379013
Epoch 9.34: Loss = 0.41832
Epoch 9.35: Loss = 0.369553
Epoch 9.36: Loss = 0.439987
Epoch 9.37: Loss = 0.286026
Epoch 9.38: Loss = 0.339386
Epoch 9.39: Loss = 0.371307
Epoch 9.40: Loss = 0.371902
Epoch 9.41: Loss = 0.350784
Epoch 9.42: Loss = 0.584885
Epoch 9.43: Loss = 0.310669
Epoch 9.44: Loss = 0.319641
Epoch 9.45: Loss = 0.359894
Epoch 9.46: Loss = 0.414597
Epoch 9.47: Loss = 0.388458
Epoch 9.48: Loss = 0.434555
Epoch 9.49: Loss = 0.424759
Epoch 9.50: Loss = 0.522308
Epoch 9.51: Loss = 0.317703
Epoch 9.52: Loss = 0.374619
Epoch 9.53: Loss = 0.402542
Epoch 9.54: Loss = 0.559311
Epoch 9.55: Loss = 0.459167
Epoch 9.56: Loss = 0.413727
Epoch 9.57: Loss = 0.401321
Epoch 9.58: Loss = 0.445831
Epoch 9.59: Loss = 0.465637
Epoch 9.60: Loss = 0.515457
Epoch 9.61: Loss = 0.385666
Epoch 9.62: Loss = 0.478195
Epoch 9.63: Loss = 0.597244
Epoch 9.64: Loss = 0.490585
Epoch 9.65: Loss = 0.538803
Epoch 9.66: Loss = 0.370331
Epoch 9.67: Loss = 0.386002
Epoch 9.68: Loss = 0.271011
Epoch 9.69: Loss = 0.336487
Epoch 9.70: Loss = 0.512405
Epoch 9.71: Loss = 0.350525
Epoch 9.72: Loss = 0.267365
Epoch 9.73: Loss = 0.403488
Epoch 9.74: Loss = 0.324295
Epoch 9.75: Loss = 0.739349
Epoch 9.76: Loss = 0.469894
Epoch 9.77: Loss = 0.316849
Epoch 9.78: Loss = 0.42131
Epoch 9.79: Loss = 0.556702
Epoch 9.80: Loss = 0.402191
Epoch 9.81: Loss = 0.350403
Epoch 9.82: Loss = 0.31192
Epoch 9.83: Loss = 0.489548
Epoch 9.84: Loss = 0.385513
Epoch 9.85: Loss = 0.544754
Epoch 9.86: Loss = 0.538605
Epoch 9.87: Loss = 0.284668
Epoch 9.88: Loss = 0.439835
Epoch 9.89: Loss = 0.423721
Epoch 9.90: Loss = 0.375565
Epoch 9.91: Loss = 0.503387
Epoch 9.92: Loss = 0.477036
Epoch 9.93: Loss = 0.572784
Epoch 9.94: Loss = 0.342392
Epoch 9.95: Loss = 0.39621
Epoch 9.96: Loss = 0.4543
Epoch 9.97: Loss = 0.338531
Epoch 9.98: Loss = 0.357788
Epoch 9.99: Loss = 0.510376
Epoch 9.100: Loss = 0.596207
Epoch 9.101: Loss = 0.558472
Epoch 9.102: Loss = 0.386124
Epoch 9.103: Loss = 0.332169
Epoch 9.104: Loss = 0.362534
Epoch 9.105: Loss = 0.571686
Epoch 9.106: Loss = 0.54982
Epoch 9.107: Loss = 0.352066
Epoch 9.108: Loss = 0.490448
Epoch 9.109: Loss = 0.349472
Epoch 9.110: Loss = 0.409042
Epoch 9.111: Loss = 0.32959
Epoch 9.112: Loss = 0.331223
Epoch 9.113: Loss = 0.371414
Epoch 9.114: Loss = 0.287811
Epoch 9.115: Loss = 0.294907
Epoch 9.116: Loss = 0.368805
Epoch 9.117: Loss = 0.230042
Epoch 9.118: Loss = 0.188492
Epoch 9.119: Loss = 0.262054
Epoch 9.120: Loss = 0.380905
TRAIN LOSS = 0.411346
TRAIN ACC = 89.7903 % (53877/60000)
Loss = 0.392258
Loss = 0.486343
Loss = 0.503784
Loss = 0.5979
Loss = 0.563431
Loss = 0.396805
Loss = 0.350174
Loss = 0.667328
Loss = 0.575027
Loss = 0.465668
Loss = 0.143173
Loss = 0.303131
Loss = 0.318893
Loss = 0.390411
Loss = 0.175491
Loss = 0.270889
Loss = 0.206467
Loss = 0.0548096
Loss = 0.294678
Loss = 0.621048
TEST LOSS = 0.388885
TEST ACC = 538.77 % (9031/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.375168
Epoch 10.2: Loss = 0.521576
Epoch 10.3: Loss = 0.518341
Epoch 10.4: Loss = 0.310608
Epoch 10.5: Loss = 0.295792
Epoch 10.6: Loss = 0.313782
Epoch 10.7: Loss = 0.262772
Epoch 10.8: Loss = 0.341843
Epoch 10.9: Loss = 0.36554
Epoch 10.10: Loss = 0.395996
Epoch 10.11: Loss = 0.406723
Epoch 10.12: Loss = 0.403076
Epoch 10.13: Loss = 0.28334
Epoch 10.14: Loss = 0.384598
Epoch 10.15: Loss = 0.40976
Epoch 10.16: Loss = 0.455276
Epoch 10.17: Loss = 0.539093
Epoch 10.18: Loss = 0.655457
Epoch 10.19: Loss = 0.453415
Epoch 10.20: Loss = 0.303848
Epoch 10.21: Loss = 0.332108
Epoch 10.22: Loss = 0.257294
Epoch 10.23: Loss = 0.30867
Epoch 10.24: Loss = 0.544434
Epoch 10.25: Loss = 0.473022
Epoch 10.26: Loss = 0.589615
Epoch 10.27: Loss = 0.52182
Epoch 10.28: Loss = 0.410324
Epoch 10.29: Loss = 0.54599
Epoch 10.30: Loss = 0.595917
Epoch 10.31: Loss = 0.364014
Epoch 10.32: Loss = 0.48793
Epoch 10.33: Loss = 0.378677
Epoch 10.34: Loss = 0.429642
Epoch 10.35: Loss = 0.355301
Epoch 10.36: Loss = 0.42601
Epoch 10.37: Loss = 0.276672
Epoch 10.38: Loss = 0.355484
Epoch 10.39: Loss = 0.363968
Epoch 10.40: Loss = 0.379028
Epoch 10.41: Loss = 0.365768
Epoch 10.42: Loss = 0.59198
Epoch 10.43: Loss = 0.315491
Epoch 10.44: Loss = 0.316315
Epoch 10.45: Loss = 0.374054
Epoch 10.46: Loss = 0.395203
Epoch 10.47: Loss = 0.41127
Epoch 10.48: Loss = 0.439972
Epoch 10.49: Loss = 0.397537
Epoch 10.50: Loss = 0.514038
Epoch 10.51: Loss = 0.333298
Epoch 10.52: Loss = 0.34346
Epoch 10.53: Loss = 0.386734
Epoch 10.54: Loss = 0.561188
Epoch 10.55: Loss = 0.437363
Epoch 10.56: Loss = 0.406067
Epoch 10.57: Loss = 0.413284
Epoch 10.58: Loss = 0.42836
Epoch 10.59: Loss = 0.46698
Epoch 10.60: Loss = 0.493759
Epoch 10.61: Loss = 0.406815
Epoch 10.62: Loss = 0.459305
Epoch 10.63: Loss = 0.573441
Epoch 10.64: Loss = 0.461975
Epoch 10.65: Loss = 0.53392
Epoch 10.66: Loss = 0.383835
Epoch 10.67: Loss = 0.38208
Epoch 10.68: Loss = 0.24231
Epoch 10.69: Loss = 0.355362
Epoch 10.70: Loss = 0.518051
Epoch 10.71: Loss = 0.368896
Epoch 10.72: Loss = 0.2724
Epoch 10.73: Loss = 0.43602
Epoch 10.74: Loss = 0.321701
Epoch 10.75: Loss = 0.757263
Epoch 10.76: Loss = 0.462173
Epoch 10.77: Loss = 0.29332
Epoch 10.78: Loss = 0.423035
Epoch 10.79: Loss = 0.563385
Epoch 10.80: Loss = 0.411606
Epoch 10.81: Loss = 0.354523
Epoch 10.82: Loss = 0.321762
Epoch 10.83: Loss = 0.51355
Epoch 10.84: Loss = 0.365768
Epoch 10.85: Loss = 0.520142
Epoch 10.86: Loss = 0.552414
Epoch 10.87: Loss = 0.28746
Epoch 10.88: Loss = 0.414169
Epoch 10.89: Loss = 0.420914
Epoch 10.90: Loss = 0.369064
Epoch 10.91: Loss = 0.473679
Epoch 10.92: Loss = 0.459824
Epoch 10.93: Loss = 0.58345
Epoch 10.94: Loss = 0.305603
Epoch 10.95: Loss = 0.398438
Epoch 10.96: Loss = 0.481445
Epoch 10.97: Loss = 0.314011
Epoch 10.98: Loss = 0.342499
Epoch 10.99: Loss = 0.511063
Epoch 10.100: Loss = 0.608261
Epoch 10.101: Loss = 0.572968
Epoch 10.102: Loss = 0.368073
Epoch 10.103: Loss = 0.330109
Epoch 10.104: Loss = 0.341217
Epoch 10.105: Loss = 0.558807
Epoch 10.106: Loss = 0.554214
Epoch 10.107: Loss = 0.335129
Epoch 10.108: Loss = 0.480026
Epoch 10.109: Loss = 0.340271
Epoch 10.110: Loss = 0.391663
Epoch 10.111: Loss = 0.314484
Epoch 10.112: Loss = 0.347626
Epoch 10.113: Loss = 0.378403
Epoch 10.114: Loss = 0.279312
Epoch 10.115: Loss = 0.296539
Epoch 10.116: Loss = 0.378433
Epoch 10.117: Loss = 0.222305
Epoch 10.118: Loss = 0.194046
Epoch 10.119: Loss = 0.259155
Epoch 10.120: Loss = 0.371475
TRAIN LOSS = 0.411026
TRAIN ACC = 89.9673 % (53983/60000)
Loss = 0.410141
Loss = 0.460281
Loss = 0.486252
Loss = 0.559769
Loss = 0.531052
Loss = 0.382263
Loss = 0.35553
Loss = 0.664703
Loss = 0.520401
Loss = 0.451172
Loss = 0.162521
Loss = 0.304413
Loss = 0.343979
Loss = 0.418198
Loss = 0.173965
Loss = 0.237305
Loss = 0.201614
Loss = 0.0472717
Loss = 0.278122
Loss = 0.632736
TEST LOSS = 0.381084
TEST ACC = 539.828 % (9062/10000)
