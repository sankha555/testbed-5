Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.36541
Epoch 1.2: Loss = 2.30972
Epoch 1.3: Loss = 2.25129
Epoch 1.4: Loss = 2.21618
Epoch 1.5: Loss = 2.14493
Epoch 1.6: Loss = 2.08788
Epoch 1.7: Loss = 2.06882
Epoch 1.8: Loss = 2.02145
Epoch 1.9: Loss = 1.97233
Epoch 1.10: Loss = 1.94618
Epoch 1.11: Loss = 1.85243
Epoch 1.12: Loss = 1.85999
Epoch 1.13: Loss = 1.77046
Epoch 1.14: Loss = 1.8043
Epoch 1.15: Loss = 1.84449
Epoch 1.16: Loss = 1.76416
Epoch 1.17: Loss = 1.68037
Epoch 1.18: Loss = 1.67094
Epoch 1.19: Loss = 1.65384
Epoch 1.20: Loss = 1.57509
Epoch 1.21: Loss = 1.52096
Epoch 1.22: Loss = 1.51178
Epoch 1.23: Loss = 1.45293
Epoch 1.24: Loss = 1.54277
Epoch 1.25: Loss = 1.49562
Epoch 1.26: Loss = 1.50137
Epoch 1.27: Loss = 1.43494
Epoch 1.28: Loss = 1.41277
Epoch 1.29: Loss = 1.42698
Epoch 1.30: Loss = 1.49816
Epoch 1.31: Loss = 1.31981
Epoch 1.32: Loss = 1.3316
Epoch 1.33: Loss = 1.28679
Epoch 1.34: Loss = 1.30737
Epoch 1.35: Loss = 1.25193
Epoch 1.36: Loss = 1.35801
Epoch 1.37: Loss = 1.19035
Epoch 1.38: Loss = 1.15193
Epoch 1.39: Loss = 1.16737
Epoch 1.40: Loss = 1.10824
Epoch 1.41: Loss = 1.14452
Epoch 1.42: Loss = 1.13042
Epoch 1.43: Loss = 1.08764
Epoch 1.44: Loss = 0.96323
Epoch 1.45: Loss = 1.13177
Epoch 1.46: Loss = 1.05401
Epoch 1.47: Loss = 0.979241
Epoch 1.48: Loss = 1.04983
Epoch 1.49: Loss = 0.999834
Epoch 1.50: Loss = 1.06535
Epoch 1.51: Loss = 0.904989
Epoch 1.52: Loss = 0.92011
Epoch 1.53: Loss = 0.992284
Epoch 1.54: Loss = 0.993271
Epoch 1.55: Loss = 0.99453
Epoch 1.56: Loss = 0.902584
Epoch 1.57: Loss = 0.838035
Epoch 1.58: Loss = 0.877317
Epoch 1.59: Loss = 0.913034
Epoch 1.60: Loss = 0.995084
Epoch 1.61: Loss = 0.948448
Epoch 1.62: Loss = 0.961849
Epoch 1.63: Loss = 0.967169
Epoch 1.64: Loss = 0.959072
Epoch 1.65: Loss = 0.961737
Epoch 1.66: Loss = 0.854414
Epoch 1.67: Loss = 0.873629
Epoch 1.68: Loss = 0.696918
Epoch 1.69: Loss = 0.789995
Epoch 1.70: Loss = 0.870896
Epoch 1.71: Loss = 0.774102
Epoch 1.72: Loss = 0.795797
Epoch 1.73: Loss = 0.819899
Epoch 1.74: Loss = 0.67631
Epoch 1.75: Loss = 0.814078
Epoch 1.76: Loss = 0.786518
Epoch 1.77: Loss = 0.733288
Epoch 1.78: Loss = 0.716361
Epoch 1.79: Loss = 0.755475
Epoch 1.80: Loss = 0.834816
Epoch 1.81: Loss = 0.70297
Epoch 1.82: Loss = 0.655361
Epoch 1.83: Loss = 0.842779
Epoch 1.84: Loss = 0.740263
Epoch 1.85: Loss = 0.830053
Epoch 1.86: Loss = 0.728979
Epoch 1.87: Loss = 0.612659
Epoch 1.88: Loss = 0.687761
Epoch 1.89: Loss = 0.765483
Epoch 1.90: Loss = 0.665716
Epoch 1.91: Loss = 0.725094
Epoch 1.92: Loss = 0.716162
Epoch 1.93: Loss = 0.731393
Epoch 1.94: Loss = 0.594639
Epoch 1.95: Loss = 0.711568
Epoch 1.96: Loss = 0.670781
Epoch 1.97: Loss = 0.50695
Epoch 1.98: Loss = 0.653039
Epoch 1.99: Loss = 0.718163
Epoch 1.100: Loss = 0.809527
Epoch 1.101: Loss = 0.703434
Epoch 1.102: Loss = 0.651617
Epoch 1.103: Loss = 0.608168
Epoch 1.104: Loss = 0.557163
Epoch 1.105: Loss = 0.664193
Epoch 1.106: Loss = 0.663178
Epoch 1.107: Loss = 0.569106
Epoch 1.108: Loss = 0.638958
Epoch 1.109: Loss = 0.600906
Epoch 1.110: Loss = 0.627606
Epoch 1.111: Loss = 0.510662
Epoch 1.112: Loss = 0.519518
Epoch 1.113: Loss = 0.581278
Epoch 1.114: Loss = 0.515815
Epoch 1.115: Loss = 0.569985
Epoch 1.116: Loss = 0.572445
Epoch 1.117: Loss = 0.457408
Epoch 1.118: Loss = 0.414056
Epoch 1.119: Loss = 0.444141
Epoch 1.120: Loss = 0.467488
TRAIN LOSS = 1.06722
TRAIN ACC = -218.443 % (42286/60000)
Loss = 0.607445
Loss = 0.648583
Loss = 0.74683
Loss = 0.707398
Loss = 0.731859
Loss = 0.623392
Loss = 0.609541
Loss = 0.762521
Loss = 0.732071
Loss = 0.673038
Loss = 0.328826
Loss = 0.495619
Loss = 0.35686
Loss = 0.52008
Loss = 0.458062
Loss = 0.422638
Loss = 0.406
Loss = 0.243029
Loss = 0.413116
Loss = 0.662461
TEST LOSS = 0.557468
TEST ACC = 422.86 % (8394/10000)
Reducing learning rate to 0.1
Epoch 2.1: Loss = 0.534322
Epoch 2.2: Loss = 0.660255
Epoch 2.3: Loss = 0.625856
Epoch 2.4: Loss = 0.494854
Epoch 2.5: Loss = 0.515884
Epoch 2.6: Loss = 0.515342
Epoch 2.7: Loss = 0.557687
Epoch 2.8: Loss = 0.547139
Epoch 2.9: Loss = 0.526629
Epoch 2.10: Loss = 0.533868
Epoch 2.11: Loss = 0.52715
Epoch 2.12: Loss = 0.534688
Epoch 2.13: Loss = 0.443023
Epoch 2.14: Loss = 0.494859
Epoch 2.15: Loss = 0.625743
Epoch 2.16: Loss = 0.60335
Epoch 2.17: Loss = 0.584772
Epoch 2.18: Loss = 0.663262
Epoch 2.19: Loss = 0.526616
Epoch 2.20: Loss = 0.488536
Epoch 2.21: Loss = 0.447599
Epoch 2.22: Loss = 0.44724
Epoch 2.23: Loss = 0.441073
Epoch 2.24: Loss = 0.638056
Epoch 2.25: Loss = 0.56859
Epoch 2.26: Loss = 0.623198
Epoch 2.27: Loss = 0.603307
Epoch 2.28: Loss = 0.58036
Epoch 2.29: Loss = 0.648365
Epoch 2.30: Loss = 0.696554
Epoch 2.31: Loss = 0.479574
Epoch 2.32: Loss = 0.599038
Epoch 2.33: Loss = 0.506412
Epoch 2.34: Loss = 0.560724
Epoch 2.35: Loss = 0.552621
Epoch 2.36: Loss = 0.622479
Epoch 2.37: Loss = 0.460338
Epoch 2.38: Loss = 0.416878
Epoch 2.39: Loss = 0.509811
Epoch 2.40: Loss = 0.4666
Epoch 2.41: Loss = 0.549255
Epoch 2.42: Loss = 0.589638
Epoch 2.43: Loss = 0.467141
Epoch 2.44: Loss = 0.402079
Epoch 2.45: Loss = 0.551929
Epoch 2.46: Loss = 0.526929
Epoch 2.47: Loss = 0.461764
Epoch 2.48: Loss = 0.519872
Epoch 2.49: Loss = 0.519556
Epoch 2.50: Loss = 0.57167
Epoch 2.51: Loss = 0.440834
Epoch 2.52: Loss = 0.433965
Epoch 2.53: Loss = 0.505054
Epoch 2.54: Loss = 0.56935
Epoch 2.55: Loss = 0.53149
Epoch 2.56: Loss = 0.472207
Epoch 2.57: Loss = 0.42807
Epoch 2.58: Loss = 0.49905
Epoch 2.59: Loss = 0.533833
Epoch 2.60: Loss = 0.594043
Epoch 2.61: Loss = 0.568009
Epoch 2.62: Loss = 0.579444
Epoch 2.63: Loss = 0.623528
Epoch 2.64: Loss = 0.586853
Epoch 2.65: Loss = 0.629345
Epoch 2.66: Loss = 0.497166
Epoch 2.67: Loss = 0.551885
Epoch 2.68: Loss = 0.352022
Epoch 2.69: Loss = 0.433855
Epoch 2.70: Loss = 0.580817
Epoch 2.71: Loss = 0.438931
Epoch 2.72: Loss = 0.464075
Epoch 2.73: Loss = 0.499958
Epoch 2.74: Loss = 0.37017
Epoch 2.75: Loss = 0.593402
Epoch 2.76: Loss = 0.505398
Epoch 2.77: Loss = 0.43747
Epoch 2.78: Loss = 0.458645
Epoch 2.79: Loss = 0.532393
Epoch 2.80: Loss = 0.555109
Epoch 2.81: Loss = 0.44218
Epoch 2.82: Loss = 0.387101
Epoch 2.83: Loss = 0.572915
Epoch 2.84: Loss = 0.481574
Epoch 2.85: Loss = 0.630951
Epoch 2.86: Loss = 0.518806
Epoch 2.87: Loss = 0.368471
Epoch 2.88: Loss = 0.465901
Epoch 2.89: Loss = 0.544311
Epoch 2.90: Loss = 0.411643
Epoch 2.91: Loss = 0.50949
Epoch 2.92: Loss = 0.525845
Epoch 2.93: Loss = 0.554281
Epoch 2.94: Loss = 0.384959
Epoch 2.95: Loss = 0.491712
Epoch 2.96: Loss = 0.515488
Epoch 2.97: Loss = 0.346901
Epoch 2.98: Loss = 0.460538
Epoch 2.99: Loss = 0.549491
Epoch 2.100: Loss = 0.609532
Epoch 2.101: Loss = 0.535512
Epoch 2.102: Loss = 0.454212
Epoch 2.103: Loss = 0.429339
Epoch 2.104: Loss = 0.378146
Epoch 2.105: Loss = 0.526891
Epoch 2.106: Loss = 0.530299
Epoch 2.107: Loss = 0.398901
Epoch 2.108: Loss = 0.487407
Epoch 2.109: Loss = 0.429054
Epoch 2.110: Loss = 0.477764
Epoch 2.111: Loss = 0.363669
Epoch 2.112: Loss = 0.369243
Epoch 2.113: Loss = 0.414626
Epoch 2.114: Loss = 0.361842
Epoch 2.115: Loss = 0.383008
Epoch 2.116: Loss = 0.423127
Epoch 2.117: Loss = 0.28911
Epoch 2.118: Loss = 0.255939
Epoch 2.119: Loss = 0.314964
Epoch 2.120: Loss = 0.336892
TRAIN LOSS = 0.501924
TRAIN ACC = -436.887 % (51164/60000)
Loss = 0.446184
Loss = 0.516499
Loss = 0.588327
Loss = 0.567401
Loss = 0.612551
Loss = 0.470571
Loss = 0.450185
Loss = 0.636332
Loss = 0.579037
Loss = 0.538393
Loss = 0.215889
Loss = 0.352095
Loss = 0.274383
Loss = 0.392472
Loss = 0.296822
Loss = 0.317902
Loss = 0.266563
Loss = 0.118593
Loss = 0.28732
Loss = 0.546397
TEST LOSS = 0.423696
TEST ACC = 511.64 % (8753/10000)
Reducing learning rate to 0.1
Epoch 3.1: Loss = 0.413313
Epoch 3.2: Loss = 0.522295
Epoch 3.3: Loss = 0.508259
Epoch 3.4: Loss = 0.364735
Epoch 3.5: Loss = 0.387111
Epoch 3.6: Loss = 0.396011
Epoch 3.7: Loss = 0.403278
Epoch 3.8: Loss = 0.41617
Epoch 3.9: Loss = 0.393645
Epoch 3.10: Loss = 0.418873
Epoch 3.11: Loss = 0.422393
Epoch 3.12: Loss = 0.41531
Epoch 3.13: Loss = 0.335809
Epoch 3.14: Loss = 0.383684
Epoch 3.15: Loss = 0.477581
Epoch 3.16: Loss = 0.483752
Epoch 3.17: Loss = 0.489949
Epoch 3.18: Loss = 0.581867
Epoch 3.19: Loss = 0.422125
Epoch 3.20: Loss = 0.386533
Epoch 3.21: Loss = 0.349968
Epoch 3.22: Loss = 0.34057
Epoch 3.23: Loss = 0.349102
Epoch 3.24: Loss = 0.540096
Epoch 3.25: Loss = 0.468399
Epoch 3.26: Loss = 0.536152
Epoch 3.27: Loss = 0.507951
Epoch 3.28: Loss = 0.484311
Epoch 3.29: Loss = 0.553718
Epoch 3.30: Loss = 0.582648
Epoch 3.31: Loss = 0.391211
Epoch 3.32: Loss = 0.514224
Epoch 3.33: Loss = 0.407936
Epoch 3.34: Loss = 0.467595
Epoch 3.35: Loss = 0.463677
Epoch 3.36: Loss = 0.525237
Epoch 3.37: Loss = 0.355251
Epoch 3.38: Loss = 0.333996
Epoch 3.39: Loss = 0.412426
Epoch 3.40: Loss = 0.376937
Epoch 3.41: Loss = 0.447993
Epoch 3.42: Loss = 0.545036
Epoch 3.43: Loss = 0.378373
Epoch 3.44: Loss = 0.33472
Epoch 3.45: Loss = 0.454122
Epoch 3.46: Loss = 0.456016
Epoch 3.47: Loss = 0.389714
Epoch 3.48: Loss = 0.43383
Epoch 3.49: Loss = 0.434855
Epoch 3.50: Loss = 0.497665
Epoch 3.51: Loss = 0.360866
Epoch 3.52: Loss = 0.345279
Epoch 3.53: Loss = 0.422561
Epoch 3.54: Loss = 0.499032
Epoch 3.55: Loss = 0.43413
Epoch 3.56: Loss = 0.399526
Epoch 3.57: Loss = 0.370777
Epoch 3.58: Loss = 0.426288
Epoch 3.59: Loss = 0.4696
Epoch 3.60: Loss = 0.510825
Epoch 3.61: Loss = 0.46967
Epoch 3.62: Loss = 0.512079
Epoch 3.63: Loss = 0.56725
Epoch 3.64: Loss = 0.524241
Epoch 3.65: Loss = 0.567813
Epoch 3.66: Loss = 0.424767
Epoch 3.67: Loss = 0.472521
Epoch 3.68: Loss = 0.276778
Epoch 3.69: Loss = 0.359445
Epoch 3.70: Loss = 0.527902
Epoch 3.71: Loss = 0.377726
Epoch 3.72: Loss = 0.381767
Epoch 3.73: Loss = 0.436397
Epoch 3.74: Loss = 0.317753
Epoch 3.75: Loss = 0.577095
Epoch 3.76: Loss = 0.445232
Epoch 3.77: Loss = 0.37106
Epoch 3.78: Loss = 0.411919
Epoch 3.79: Loss = 0.492391
Epoch 3.80: Loss = 0.48843
Epoch 3.81: Loss = 0.371607
Epoch 3.82: Loss = 0.331877
Epoch 3.83: Loss = 0.501703
Epoch 3.84: Loss = 0.422718
Epoch 3.85: Loss = 0.580622
Epoch 3.86: Loss = 0.484688
Epoch 3.87: Loss = 0.303094
Epoch 3.88: Loss = 0.411441
Epoch 3.89: Loss = 0.484806
Epoch 3.90: Loss = 0.353679
Epoch 3.91: Loss = 0.461687
Epoch 3.92: Loss = 0.486237
Epoch 3.93: Loss = 0.510813
Epoch 3.94: Loss = 0.329355
Epoch 3.95: Loss = 0.437598
Epoch 3.96: Loss = 0.480176
Epoch 3.97: Loss = 0.310063
Epoch 3.98: Loss = 0.40147
Epoch 3.99: Loss = 0.496136
Epoch 3.100: Loss = 0.563425
Epoch 3.101: Loss = 0.499871
Epoch 3.102: Loss = 0.394759
Epoch 3.103: Loss = 0.383026
Epoch 3.104: Loss = 0.336061
Epoch 3.105: Loss = 0.492666
Epoch 3.106: Loss = 0.498143
Epoch 3.107: Loss = 0.338088
Epoch 3.108: Loss = 0.449547
Epoch 3.109: Loss = 0.376728
Epoch 3.110: Loss = 0.438165
Epoch 3.111: Loss = 0.318523
Epoch 3.112: Loss = 0.34188
Epoch 3.113: Loss = 0.369306
Epoch 3.114: Loss = 0.316389
Epoch 3.115: Loss = 0.319902
Epoch 3.116: Loss = 0.383678
Epoch 3.117: Loss = 0.244522
Epoch 3.118: Loss = 0.214556
Epoch 3.119: Loss = 0.282958
Epoch 3.120: Loss = 0.310188
TRAIN LOSS = 0.426914
TRAIN ACC = 109.222 % (52417/60000)
Loss = 0.397418
Loss = 0.484676
Loss = 0.539743
Loss = 0.529255
Loss = 0.574414
Loss = 0.421488
Loss = 0.398914
Loss = 0.61162
Loss = 0.533329
Loss = 0.50442
Loss = 0.189278
Loss = 0.319779
Loss = 0.259159
Loss = 0.358012
Loss = 0.243039
Loss = 0.28875
Loss = 0.223947
Loss = 0.0838936
Loss = 0.240503
Loss = 0.508505
TEST LOSS = 0.385507
TEST ACC = 524.17 % (8875/10000)
Reducing learning rate to 0.1
Epoch 4.1: Loss = 0.377398
Epoch 4.2: Loss = 0.473431
Epoch 4.3: Loss = 0.471849
Epoch 4.4: Loss = 0.325584
Epoch 4.5: Loss = 0.348733
Epoch 4.6: Loss = 0.363526
Epoch 4.7: Loss = 0.352592
Epoch 4.8: Loss = 0.380795
Epoch 4.9: Loss = 0.361225
Epoch 4.10: Loss = 0.384947
Epoch 4.11: Loss = 0.40148
Epoch 4.12: Loss = 0.38623
Epoch 4.13: Loss = 0.305924
Epoch 4.14: Loss = 0.347
Epoch 4.15: Loss = 0.426377
Epoch 4.16: Loss = 0.446532
Epoch 4.17: Loss = 0.470462
Epoch 4.18: Loss = 0.559395
Epoch 4.19: Loss = 0.393353
Epoch 4.20: Loss = 0.346949
Epoch 4.21: Loss = 0.314707
Epoch 4.22: Loss = 0.307495
Epoch 4.23: Loss = 0.310943
Epoch 4.24: Loss = 0.515473
Epoch 4.25: Loss = 0.440923
Epoch 4.26: Loss = 0.521165
Epoch 4.27: Loss = 0.486068
Epoch 4.28: Loss = 0.456795
Epoch 4.29: Loss = 0.531275
Epoch 4.30: Loss = 0.540952
Epoch 4.31: Loss = 0.366026
Epoch 4.32: Loss = 0.468529
Epoch 4.33: Loss = 0.365786
Epoch 4.34: Loss = 0.433815
Epoch 4.35: Loss = 0.419121
Epoch 4.36: Loss = 0.49529
Epoch 4.37: Loss = 0.312056
Epoch 4.38: Loss = 0.310219
Epoch 4.39: Loss = 0.36824
Epoch 4.40: Loss = 0.352889
Epoch 4.41: Loss = 0.41251
Epoch 4.42: Loss = 0.539172
Epoch 4.43: Loss = 0.343164
Epoch 4.44: Loss = 0.311947
Epoch 4.45: Loss = 0.414354
Epoch 4.46: Loss = 0.426136
Epoch 4.47: Loss = 0.36467
Epoch 4.48: Loss = 0.410133
Epoch 4.49: Loss = 0.400329
Epoch 4.50: Loss = 0.47514
Epoch 4.51: Loss = 0.337327
Epoch 4.52: Loss = 0.322481
Epoch 4.53: Loss = 0.387522
Epoch 4.54: Loss = 0.477249
Epoch 4.55: Loss = 0.413448
Epoch 4.56: Loss = 0.378947
Epoch 4.57: Loss = 0.352053
Epoch 4.58: Loss = 0.405358
Epoch 4.59: Loss = 0.446765
Epoch 4.60: Loss = 0.473761
Epoch 4.61: Loss = 0.428403
Epoch 4.62: Loss = 0.488711
Epoch 4.63: Loss = 0.550509
Epoch 4.64: Loss = 0.494126
Epoch 4.65: Loss = 0.557254
Epoch 4.66: Loss = 0.402417
Epoch 4.67: Loss = 0.428333
Epoch 4.68: Loss = 0.248425
Epoch 4.69: Loss = 0.332898
Epoch 4.70: Loss = 0.50262
Epoch 4.71: Loss = 0.347866
Epoch 4.72: Loss = 0.339889
Epoch 4.73: Loss = 0.408875
Epoch 4.74: Loss = 0.303357
Epoch 4.75: Loss = 0.589046
Epoch 4.76: Loss = 0.425649
Epoch 4.77: Loss = 0.337964
Epoch 4.78: Loss = 0.399649
Epoch 4.79: Loss = 0.465503
Epoch 4.80: Loss = 0.447394
Epoch 4.81: Loss = 0.33663
Epoch 4.82: Loss = 0.309926
Epoch 4.83: Loss = 0.464471
Epoch 4.84: Loss = 0.392033
Epoch 4.85: Loss = 0.568054
Epoch 4.86: Loss = 0.467998
Epoch 4.87: Loss = 0.280746
Epoch 4.88: Loss = 0.383412
Epoch 4.89: Loss = 0.451864
Epoch 4.90: Loss = 0.331493
Epoch 4.91: Loss = 0.450324
Epoch 4.92: Loss = 0.47194
Epoch 4.93: Loss = 0.497957
Epoch 4.94: Loss = 0.304668
Epoch 4.95: Loss = 0.41579
Epoch 4.96: Loss = 0.464183
Epoch 4.97: Loss = 0.29735
Epoch 4.98: Loss = 0.367915
Epoch 4.99: Loss = 0.467171
Epoch 4.100: Loss = 0.556339
Epoch 4.101: Loss = 0.49764
Epoch 4.102: Loss = 0.373125
Epoch 4.103: Loss = 0.365651
Epoch 4.104: Loss = 0.32189
Epoch 4.105: Loss = 0.479176
Epoch 4.106: Loss = 0.491858
Epoch 4.107: Loss = 0.314638
Epoch 4.108: Loss = 0.431318
Epoch 4.109: Loss = 0.349216
Epoch 4.110: Loss = 0.419523
Epoch 4.111: Loss = 0.3078
Epoch 4.112: Loss = 0.323047
Epoch 4.113: Loss = 0.343237
Epoch 4.114: Loss = 0.306929
Epoch 4.115: Loss = 0.294634
Epoch 4.116: Loss = 0.36491
Epoch 4.117: Loss = 0.220222
Epoch 4.118: Loss = 0.198321
Epoch 4.119: Loss = 0.275126
Epoch 4.120: Loss = 0.294972
TRAIN LOSS = 0.40072
TRAIN ACC = 218.443 % (52914/60000)
Loss = 0.372754
Loss = 0.464475
Loss = 0.51819
Loss = 0.511112
Loss = 0.549573
Loss = 0.39563
Loss = 0.36904
Loss = 0.592206
Loss = 0.508811
Loss = 0.478946
Loss = 0.171744
Loss = 0.300307
Loss = 0.255725
Loss = 0.333827
Loss = 0.217408
Loss = 0.278519
Loss = 0.198998
Loss = 0.0661007
Loss = 0.230206
Loss = 0.487805
TEST LOSS = 0.365069
TEST ACC = 529.14 % (8953/10000)
Reducing learning rate to 0.1
Epoch 5.1: Loss = 0.362884
Epoch 5.2: Loss = 0.454774
Epoch 5.3: Loss = 0.4608
Epoch 5.4: Loss = 0.305235
Epoch 5.5: Loss = 0.330331
Epoch 5.6: Loss = 0.3381
Epoch 5.7: Loss = 0.331291
Epoch 5.8: Loss = 0.364923
Epoch 5.9: Loss = 0.344261
Epoch 5.10: Loss = 0.371534
Epoch 5.11: Loss = 0.380884
Epoch 5.12: Loss = 0.360159
Epoch 5.13: Loss = 0.290139
Epoch 5.14: Loss = 0.335093
Epoch 5.15: Loss = 0.396837
Epoch 5.16: Loss = 0.431797
Epoch 5.17: Loss = 0.450206
Epoch 5.18: Loss = 0.555007
Epoch 5.19: Loss = 0.388131
Epoch 5.20: Loss = 0.329986
Epoch 5.21: Loss = 0.300634
Epoch 5.22: Loss = 0.287022
Epoch 5.23: Loss = 0.295805
Epoch 5.24: Loss = 0.502882
Epoch 5.25: Loss = 0.440386
Epoch 5.26: Loss = 0.508296
Epoch 5.27: Loss = 0.465789
Epoch 5.28: Loss = 0.440152
Epoch 5.29: Loss = 0.520602
Epoch 5.30: Loss = 0.522177
Epoch 5.31: Loss = 0.352672
Epoch 5.32: Loss = 0.441602
Epoch 5.33: Loss = 0.347058
Epoch 5.34: Loss = 0.412586
Epoch 5.35: Loss = 0.407999
Epoch 5.36: Loss = 0.478048
Epoch 5.37: Loss = 0.289337
Epoch 5.38: Loss = 0.295886
Epoch 5.39: Loss = 0.352334
Epoch 5.40: Loss = 0.341262
Epoch 5.41: Loss = 0.402018
Epoch 5.42: Loss = 0.528464
Epoch 5.43: Loss = 0.317043
Epoch 5.44: Loss = 0.298464
Epoch 5.45: Loss = 0.396487
Epoch 5.46: Loss = 0.412918
Epoch 5.47: Loss = 0.357857
Epoch 5.48: Loss = 0.403577
Epoch 5.49: Loss = 0.386405
Epoch 5.50: Loss = 0.461355
Epoch 5.51: Loss = 0.323266
Epoch 5.52: Loss = 0.310498
Epoch 5.53: Loss = 0.382034
Epoch 5.54: Loss = 0.47172
Epoch 5.55: Loss = 0.393121
Epoch 5.56: Loss = 0.375702
Epoch 5.57: Loss = 0.344575
Epoch 5.58: Loss = 0.39022
Epoch 5.59: Loss = 0.429521
Epoch 5.60: Loss = 0.454524
Epoch 5.61: Loss = 0.401031
Epoch 5.62: Loss = 0.472958
Epoch 5.63: Loss = 0.535107
Epoch 5.64: Loss = 0.478672
Epoch 5.65: Loss = 0.541742
Epoch 5.66: Loss = 0.391419
Epoch 5.67: Loss = 0.413736
Epoch 5.68: Loss = 0.234734
Epoch 5.69: Loss = 0.322877
Epoch 5.70: Loss = 0.484281
Epoch 5.71: Loss = 0.331945
Epoch 5.72: Loss = 0.317263
Epoch 5.73: Loss = 0.394683
Epoch 5.74: Loss = 0.291866
Epoch 5.75: Loss = 0.588484
Epoch 5.76: Loss = 0.411214
Epoch 5.77: Loss = 0.314268
Epoch 5.78: Loss = 0.382785
Epoch 5.79: Loss = 0.452383
Epoch 5.80: Loss = 0.423218
Epoch 5.81: Loss = 0.321767
Epoch 5.82: Loss = 0.298917
Epoch 5.83: Loss = 0.445808
Epoch 5.84: Loss = 0.374221
Epoch 5.85: Loss = 0.545908
Epoch 5.86: Loss = 0.466887
Epoch 5.87: Loss = 0.263495
Epoch 5.88: Loss = 0.363631
Epoch 5.89: Loss = 0.440011
Epoch 5.90: Loss = 0.320399
Epoch 5.91: Loss = 0.439972
Epoch 5.92: Loss = 0.45893
Epoch 5.93: Loss = 0.488215
Epoch 5.94: Loss = 0.286806
Epoch 5.95: Loss = 0.395116
Epoch 5.96: Loss = 0.45735
Epoch 5.97: Loss = 0.287994
Epoch 5.98: Loss = 0.34924
Epoch 5.99: Loss = 0.445582
Epoch 5.100: Loss = 0.541036
Epoch 5.101: Loss = 0.492528
Epoch 5.102: Loss = 0.359616
Epoch 5.103: Loss = 0.350867
Epoch 5.104: Loss = 0.315012
Epoch 5.105: Loss = 0.470732
Epoch 5.106: Loss = 0.487968
Epoch 5.107: Loss = 0.30462
Epoch 5.108: Loss = 0.418055
Epoch 5.109: Loss = 0.338026
Epoch 5.110: Loss = 0.409104
Epoch 5.111: Loss = 0.305608
Epoch 5.112: Loss = 0.315203
Epoch 5.113: Loss = 0.336588
Epoch 5.114: Loss = 0.293571
Epoch 5.115: Loss = 0.278677
Epoch 5.116: Loss = 0.352958
Epoch 5.117: Loss = 0.204739
Epoch 5.118: Loss = 0.186185
Epoch 5.119: Loss = 0.272106
Epoch 5.120: Loss = 0.296963
TRAIN LOSS = 0.386581
TRAIN ACC = -218.443 % (53334/60000)
Loss = 0.354783
Loss = 0.451226
Loss = 0.502826
Loss = 0.499436
Loss = 0.541707
Loss = 0.383141
Loss = 0.353788
Loss = 0.589244
Loss = 0.497426
Loss = 0.465791
Loss = 0.156712
Loss = 0.295233
Loss = 0.260884
Loss = 0.316999
Loss = 0.196709
Loss = 0.268192
Loss = 0.17964
Loss = 0.0585014
Loss = 0.224527
Loss = 0.486278
TEST LOSS = 0.354152
TEST ACC = 533.34 % (8996/10000)
Reducing learning rate to 0.1
Epoch 6.1: Loss = 0.348627
Epoch 6.2: Loss = 0.449228
Epoch 6.3: Loss = 0.456861
Epoch 6.4: Loss = 0.294095
Epoch 6.5: Loss = 0.326779
Epoch 6.6: Loss = 0.320762
Epoch 6.7: Loss = 0.313867
Epoch 6.8: Loss = 0.364358
Epoch 6.9: Loss = 0.339368
Epoch 6.10: Loss = 0.362448
Epoch 6.11: Loss = 0.381054
Epoch 6.12: Loss = 0.355411
Epoch 6.13: Loss = 0.282557
Epoch 6.14: Loss = 0.322044
Epoch 6.15: Loss = 0.385398
Epoch 6.16: Loss = 0.42316
Epoch 6.17: Loss = 0.451665
Epoch 6.18: Loss = 0.553477
Epoch 6.19: Loss = 0.381641
Epoch 6.20: Loss = 0.311077
Epoch 6.21: Loss = 0.291177
Epoch 6.22: Loss = 0.275636
Epoch 6.23: Loss = 0.294534
Epoch 6.24: Loss = 0.503635
Epoch 6.25: Loss = 0.438702
Epoch 6.26: Loss = 0.510864
Epoch 6.27: Loss = 0.461785
Epoch 6.28: Loss = 0.424223
Epoch 6.29: Loss = 0.509179
Epoch 6.30: Loss = 0.507986
Epoch 6.31: Loss = 0.335785
Epoch 6.32: Loss = 0.43045
Epoch 6.33: Loss = 0.33467
Epoch 6.34: Loss = 0.397094
Epoch 6.35: Loss = 0.391233
Epoch 6.36: Loss = 0.463646
Epoch 6.37: Loss = 0.277471
Epoch 6.38: Loss = 0.291302
Epoch 6.39: Loss = 0.333819
Epoch 6.40: Loss = 0.335395
Epoch 6.41: Loss = 0.382705
Epoch 6.42: Loss = 0.541014
Epoch 6.43: Loss = 0.306653
Epoch 6.44: Loss = 0.281771
Epoch 6.45: Loss = 0.373545
Epoch 6.46: Loss = 0.407853
Epoch 6.47: Loss = 0.353635
Epoch 6.48: Loss = 0.392499
Epoch 6.49: Loss = 0.367009
Epoch 6.50: Loss = 0.449206
Epoch 6.51: Loss = 0.308182
Epoch 6.52: Loss = 0.305032
Epoch 6.53: Loss = 0.368242
Epoch 6.54: Loss = 0.459903
Epoch 6.55: Loss = 0.393798
Epoch 6.56: Loss = 0.363943
Epoch 6.57: Loss = 0.337598
Epoch 6.58: Loss = 0.382474
Epoch 6.59: Loss = 0.415855
Epoch 6.60: Loss = 0.444653
Epoch 6.61: Loss = 0.384124
Epoch 6.62: Loss = 0.472287
Epoch 6.63: Loss = 0.539393
Epoch 6.64: Loss = 0.467457
Epoch 6.65: Loss = 0.538509
Epoch 6.66: Loss = 0.377762
Epoch 6.67: Loss = 0.396147
Epoch 6.68: Loss = 0.223213
Epoch 6.69: Loss = 0.313104
Epoch 6.70: Loss = 0.468499
Epoch 6.71: Loss = 0.32181
Epoch 6.72: Loss = 0.294248
Epoch 6.73: Loss = 0.384038
Epoch 6.74: Loss = 0.289732
Epoch 6.75: Loss = 0.593648
Epoch 6.76: Loss = 0.405312
Epoch 6.77: Loss = 0.307886
Epoch 6.78: Loss = 0.386789
Epoch 6.79: Loss = 0.453683
Epoch 6.80: Loss = 0.413992
Epoch 6.81: Loss = 0.310338
Epoch 6.82: Loss = 0.2876
Epoch 6.83: Loss = 0.4436
Epoch 6.84: Loss = 0.361699
Epoch 6.85: Loss = 0.536334
Epoch 6.86: Loss = 0.45505
Epoch 6.87: Loss = 0.257459
Epoch 6.88: Loss = 0.353558
Epoch 6.89: Loss = 0.422123
Epoch 6.90: Loss = 0.318083
Epoch 6.91: Loss = 0.438188
Epoch 6.92: Loss = 0.443602
Epoch 6.93: Loss = 0.480023
Epoch 6.94: Loss = 0.278929
Epoch 6.95: Loss = 0.384569
Epoch 6.96: Loss = 0.446953
Epoch 6.97: Loss = 0.280858
Epoch 6.98: Loss = 0.337545
Epoch 6.99: Loss = 0.430775
Epoch 6.100: Loss = 0.534631
Epoch 6.101: Loss = 0.491216
Epoch 6.102: Loss = 0.351331
Epoch 6.103: Loss = 0.337683
Epoch 6.104: Loss = 0.301444
Epoch 6.105: Loss = 0.455013
Epoch 6.106: Loss = 0.488179
Epoch 6.107: Loss = 0.301361
Epoch 6.108: Loss = 0.41305
Epoch 6.109: Loss = 0.332548
Epoch 6.110: Loss = 0.406968
Epoch 6.111: Loss = 0.299711
Epoch 6.112: Loss = 0.302136
Epoch 6.113: Loss = 0.328829
Epoch 6.114: Loss = 0.28233
Epoch 6.115: Loss = 0.264147
Epoch 6.116: Loss = 0.349374
Epoch 6.117: Loss = 0.191947
Epoch 6.118: Loss = 0.180546
Epoch 6.119: Loss = 0.270194
Epoch 6.120: Loss = 0.287626
TRAIN LOSS = 0.37781
TRAIN ACC = -436.887 % (53588/60000)
Loss = 0.343229
Loss = 0.444478
Loss = 0.490358
Loss = 0.498875
Loss = 0.53046
Loss = 0.372745
Loss = 0.335521
Loss = 0.585882
Loss = 0.479009
Loss = 0.459373
Loss = 0.154373
Loss = 0.288292
Loss = 0.263215
Loss = 0.317645
Loss = 0.190252
Loss = 0.267218
Loss = 0.179114
Loss = 0.0539598
Loss = 0.215459
Loss = 0.471822
TEST LOSS = 0.347064
TEST ACC = 535.88 % (9027/10000)
Reducing learning rate to 0.1
Epoch 7.1: Loss = 0.34721
Epoch 7.2: Loss = 0.434987
Epoch 7.3: Loss = 0.450211
Epoch 7.4: Loss = 0.292859
Epoch 7.5: Loss = 0.313831
Epoch 7.6: Loss = 0.314712
Epoch 7.7: Loss = 0.307617
Epoch 7.8: Loss = 0.347326
Epoch 7.9: Loss = 0.331668
Epoch 7.10: Loss = 0.36036
Epoch 7.11: Loss = 0.372244
Epoch 7.12: Loss = 0.354423
Epoch 7.13: Loss = 0.277572
Epoch 7.14: Loss = 0.305307
Epoch 7.15: Loss = 0.372572
Epoch 7.16: Loss = 0.412638
Epoch 7.17: Loss = 0.437881
Epoch 7.18: Loss = 0.55365
Epoch 7.19: Loss = 0.379282
Epoch 7.20: Loss = 0.300656
Epoch 7.21: Loss = 0.289714
Epoch 7.22: Loss = 0.273631
Epoch 7.23: Loss = 0.285661
Epoch 7.24: Loss = 0.496143
Epoch 7.25: Loss = 0.426217
Epoch 7.26: Loss = 0.497751
Epoch 7.27: Loss = 0.457685
Epoch 7.28: Loss = 0.414939
Epoch 7.29: Loss = 0.495276
Epoch 7.30: Loss = 0.487813
Epoch 7.31: Loss = 0.329596
Epoch 7.32: Loss = 0.423089
Epoch 7.33: Loss = 0.324871
Epoch 7.34: Loss = 0.387063
Epoch 7.35: Loss = 0.383806
Epoch 7.36: Loss = 0.453526
Epoch 7.37: Loss = 0.274599
Epoch 7.38: Loss = 0.286921
Epoch 7.39: Loss = 0.308899
Epoch 7.40: Loss = 0.323607
Epoch 7.41: Loss = 0.368146
Epoch 7.42: Loss = 0.527635
Epoch 7.43: Loss = 0.294731
Epoch 7.44: Loss = 0.286867
Epoch 7.45: Loss = 0.373753
Epoch 7.46: Loss = 0.385275
Epoch 7.47: Loss = 0.3478
Epoch 7.48: Loss = 0.387783
Epoch 7.49: Loss = 0.355518
Epoch 7.50: Loss = 0.443208
Epoch 7.51: Loss = 0.298819
Epoch 7.52: Loss = 0.295755
Epoch 7.53: Loss = 0.35051
Epoch 7.54: Loss = 0.449702
Epoch 7.55: Loss = 0.384151
Epoch 7.56: Loss = 0.361316
Epoch 7.57: Loss = 0.340974
Epoch 7.58: Loss = 0.365652
Epoch 7.59: Loss = 0.399375
Epoch 7.60: Loss = 0.429601
Epoch 7.61: Loss = 0.36328
Epoch 7.62: Loss = 0.464589
Epoch 7.63: Loss = 0.533546
Epoch 7.64: Loss = 0.450741
Epoch 7.65: Loss = 0.517377
Epoch 7.66: Loss = 0.359446
Epoch 7.67: Loss = 0.378343
Epoch 7.68: Loss = 0.220807
Epoch 7.69: Loss = 0.308667
Epoch 7.70: Loss = 0.449553
Epoch 7.71: Loss = 0.312215
Epoch 7.72: Loss = 0.281649
Epoch 7.73: Loss = 0.377247
Epoch 7.74: Loss = 0.285605
Epoch 7.75: Loss = 0.587993
Epoch 7.76: Loss = 0.394651
Epoch 7.77: Loss = 0.295518
Epoch 7.78: Loss = 0.376939
Epoch 7.79: Loss = 0.453134
Epoch 7.80: Loss = 0.399647
Epoch 7.81: Loss = 0.299841
Epoch 7.82: Loss = 0.288049
Epoch 7.83: Loss = 0.427217
Epoch 7.84: Loss = 0.348928
Epoch 7.85: Loss = 0.527937
Epoch 7.86: Loss = 0.457818
Epoch 7.87: Loss = 0.253073
Epoch 7.88: Loss = 0.350133
Epoch 7.89: Loss = 0.420004
Epoch 7.90: Loss = 0.31464
Epoch 7.91: Loss = 0.43173
Epoch 7.92: Loss = 0.436518
Epoch 7.93: Loss = 0.486968
Epoch 7.94: Loss = 0.262017
Epoch 7.95: Loss = 0.380138
Epoch 7.96: Loss = 0.448541
Epoch 7.97: Loss = 0.273428
Epoch 7.98: Loss = 0.331985
Epoch 7.99: Loss = 0.425838
Epoch 7.100: Loss = 0.522254
Epoch 7.101: Loss = 0.492161
Epoch 7.102: Loss = 0.351631
Epoch 7.103: Loss = 0.329769
Epoch 7.104: Loss = 0.295207
Epoch 7.105: Loss = 0.446224
Epoch 7.106: Loss = 0.48699
Epoch 7.107: Loss = 0.286866
Epoch 7.108: Loss = 0.417077
Epoch 7.109: Loss = 0.325694
Epoch 7.110: Loss = 0.397658
Epoch 7.111: Loss = 0.297835
Epoch 7.112: Loss = 0.301246
Epoch 7.113: Loss = 0.329596
Epoch 7.114: Loss = 0.268398
Epoch 7.115: Loss = 0.251301
Epoch 7.116: Loss = 0.347411
Epoch 7.117: Loss = 0.185617
Epoch 7.118: Loss = 0.167651
Epoch 7.119: Loss = 0.26499
Epoch 7.120: Loss = 0.29093
TRAIN LOSS = 0.369722
TRAIN ACC = 218.443 % (53770/60000)
Loss = 0.33028
Loss = 0.429639
Loss = 0.490398
Loss = 0.488096
Loss = 0.531121
Loss = 0.36989
Loss = 0.321748
Loss = 0.583136
Loss = 0.473147
Loss = 0.449647
Loss = 0.15104
Loss = 0.282593
Loss = 0.250849
Loss = 0.3086
Loss = 0.1742
Loss = 0.276233
Loss = 0.16669
Loss = 0.0484065
Loss = 0.211537
Loss = 0.463182
TEST LOSS = 0.340022
TEST ACC = 537.7 % (9062/10000)
Reducing learning rate to 0.1
Epoch 8.1: Loss = 0.337785
Epoch 8.2: Loss = 0.437358
Epoch 8.3: Loss = 0.459407
Epoch 8.4: Loss = 0.280616
Epoch 8.5: Loss = 0.31357
Epoch 8.6: Loss = 0.312639
Epoch 8.7: Loss = 0.304219
Epoch 8.8: Loss = 0.342463
Epoch 8.9: Loss = 0.331266
Epoch 8.10: Loss = 0.352258
Epoch 8.11: Loss = 0.363103
Epoch 8.12: Loss = 0.347437
Epoch 8.13: Loss = 0.274524
Epoch 8.14: Loss = 0.309642
Epoch 8.15: Loss = 0.361657
Epoch 8.16: Loss = 0.396559
Epoch 8.17: Loss = 0.440373
Epoch 8.18: Loss = 0.549252
Epoch 8.19: Loss = 0.377582
Epoch 8.20: Loss = 0.287653
Epoch 8.21: Loss = 0.287917
Epoch 8.22: Loss = 0.268882
Epoch 8.23: Loss = 0.277803
Epoch 8.24: Loss = 0.501634
Epoch 8.25: Loss = 0.427683
Epoch 8.26: Loss = 0.49058
Epoch 8.27: Loss = 0.455566
Epoch 8.28: Loss = 0.404588
Epoch 8.29: Loss = 0.486551
Epoch 8.30: Loss = 0.472843
Epoch 8.31: Loss = 0.317125
Epoch 8.32: Loss = 0.415271
Epoch 8.33: Loss = 0.317629
Epoch 8.34: Loss = 0.380876
Epoch 8.35: Loss = 0.370769
Epoch 8.36: Loss = 0.442011
Epoch 8.37: Loss = 0.257851
Epoch 8.38: Loss = 0.285792
Epoch 8.39: Loss = 0.301905
Epoch 8.40: Loss = 0.322797
Epoch 8.41: Loss = 0.361342
Epoch 8.42: Loss = 0.531455
Epoch 8.43: Loss = 0.287417
Epoch 8.44: Loss = 0.274447
Epoch 8.45: Loss = 0.363031
Epoch 8.46: Loss = 0.385259
Epoch 8.47: Loss = 0.337946
Epoch 8.48: Loss = 0.383565
Epoch 8.49: Loss = 0.348733
Epoch 8.50: Loss = 0.43711
Epoch 8.51: Loss = 0.293993
Epoch 8.52: Loss = 0.284669
Epoch 8.53: Loss = 0.345331
Epoch 8.54: Loss = 0.444686
Epoch 8.55: Loss = 0.383472
Epoch 8.56: Loss = 0.353949
Epoch 8.57: Loss = 0.337643
Epoch 8.58: Loss = 0.356082
Epoch 8.59: Loss = 0.406285
Epoch 8.60: Loss = 0.433774
Epoch 8.61: Loss = 0.354899
Epoch 8.62: Loss = 0.450841
Epoch 8.63: Loss = 0.52932
Epoch 8.64: Loss = 0.445926
Epoch 8.65: Loss = 0.524191
Epoch 8.66: Loss = 0.360317
Epoch 8.67: Loss = 0.37231
Epoch 8.68: Loss = 0.206877
Epoch 8.69: Loss = 0.30321
Epoch 8.70: Loss = 0.455292
Epoch 8.71: Loss = 0.310896
Epoch 8.72: Loss = 0.273885
Epoch 8.73: Loss = 0.368323
Epoch 8.74: Loss = 0.279247
Epoch 8.75: Loss = 0.609648
Epoch 8.76: Loss = 0.403062
Epoch 8.77: Loss = 0.290445
Epoch 8.78: Loss = 0.370687
Epoch 8.79: Loss = 0.45425
Epoch 8.80: Loss = 0.392412
Epoch 8.81: Loss = 0.293956
Epoch 8.82: Loss = 0.287455
Epoch 8.83: Loss = 0.43019
Epoch 8.84: Loss = 0.348645
Epoch 8.85: Loss = 0.532563
Epoch 8.86: Loss = 0.453837
Epoch 8.87: Loss = 0.2428
Epoch 8.88: Loss = 0.351997
Epoch 8.89: Loss = 0.420536
Epoch 8.90: Loss = 0.313297
Epoch 8.91: Loss = 0.428659
Epoch 8.92: Loss = 0.437267
Epoch 8.93: Loss = 0.482485
Epoch 8.94: Loss = 0.253969
Epoch 8.95: Loss = 0.383303
Epoch 8.96: Loss = 0.446358
Epoch 8.97: Loss = 0.272375
Epoch 8.98: Loss = 0.320669
Epoch 8.99: Loss = 0.418198
Epoch 8.100: Loss = 0.531643
Epoch 8.101: Loss = 0.492334
Epoch 8.102: Loss = 0.339993
Epoch 8.103: Loss = 0.326092
Epoch 8.104: Loss = 0.295767
Epoch 8.105: Loss = 0.439092
Epoch 8.106: Loss = 0.482511
Epoch 8.107: Loss = 0.275628
Epoch 8.108: Loss = 0.41488
Epoch 8.109: Loss = 0.3156
Epoch 8.110: Loss = 0.39592
Epoch 8.111: Loss = 0.295587
Epoch 8.112: Loss = 0.298733
Epoch 8.113: Loss = 0.319688
Epoch 8.114: Loss = 0.264859
Epoch 8.115: Loss = 0.236132
Epoch 8.116: Loss = 0.336478
Epoch 8.117: Loss = 0.178613
Epoch 8.118: Loss = 0.164013
Epoch 8.119: Loss = 0.267942
Epoch 8.120: Loss = 0.290569
TRAIN LOSS = 0.365436
TRAIN ACC = 218.443 % (53898/60000)
Loss = 0.324836
Loss = 0.427293
Loss = 0.487261
Loss = 0.486712
Loss = 0.524278
Loss = 0.356946
Loss = 0.314374
Loss = 0.577353
Loss = 0.467068
Loss = 0.443292
Loss = 0.140739
Loss = 0.278001
Loss = 0.241283
Loss = 0.307141
Loss = 0.169392
Loss = 0.270486
Loss = 0.156913
Loss = 0.0469203
Loss = 0.208703
Loss = 0.466567
TEST LOSS = 0.334778
TEST ACC = 538.98 % (9084/10000)
Reducing learning rate to 0.1
Epoch 9.1: Loss = 0.329152
Epoch 9.2: Loss = 0.44216
Epoch 9.3: Loss = 0.456365
Epoch 9.4: Loss = 0.267627
Epoch 9.5: Loss = 0.301538
Epoch 9.6: Loss = 0.30258
Epoch 9.7: Loss = 0.294367
Epoch 9.8: Loss = 0.332584
Epoch 9.9: Loss = 0.33501
Epoch 9.10: Loss = 0.342592
Epoch 9.11: Loss = 0.358443
Epoch 9.12: Loss = 0.33891
Epoch 9.13: Loss = 0.269906
Epoch 9.14: Loss = 0.307423
Epoch 9.15: Loss = 0.355532
Epoch 9.16: Loss = 0.391942
Epoch 9.17: Loss = 0.436557
Epoch 9.18: Loss = 0.543474
Epoch 9.19: Loss = 0.37395
Epoch 9.20: Loss = 0.285066
Epoch 9.21: Loss = 0.287138
Epoch 9.22: Loss = 0.261668
Epoch 9.23: Loss = 0.27039
Epoch 9.24: Loss = 0.487679
Epoch 9.25: Loss = 0.430071
Epoch 9.26: Loss = 0.479519
Epoch 9.27: Loss = 0.43933
Epoch 9.28: Loss = 0.395957
Epoch 9.29: Loss = 0.48558
Epoch 9.30: Loss = 0.486662
Epoch 9.31: Loss = 0.315431
Epoch 9.32: Loss = 0.414245
Epoch 9.33: Loss = 0.310059
Epoch 9.34: Loss = 0.378114
Epoch 9.35: Loss = 0.362184
Epoch 9.36: Loss = 0.442951
Epoch 9.37: Loss = 0.254711
Epoch 9.38: Loss = 0.289039
Epoch 9.39: Loss = 0.297685
Epoch 9.40: Loss = 0.32153
Epoch 9.41: Loss = 0.350811
Epoch 9.42: Loss = 0.533046
Epoch 9.43: Loss = 0.289249
Epoch 9.44: Loss = 0.267072
Epoch 9.45: Loss = 0.342294
Epoch 9.46: Loss = 0.383814
Epoch 9.47: Loss = 0.341615
Epoch 9.48: Loss = 0.380906
Epoch 9.49: Loss = 0.340413
Epoch 9.50: Loss = 0.43607
Epoch 9.51: Loss = 0.284821
Epoch 9.52: Loss = 0.275759
Epoch 9.53: Loss = 0.339787
Epoch 9.54: Loss = 0.439948
Epoch 9.55: Loss = 0.37414
Epoch 9.56: Loss = 0.337103
Epoch 9.57: Loss = 0.330725
Epoch 9.58: Loss = 0.357569
Epoch 9.59: Loss = 0.402058
Epoch 9.60: Loss = 0.42684
Epoch 9.61: Loss = 0.342077
Epoch 9.62: Loss = 0.445037
Epoch 9.63: Loss = 0.52364
Epoch 9.64: Loss = 0.435624
Epoch 9.65: Loss = 0.530576
Epoch 9.66: Loss = 0.359845
Epoch 9.67: Loss = 0.377109
Epoch 9.68: Loss = 0.202854
Epoch 9.69: Loss = 0.296896
Epoch 9.70: Loss = 0.438679
Epoch 9.71: Loss = 0.311367
Epoch 9.72: Loss = 0.265241
Epoch 9.73: Loss = 0.359947
Epoch 9.74: Loss = 0.273879
Epoch 9.75: Loss = 0.610437
Epoch 9.76: Loss = 0.402827
Epoch 9.77: Loss = 0.281793
Epoch 9.78: Loss = 0.367881
Epoch 9.79: Loss = 0.456607
Epoch 9.80: Loss = 0.379933
Epoch 9.81: Loss = 0.282728
Epoch 9.82: Loss = 0.278487
Epoch 9.83: Loss = 0.421944
Epoch 9.84: Loss = 0.33749
Epoch 9.85: Loss = 0.52242
Epoch 9.86: Loss = 0.453286
Epoch 9.87: Loss = 0.23889
Epoch 9.88: Loss = 0.340118
Epoch 9.89: Loss = 0.419453
Epoch 9.90: Loss = 0.301019
Epoch 9.91: Loss = 0.419378
Epoch 9.92: Loss = 0.434083
Epoch 9.93: Loss = 0.486297
Epoch 9.94: Loss = 0.251201
Epoch 9.95: Loss = 0.374318
Epoch 9.96: Loss = 0.424258
Epoch 9.97: Loss = 0.277382
Epoch 9.98: Loss = 0.316291
Epoch 9.99: Loss = 0.40447
Epoch 9.100: Loss = 0.516159
Epoch 9.101: Loss = 0.491976
Epoch 9.102: Loss = 0.333621
Epoch 9.103: Loss = 0.320481
Epoch 9.104: Loss = 0.295345
Epoch 9.105: Loss = 0.4308
Epoch 9.106: Loss = 0.489684
Epoch 9.107: Loss = 0.267601
Epoch 9.108: Loss = 0.408811
Epoch 9.109: Loss = 0.314043
Epoch 9.110: Loss = 0.392484
Epoch 9.111: Loss = 0.29321
Epoch 9.112: Loss = 0.286717
Epoch 9.113: Loss = 0.312438
Epoch 9.114: Loss = 0.25693
Epoch 9.115: Loss = 0.23914
Epoch 9.116: Loss = 0.338029
Epoch 9.117: Loss = 0.1766
Epoch 9.118: Loss = 0.163249
Epoch 9.119: Loss = 0.251097
Epoch 9.120: Loss = 0.298296
TRAIN LOSS = 0.360264
TRAIN ACC = -327.666 % (54093/60000)
Loss = 0.322424
Loss = 0.423868
Loss = 0.481613
Loss = 0.489998
Loss = 0.517265
Loss = 0.347425
Loss = 0.311011
Loss = 0.573824
Loss = 0.472041
Loss = 0.433259
Loss = 0.13522
Loss = 0.263425
Loss = 0.238618
Loss = 0.304097
Loss = 0.159958
Loss = 0.260176
Loss = 0.149953
Loss = 0.0430514
Loss = 0.213944
Loss = 0.458034
TEST LOSS = 0.32996
TEST ACC = 540.93 % (9118/10000)
Reducing learning rate to 0.1
Epoch 10.1: Loss = 0.322445
Epoch 10.2: Loss = 0.434453
Epoch 10.3: Loss = 0.450728
Epoch 10.4: Loss = 0.264937
Epoch 10.5: Loss = 0.29359
Epoch 10.6: Loss = 0.295738
Epoch 10.7: Loss = 0.286485
Epoch 10.8: Loss = 0.325133
Epoch 10.9: Loss = 0.32243
Epoch 10.10: Loss = 0.33492
Epoch 10.11: Loss = 0.354043
Epoch 10.12: Loss = 0.335207
Epoch 10.13: Loss = 0.259968
Epoch 10.14: Loss = 0.299382
Epoch 10.15: Loss = 0.360033
Epoch 10.16: Loss = 0.382596
Epoch 10.17: Loss = 0.421676
Epoch 10.18: Loss = 0.556152
Epoch 10.19: Loss = 0.367066
Epoch 10.20: Loss = 0.273859
Epoch 10.21: Loss = 0.285326
Epoch 10.22: Loss = 0.25323
Epoch 10.23: Loss = 0.260277
Epoch 10.24: Loss = 0.48554
Epoch 10.25: Loss = 0.434474
Epoch 10.26: Loss = 0.477832
Epoch 10.27: Loss = 0.444348
Epoch 10.28: Loss = 0.389815
Epoch 10.29: Loss = 0.473848
Epoch 10.30: Loss = 0.461793
Epoch 10.31: Loss = 0.314552
Epoch 10.32: Loss = 0.393159
Epoch 10.33: Loss = 0.30024
Epoch 10.34: Loss = 0.369009
Epoch 10.35: Loss = 0.364755
Epoch 10.36: Loss = 0.428823
Epoch 10.37: Loss = 0.251032
Epoch 10.38: Loss = 0.286245
Epoch 10.39: Loss = 0.285501
Epoch 10.40: Loss = 0.321002
Epoch 10.41: Loss = 0.344238
Epoch 10.42: Loss = 0.528159
Epoch 10.43: Loss = 0.282111
Epoch 10.44: Loss = 0.263617
Epoch 10.45: Loss = 0.335249
Epoch 10.46: Loss = 0.372709
Epoch 10.47: Loss = 0.337988
Epoch 10.48: Loss = 0.375695
Epoch 10.49: Loss = 0.326405
Epoch 10.50: Loss = 0.424077
Epoch 10.51: Loss = 0.274167
Epoch 10.52: Loss = 0.274615
Epoch 10.53: Loss = 0.327188
Epoch 10.54: Loss = 0.445676
Epoch 10.55: Loss = 0.373672
Epoch 10.56: Loss = 0.336206
Epoch 10.57: Loss = 0.326093
Epoch 10.58: Loss = 0.345397
Epoch 10.59: Loss = 0.393497
Epoch 10.60: Loss = 0.420388
Epoch 10.61: Loss = 0.333863
Epoch 10.62: Loss = 0.43653
Epoch 10.63: Loss = 0.52283
Epoch 10.64: Loss = 0.431665
Epoch 10.65: Loss = 0.516761
Epoch 10.66: Loss = 0.346528
Epoch 10.67: Loss = 0.366526
Epoch 10.68: Loss = 0.196521
Epoch 10.69: Loss = 0.29433
Epoch 10.70: Loss = 0.430535
Epoch 10.71: Loss = 0.306766
Epoch 10.72: Loss = 0.248619
Epoch 10.73: Loss = 0.366218
Epoch 10.74: Loss = 0.282963
Epoch 10.75: Loss = 0.610875
Epoch 10.76: Loss = 0.402126
Epoch 10.77: Loss = 0.275573
Epoch 10.78: Loss = 0.366727
Epoch 10.79: Loss = 0.45293
Epoch 10.80: Loss = 0.37397
Epoch 10.81: Loss = 0.280064
Epoch 10.82: Loss = 0.277593
Epoch 10.83: Loss = 0.406845
Epoch 10.84: Loss = 0.327825
Epoch 10.85: Loss = 0.505902
Epoch 10.86: Loss = 0.444084
Epoch 10.87: Loss = 0.234191
Epoch 10.88: Loss = 0.337535
Epoch 10.89: Loss = 0.413027
Epoch 10.90: Loss = 0.296476
Epoch 10.91: Loss = 0.424156
Epoch 10.92: Loss = 0.438148
Epoch 10.93: Loss = 0.481425
Epoch 10.94: Loss = 0.257294
Epoch 10.95: Loss = 0.364761
Epoch 10.96: Loss = 0.418895
Epoch 10.97: Loss = 0.274916
Epoch 10.98: Loss = 0.321246
Epoch 10.99: Loss = 0.392591
Epoch 10.100: Loss = 0.506892
Epoch 10.101: Loss = 0.491721
Epoch 10.102: Loss = 0.335213
Epoch 10.103: Loss = 0.314804
Epoch 10.104: Loss = 0.300999
Epoch 10.105: Loss = 0.427053
Epoch 10.106: Loss = 0.500292
Epoch 10.107: Loss = 0.262089
Epoch 10.108: Loss = 0.41827
Epoch 10.109: Loss = 0.30687
Epoch 10.110: Loss = 0.387088
Epoch 10.111: Loss = 0.289985
Epoch 10.112: Loss = 0.289396
Epoch 10.113: Loss = 0.312726
Epoch 10.114: Loss = 0.253826
Epoch 10.115: Loss = 0.228938
Epoch 10.116: Loss = 0.339315
Epoch 10.117: Loss = 0.169559
Epoch 10.118: Loss = 0.160737
Epoch 10.119: Loss = 0.254946
Epoch 10.120: Loss = 0.287179
TRAIN LOSS = 0.355221
TRAIN ACC = 327.666 % (54227/60000)
Loss = 0.321815
Loss = 0.421997
Loss = 0.468326
Loss = 0.492042
Loss = 0.5211
Loss = 0.345605
Loss = 0.307595
Loss = 0.575655
Loss = 0.472692
Loss = 0.430607
Loss = 0.125336
Loss = 0.27736
Loss = 0.252259
Loss = 0.296444
Loss = 0.153534
Loss = 0.247915
Loss = 0.156197
Loss = 0.0442211
Loss = 0.203446
Loss = 0.451183
TEST LOSS = 0.328266
TEST ACC = 542.27 % (9126/10000)
