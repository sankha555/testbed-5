Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.5116
Epoch 1.2: Loss = 2.41278
Epoch 1.3: Loss = 2.36221
Epoch 1.4: Loss = 2.34193
Epoch 1.5: Loss = 2.25288
Epoch 1.6: Loss = 2.23772
Epoch 1.7: Loss = 2.21364
Epoch 1.8: Loss = 2.18407
Epoch 1.9: Loss = 2.1358
Epoch 1.10: Loss = 2.08774
Epoch 1.11: Loss = 2.05707
Epoch 1.12: Loss = 2.02711
Epoch 1.13: Loss = 1.93976
Epoch 1.14: Loss = 1.96591
Epoch 1.15: Loss = 1.98831
Epoch 1.16: Loss = 1.90781
Epoch 1.17: Loss = 1.87531
Epoch 1.18: Loss = 1.8531
Epoch 1.19: Loss = 1.79094
Epoch 1.20: Loss = 1.74734
Epoch 1.21: Loss = 1.68451
Epoch 1.22: Loss = 1.67413
Epoch 1.23: Loss = 1.63132
Epoch 1.24: Loss = 1.70485
Epoch 1.25: Loss = 1.64896
Epoch 1.26: Loss = 1.64836
Epoch 1.27: Loss = 1.58141
Epoch 1.28: Loss = 1.56636
Epoch 1.29: Loss = 1.55122
Epoch 1.30: Loss = 1.58971
Epoch 1.31: Loss = 1.463
Epoch 1.32: Loss = 1.48564
Epoch 1.33: Loss = 1.41191
Epoch 1.34: Loss = 1.43649
Epoch 1.35: Loss = 1.37886
Epoch 1.36: Loss = 1.46376
Epoch 1.37: Loss = 1.32939
Epoch 1.38: Loss = 1.26375
Epoch 1.39: Loss = 1.27931
Epoch 1.40: Loss = 1.22118
Epoch 1.41: Loss = 1.24992
Epoch 1.42: Loss = 1.20601
Epoch 1.43: Loss = 1.19507
Epoch 1.44: Loss = 1.08601
Epoch 1.45: Loss = 1.21001
Epoch 1.46: Loss = 1.16119
Epoch 1.47: Loss = 1.07303
Epoch 1.48: Loss = 1.15762
Epoch 1.49: Loss = 1.08644
Epoch 1.50: Loss = 1.16003
Epoch 1.51: Loss = 0.970978
Epoch 1.52: Loss = 1.00427
Epoch 1.53: Loss = 1.06197
Epoch 1.54: Loss = 1.07918
Epoch 1.55: Loss = 1.06091
Epoch 1.56: Loss = 0.986465
Epoch 1.57: Loss = 0.905609
Epoch 1.58: Loss = 0.947342
Epoch 1.59: Loss = 0.941956
Epoch 1.60: Loss = 1.05417
Epoch 1.61: Loss = 0.990204
Epoch 1.62: Loss = 1.00673
Epoch 1.63: Loss = 1.01909
Epoch 1.64: Loss = 0.996017
Epoch 1.65: Loss = 1.01538
Epoch 1.66: Loss = 0.904648
Epoch 1.67: Loss = 0.914581
Epoch 1.68: Loss = 0.777969
Epoch 1.69: Loss = 0.823349
Epoch 1.70: Loss = 0.908447
Epoch 1.71: Loss = 0.837082
Epoch 1.72: Loss = 0.862839
Epoch 1.73: Loss = 0.846863
Epoch 1.74: Loss = 0.689941
Epoch 1.75: Loss = 0.844757
Epoch 1.76: Loss = 0.800949
Epoch 1.77: Loss = 0.774109
Epoch 1.78: Loss = 0.748016
Epoch 1.79: Loss = 0.768204
Epoch 1.80: Loss = 0.86261
Epoch 1.81: Loss = 0.72171
Epoch 1.82: Loss = 0.697968
Epoch 1.83: Loss = 0.861496
Epoch 1.84: Loss = 0.796875
Epoch 1.85: Loss = 0.88237
Epoch 1.86: Loss = 0.74823
Epoch 1.87: Loss = 0.701889
Epoch 1.88: Loss = 0.731064
Epoch 1.89: Loss = 0.800598
Epoch 1.90: Loss = 0.701248
Epoch 1.91: Loss = 0.779984
Epoch 1.92: Loss = 0.774002
Epoch 1.93: Loss = 0.786789
Epoch 1.94: Loss = 0.62677
Epoch 1.95: Loss = 0.732666
Epoch 1.96: Loss = 0.689362
Epoch 1.97: Loss = 0.544052
Epoch 1.98: Loss = 0.663589
Epoch 1.99: Loss = 0.742416
Epoch 1.100: Loss = 0.83992
Epoch 1.101: Loss = 0.750107
Epoch 1.102: Loss = 0.685318
Epoch 1.103: Loss = 0.617874
Epoch 1.104: Loss = 0.606949
Epoch 1.105: Loss = 0.700424
Epoch 1.106: Loss = 0.709259
Epoch 1.107: Loss = 0.597214
Epoch 1.108: Loss = 0.647064
Epoch 1.109: Loss = 0.611359
Epoch 1.110: Loss = 0.643692
Epoch 1.111: Loss = 0.566208
Epoch 1.112: Loss = 0.529205
Epoch 1.113: Loss = 0.634903
Epoch 1.114: Loss = 0.525436
Epoch 1.115: Loss = 0.620667
Epoch 1.116: Loss = 0.594131
Epoch 1.117: Loss = 0.505219
Epoch 1.118: Loss = 0.438324
Epoch 1.119: Loss = 0.479935
Epoch 1.120: Loss = 0.481705
TRAIN LOSS = 1.14722
TRAIN ACC = 68.8049 % (41285/60000)
Loss = 0.654037
Loss = 0.667755
Loss = 0.785583
Loss = 0.721939
Loss = 0.754242
Loss = 0.653351
Loss = 0.632278
Loss = 0.810791
Loss = 0.753616
Loss = 0.675995
Loss = 0.364868
Loss = 0.551926
Loss = 0.397644
Loss = 0.558243
Loss = 0.480911
Loss = 0.446243
Loss = 0.420776
Loss = 0.281296
Loss = 0.441681
Loss = 0.667389
TEST LOSS = 0.586028
TEST ACC = 412.849 % (8252/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.530914
Epoch 2.2: Loss = 0.666016
Epoch 2.3: Loss = 0.656265
Epoch 2.4: Loss = 0.545258
Epoch 2.5: Loss = 0.555847
Epoch 2.6: Loss = 0.54245
Epoch 2.7: Loss = 0.610916
Epoch 2.8: Loss = 0.566925
Epoch 2.9: Loss = 0.567612
Epoch 2.10: Loss = 0.553986
Epoch 2.11: Loss = 0.554123
Epoch 2.12: Loss = 0.538544
Epoch 2.13: Loss = 0.468063
Epoch 2.14: Loss = 0.522278
Epoch 2.15: Loss = 0.6763
Epoch 2.16: Loss = 0.594742
Epoch 2.17: Loss = 0.624695
Epoch 2.18: Loss = 0.666122
Epoch 2.19: Loss = 0.535416
Epoch 2.20: Loss = 0.491043
Epoch 2.21: Loss = 0.469604
Epoch 2.22: Loss = 0.466675
Epoch 2.23: Loss = 0.485748
Epoch 2.24: Loss = 0.66687
Epoch 2.25: Loss = 0.542862
Epoch 2.26: Loss = 0.627457
Epoch 2.27: Loss = 0.602951
Epoch 2.28: Loss = 0.619614
Epoch 2.29: Loss = 0.65123
Epoch 2.30: Loss = 0.723862
Epoch 2.31: Loss = 0.503693
Epoch 2.32: Loss = 0.627731
Epoch 2.33: Loss = 0.519562
Epoch 2.34: Loss = 0.598053
Epoch 2.35: Loss = 0.571518
Epoch 2.36: Loss = 0.673584
Epoch 2.37: Loss = 0.493301
Epoch 2.38: Loss = 0.439514
Epoch 2.39: Loss = 0.540482
Epoch 2.40: Loss = 0.49408
Epoch 2.41: Loss = 0.556473
Epoch 2.42: Loss = 0.599945
Epoch 2.43: Loss = 0.503525
Epoch 2.44: Loss = 0.424057
Epoch 2.45: Loss = 0.530624
Epoch 2.46: Loss = 0.585831
Epoch 2.47: Loss = 0.474625
Epoch 2.48: Loss = 0.563553
Epoch 2.49: Loss = 0.518784
Epoch 2.50: Loss = 0.602905
Epoch 2.51: Loss = 0.460495
Epoch 2.52: Loss = 0.452042
Epoch 2.53: Loss = 0.502396
Epoch 2.54: Loss = 0.597504
Epoch 2.55: Loss = 0.536987
Epoch 2.56: Loss = 0.490463
Epoch 2.57: Loss = 0.471695
Epoch 2.58: Loss = 0.491714
Epoch 2.59: Loss = 0.542603
Epoch 2.60: Loss = 0.607651
Epoch 2.61: Loss = 0.577881
Epoch 2.62: Loss = 0.580017
Epoch 2.63: Loss = 0.62529
Epoch 2.64: Loss = 0.603912
Epoch 2.65: Loss = 0.635605
Epoch 2.66: Loss = 0.532135
Epoch 2.67: Loss = 0.521851
Epoch 2.68: Loss = 0.384323
Epoch 2.69: Loss = 0.436584
Epoch 2.70: Loss = 0.587814
Epoch 2.71: Loss = 0.457718
Epoch 2.72: Loss = 0.511108
Epoch 2.73: Loss = 0.50769
Epoch 2.74: Loss = 0.383026
Epoch 2.75: Loss = 0.611969
Epoch 2.76: Loss = 0.532867
Epoch 2.77: Loss = 0.463486
Epoch 2.78: Loss = 0.461227
Epoch 2.79: Loss = 0.51033
Epoch 2.80: Loss = 0.568451
Epoch 2.81: Loss = 0.452179
Epoch 2.82: Loss = 0.408829
Epoch 2.83: Loss = 0.600983
Epoch 2.84: Loss = 0.507202
Epoch 2.85: Loss = 0.643494
Epoch 2.86: Loss = 0.506256
Epoch 2.87: Loss = 0.420151
Epoch 2.88: Loss = 0.476563
Epoch 2.89: Loss = 0.573944
Epoch 2.90: Loss = 0.426331
Epoch 2.91: Loss = 0.554764
Epoch 2.92: Loss = 0.538788
Epoch 2.93: Loss = 0.558716
Epoch 2.94: Loss = 0.406937
Epoch 2.95: Loss = 0.492722
Epoch 2.96: Loss = 0.524872
Epoch 2.97: Loss = 0.367249
Epoch 2.98: Loss = 0.446274
Epoch 2.99: Loss = 0.556335
Epoch 2.100: Loss = 0.645294
Epoch 2.101: Loss = 0.573807
Epoch 2.102: Loss = 0.483902
Epoch 2.103: Loss = 0.423721
Epoch 2.104: Loss = 0.396439
Epoch 2.105: Loss = 0.553802
Epoch 2.106: Loss = 0.550735
Epoch 2.107: Loss = 0.408951
Epoch 2.108: Loss = 0.491394
Epoch 2.109: Loss = 0.414413
Epoch 2.110: Loss = 0.484146
Epoch 2.111: Loss = 0.388565
Epoch 2.112: Loss = 0.393524
Epoch 2.113: Loss = 0.453094
Epoch 2.114: Loss = 0.380905
Epoch 2.115: Loss = 0.427063
Epoch 2.116: Loss = 0.441452
Epoch 2.117: Loss = 0.358597
Epoch 2.118: Loss = 0.257156
Epoch 2.119: Loss = 0.340607
Epoch 2.120: Loss = 0.347015
TRAIN LOSS = 0.520599
TRAIN ACC = 84.523 % (50716/60000)
Loss = 0.494843
Loss = 0.548172
Loss = 0.646835
Loss = 0.599899
Loss = 0.644699
Loss = 0.486832
Loss = 0.472092
Loss = 0.66629
Loss = 0.61554
Loss = 0.532623
Loss = 0.227112
Loss = 0.404221
Loss = 0.290298
Loss = 0.408752
Loss = 0.311523
Loss = 0.312408
Loss = 0.276978
Loss = 0.141724
Loss = 0.284836
Loss = 0.581177
TEST LOSS = 0.447343
TEST ACC = 507.159 % (8633/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.40358
Epoch 3.2: Loss = 0.544571
Epoch 3.3: Loss = 0.535995
Epoch 3.4: Loss = 0.389023
Epoch 3.5: Loss = 0.409439
Epoch 3.6: Loss = 0.424286
Epoch 3.7: Loss = 0.446198
Epoch 3.8: Loss = 0.427979
Epoch 3.9: Loss = 0.430389
Epoch 3.10: Loss = 0.430634
Epoch 3.11: Loss = 0.440536
Epoch 3.12: Loss = 0.414749
Epoch 3.13: Loss = 0.367691
Epoch 3.14: Loss = 0.410797
Epoch 3.15: Loss = 0.528183
Epoch 3.16: Loss = 0.457443
Epoch 3.17: Loss = 0.538986
Epoch 3.18: Loss = 0.611893
Epoch 3.19: Loss = 0.447067
Epoch 3.20: Loss = 0.396072
Epoch 3.21: Loss = 0.373245
Epoch 3.22: Loss = 0.35025
Epoch 3.23: Loss = 0.406265
Epoch 3.24: Loss = 0.589706
Epoch 3.25: Loss = 0.454407
Epoch 3.26: Loss = 0.548477
Epoch 3.27: Loss = 0.52153
Epoch 3.28: Loss = 0.53569
Epoch 3.29: Loss = 0.565109
Epoch 3.30: Loss = 0.635162
Epoch 3.31: Loss = 0.415268
Epoch 3.32: Loss = 0.549042
Epoch 3.33: Loss = 0.422546
Epoch 3.34: Loss = 0.503967
Epoch 3.35: Loss = 0.46666
Epoch 3.36: Loss = 0.585571
Epoch 3.37: Loss = 0.383438
Epoch 3.38: Loss = 0.366272
Epoch 3.39: Loss = 0.437363
Epoch 3.40: Loss = 0.412689
Epoch 3.41: Loss = 0.455307
Epoch 3.42: Loss = 0.550262
Epoch 3.43: Loss = 0.418732
Epoch 3.44: Loss = 0.349976
Epoch 3.45: Loss = 0.443207
Epoch 3.46: Loss = 0.508972
Epoch 3.47: Loss = 0.401901
Epoch 3.48: Loss = 0.490723
Epoch 3.49: Loss = 0.439102
Epoch 3.50: Loss = 0.541382
Epoch 3.51: Loss = 0.394485
Epoch 3.52: Loss = 0.377625
Epoch 3.53: Loss = 0.427292
Epoch 3.54: Loss = 0.544418
Epoch 3.55: Loss = 0.444672
Epoch 3.56: Loss = 0.440765
Epoch 3.57: Loss = 0.417664
Epoch 3.58: Loss = 0.42099
Epoch 3.59: Loss = 0.488327
Epoch 3.60: Loss = 0.544418
Epoch 3.61: Loss = 0.509506
Epoch 3.62: Loss = 0.506256
Epoch 3.63: Loss = 0.569672
Epoch 3.64: Loss = 0.538391
Epoch 3.65: Loss = 0.587784
Epoch 3.66: Loss = 0.442032
Epoch 3.67: Loss = 0.44371
Epoch 3.68: Loss = 0.31958
Epoch 3.69: Loss = 0.361511
Epoch 3.70: Loss = 0.550446
Epoch 3.71: Loss = 0.389938
Epoch 3.72: Loss = 0.418503
Epoch 3.73: Loss = 0.419861
Epoch 3.74: Loss = 0.324875
Epoch 3.75: Loss = 0.581558
Epoch 3.76: Loss = 0.490051
Epoch 3.77: Loss = 0.397354
Epoch 3.78: Loss = 0.399857
Epoch 3.79: Loss = 0.49617
Epoch 3.80: Loss = 0.511765
Epoch 3.81: Loss = 0.384903
Epoch 3.82: Loss = 0.342651
Epoch 3.83: Loss = 0.523941
Epoch 3.84: Loss = 0.447922
Epoch 3.85: Loss = 0.582108
Epoch 3.86: Loss = 0.475342
Epoch 3.87: Loss = 0.35881
Epoch 3.88: Loss = 0.434891
Epoch 3.89: Loss = 0.521561
Epoch 3.90: Loss = 0.373978
Epoch 3.91: Loss = 0.504257
Epoch 3.92: Loss = 0.495819
Epoch 3.93: Loss = 0.520599
Epoch 3.94: Loss = 0.354691
Epoch 3.95: Loss = 0.436279
Epoch 3.96: Loss = 0.492706
Epoch 3.97: Loss = 0.334366
Epoch 3.98: Loss = 0.407806
Epoch 3.99: Loss = 0.501556
Epoch 3.100: Loss = 0.618027
Epoch 3.101: Loss = 0.565384
Epoch 3.102: Loss = 0.439529
Epoch 3.103: Loss = 0.37735
Epoch 3.104: Loss = 0.341873
Epoch 3.105: Loss = 0.511887
Epoch 3.106: Loss = 0.525955
Epoch 3.107: Loss = 0.374466
Epoch 3.108: Loss = 0.458725
Epoch 3.109: Loss = 0.368637
Epoch 3.110: Loss = 0.43251
Epoch 3.111: Loss = 0.347641
Epoch 3.112: Loss = 0.353943
Epoch 3.113: Loss = 0.414627
Epoch 3.114: Loss = 0.342865
Epoch 3.115: Loss = 0.374268
Epoch 3.116: Loss = 0.383316
Epoch 3.117: Loss = 0.305511
Epoch 3.118: Loss = 0.210251
Epoch 3.119: Loss = 0.310333
Epoch 3.120: Loss = 0.328568
TRAIN LOSS = 0.448471
TRAIN ACC = 86.4456 % (51869/60000)
Loss = 0.424881
Loss = 0.493042
Loss = 0.59967
Loss = 0.544449
Loss = 0.604233
Loss = 0.437088
Loss = 0.40329
Loss = 0.631317
Loss = 0.575821
Loss = 0.485321
Loss = 0.186523
Loss = 0.354279
Loss = 0.270126
Loss = 0.381424
Loss = 0.255478
Loss = 0.283829
Loss = 0.240173
Loss = 0.0971832
Loss = 0.235626
Loss = 0.554993
TEST LOSS = 0.402937
TEST ACC = 518.689 % (8790/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.36969
Epoch 4.2: Loss = 0.513306
Epoch 4.3: Loss = 0.502182
Epoch 4.4: Loss = 0.341171
Epoch 4.5: Loss = 0.381348
Epoch 4.6: Loss = 0.398087
Epoch 4.7: Loss = 0.399261
Epoch 4.8: Loss = 0.395706
Epoch 4.9: Loss = 0.391724
Epoch 4.10: Loss = 0.384308
Epoch 4.11: Loss = 0.410156
Epoch 4.12: Loss = 0.393372
Epoch 4.13: Loss = 0.348114
Epoch 4.14: Loss = 0.366501
Epoch 4.15: Loss = 0.476608
Epoch 4.16: Loss = 0.409729
Epoch 4.17: Loss = 0.501282
Epoch 4.18: Loss = 0.595505
Epoch 4.19: Loss = 0.430161
Epoch 4.20: Loss = 0.370834
Epoch 4.21: Loss = 0.333542
Epoch 4.22: Loss = 0.30397
Epoch 4.23: Loss = 0.384827
Epoch 4.24: Loss = 0.551483
Epoch 4.25: Loss = 0.442062
Epoch 4.26: Loss = 0.528137
Epoch 4.27: Loss = 0.483612
Epoch 4.28: Loss = 0.49176
Epoch 4.29: Loss = 0.548431
Epoch 4.30: Loss = 0.588348
Epoch 4.31: Loss = 0.370911
Epoch 4.32: Loss = 0.514343
Epoch 4.33: Loss = 0.386993
Epoch 4.34: Loss = 0.466019
Epoch 4.35: Loss = 0.406784
Epoch 4.36: Loss = 0.551987
Epoch 4.37: Loss = 0.334656
Epoch 4.38: Loss = 0.353958
Epoch 4.39: Loss = 0.412704
Epoch 4.40: Loss = 0.39209
Epoch 4.41: Loss = 0.428772
Epoch 4.42: Loss = 0.565903
Epoch 4.43: Loss = 0.390503
Epoch 4.44: Loss = 0.32341
Epoch 4.45: Loss = 0.408096
Epoch 4.46: Loss = 0.475327
Epoch 4.47: Loss = 0.384216
Epoch 4.48: Loss = 0.447525
Epoch 4.49: Loss = 0.40097
Epoch 4.50: Loss = 0.515701
Epoch 4.51: Loss = 0.353271
Epoch 4.52: Loss = 0.356049
Epoch 4.53: Loss = 0.385452
Epoch 4.54: Loss = 0.513367
Epoch 4.55: Loss = 0.404999
Epoch 4.56: Loss = 0.41243
Epoch 4.57: Loss = 0.39537
Epoch 4.58: Loss = 0.389542
Epoch 4.59: Loss = 0.458649
Epoch 4.60: Loss = 0.515457
Epoch 4.61: Loss = 0.460724
Epoch 4.62: Loss = 0.488113
Epoch 4.63: Loss = 0.564789
Epoch 4.64: Loss = 0.519012
Epoch 4.65: Loss = 0.594894
Epoch 4.66: Loss = 0.415131
Epoch 4.67: Loss = 0.412247
Epoch 4.68: Loss = 0.293015
Epoch 4.69: Loss = 0.331299
Epoch 4.70: Loss = 0.528336
Epoch 4.71: Loss = 0.356155
Epoch 4.72: Loss = 0.371765
Epoch 4.73: Loss = 0.39592
Epoch 4.74: Loss = 0.311386
Epoch 4.75: Loss = 0.60379
Epoch 4.76: Loss = 0.454529
Epoch 4.77: Loss = 0.372833
Epoch 4.78: Loss = 0.384064
Epoch 4.79: Loss = 0.474426
Epoch 4.80: Loss = 0.462769
Epoch 4.81: Loss = 0.367981
Epoch 4.82: Loss = 0.324097
Epoch 4.83: Loss = 0.510681
Epoch 4.84: Loss = 0.439148
Epoch 4.85: Loss = 0.588043
Epoch 4.86: Loss = 0.465179
Epoch 4.87: Loss = 0.333252
Epoch 4.88: Loss = 0.39798
Epoch 4.89: Loss = 0.495026
Epoch 4.90: Loss = 0.346634
Epoch 4.91: Loss = 0.49968
Epoch 4.92: Loss = 0.481003
Epoch 4.93: Loss = 0.507355
Epoch 4.94: Loss = 0.328735
Epoch 4.95: Loss = 0.41745
Epoch 4.96: Loss = 0.464844
Epoch 4.97: Loss = 0.316895
Epoch 4.98: Loss = 0.396973
Epoch 4.99: Loss = 0.495392
Epoch 4.100: Loss = 0.568741
Epoch 4.101: Loss = 0.53923
Epoch 4.102: Loss = 0.427261
Epoch 4.103: Loss = 0.373657
Epoch 4.104: Loss = 0.328766
Epoch 4.105: Loss = 0.503448
Epoch 4.106: Loss = 0.508759
Epoch 4.107: Loss = 0.338989
Epoch 4.108: Loss = 0.449371
Epoch 4.109: Loss = 0.343979
Epoch 4.110: Loss = 0.40329
Epoch 4.111: Loss = 0.332626
Epoch 4.112: Loss = 0.339233
Epoch 4.113: Loss = 0.393402
Epoch 4.114: Loss = 0.336716
Epoch 4.115: Loss = 0.352219
Epoch 4.116: Loss = 0.371262
Epoch 4.117: Loss = 0.271225
Epoch 4.118: Loss = 0.20639
Epoch 4.119: Loss = 0.287827
Epoch 4.120: Loss = 0.321487
TRAIN LOSS = 0.422668
TRAIN ACC = 87.4023 % (52444/60000)
Loss = 0.395828
Loss = 0.488144
Loss = 0.562012
Loss = 0.543045
Loss = 0.577698
Loss = 0.418793
Loss = 0.402527
Loss = 0.618484
Loss = 0.553268
Loss = 0.479111
Loss = 0.17572
Loss = 0.346512
Loss = 0.240479
Loss = 0.358902
Loss = 0.229279
Loss = 0.290741
Loss = 0.215347
Loss = 0.0815582
Loss = 0.22406
Loss = 0.522491
TEST LOSS = 0.3862
TEST ACC = 524.438 % (8866/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.341385
Epoch 5.2: Loss = 0.498123
Epoch 5.3: Loss = 0.485229
Epoch 5.4: Loss = 0.323456
Epoch 5.5: Loss = 0.359161
Epoch 5.6: Loss = 0.388474
Epoch 5.7: Loss = 0.379959
Epoch 5.8: Loss = 0.365341
Epoch 5.9: Loss = 0.358597
Epoch 5.10: Loss = 0.364212
Epoch 5.11: Loss = 0.382874
Epoch 5.12: Loss = 0.374329
Epoch 5.13: Loss = 0.31041
Epoch 5.14: Loss = 0.351212
Epoch 5.15: Loss = 0.446167
Epoch 5.16: Loss = 0.382339
Epoch 5.17: Loss = 0.491745
Epoch 5.18: Loss = 0.581863
Epoch 5.19: Loss = 0.416595
Epoch 5.20: Loss = 0.352829
Epoch 5.21: Loss = 0.311157
Epoch 5.22: Loss = 0.288803
Epoch 5.23: Loss = 0.354126
Epoch 5.24: Loss = 0.542618
Epoch 5.25: Loss = 0.417145
Epoch 5.26: Loss = 0.520844
Epoch 5.27: Loss = 0.479797
Epoch 5.28: Loss = 0.472549
Epoch 5.29: Loss = 0.538223
Epoch 5.30: Loss = 0.574951
Epoch 5.31: Loss = 0.362228
Epoch 5.32: Loss = 0.467712
Epoch 5.33: Loss = 0.363937
Epoch 5.34: Loss = 0.443283
Epoch 5.35: Loss = 0.39238
Epoch 5.36: Loss = 0.518723
Epoch 5.37: Loss = 0.315048
Epoch 5.38: Loss = 0.33284
Epoch 5.39: Loss = 0.386948
Epoch 5.40: Loss = 0.379257
Epoch 5.41: Loss = 0.415054
Epoch 5.42: Loss = 0.573288
Epoch 5.43: Loss = 0.376144
Epoch 5.44: Loss = 0.30365
Epoch 5.45: Loss = 0.396439
Epoch 5.46: Loss = 0.437744
Epoch 5.47: Loss = 0.381989
Epoch 5.48: Loss = 0.430954
Epoch 5.49: Loss = 0.382095
Epoch 5.50: Loss = 0.473724
Epoch 5.51: Loss = 0.330078
Epoch 5.52: Loss = 0.342163
Epoch 5.53: Loss = 0.379868
Epoch 5.54: Loss = 0.515869
Epoch 5.55: Loss = 0.378006
Epoch 5.56: Loss = 0.392746
Epoch 5.57: Loss = 0.384659
Epoch 5.58: Loss = 0.383316
Epoch 5.59: Loss = 0.451172
Epoch 5.60: Loss = 0.499283
Epoch 5.61: Loss = 0.43248
Epoch 5.62: Loss = 0.479553
Epoch 5.63: Loss = 0.563309
Epoch 5.64: Loss = 0.505936
Epoch 5.65: Loss = 0.568665
Epoch 5.66: Loss = 0.404709
Epoch 5.67: Loss = 0.394119
Epoch 5.68: Loss = 0.2724
Epoch 5.69: Loss = 0.311493
Epoch 5.70: Loss = 0.523315
Epoch 5.71: Loss = 0.345718
Epoch 5.72: Loss = 0.337326
Epoch 5.73: Loss = 0.372314
Epoch 5.74: Loss = 0.297043
Epoch 5.75: Loss = 0.614166
Epoch 5.76: Loss = 0.452362
Epoch 5.77: Loss = 0.345642
Epoch 5.78: Loss = 0.376678
Epoch 5.79: Loss = 0.475922
Epoch 5.80: Loss = 0.449722
Epoch 5.81: Loss = 0.347061
Epoch 5.82: Loss = 0.320923
Epoch 5.83: Loss = 0.477966
Epoch 5.84: Loss = 0.41243
Epoch 5.85: Loss = 0.563919
Epoch 5.86: Loss = 0.476532
Epoch 5.87: Loss = 0.323532
Epoch 5.88: Loss = 0.383636
Epoch 5.89: Loss = 0.491837
Epoch 5.90: Loss = 0.342163
Epoch 5.91: Loss = 0.486069
Epoch 5.92: Loss = 0.460342
Epoch 5.93: Loss = 0.499664
Epoch 5.94: Loss = 0.324463
Epoch 5.95: Loss = 0.413269
Epoch 5.96: Loss = 0.455872
Epoch 5.97: Loss = 0.312424
Epoch 5.98: Loss = 0.395767
Epoch 5.99: Loss = 0.475143
Epoch 5.100: Loss = 0.551147
Epoch 5.101: Loss = 0.533173
Epoch 5.102: Loss = 0.431747
Epoch 5.103: Loss = 0.354385
Epoch 5.104: Loss = 0.327271
Epoch 5.105: Loss = 0.509125
Epoch 5.106: Loss = 0.520981
Epoch 5.107: Loss = 0.312912
Epoch 5.108: Loss = 0.436432
Epoch 5.109: Loss = 0.319473
Epoch 5.110: Loss = 0.406204
Epoch 5.111: Loss = 0.328659
Epoch 5.112: Loss = 0.335342
Epoch 5.113: Loss = 0.385223
Epoch 5.114: Loss = 0.337433
Epoch 5.115: Loss = 0.329834
Epoch 5.116: Loss = 0.36882
Epoch 5.117: Loss = 0.276962
Epoch 5.118: Loss = 0.188553
Epoch 5.119: Loss = 0.281036
Epoch 5.120: Loss = 0.311707
TRAIN LOSS = 0.408157
TRAIN ACC = 88.1424 % (52887/60000)
Loss = 0.390884
Loss = 0.483551
Loss = 0.559158
Loss = 0.554306
Loss = 0.580399
Loss = 0.414856
Loss = 0.380814
Loss = 0.62323
Loss = 0.534302
Loss = 0.469635
Loss = 0.174652
Loss = 0.322922
Loss = 0.24118
Loss = 0.35144
Loss = 0.211273
Loss = 0.29892
Loss = 0.216492
Loss = 0.0727997
Loss = 0.217407
Loss = 0.526794
TEST LOSS = 0.381251
TEST ACC = 528.87 % (8904/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.341339
Epoch 6.2: Loss = 0.485992
Epoch 6.3: Loss = 0.476318
Epoch 6.4: Loss = 0.307861
Epoch 6.5: Loss = 0.354874
Epoch 6.6: Loss = 0.376541
Epoch 6.7: Loss = 0.369873
Epoch 6.8: Loss = 0.368301
Epoch 6.9: Loss = 0.349869
Epoch 6.10: Loss = 0.35582
Epoch 6.11: Loss = 0.384262
Epoch 6.12: Loss = 0.377304
Epoch 6.13: Loss = 0.308212
Epoch 6.14: Loss = 0.345016
Epoch 6.15: Loss = 0.438828
Epoch 6.16: Loss = 0.377045
Epoch 6.17: Loss = 0.488953
Epoch 6.18: Loss = 0.572403
Epoch 6.19: Loss = 0.419922
Epoch 6.20: Loss = 0.355255
Epoch 6.21: Loss = 0.290466
Epoch 6.22: Loss = 0.290588
Epoch 6.23: Loss = 0.32164
Epoch 6.24: Loss = 0.530243
Epoch 6.25: Loss = 0.407257
Epoch 6.26: Loss = 0.533478
Epoch 6.27: Loss = 0.475693
Epoch 6.28: Loss = 0.472778
Epoch 6.29: Loss = 0.523941
Epoch 6.30: Loss = 0.543777
Epoch 6.31: Loss = 0.361206
Epoch 6.32: Loss = 0.448364
Epoch 6.33: Loss = 0.359756
Epoch 6.34: Loss = 0.434937
Epoch 6.35: Loss = 0.390167
Epoch 6.36: Loss = 0.530151
Epoch 6.37: Loss = 0.293869
Epoch 6.38: Loss = 0.337128
Epoch 6.39: Loss = 0.385788
Epoch 6.40: Loss = 0.383743
Epoch 6.41: Loss = 0.403244
Epoch 6.42: Loss = 0.576874
Epoch 6.43: Loss = 0.369446
Epoch 6.44: Loss = 0.301529
Epoch 6.45: Loss = 0.389832
Epoch 6.46: Loss = 0.428314
Epoch 6.47: Loss = 0.367126
Epoch 6.48: Loss = 0.420181
Epoch 6.49: Loss = 0.38797
Epoch 6.50: Loss = 0.469254
Epoch 6.51: Loss = 0.332138
Epoch 6.52: Loss = 0.319733
Epoch 6.53: Loss = 0.374634
Epoch 6.54: Loss = 0.505875
Epoch 6.55: Loss = 0.36351
Epoch 6.56: Loss = 0.390778
Epoch 6.57: Loss = 0.393982
Epoch 6.58: Loss = 0.384766
Epoch 6.59: Loss = 0.461472
Epoch 6.60: Loss = 0.508957
Epoch 6.61: Loss = 0.425583
Epoch 6.62: Loss = 0.496277
Epoch 6.63: Loss = 0.572495
Epoch 6.64: Loss = 0.499115
Epoch 6.65: Loss = 0.553879
Epoch 6.66: Loss = 0.401352
Epoch 6.67: Loss = 0.398254
Epoch 6.68: Loss = 0.26973
Epoch 6.69: Loss = 0.313431
Epoch 6.70: Loss = 0.513687
Epoch 6.71: Loss = 0.347626
Epoch 6.72: Loss = 0.331543
Epoch 6.73: Loss = 0.379456
Epoch 6.74: Loss = 0.28241
Epoch 6.75: Loss = 0.606796
Epoch 6.76: Loss = 0.464157
Epoch 6.77: Loss = 0.337036
Epoch 6.78: Loss = 0.384003
Epoch 6.79: Loss = 0.487427
Epoch 6.80: Loss = 0.432373
Epoch 6.81: Loss = 0.343643
Epoch 6.82: Loss = 0.322159
Epoch 6.83: Loss = 0.465576
Epoch 6.84: Loss = 0.408737
Epoch 6.85: Loss = 0.564423
Epoch 6.86: Loss = 0.469955
Epoch 6.87: Loss = 0.322708
Epoch 6.88: Loss = 0.388153
Epoch 6.89: Loss = 0.491135
Epoch 6.90: Loss = 0.339462
Epoch 6.91: Loss = 0.487381
Epoch 6.92: Loss = 0.462967
Epoch 6.93: Loss = 0.533844
Epoch 6.94: Loss = 0.339523
Epoch 6.95: Loss = 0.420334
Epoch 6.96: Loss = 0.453873
Epoch 6.97: Loss = 0.316086
Epoch 6.98: Loss = 0.381699
Epoch 6.99: Loss = 0.470398
Epoch 6.100: Loss = 0.566208
Epoch 6.101: Loss = 0.533676
Epoch 6.102: Loss = 0.434052
Epoch 6.103: Loss = 0.349915
Epoch 6.104: Loss = 0.32399
Epoch 6.105: Loss = 0.481674
Epoch 6.106: Loss = 0.518753
Epoch 6.107: Loss = 0.294922
Epoch 6.108: Loss = 0.440979
Epoch 6.109: Loss = 0.338226
Epoch 6.110: Loss = 0.409012
Epoch 6.111: Loss = 0.320297
Epoch 6.112: Loss = 0.333557
Epoch 6.113: Loss = 0.388351
Epoch 6.114: Loss = 0.328217
Epoch 6.115: Loss = 0.318573
Epoch 6.116: Loss = 0.355927
Epoch 6.117: Loss = 0.264511
Epoch 6.118: Loss = 0.185928
Epoch 6.119: Loss = 0.281387
Epoch 6.120: Loss = 0.318481
TRAIN LOSS = 0.404861
TRAIN ACC = 88.4872 % (53095/60000)
Loss = 0.381363
Loss = 0.489731
Loss = 0.537933
Loss = 0.553375
Loss = 0.578522
Loss = 0.394974
Loss = 0.377762
Loss = 0.623932
Loss = 0.516815
Loss = 0.463531
Loss = 0.175476
Loss = 0.323151
Loss = 0.253845
Loss = 0.343216
Loss = 0.210342
Loss = 0.263794
Loss = 0.225372
Loss = 0.0674133
Loss = 0.223923
Loss = 0.526489
TEST LOSS = 0.376548
TEST ACC = 530.949 % (8946/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.348038
Epoch 7.2: Loss = 0.482437
Epoch 7.3: Loss = 0.48558
Epoch 7.4: Loss = 0.293396
Epoch 7.5: Loss = 0.352371
Epoch 7.6: Loss = 0.367233
Epoch 7.7: Loss = 0.3508
Epoch 7.8: Loss = 0.351425
Epoch 7.9: Loss = 0.346619
Epoch 7.10: Loss = 0.346664
Epoch 7.11: Loss = 0.380127
Epoch 7.12: Loss = 0.370758
Epoch 7.13: Loss = 0.296936
Epoch 7.14: Loss = 0.33757
Epoch 7.15: Loss = 0.433197
Epoch 7.16: Loss = 0.357254
Epoch 7.17: Loss = 0.469818
Epoch 7.18: Loss = 0.567352
Epoch 7.19: Loss = 0.417801
Epoch 7.20: Loss = 0.341232
Epoch 7.21: Loss = 0.282883
Epoch 7.22: Loss = 0.281494
Epoch 7.23: Loss = 0.318146
Epoch 7.24: Loss = 0.530304
Epoch 7.25: Loss = 0.424561
Epoch 7.26: Loss = 0.531219
Epoch 7.27: Loss = 0.463715
Epoch 7.28: Loss = 0.480423
Epoch 7.29: Loss = 0.517548
Epoch 7.30: Loss = 0.563599
Epoch 7.31: Loss = 0.360153
Epoch 7.32: Loss = 0.435455
Epoch 7.33: Loss = 0.351028
Epoch 7.34: Loss = 0.419952
Epoch 7.35: Loss = 0.383148
Epoch 7.36: Loss = 0.520691
Epoch 7.37: Loss = 0.285645
Epoch 7.38: Loss = 0.321152
Epoch 7.39: Loss = 0.360214
Epoch 7.40: Loss = 0.362198
Epoch 7.41: Loss = 0.375793
Epoch 7.42: Loss = 0.580521
Epoch 7.43: Loss = 0.358917
Epoch 7.44: Loss = 0.301559
Epoch 7.45: Loss = 0.400574
Epoch 7.46: Loss = 0.417191
Epoch 7.47: Loss = 0.367935
Epoch 7.48: Loss = 0.431259
Epoch 7.49: Loss = 0.392929
Epoch 7.50: Loss = 0.492783
Epoch 7.51: Loss = 0.323151
Epoch 7.52: Loss = 0.329544
Epoch 7.53: Loss = 0.372971
Epoch 7.54: Loss = 0.494171
Epoch 7.55: Loss = 0.359222
Epoch 7.56: Loss = 0.383087
Epoch 7.57: Loss = 0.399765
Epoch 7.58: Loss = 0.365005
Epoch 7.59: Loss = 0.454727
Epoch 7.60: Loss = 0.486511
Epoch 7.61: Loss = 0.415543
Epoch 7.62: Loss = 0.490005
Epoch 7.63: Loss = 0.561676
Epoch 7.64: Loss = 0.487091
Epoch 7.65: Loss = 0.532379
Epoch 7.66: Loss = 0.394531
Epoch 7.67: Loss = 0.38382
Epoch 7.68: Loss = 0.25502
Epoch 7.69: Loss = 0.309204
Epoch 7.70: Loss = 0.504395
Epoch 7.71: Loss = 0.342499
Epoch 7.72: Loss = 0.307251
Epoch 7.73: Loss = 0.363739
Epoch 7.74: Loss = 0.280426
Epoch 7.75: Loss = 0.608856
Epoch 7.76: Loss = 0.44754
Epoch 7.77: Loss = 0.32608
Epoch 7.78: Loss = 0.385803
Epoch 7.79: Loss = 0.476822
Epoch 7.80: Loss = 0.42392
Epoch 7.81: Loss = 0.349152
Epoch 7.82: Loss = 0.315414
Epoch 7.83: Loss = 0.464951
Epoch 7.84: Loss = 0.400421
Epoch 7.85: Loss = 0.531143
Epoch 7.86: Loss = 0.462646
Epoch 7.87: Loss = 0.316147
Epoch 7.88: Loss = 0.397705
Epoch 7.89: Loss = 0.465851
Epoch 7.90: Loss = 0.339111
Epoch 7.91: Loss = 0.457458
Epoch 7.92: Loss = 0.453979
Epoch 7.93: Loss = 0.526566
Epoch 7.94: Loss = 0.318253
Epoch 7.95: Loss = 0.413055
Epoch 7.96: Loss = 0.461868
Epoch 7.97: Loss = 0.302872
Epoch 7.98: Loss = 0.362732
Epoch 7.99: Loss = 0.480316
Epoch 7.100: Loss = 0.567352
Epoch 7.101: Loss = 0.537186
Epoch 7.102: Loss = 0.41568
Epoch 7.103: Loss = 0.322449
Epoch 7.104: Loss = 0.312637
Epoch 7.105: Loss = 0.475235
Epoch 7.106: Loss = 0.5298
Epoch 7.107: Loss = 0.286301
Epoch 7.108: Loss = 0.446167
Epoch 7.109: Loss = 0.331879
Epoch 7.110: Loss = 0.386414
Epoch 7.111: Loss = 0.317429
Epoch 7.112: Loss = 0.320053
Epoch 7.113: Loss = 0.373047
Epoch 7.114: Loss = 0.312622
Epoch 7.115: Loss = 0.296814
Epoch 7.116: Loss = 0.36026
Epoch 7.117: Loss = 0.244904
Epoch 7.118: Loss = 0.173325
Epoch 7.119: Loss = 0.285309
Epoch 7.120: Loss = 0.317688
TRAIN LOSS = 0.397552
TRAIN ACC = 88.945 % (53370/60000)
Loss = 0.376297
Loss = 0.46875
Loss = 0.53717
Loss = 0.560226
Loss = 0.566193
Loss = 0.392639
Loss = 0.359451
Loss = 0.622787
Loss = 0.500412
Loss = 0.456833
Loss = 0.15773
Loss = 0.323654
Loss = 0.248352
Loss = 0.347321
Loss = 0.206192
Loss = 0.287247
Loss = 0.215668
Loss = 0.0712738
Loss = 0.223541
Loss = 0.517563
TEST LOSS = 0.371965
TEST ACC = 533.699 % (8942/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.354492
Epoch 8.2: Loss = 0.475967
Epoch 8.3: Loss = 0.492386
Epoch 8.4: Loss = 0.300446
Epoch 8.5: Loss = 0.349533
Epoch 8.6: Loss = 0.361694
Epoch 8.7: Loss = 0.345322
Epoch 8.8: Loss = 0.325256
Epoch 8.9: Loss = 0.340012
Epoch 8.10: Loss = 0.335556
Epoch 8.11: Loss = 0.373291
Epoch 8.12: Loss = 0.361877
Epoch 8.13: Loss = 0.28244
Epoch 8.14: Loss = 0.331619
Epoch 8.15: Loss = 0.430847
Epoch 8.16: Loss = 0.346634
Epoch 8.17: Loss = 0.466232
Epoch 8.18: Loss = 0.579422
Epoch 8.19: Loss = 0.423706
Epoch 8.20: Loss = 0.329865
Epoch 8.21: Loss = 0.285416
Epoch 8.22: Loss = 0.267502
Epoch 8.23: Loss = 0.310593
Epoch 8.24: Loss = 0.533203
Epoch 8.25: Loss = 0.407166
Epoch 8.26: Loss = 0.495544
Epoch 8.27: Loss = 0.460281
Epoch 8.28: Loss = 0.492126
Epoch 8.29: Loss = 0.512314
Epoch 8.30: Loss = 0.5634
Epoch 8.31: Loss = 0.347488
Epoch 8.32: Loss = 0.423798
Epoch 8.33: Loss = 0.354599
Epoch 8.34: Loss = 0.401489
Epoch 8.35: Loss = 0.383621
Epoch 8.36: Loss = 0.499084
Epoch 8.37: Loss = 0.278366
Epoch 8.38: Loss = 0.326172
Epoch 8.39: Loss = 0.346237
Epoch 8.40: Loss = 0.360199
Epoch 8.41: Loss = 0.396042
Epoch 8.42: Loss = 0.617676
Epoch 8.43: Loss = 0.367783
Epoch 8.44: Loss = 0.289536
Epoch 8.45: Loss = 0.372162
Epoch 8.46: Loss = 0.434433
Epoch 8.47: Loss = 0.391144
Epoch 8.48: Loss = 0.420532
Epoch 8.49: Loss = 0.395294
Epoch 8.50: Loss = 0.47522
Epoch 8.51: Loss = 0.333847
Epoch 8.52: Loss = 0.335342
Epoch 8.53: Loss = 0.357788
Epoch 8.54: Loss = 0.49942
Epoch 8.55: Loss = 0.34288
Epoch 8.56: Loss = 0.397324
Epoch 8.57: Loss = 0.385986
Epoch 8.58: Loss = 0.353989
Epoch 8.59: Loss = 0.46843
Epoch 8.60: Loss = 0.477524
Epoch 8.61: Loss = 0.441376
Epoch 8.62: Loss = 0.498093
Epoch 8.63: Loss = 0.579865
Epoch 8.64: Loss = 0.504242
Epoch 8.65: Loss = 0.545181
Epoch 8.66: Loss = 0.395569
Epoch 8.67: Loss = 0.398438
Epoch 8.68: Loss = 0.242325
Epoch 8.69: Loss = 0.313675
Epoch 8.70: Loss = 0.524719
Epoch 8.71: Loss = 0.332748
Epoch 8.72: Loss = 0.304871
Epoch 8.73: Loss = 0.386124
Epoch 8.74: Loss = 0.297348
Epoch 8.75: Loss = 0.619858
Epoch 8.76: Loss = 0.438522
Epoch 8.77: Loss = 0.334091
Epoch 8.78: Loss = 0.404419
Epoch 8.79: Loss = 0.465805
Epoch 8.80: Loss = 0.40303
Epoch 8.81: Loss = 0.360565
Epoch 8.82: Loss = 0.322891
Epoch 8.83: Loss = 0.475845
Epoch 8.84: Loss = 0.411392
Epoch 8.85: Loss = 0.532043
Epoch 8.86: Loss = 0.456497
Epoch 8.87: Loss = 0.311417
Epoch 8.88: Loss = 0.383163
Epoch 8.89: Loss = 0.467529
Epoch 8.90: Loss = 0.328384
Epoch 8.91: Loss = 0.471008
Epoch 8.92: Loss = 0.471283
Epoch 8.93: Loss = 0.546555
Epoch 8.94: Loss = 0.310196
Epoch 8.95: Loss = 0.413483
Epoch 8.96: Loss = 0.468796
Epoch 8.97: Loss = 0.290039
Epoch 8.98: Loss = 0.368118
Epoch 8.99: Loss = 0.480606
Epoch 8.100: Loss = 0.558792
Epoch 8.101: Loss = 0.542053
Epoch 8.102: Loss = 0.419189
Epoch 8.103: Loss = 0.314743
Epoch 8.104: Loss = 0.306198
Epoch 8.105: Loss = 0.469101
Epoch 8.106: Loss = 0.551132
Epoch 8.107: Loss = 0.304001
Epoch 8.108: Loss = 0.455292
Epoch 8.109: Loss = 0.333008
Epoch 8.110: Loss = 0.386307
Epoch 8.111: Loss = 0.310089
Epoch 8.112: Loss = 0.317429
Epoch 8.113: Loss = 0.35907
Epoch 8.114: Loss = 0.30603
Epoch 8.115: Loss = 0.292328
Epoch 8.116: Loss = 0.353287
Epoch 8.117: Loss = 0.244537
Epoch 8.118: Loss = 0.191498
Epoch 8.119: Loss = 0.295593
Epoch 8.120: Loss = 0.309967
TRAIN LOSS = 0.397385
TRAIN ACC = 89.0808 % (53451/60000)
Loss = 0.366669
Loss = 0.470139
Loss = 0.52916
Loss = 0.5513
Loss = 0.564804
Loss = 0.398758
Loss = 0.356583
Loss = 0.622787
Loss = 0.522079
Loss = 0.451721
Loss = 0.153732
Loss = 0.321808
Loss = 0.265778
Loss = 0.349457
Loss = 0.189468
Loss = 0.2556
Loss = 0.192276
Loss = 0.0571747
Loss = 0.217026
Loss = 0.509323
TEST LOSS = 0.367282
TEST ACC = 534.509 % (8991/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.354584
Epoch 9.2: Loss = 0.485977
Epoch 9.3: Loss = 0.48175
Epoch 9.4: Loss = 0.297058
Epoch 9.5: Loss = 0.334
Epoch 9.6: Loss = 0.374634
Epoch 9.7: Loss = 0.335541
Epoch 9.8: Loss = 0.327621
Epoch 9.9: Loss = 0.348526
Epoch 9.10: Loss = 0.345291
Epoch 9.11: Loss = 0.372818
Epoch 9.12: Loss = 0.372482
Epoch 9.13: Loss = 0.277008
Epoch 9.14: Loss = 0.320908
Epoch 9.15: Loss = 0.421371
Epoch 9.16: Loss = 0.368378
Epoch 9.17: Loss = 0.462143
Epoch 9.18: Loss = 0.598251
Epoch 9.19: Loss = 0.430725
Epoch 9.20: Loss = 0.324432
Epoch 9.21: Loss = 0.285568
Epoch 9.22: Loss = 0.271423
Epoch 9.23: Loss = 0.298584
Epoch 9.24: Loss = 0.531128
Epoch 9.25: Loss = 0.417343
Epoch 9.26: Loss = 0.519424
Epoch 9.27: Loss = 0.457779
Epoch 9.28: Loss = 0.48674
Epoch 9.29: Loss = 0.531357
Epoch 9.30: Loss = 0.53801
Epoch 9.31: Loss = 0.339676
Epoch 9.32: Loss = 0.415558
Epoch 9.33: Loss = 0.332092
Epoch 9.34: Loss = 0.408112
Epoch 9.35: Loss = 0.390884
Epoch 9.36: Loss = 0.529129
Epoch 9.37: Loss = 0.273346
Epoch 9.38: Loss = 0.330826
Epoch 9.39: Loss = 0.353607
Epoch 9.40: Loss = 0.366226
Epoch 9.41: Loss = 0.385162
Epoch 9.42: Loss = 0.607788
Epoch 9.43: Loss = 0.356613
Epoch 9.44: Loss = 0.286621
Epoch 9.45: Loss = 0.371796
Epoch 9.46: Loss = 0.438782
Epoch 9.47: Loss = 0.390366
Epoch 9.48: Loss = 0.415985
Epoch 9.49: Loss = 0.392654
Epoch 9.50: Loss = 0.453033
Epoch 9.51: Loss = 0.322403
Epoch 9.52: Loss = 0.345596
Epoch 9.53: Loss = 0.354568
Epoch 9.54: Loss = 0.496124
Epoch 9.55: Loss = 0.346649
Epoch 9.56: Loss = 0.388229
Epoch 9.57: Loss = 0.405472
Epoch 9.58: Loss = 0.344772
Epoch 9.59: Loss = 0.45723
Epoch 9.60: Loss = 0.468628
Epoch 9.61: Loss = 0.419235
Epoch 9.62: Loss = 0.490921
Epoch 9.63: Loss = 0.562729
Epoch 9.64: Loss = 0.505203
Epoch 9.65: Loss = 0.528534
Epoch 9.66: Loss = 0.393646
Epoch 9.67: Loss = 0.379639
Epoch 9.68: Loss = 0.226776
Epoch 9.69: Loss = 0.319885
Epoch 9.70: Loss = 0.517181
Epoch 9.71: Loss = 0.344757
Epoch 9.72: Loss = 0.30748
Epoch 9.73: Loss = 0.379608
Epoch 9.74: Loss = 0.299896
Epoch 9.75: Loss = 0.59523
Epoch 9.76: Loss = 0.438843
Epoch 9.77: Loss = 0.324768
Epoch 9.78: Loss = 0.397278
Epoch 9.79: Loss = 0.468658
Epoch 9.80: Loss = 0.408264
Epoch 9.81: Loss = 0.351944
Epoch 9.82: Loss = 0.324158
Epoch 9.83: Loss = 0.487747
Epoch 9.84: Loss = 0.401489
Epoch 9.85: Loss = 0.536728
Epoch 9.86: Loss = 0.469055
Epoch 9.87: Loss = 0.296097
Epoch 9.88: Loss = 0.374496
Epoch 9.89: Loss = 0.464066
Epoch 9.90: Loss = 0.325287
Epoch 9.91: Loss = 0.475601
Epoch 9.92: Loss = 0.468491
Epoch 9.93: Loss = 0.564133
Epoch 9.94: Loss = 0.314316
Epoch 9.95: Loss = 0.413406
Epoch 9.96: Loss = 0.485428
Epoch 9.97: Loss = 0.299454
Epoch 9.98: Loss = 0.370178
Epoch 9.99: Loss = 0.45723
Epoch 9.100: Loss = 0.574112
Epoch 9.101: Loss = 0.558624
Epoch 9.102: Loss = 0.445755
Epoch 9.103: Loss = 0.306931
Epoch 9.104: Loss = 0.312683
Epoch 9.105: Loss = 0.488037
Epoch 9.106: Loss = 0.554779
Epoch 9.107: Loss = 0.300003
Epoch 9.108: Loss = 0.468307
Epoch 9.109: Loss = 0.331009
Epoch 9.110: Loss = 0.388931
Epoch 9.111: Loss = 0.317963
Epoch 9.112: Loss = 0.336182
Epoch 9.113: Loss = 0.359619
Epoch 9.114: Loss = 0.309952
Epoch 9.115: Loss = 0.292007
Epoch 9.116: Loss = 0.339737
Epoch 9.117: Loss = 0.24231
Epoch 9.118: Loss = 0.186188
Epoch 9.119: Loss = 0.275986
Epoch 9.120: Loss = 0.323669
TRAIN LOSS = 0.396988
TRAIN ACC = 89.2136 % (53530/60000)
Loss = 0.377228
Loss = 0.4776
Loss = 0.548477
Loss = 0.576324
Loss = 0.581284
Loss = 0.387329
Loss = 0.368454
Loss = 0.605133
Loss = 0.522766
Loss = 0.470734
Loss = 0.160202
Loss = 0.325928
Loss = 0.268402
Loss = 0.373184
Loss = 0.20105
Loss = 0.255569
Loss = 0.205414
Loss = 0.0610504
Loss = 0.222992
Loss = 0.546371
TEST LOSS = 0.376774
TEST ACC = 535.3 % (8976/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.35675
Epoch 10.2: Loss = 0.475616
Epoch 10.3: Loss = 0.485443
Epoch 10.4: Loss = 0.30687
Epoch 10.5: Loss = 0.344009
Epoch 10.6: Loss = 0.384842
Epoch 10.7: Loss = 0.342087
Epoch 10.8: Loss = 0.308929
Epoch 10.9: Loss = 0.37529
Epoch 10.10: Loss = 0.371216
Epoch 10.11: Loss = 0.37764
Epoch 10.12: Loss = 0.379562
Epoch 10.13: Loss = 0.294373
Epoch 10.14: Loss = 0.316635
Epoch 10.15: Loss = 0.410507
Epoch 10.16: Loss = 0.369202
Epoch 10.17: Loss = 0.478836
Epoch 10.18: Loss = 0.61441
Epoch 10.19: Loss = 0.419495
Epoch 10.20: Loss = 0.3181
Epoch 10.21: Loss = 0.2892
Epoch 10.22: Loss = 0.27916
Epoch 10.23: Loss = 0.295258
Epoch 10.24: Loss = 0.577408
Epoch 10.25: Loss = 0.432846
Epoch 10.26: Loss = 0.511185
Epoch 10.27: Loss = 0.490051
Epoch 10.28: Loss = 0.497803
Epoch 10.29: Loss = 0.524277
Epoch 10.30: Loss = 0.550964
Epoch 10.31: Loss = 0.353363
Epoch 10.32: Loss = 0.414703
Epoch 10.33: Loss = 0.343979
Epoch 10.34: Loss = 0.412109
Epoch 10.35: Loss = 0.405212
Epoch 10.36: Loss = 0.527954
Epoch 10.37: Loss = 0.275909
Epoch 10.38: Loss = 0.333374
Epoch 10.39: Loss = 0.345642
Epoch 10.40: Loss = 0.369965
Epoch 10.41: Loss = 0.40387
Epoch 10.42: Loss = 0.62796
Epoch 10.43: Loss = 0.361389
Epoch 10.44: Loss = 0.293015
Epoch 10.45: Loss = 0.379425
Epoch 10.46: Loss = 0.471588
Epoch 10.47: Loss = 0.389618
Epoch 10.48: Loss = 0.428009
Epoch 10.49: Loss = 0.409729
Epoch 10.50: Loss = 0.482056
Epoch 10.51: Loss = 0.318283
Epoch 10.52: Loss = 0.343307
Epoch 10.53: Loss = 0.366516
Epoch 10.54: Loss = 0.505554
Epoch 10.55: Loss = 0.379044
Epoch 10.56: Loss = 0.372681
Epoch 10.57: Loss = 0.404785
Epoch 10.58: Loss = 0.357498
Epoch 10.59: Loss = 0.444443
Epoch 10.60: Loss = 0.491577
Epoch 10.61: Loss = 0.411194
Epoch 10.62: Loss = 0.478653
Epoch 10.63: Loss = 0.57933
Epoch 10.64: Loss = 0.499268
Epoch 10.65: Loss = 0.538605
Epoch 10.66: Loss = 0.412582
Epoch 10.67: Loss = 0.375366
Epoch 10.68: Loss = 0.231873
Epoch 10.69: Loss = 0.312119
Epoch 10.70: Loss = 0.52803
Epoch 10.71: Loss = 0.343796
Epoch 10.72: Loss = 0.308289
Epoch 10.73: Loss = 0.394028
Epoch 10.74: Loss = 0.290146
Epoch 10.75: Loss = 0.641556
Epoch 10.76: Loss = 0.466507
Epoch 10.77: Loss = 0.333267
Epoch 10.78: Loss = 0.40657
Epoch 10.79: Loss = 0.475906
Epoch 10.80: Loss = 0.424789
Epoch 10.81: Loss = 0.362289
Epoch 10.82: Loss = 0.314087
Epoch 10.83: Loss = 0.495712
Epoch 10.84: Loss = 0.406906
Epoch 10.85: Loss = 0.595322
Epoch 10.86: Loss = 0.479675
Epoch 10.87: Loss = 0.289444
Epoch 10.88: Loss = 0.361206
Epoch 10.89: Loss = 0.466919
Epoch 10.90: Loss = 0.325317
Epoch 10.91: Loss = 0.499313
Epoch 10.92: Loss = 0.465607
Epoch 10.93: Loss = 0.549042
Epoch 10.94: Loss = 0.312332
Epoch 10.95: Loss = 0.415573
Epoch 10.96: Loss = 0.482086
Epoch 10.97: Loss = 0.307144
Epoch 10.98: Loss = 0.374298
Epoch 10.99: Loss = 0.44635
Epoch 10.100: Loss = 0.596863
Epoch 10.101: Loss = 0.550308
Epoch 10.102: Loss = 0.434158
Epoch 10.103: Loss = 0.327545
Epoch 10.104: Loss = 0.318451
Epoch 10.105: Loss = 0.501678
Epoch 10.106: Loss = 0.584503
Epoch 10.107: Loss = 0.292725
Epoch 10.108: Loss = 0.447906
Epoch 10.109: Loss = 0.328888
Epoch 10.110: Loss = 0.398056
Epoch 10.111: Loss = 0.329453
Epoch 10.112: Loss = 0.345261
Epoch 10.113: Loss = 0.365982
Epoch 10.114: Loss = 0.306488
Epoch 10.115: Loss = 0.284439
Epoch 10.116: Loss = 0.347321
Epoch 10.117: Loss = 0.229721
Epoch 10.118: Loss = 0.18721
Epoch 10.119: Loss = 0.293945
Epoch 10.120: Loss = 0.336365
TRAIN LOSS = 0.403305
TRAIN ACC = 89.3234 % (53597/60000)
Loss = 0.357269
Loss = 0.464676
Loss = 0.544708
Loss = 0.601303
Loss = 0.587265
Loss = 0.388535
Loss = 0.379227
Loss = 0.595673
Loss = 0.521896
Loss = 0.462082
Loss = 0.18129
Loss = 0.324066
Loss = 0.286346
Loss = 0.405182
Loss = 0.197021
Loss = 0.262665
Loss = 0.222595
Loss = 0.0568848
Loss = 0.233444
Loss = 0.552658
TEST LOSS = 0.381239
TEST ACC = 535.97 % (8981/10000)
