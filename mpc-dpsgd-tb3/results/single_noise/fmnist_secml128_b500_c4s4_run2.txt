Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.32594
Epoch 1.2: Loss = 2.22009
Epoch 1.3: Loss = 2.16219
Epoch 1.4: Loss = 2.08807
Epoch 1.5: Loss = 2.04823
Epoch 1.6: Loss = 1.99751
Epoch 1.7: Loss = 1.95401
Epoch 1.8: Loss = 1.89961
Epoch 1.9: Loss = 1.83481
Epoch 1.10: Loss = 1.79385
Epoch 1.11: Loss = 1.81905
Epoch 1.12: Loss = 1.73959
Epoch 1.13: Loss = 1.69948
Epoch 1.14: Loss = 1.67061
Epoch 1.15: Loss = 1.6012
Epoch 1.16: Loss = 1.60446
Epoch 1.17: Loss = 1.57401
Epoch 1.18: Loss = 1.53711
Epoch 1.19: Loss = 1.48895
Epoch 1.20: Loss = 1.47736
Epoch 1.21: Loss = 1.43637
Epoch 1.22: Loss = 1.39809
Epoch 1.23: Loss = 1.37001
Epoch 1.24: Loss = 1.42108
Epoch 1.25: Loss = 1.33395
Epoch 1.26: Loss = 1.2746
Epoch 1.27: Loss = 1.25795
Epoch 1.28: Loss = 1.27809
Epoch 1.29: Loss = 1.25638
Epoch 1.30: Loss = 1.2415
Epoch 1.31: Loss = 1.25874
Epoch 1.32: Loss = 1.20523
Epoch 1.33: Loss = 1.1123
Epoch 1.34: Loss = 1.18324
Epoch 1.35: Loss = 1.21747
Epoch 1.36: Loss = 1.17705
Epoch 1.37: Loss = 1.1591
Epoch 1.38: Loss = 1.11563
Epoch 1.39: Loss = 1.09552
Epoch 1.40: Loss = 1.09244
Epoch 1.41: Loss = 1.14006
Epoch 1.42: Loss = 1.06192
Epoch 1.43: Loss = 1.032
Epoch 1.44: Loss = 1.00897
Epoch 1.45: Loss = 1.03371
Epoch 1.46: Loss = 1.04543
Epoch 1.47: Loss = 1.01129
Epoch 1.48: Loss = 0.994049
Epoch 1.49: Loss = 1.02336
Epoch 1.50: Loss = 0.979248
Epoch 1.51: Loss = 0.947708
Epoch 1.52: Loss = 1.03339
Epoch 1.53: Loss = 1.03445
Epoch 1.54: Loss = 0.87619
Epoch 1.55: Loss = 0.981598
Epoch 1.56: Loss = 0.99379
Epoch 1.57: Loss = 1.00166
Epoch 1.58: Loss = 0.955948
Epoch 1.59: Loss = 0.978821
Epoch 1.60: Loss = 0.985107
Epoch 1.61: Loss = 0.872131
Epoch 1.62: Loss = 0.95546
Epoch 1.63: Loss = 0.844772
Epoch 1.64: Loss = 0.866013
Epoch 1.65: Loss = 0.890442
Epoch 1.66: Loss = 0.897888
Epoch 1.67: Loss = 0.821274
Epoch 1.68: Loss = 0.961731
Epoch 1.69: Loss = 0.908356
Epoch 1.70: Loss = 0.873657
Epoch 1.71: Loss = 0.803635
Epoch 1.72: Loss = 0.818741
Epoch 1.73: Loss = 0.923767
Epoch 1.74: Loss = 0.900467
Epoch 1.75: Loss = 0.849335
Epoch 1.76: Loss = 0.866348
Epoch 1.77: Loss = 0.822357
Epoch 1.78: Loss = 0.845657
Epoch 1.79: Loss = 0.768311
Epoch 1.80: Loss = 0.832504
Epoch 1.81: Loss = 0.8004
Epoch 1.82: Loss = 0.835342
Epoch 1.83: Loss = 0.860947
Epoch 1.84: Loss = 0.83168
Epoch 1.85: Loss = 0.800949
Epoch 1.86: Loss = 0.881561
Epoch 1.87: Loss = 0.890732
Epoch 1.88: Loss = 0.750229
Epoch 1.89: Loss = 0.873581
Epoch 1.90: Loss = 0.808517
Epoch 1.91: Loss = 0.883636
Epoch 1.92: Loss = 0.842728
Epoch 1.93: Loss = 0.847641
Epoch 1.94: Loss = 0.801849
Epoch 1.95: Loss = 0.847321
Epoch 1.96: Loss = 0.790146
Epoch 1.97: Loss = 0.691223
Epoch 1.98: Loss = 0.817474
Epoch 1.99: Loss = 0.794189
Epoch 1.100: Loss = 0.761917
Epoch 1.101: Loss = 0.845001
Epoch 1.102: Loss = 0.827301
Epoch 1.103: Loss = 0.811142
Epoch 1.104: Loss = 0.783997
Epoch 1.105: Loss = 0.731125
Epoch 1.106: Loss = 0.884827
Epoch 1.107: Loss = 0.812851
Epoch 1.108: Loss = 0.809906
Epoch 1.109: Loss = 0.788422
Epoch 1.110: Loss = 0.818665
Epoch 1.111: Loss = 0.71875
Epoch 1.112: Loss = 0.715027
Epoch 1.113: Loss = 0.775696
Epoch 1.114: Loss = 0.800278
Epoch 1.115: Loss = 0.795303
Epoch 1.116: Loss = 0.692017
Epoch 1.117: Loss = 0.862595
Epoch 1.118: Loss = 0.716782
Epoch 1.119: Loss = 0.763977
Epoch 1.120: Loss = 0.733582
TRAIN LOSS = 1.09135
TRAIN ACC = 62.4161 % (37451/60000)
Loss = 0.721802
Loss = 0.835938
Loss = 0.817917
Loss = 0.717331
Loss = 0.703064
Loss = 0.891586
Loss = 0.906204
Loss = 0.846313
Loss = 0.773193
Loss = 0.737228
Loss = 0.835205
Loss = 0.80632
Loss = 0.793701
Loss = 0.806961
Loss = 0.796524
Loss = 0.838776
Loss = 0.751816
Loss = 0.810913
Loss = 0.833496
Loss = 0.807632
TEST LOSS = 0.801596
TEST ACC = 374.509 % (7094/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.803619
Epoch 2.2: Loss = 0.743698
Epoch 2.3: Loss = 0.821579
Epoch 2.4: Loss = 0.695663
Epoch 2.5: Loss = 0.763107
Epoch 2.6: Loss = 0.8573
Epoch 2.7: Loss = 0.776245
Epoch 2.8: Loss = 0.824677
Epoch 2.9: Loss = 0.707031
Epoch 2.10: Loss = 0.624359
Epoch 2.11: Loss = 0.843719
Epoch 2.12: Loss = 0.780014
Epoch 2.13: Loss = 0.76796
Epoch 2.14: Loss = 0.773438
Epoch 2.15: Loss = 0.755127
Epoch 2.16: Loss = 0.82225
Epoch 2.17: Loss = 0.724701
Epoch 2.18: Loss = 0.758438
Epoch 2.19: Loss = 0.752625
Epoch 2.20: Loss = 0.862732
Epoch 2.21: Loss = 0.737442
Epoch 2.22: Loss = 0.651367
Epoch 2.23: Loss = 0.757813
Epoch 2.24: Loss = 0.859695
Epoch 2.25: Loss = 0.737274
Epoch 2.26: Loss = 0.684357
Epoch 2.27: Loss = 0.740753
Epoch 2.28: Loss = 0.750885
Epoch 2.29: Loss = 0.770782
Epoch 2.30: Loss = 0.744888
Epoch 2.31: Loss = 0.828888
Epoch 2.32: Loss = 0.751144
Epoch 2.33: Loss = 0.64119
Epoch 2.34: Loss = 0.81395
Epoch 2.35: Loss = 0.801559
Epoch 2.36: Loss = 0.796143
Epoch 2.37: Loss = 0.792603
Epoch 2.38: Loss = 0.737823
Epoch 2.39: Loss = 0.826233
Epoch 2.40: Loss = 0.742981
Epoch 2.41: Loss = 0.794952
Epoch 2.42: Loss = 0.750275
Epoch 2.43: Loss = 0.731369
Epoch 2.44: Loss = 0.694244
Epoch 2.45: Loss = 0.775101
Epoch 2.46: Loss = 0.844788
Epoch 2.47: Loss = 0.694946
Epoch 2.48: Loss = 0.705292
Epoch 2.49: Loss = 0.769928
Epoch 2.50: Loss = 0.74614
Epoch 2.51: Loss = 0.646011
Epoch 2.52: Loss = 0.792068
Epoch 2.53: Loss = 0.843018
Epoch 2.54: Loss = 0.655334
Epoch 2.55: Loss = 0.764771
Epoch 2.56: Loss = 0.793915
Epoch 2.57: Loss = 0.782196
Epoch 2.58: Loss = 0.779694
Epoch 2.59: Loss = 0.770432
Epoch 2.60: Loss = 0.790237
Epoch 2.61: Loss = 0.670135
Epoch 2.62: Loss = 0.778229
Epoch 2.63: Loss = 0.629501
Epoch 2.64: Loss = 0.663269
Epoch 2.65: Loss = 0.713181
Epoch 2.66: Loss = 0.693283
Epoch 2.67: Loss = 0.666122
Epoch 2.68: Loss = 0.814758
Epoch 2.69: Loss = 0.736908
Epoch 2.70: Loss = 0.75386
Epoch 2.71: Loss = 0.615082
Epoch 2.72: Loss = 0.715836
Epoch 2.73: Loss = 0.827377
Epoch 2.74: Loss = 0.7939
Epoch 2.75: Loss = 0.699295
Epoch 2.76: Loss = 0.669647
Epoch 2.77: Loss = 0.705795
Epoch 2.78: Loss = 0.730438
Epoch 2.79: Loss = 0.675415
Epoch 2.80: Loss = 0.682236
Epoch 2.81: Loss = 0.679718
Epoch 2.82: Loss = 0.667694
Epoch 2.83: Loss = 0.75061
Epoch 2.84: Loss = 0.680588
Epoch 2.85: Loss = 0.681015
Epoch 2.86: Loss = 0.764984
Epoch 2.87: Loss = 0.744141
Epoch 2.88: Loss = 0.624619
Epoch 2.89: Loss = 0.787704
Epoch 2.90: Loss = 0.701569
Epoch 2.91: Loss = 0.79451
Epoch 2.92: Loss = 0.71048
Epoch 2.93: Loss = 0.761765
Epoch 2.94: Loss = 0.731003
Epoch 2.95: Loss = 0.782867
Epoch 2.96: Loss = 0.683884
Epoch 2.97: Loss = 0.600906
Epoch 2.98: Loss = 0.69548
Epoch 2.99: Loss = 0.683029
Epoch 2.100: Loss = 0.68631
Epoch 2.101: Loss = 0.772964
Epoch 2.102: Loss = 0.741104
Epoch 2.103: Loss = 0.69136
Epoch 2.104: Loss = 0.683441
Epoch 2.105: Loss = 0.633881
Epoch 2.106: Loss = 0.801529
Epoch 2.107: Loss = 0.746506
Epoch 2.108: Loss = 0.755905
Epoch 2.109: Loss = 0.739273
Epoch 2.110: Loss = 0.745163
Epoch 2.111: Loss = 0.640244
Epoch 2.112: Loss = 0.656799
Epoch 2.113: Loss = 0.692841
Epoch 2.114: Loss = 0.728333
Epoch 2.115: Loss = 0.731766
Epoch 2.116: Loss = 0.63942
Epoch 2.117: Loss = 0.789688
Epoch 2.118: Loss = 0.65062
Epoch 2.119: Loss = 0.691055
Epoch 2.120: Loss = 0.678162
TRAIN LOSS = 0.736755
TRAIN ACC = 74.7498 % (44852/60000)
Loss = 0.645157
Loss = 0.789795
Loss = 0.74054
Loss = 0.634964
Loss = 0.64949
Loss = 0.826233
Loss = 0.86557
Loss = 0.803299
Loss = 0.727356
Loss = 0.661041
Loss = 0.790833
Loss = 0.785812
Loss = 0.748154
Loss = 0.745483
Loss = 0.743927
Loss = 0.782196
Loss = 0.699814
Loss = 0.761078
Loss = 0.795715
Loss = 0.7547
TEST LOSS = 0.747558
TEST ACC = 448.52 % (7494/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.757721
Epoch 3.2: Loss = 0.687057
Epoch 3.3: Loss = 0.756256
Epoch 3.4: Loss = 0.633072
Epoch 3.5: Loss = 0.6875
Epoch 3.6: Loss = 0.823853
Epoch 3.7: Loss = 0.696503
Epoch 3.8: Loss = 0.770111
Epoch 3.9: Loss = 0.619293
Epoch 3.10: Loss = 0.566803
Epoch 3.11: Loss = 0.81131
Epoch 3.12: Loss = 0.72438
Epoch 3.13: Loss = 0.721802
Epoch 3.14: Loss = 0.724609
Epoch 3.15: Loss = 0.713455
Epoch 3.16: Loss = 0.77002
Epoch 3.17: Loss = 0.687576
Epoch 3.18: Loss = 0.709763
Epoch 3.19: Loss = 0.704346
Epoch 3.20: Loss = 0.822235
Epoch 3.21: Loss = 0.646851
Epoch 3.22: Loss = 0.598892
Epoch 3.23: Loss = 0.705566
Epoch 3.24: Loss = 0.839996
Epoch 3.25: Loss = 0.675873
Epoch 3.26: Loss = 0.641968
Epoch 3.27: Loss = 0.70578
Epoch 3.28: Loss = 0.69397
Epoch 3.29: Loss = 0.73436
Epoch 3.30: Loss = 0.717651
Epoch 3.31: Loss = 0.784058
Epoch 3.32: Loss = 0.713928
Epoch 3.33: Loss = 0.57959
Epoch 3.34: Loss = 0.791855
Epoch 3.35: Loss = 0.757507
Epoch 3.36: Loss = 0.756042
Epoch 3.37: Loss = 0.761398
Epoch 3.38: Loss = 0.711777
Epoch 3.39: Loss = 0.785187
Epoch 3.40: Loss = 0.718124
Epoch 3.41: Loss = 0.776932
Epoch 3.42: Loss = 0.720963
Epoch 3.43: Loss = 0.722794
Epoch 3.44: Loss = 0.629532
Epoch 3.45: Loss = 0.731888
Epoch 3.46: Loss = 0.825592
Epoch 3.47: Loss = 0.655258
Epoch 3.48: Loss = 0.662857
Epoch 3.49: Loss = 0.744553
Epoch 3.50: Loss = 0.720734
Epoch 3.51: Loss = 0.594345
Epoch 3.52: Loss = 0.74501
Epoch 3.53: Loss = 0.8246
Epoch 3.54: Loss = 0.601074
Epoch 3.55: Loss = 0.738724
Epoch 3.56: Loss = 0.766846
Epoch 3.57: Loss = 0.764542
Epoch 3.58: Loss = 0.750061
Epoch 3.59: Loss = 0.775696
Epoch 3.60: Loss = 0.783813
Epoch 3.61: Loss = 0.656845
Epoch 3.62: Loss = 0.769394
Epoch 3.63: Loss = 0.616394
Epoch 3.64: Loss = 0.621689
Epoch 3.65: Loss = 0.729156
Epoch 3.66: Loss = 0.649521
Epoch 3.67: Loss = 0.674423
Epoch 3.68: Loss = 0.791397
Epoch 3.69: Loss = 0.749695
Epoch 3.70: Loss = 0.761078
Epoch 3.71: Loss = 0.614929
Epoch 3.72: Loss = 0.701797
Epoch 3.73: Loss = 0.823746
Epoch 3.74: Loss = 0.754333
Epoch 3.75: Loss = 0.68512
Epoch 3.76: Loss = 0.665802
Epoch 3.77: Loss = 0.690262
Epoch 3.78: Loss = 0.724304
Epoch 3.79: Loss = 0.68602
Epoch 3.80: Loss = 0.655365
Epoch 3.81: Loss = 0.673248
Epoch 3.82: Loss = 0.676437
Epoch 3.83: Loss = 0.768555
Epoch 3.84: Loss = 0.690796
Epoch 3.85: Loss = 0.681442
Epoch 3.86: Loss = 0.774292
Epoch 3.87: Loss = 0.726517
Epoch 3.88: Loss = 0.614471
Epoch 3.89: Loss = 0.788467
Epoch 3.90: Loss = 0.703125
Epoch 3.91: Loss = 0.786438
Epoch 3.92: Loss = 0.722733
Epoch 3.93: Loss = 0.731384
Epoch 3.94: Loss = 0.692001
Epoch 3.95: Loss = 0.747177
Epoch 3.96: Loss = 0.677597
Epoch 3.97: Loss = 0.577957
Epoch 3.98: Loss = 0.698715
Epoch 3.99: Loss = 0.708359
Epoch 3.100: Loss = 0.698196
Epoch 3.101: Loss = 0.770813
Epoch 3.102: Loss = 0.76712
Epoch 3.103: Loss = 0.672623
Epoch 3.104: Loss = 0.700897
Epoch 3.105: Loss = 0.641541
Epoch 3.106: Loss = 0.793167
Epoch 3.107: Loss = 0.772675
Epoch 3.108: Loss = 0.774567
Epoch 3.109: Loss = 0.757111
Epoch 3.110: Loss = 0.748657
Epoch 3.111: Loss = 0.650223
Epoch 3.112: Loss = 0.665741
Epoch 3.113: Loss = 0.699921
Epoch 3.114: Loss = 0.732437
Epoch 3.115: Loss = 0.738449
Epoch 3.116: Loss = 0.658386
Epoch 3.117: Loss = 0.804077
Epoch 3.118: Loss = 0.638412
Epoch 3.119: Loss = 0.717941
Epoch 3.120: Loss = 0.676483
TRAIN LOSS = 0.714874
TRAIN ACC = 76.5167 % (45912/60000)
Loss = 0.649017
Loss = 0.829056
Loss = 0.730789
Loss = 0.618988
Loss = 0.661377
Loss = 0.843735
Loss = 0.915024
Loss = 0.790695
Loss = 0.756592
Loss = 0.667526
Loss = 0.80658
Loss = 0.867401
Loss = 0.769745
Loss = 0.788361
Loss = 0.766388
Loss = 0.797165
Loss = 0.727188
Loss = 0.804886
Loss = 0.832474
Loss = 0.784958
TEST LOSS = 0.770397
TEST ACC = 459.119 % (7594/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.735672
Epoch 4.2: Loss = 0.690063
Epoch 4.3: Loss = 0.731964
Epoch 4.4: Loss = 0.625076
Epoch 4.5: Loss = 0.673752
Epoch 4.6: Loss = 0.809998
Epoch 4.7: Loss = 0.707642
Epoch 4.8: Loss = 0.795364
Epoch 4.9: Loss = 0.610519
Epoch 4.10: Loss = 0.537796
Epoch 4.11: Loss = 0.825333
Epoch 4.12: Loss = 0.726135
Epoch 4.13: Loss = 0.700943
Epoch 4.14: Loss = 0.711334
Epoch 4.15: Loss = 0.694519
Epoch 4.16: Loss = 0.756439
Epoch 4.17: Loss = 0.647583
Epoch 4.18: Loss = 0.689911
Epoch 4.19: Loss = 0.709763
Epoch 4.20: Loss = 0.837875
Epoch 4.21: Loss = 0.636093
Epoch 4.22: Loss = 0.566879
Epoch 4.23: Loss = 0.708221
Epoch 4.24: Loss = 0.84436
Epoch 4.25: Loss = 0.639053
Epoch 4.26: Loss = 0.648148
Epoch 4.27: Loss = 0.676529
Epoch 4.28: Loss = 0.697281
Epoch 4.29: Loss = 0.723175
Epoch 4.30: Loss = 0.711334
Epoch 4.31: Loss = 0.772842
Epoch 4.32: Loss = 0.733566
Epoch 4.33: Loss = 0.571136
Epoch 4.34: Loss = 0.792633
Epoch 4.35: Loss = 0.743958
Epoch 4.36: Loss = 0.770554
Epoch 4.37: Loss = 0.765778
Epoch 4.38: Loss = 0.718964
Epoch 4.39: Loss = 0.791626
Epoch 4.40: Loss = 0.674286
Epoch 4.41: Loss = 0.750549
Epoch 4.42: Loss = 0.68634
Epoch 4.43: Loss = 0.71965
Epoch 4.44: Loss = 0.613266
Epoch 4.45: Loss = 0.733231
Epoch 4.46: Loss = 0.812531
Epoch 4.47: Loss = 0.659241
Epoch 4.48: Loss = 0.619385
Epoch 4.49: Loss = 0.707016
Epoch 4.50: Loss = 0.713516
Epoch 4.51: Loss = 0.56958
Epoch 4.52: Loss = 0.740051
Epoch 4.53: Loss = 0.800491
Epoch 4.54: Loss = 0.576782
Epoch 4.55: Loss = 0.739212
Epoch 4.56: Loss = 0.728638
Epoch 4.57: Loss = 0.75705
Epoch 4.58: Loss = 0.715088
Epoch 4.59: Loss = 0.789902
Epoch 4.60: Loss = 0.791351
Epoch 4.61: Loss = 0.633606
Epoch 4.62: Loss = 0.731949
Epoch 4.63: Loss = 0.614532
Epoch 4.64: Loss = 0.580032
Epoch 4.65: Loss = 0.728668
Epoch 4.66: Loss = 0.622665
Epoch 4.67: Loss = 0.690903
Epoch 4.68: Loss = 0.815643
Epoch 4.69: Loss = 0.747742
Epoch 4.70: Loss = 0.751907
Epoch 4.71: Loss = 0.590164
Epoch 4.72: Loss = 0.697021
Epoch 4.73: Loss = 0.807297
Epoch 4.74: Loss = 0.728882
Epoch 4.75: Loss = 0.658356
Epoch 4.76: Loss = 0.650986
Epoch 4.77: Loss = 0.679245
Epoch 4.78: Loss = 0.724899
Epoch 4.79: Loss = 0.684006
Epoch 4.80: Loss = 0.64238
Epoch 4.81: Loss = 0.642761
Epoch 4.82: Loss = 0.662399
Epoch 4.83: Loss = 0.736496
Epoch 4.84: Loss = 0.66539
Epoch 4.85: Loss = 0.682892
Epoch 4.86: Loss = 0.747849
Epoch 4.87: Loss = 0.689255
Epoch 4.88: Loss = 0.613297
Epoch 4.89: Loss = 0.800232
Epoch 4.90: Loss = 0.705383
Epoch 4.91: Loss = 0.743225
Epoch 4.92: Loss = 0.709045
Epoch 4.93: Loss = 0.733353
Epoch 4.94: Loss = 0.699707
Epoch 4.95: Loss = 0.755234
Epoch 4.96: Loss = 0.668442
Epoch 4.97: Loss = 0.570908
Epoch 4.98: Loss = 0.663132
Epoch 4.99: Loss = 0.671997
Epoch 4.100: Loss = 0.683319
Epoch 4.101: Loss = 0.738876
Epoch 4.102: Loss = 0.739624
Epoch 4.103: Loss = 0.655487
Epoch 4.104: Loss = 0.660797
Epoch 4.105: Loss = 0.611801
Epoch 4.106: Loss = 0.775772
Epoch 4.107: Loss = 0.770462
Epoch 4.108: Loss = 0.785965
Epoch 4.109: Loss = 0.754547
Epoch 4.110: Loss = 0.732071
Epoch 4.111: Loss = 0.637741
Epoch 4.112: Loss = 0.67453
Epoch 4.113: Loss = 0.682602
Epoch 4.114: Loss = 0.717087
Epoch 4.115: Loss = 0.710541
Epoch 4.116: Loss = 0.650589
Epoch 4.117: Loss = 0.763092
Epoch 4.118: Loss = 0.627686
Epoch 4.119: Loss = 0.681808
Epoch 4.120: Loss = 0.649689
TRAIN LOSS = 0.702286
TRAIN ACC = 77.71 % (46628/60000)
Loss = 0.632202
Loss = 0.778946
Loss = 0.695679
Loss = 0.583588
Loss = 0.644669
Loss = 0.813629
Loss = 0.94252
Loss = 0.791107
Loss = 0.707275
Loss = 0.642532
Loss = 0.811935
Loss = 0.826355
Loss = 0.728745
Loss = 0.734116
Loss = 0.731476
Loss = 0.728333
Loss = 0.709518
Loss = 0.746033
Loss = 0.782593
Loss = 0.751328
TEST LOSS = 0.739129
TEST ACC = 466.28 % (7706/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.698227
Epoch 5.2: Loss = 0.699402
Epoch 5.3: Loss = 0.720978
Epoch 5.4: Loss = 0.600998
Epoch 5.5: Loss = 0.663055
Epoch 5.6: Loss = 0.78804
Epoch 5.7: Loss = 0.700516
Epoch 5.8: Loss = 0.778198
Epoch 5.9: Loss = 0.598038
Epoch 5.10: Loss = 0.539368
Epoch 5.11: Loss = 0.789612
Epoch 5.12: Loss = 0.734711
Epoch 5.13: Loss = 0.712021
Epoch 5.14: Loss = 0.700073
Epoch 5.15: Loss = 0.696365
Epoch 5.16: Loss = 0.751556
Epoch 5.17: Loss = 0.634979
Epoch 5.18: Loss = 0.689743
Epoch 5.19: Loss = 0.676743
Epoch 5.20: Loss = 0.797256
Epoch 5.21: Loss = 0.615936
Epoch 5.22: Loss = 0.533752
Epoch 5.23: Loss = 0.677032
Epoch 5.24: Loss = 0.819427
Epoch 5.25: Loss = 0.634933
Epoch 5.26: Loss = 0.637314
Epoch 5.27: Loss = 0.6548
Epoch 5.28: Loss = 0.687103
Epoch 5.29: Loss = 0.691193
Epoch 5.30: Loss = 0.673065
Epoch 5.31: Loss = 0.809891
Epoch 5.32: Loss = 0.701859
Epoch 5.33: Loss = 0.544815
Epoch 5.34: Loss = 0.780609
Epoch 5.35: Loss = 0.719131
Epoch 5.36: Loss = 0.74173
Epoch 5.37: Loss = 0.765839
Epoch 5.38: Loss = 0.688858
Epoch 5.39: Loss = 0.776962
Epoch 5.40: Loss = 0.680283
Epoch 5.41: Loss = 0.742035
Epoch 5.42: Loss = 0.675171
Epoch 5.43: Loss = 0.701492
Epoch 5.44: Loss = 0.588257
Epoch 5.45: Loss = 0.732376
Epoch 5.46: Loss = 0.821884
Epoch 5.47: Loss = 0.656082
Epoch 5.48: Loss = 0.664291
Epoch 5.49: Loss = 0.691544
Epoch 5.50: Loss = 0.698563
Epoch 5.51: Loss = 0.589554
Epoch 5.52: Loss = 0.746643
Epoch 5.53: Loss = 0.783051
Epoch 5.54: Loss = 0.571487
Epoch 5.55: Loss = 0.749847
Epoch 5.56: Loss = 0.74147
Epoch 5.57: Loss = 0.751099
Epoch 5.58: Loss = 0.714142
Epoch 5.59: Loss = 0.805023
Epoch 5.60: Loss = 0.76033
Epoch 5.61: Loss = 0.640137
Epoch 5.62: Loss = 0.722549
Epoch 5.63: Loss = 0.615158
Epoch 5.64: Loss = 0.570786
Epoch 5.65: Loss = 0.748566
Epoch 5.66: Loss = 0.606491
Epoch 5.67: Loss = 0.686417
Epoch 5.68: Loss = 0.841202
Epoch 5.69: Loss = 0.759537
Epoch 5.70: Loss = 0.750565
Epoch 5.71: Loss = 0.585724
Epoch 5.72: Loss = 0.706223
Epoch 5.73: Loss = 0.793396
Epoch 5.74: Loss = 0.724884
Epoch 5.75: Loss = 0.685654
Epoch 5.76: Loss = 0.668304
Epoch 5.77: Loss = 0.692352
Epoch 5.78: Loss = 0.700531
Epoch 5.79: Loss = 0.68631
Epoch 5.80: Loss = 0.653488
Epoch 5.81: Loss = 0.690857
Epoch 5.82: Loss = 0.674759
Epoch 5.83: Loss = 0.765656
Epoch 5.84: Loss = 0.658081
Epoch 5.85: Loss = 0.669403
Epoch 5.86: Loss = 0.737701
Epoch 5.87: Loss = 0.695206
Epoch 5.88: Loss = 0.614914
Epoch 5.89: Loss = 0.845718
Epoch 5.90: Loss = 0.703476
Epoch 5.91: Loss = 0.75853
Epoch 5.92: Loss = 0.686584
Epoch 5.93: Loss = 0.732941
Epoch 5.94: Loss = 0.722946
Epoch 5.95: Loss = 0.747513
Epoch 5.96: Loss = 0.67424
Epoch 5.97: Loss = 0.586212
Epoch 5.98: Loss = 0.68277
Epoch 5.99: Loss = 0.697495
Epoch 5.100: Loss = 0.693024
Epoch 5.101: Loss = 0.754028
Epoch 5.102: Loss = 0.749039
Epoch 5.103: Loss = 0.684753
Epoch 5.104: Loss = 0.643784
Epoch 5.105: Loss = 0.644577
Epoch 5.106: Loss = 0.792313
Epoch 5.107: Loss = 0.766983
Epoch 5.108: Loss = 0.822906
Epoch 5.109: Loss = 0.765594
Epoch 5.110: Loss = 0.717377
Epoch 5.111: Loss = 0.65097
Epoch 5.112: Loss = 0.680634
Epoch 5.113: Loss = 0.672607
Epoch 5.114: Loss = 0.764786
Epoch 5.115: Loss = 0.709747
Epoch 5.116: Loss = 0.613235
Epoch 5.117: Loss = 0.795715
Epoch 5.118: Loss = 0.638855
Epoch 5.119: Loss = 0.709763
Epoch 5.120: Loss = 0.682648
TRAIN LOSS = 0.70105
TRAIN ACC = 78.746 % (47250/60000)
Loss = 0.660812
Loss = 0.807663
Loss = 0.68898
Loss = 0.59256
Loss = 0.679306
Loss = 0.823151
Loss = 0.929153
Loss = 0.794556
Loss = 0.728363
Loss = 0.654205
Loss = 0.88446
Loss = 0.881409
Loss = 0.765045
Loss = 0.770752
Loss = 0.745728
Loss = 0.748169
Loss = 0.709793
Loss = 0.781418
Loss = 0.809616
Loss = 0.764282
TEST LOSS = 0.760971
TEST ACC = 472.499 % (7810/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.72052
Epoch 6.2: Loss = 0.710342
Epoch 6.3: Loss = 0.713135
Epoch 6.4: Loss = 0.606461
Epoch 6.5: Loss = 0.669556
Epoch 6.6: Loss = 0.840561
Epoch 6.7: Loss = 0.725159
Epoch 6.8: Loss = 0.789215
Epoch 6.9: Loss = 0.619186
Epoch 6.10: Loss = 0.512421
Epoch 6.11: Loss = 0.79129
Epoch 6.12: Loss = 0.69989
Epoch 6.13: Loss = 0.731598
Epoch 6.14: Loss = 0.709808
Epoch 6.15: Loss = 0.695633
Epoch 6.16: Loss = 0.777557
Epoch 6.17: Loss = 0.629135
Epoch 6.18: Loss = 0.70282
Epoch 6.19: Loss = 0.674057
Epoch 6.20: Loss = 0.831268
Epoch 6.21: Loss = 0.628586
Epoch 6.22: Loss = 0.539688
Epoch 6.23: Loss = 0.688644
Epoch 6.24: Loss = 0.814636
Epoch 6.25: Loss = 0.647476
Epoch 6.26: Loss = 0.648697
Epoch 6.27: Loss = 0.687714
Epoch 6.28: Loss = 0.70842
Epoch 6.29: Loss = 0.705002
Epoch 6.30: Loss = 0.700012
Epoch 6.31: Loss = 0.761673
Epoch 6.32: Loss = 0.738495
Epoch 6.33: Loss = 0.539383
Epoch 6.34: Loss = 0.812622
Epoch 6.35: Loss = 0.754974
Epoch 6.36: Loss = 0.753342
Epoch 6.37: Loss = 0.752106
Epoch 6.38: Loss = 0.712845
Epoch 6.39: Loss = 0.767822
Epoch 6.40: Loss = 0.681122
Epoch 6.41: Loss = 0.743179
Epoch 6.42: Loss = 0.68335
Epoch 6.43: Loss = 0.719894
Epoch 6.44: Loss = 0.581482
Epoch 6.45: Loss = 0.719193
Epoch 6.46: Loss = 0.836975
Epoch 6.47: Loss = 0.637436
Epoch 6.48: Loss = 0.614899
Epoch 6.49: Loss = 0.666107
Epoch 6.50: Loss = 0.721603
Epoch 6.51: Loss = 0.584961
Epoch 6.52: Loss = 0.774246
Epoch 6.53: Loss = 0.783417
Epoch 6.54: Loss = 0.548126
Epoch 6.55: Loss = 0.726761
Epoch 6.56: Loss = 0.736542
Epoch 6.57: Loss = 0.781128
Epoch 6.58: Loss = 0.720764
Epoch 6.59: Loss = 0.830429
Epoch 6.60: Loss = 0.781448
Epoch 6.61: Loss = 0.651489
Epoch 6.62: Loss = 0.744354
Epoch 6.63: Loss = 0.611984
Epoch 6.64: Loss = 0.578339
Epoch 6.65: Loss = 0.737564
Epoch 6.66: Loss = 0.599625
Epoch 6.67: Loss = 0.706696
Epoch 6.68: Loss = 0.829315
Epoch 6.69: Loss = 0.720779
Epoch 6.70: Loss = 0.735306
Epoch 6.71: Loss = 0.602631
Epoch 6.72: Loss = 0.705551
Epoch 6.73: Loss = 0.795639
Epoch 6.74: Loss = 0.710922
Epoch 6.75: Loss = 0.684006
Epoch 6.76: Loss = 0.669189
Epoch 6.77: Loss = 0.690079
Epoch 6.78: Loss = 0.708557
Epoch 6.79: Loss = 0.659805
Epoch 6.80: Loss = 0.62709
Epoch 6.81: Loss = 0.66713
Epoch 6.82: Loss = 0.6633
Epoch 6.83: Loss = 0.762589
Epoch 6.84: Loss = 0.633896
Epoch 6.85: Loss = 0.672989
Epoch 6.86: Loss = 0.74823
Epoch 6.87: Loss = 0.695572
Epoch 6.88: Loss = 0.609924
Epoch 6.89: Loss = 0.817429
Epoch 6.90: Loss = 0.681519
Epoch 6.91: Loss = 0.778748
Epoch 6.92: Loss = 0.672012
Epoch 6.93: Loss = 0.707062
Epoch 6.94: Loss = 0.692444
Epoch 6.95: Loss = 0.724106
Epoch 6.96: Loss = 0.657242
Epoch 6.97: Loss = 0.579849
Epoch 6.98: Loss = 0.649826
Epoch 6.99: Loss = 0.692963
Epoch 6.100: Loss = 0.69075
Epoch 6.101: Loss = 0.736938
Epoch 6.102: Loss = 0.768295
Epoch 6.103: Loss = 0.686478
Epoch 6.104: Loss = 0.614502
Epoch 6.105: Loss = 0.617416
Epoch 6.106: Loss = 0.762436
Epoch 6.107: Loss = 0.764313
Epoch 6.108: Loss = 0.828751
Epoch 6.109: Loss = 0.742813
Epoch 6.110: Loss = 0.754608
Epoch 6.111: Loss = 0.649445
Epoch 6.112: Loss = 0.696213
Epoch 6.113: Loss = 0.686127
Epoch 6.114: Loss = 0.75412
Epoch 6.115: Loss = 0.705109
Epoch 6.116: Loss = 0.641266
Epoch 6.117: Loss = 0.755127
Epoch 6.118: Loss = 0.632599
Epoch 6.119: Loss = 0.658737
Epoch 6.120: Loss = 0.662842
TRAIN LOSS = 0.700592
TRAIN ACC = 79.3869 % (47634/60000)
Loss = 0.629669
Loss = 0.79924
Loss = 0.67804
Loss = 0.607788
Loss = 0.718689
Loss = 0.836227
Loss = 0.943771
Loss = 0.812607
Loss = 0.738037
Loss = 0.66423
Loss = 0.906479
Loss = 0.90686
Loss = 0.776199
Loss = 0.785599
Loss = 0.76181
Loss = 0.769913
Loss = 0.719437
Loss = 0.788376
Loss = 0.821915
Loss = 0.749939
TEST LOSS = 0.770741
TEST ACC = 476.34 % (7854/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.736954
Epoch 7.2: Loss = 0.697891
Epoch 7.3: Loss = 0.67955
Epoch 7.4: Loss = 0.610504
Epoch 7.5: Loss = 0.657639
Epoch 7.6: Loss = 0.801483
Epoch 7.7: Loss = 0.706451
Epoch 7.8: Loss = 0.796432
Epoch 7.9: Loss = 0.586838
Epoch 7.10: Loss = 0.493103
Epoch 7.11: Loss = 0.772202
Epoch 7.12: Loss = 0.706772
Epoch 7.13: Loss = 0.685455
Epoch 7.14: Loss = 0.673752
Epoch 7.15: Loss = 0.694122
Epoch 7.16: Loss = 0.759308
Epoch 7.17: Loss = 0.628632
Epoch 7.18: Loss = 0.703812
Epoch 7.19: Loss = 0.639221
Epoch 7.20: Loss = 0.80658
Epoch 7.21: Loss = 0.651398
Epoch 7.22: Loss = 0.513168
Epoch 7.23: Loss = 0.688797
Epoch 7.24: Loss = 0.824707
Epoch 7.25: Loss = 0.638153
Epoch 7.26: Loss = 0.636642
Epoch 7.27: Loss = 0.704514
Epoch 7.28: Loss = 0.73111
Epoch 7.29: Loss = 0.69577
Epoch 7.30: Loss = 0.721954
Epoch 7.31: Loss = 0.781052
Epoch 7.32: Loss = 0.708633
Epoch 7.33: Loss = 0.529861
Epoch 7.34: Loss = 0.784912
Epoch 7.35: Loss = 0.784363
Epoch 7.36: Loss = 0.788925
Epoch 7.37: Loss = 0.783936
Epoch 7.38: Loss = 0.726501
Epoch 7.39: Loss = 0.777344
Epoch 7.40: Loss = 0.678589
Epoch 7.41: Loss = 0.75061
Epoch 7.42: Loss = 0.680496
Epoch 7.43: Loss = 0.729843
Epoch 7.44: Loss = 0.567978
Epoch 7.45: Loss = 0.768097
Epoch 7.46: Loss = 0.808197
Epoch 7.47: Loss = 0.672363
Epoch 7.48: Loss = 0.619705
Epoch 7.49: Loss = 0.697418
Epoch 7.50: Loss = 0.740829
Epoch 7.51: Loss = 0.585693
Epoch 7.52: Loss = 0.762726
Epoch 7.53: Loss = 0.782211
Epoch 7.54: Loss = 0.531723
Epoch 7.55: Loss = 0.694672
Epoch 7.56: Loss = 0.733932
Epoch 7.57: Loss = 0.749863
Epoch 7.58: Loss = 0.723251
Epoch 7.59: Loss = 0.82164
Epoch 7.60: Loss = 0.804108
Epoch 7.61: Loss = 0.645264
Epoch 7.62: Loss = 0.766953
Epoch 7.63: Loss = 0.604706
Epoch 7.64: Loss = 0.569382
Epoch 7.65: Loss = 0.712662
Epoch 7.66: Loss = 0.583908
Epoch 7.67: Loss = 0.665298
Epoch 7.68: Loss = 0.824127
Epoch 7.69: Loss = 0.68689
Epoch 7.70: Loss = 0.7332
Epoch 7.71: Loss = 0.607346
Epoch 7.72: Loss = 0.688339
Epoch 7.73: Loss = 0.79985
Epoch 7.74: Loss = 0.721848
Epoch 7.75: Loss = 0.664108
Epoch 7.76: Loss = 0.678238
Epoch 7.77: Loss = 0.695389
Epoch 7.78: Loss = 0.650391
Epoch 7.79: Loss = 0.650925
Epoch 7.80: Loss = 0.608429
Epoch 7.81: Loss = 0.684006
Epoch 7.82: Loss = 0.680588
Epoch 7.83: Loss = 0.752823
Epoch 7.84: Loss = 0.623917
Epoch 7.85: Loss = 0.671967
Epoch 7.86: Loss = 0.747528
Epoch 7.87: Loss = 0.67804
Epoch 7.88: Loss = 0.600494
Epoch 7.89: Loss = 0.823303
Epoch 7.90: Loss = 0.664993
Epoch 7.91: Loss = 0.785217
Epoch 7.92: Loss = 0.662537
Epoch 7.93: Loss = 0.665817
Epoch 7.94: Loss = 0.670471
Epoch 7.95: Loss = 0.721542
Epoch 7.96: Loss = 0.680038
Epoch 7.97: Loss = 0.577545
Epoch 7.98: Loss = 0.670944
Epoch 7.99: Loss = 0.690125
Epoch 7.100: Loss = 0.689026
Epoch 7.101: Loss = 0.735168
Epoch 7.102: Loss = 0.745834
Epoch 7.103: Loss = 0.724136
Epoch 7.104: Loss = 0.618576
Epoch 7.105: Loss = 0.621063
Epoch 7.106: Loss = 0.791061
Epoch 7.107: Loss = 0.726303
Epoch 7.108: Loss = 0.841644
Epoch 7.109: Loss = 0.758469
Epoch 7.110: Loss = 0.753067
Epoch 7.111: Loss = 0.669144
Epoch 7.112: Loss = 0.708588
Epoch 7.113: Loss = 0.706055
Epoch 7.114: Loss = 0.778473
Epoch 7.115: Loss = 0.699341
Epoch 7.116: Loss = 0.626358
Epoch 7.117: Loss = 0.771851
Epoch 7.118: Loss = 0.619461
Epoch 7.119: Loss = 0.658463
Epoch 7.120: Loss = 0.660599
TRAIN LOSS = 0.697708
TRAIN ACC = 79.8553 % (47915/60000)
Loss = 0.649582
Loss = 0.806717
Loss = 0.697983
Loss = 0.634262
Loss = 0.744965
Loss = 0.869232
Loss = 0.974564
Loss = 0.843414
Loss = 0.781418
Loss = 0.70224
Loss = 0.924438
Loss = 0.916626
Loss = 0.784012
Loss = 0.801636
Loss = 0.77504
Loss = 0.793091
Loss = 0.773544
Loss = 0.84169
Loss = 0.860016
Loss = 0.766342
TEST LOSS = 0.79704
TEST ACC = 479.149 % (7882/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.733063
Epoch 8.2: Loss = 0.732452
Epoch 8.3: Loss = 0.690826
Epoch 8.4: Loss = 0.608307
Epoch 8.5: Loss = 0.675034
Epoch 8.6: Loss = 0.866425
Epoch 8.7: Loss = 0.74852
Epoch 8.8: Loss = 0.799026
Epoch 8.9: Loss = 0.605865
Epoch 8.10: Loss = 0.498322
Epoch 8.11: Loss = 0.765427
Epoch 8.12: Loss = 0.714523
Epoch 8.13: Loss = 0.740158
Epoch 8.14: Loss = 0.663086
Epoch 8.15: Loss = 0.698395
Epoch 8.16: Loss = 0.779205
Epoch 8.17: Loss = 0.608505
Epoch 8.18: Loss = 0.705612
Epoch 8.19: Loss = 0.664383
Epoch 8.20: Loss = 0.802887
Epoch 8.21: Loss = 0.668045
Epoch 8.22: Loss = 0.5009
Epoch 8.23: Loss = 0.696854
Epoch 8.24: Loss = 0.804138
Epoch 8.25: Loss = 0.65831
Epoch 8.26: Loss = 0.597626
Epoch 8.27: Loss = 0.685684
Epoch 8.28: Loss = 0.711624
Epoch 8.29: Loss = 0.670349
Epoch 8.30: Loss = 0.697388
Epoch 8.31: Loss = 0.75946
Epoch 8.32: Loss = 0.695953
Epoch 8.33: Loss = 0.526535
Epoch 8.34: Loss = 0.816345
Epoch 8.35: Loss = 0.759155
Epoch 8.36: Loss = 0.77066
Epoch 8.37: Loss = 0.766296
Epoch 8.38: Loss = 0.728683
Epoch 8.39: Loss = 0.740204
Epoch 8.40: Loss = 0.68634
Epoch 8.41: Loss = 0.739807
Epoch 8.42: Loss = 0.673615
Epoch 8.43: Loss = 0.728043
Epoch 8.44: Loss = 0.57724
Epoch 8.45: Loss = 0.75679
Epoch 8.46: Loss = 0.868484
Epoch 8.47: Loss = 0.654343
Epoch 8.48: Loss = 0.656601
Epoch 8.49: Loss = 0.648102
Epoch 8.50: Loss = 0.719666
Epoch 8.51: Loss = 0.574661
Epoch 8.52: Loss = 0.745621
Epoch 8.53: Loss = 0.794067
Epoch 8.54: Loss = 0.569183
Epoch 8.55: Loss = 0.7332
Epoch 8.56: Loss = 0.737396
Epoch 8.57: Loss = 0.775803
Epoch 8.58: Loss = 0.700577
Epoch 8.59: Loss = 0.830597
Epoch 8.60: Loss = 0.788208
Epoch 8.61: Loss = 0.647003
Epoch 8.62: Loss = 0.786377
Epoch 8.63: Loss = 0.633514
Epoch 8.64: Loss = 0.574005
Epoch 8.65: Loss = 0.736557
Epoch 8.66: Loss = 0.61647
Epoch 8.67: Loss = 0.694778
Epoch 8.68: Loss = 0.907761
Epoch 8.69: Loss = 0.742981
Epoch 8.70: Loss = 0.750931
Epoch 8.71: Loss = 0.623825
Epoch 8.72: Loss = 0.714828
Epoch 8.73: Loss = 0.829956
Epoch 8.74: Loss = 0.722046
Epoch 8.75: Loss = 0.669724
Epoch 8.76: Loss = 0.678238
Epoch 8.77: Loss = 0.705215
Epoch 8.78: Loss = 0.648132
Epoch 8.79: Loss = 0.67186
Epoch 8.80: Loss = 0.610809
Epoch 8.81: Loss = 0.69812
Epoch 8.82: Loss = 0.695663
Epoch 8.83: Loss = 0.794571
Epoch 8.84: Loss = 0.619019
Epoch 8.85: Loss = 0.682419
Epoch 8.86: Loss = 0.751221
Epoch 8.87: Loss = 0.680588
Epoch 8.88: Loss = 0.619476
Epoch 8.89: Loss = 0.819199
Epoch 8.90: Loss = 0.689987
Epoch 8.91: Loss = 0.792419
Epoch 8.92: Loss = 0.673889
Epoch 8.93: Loss = 0.701431
Epoch 8.94: Loss = 0.670624
Epoch 8.95: Loss = 0.718719
Epoch 8.96: Loss = 0.651245
Epoch 8.97: Loss = 0.563461
Epoch 8.98: Loss = 0.649429
Epoch 8.99: Loss = 0.736801
Epoch 8.100: Loss = 0.684311
Epoch 8.101: Loss = 0.729874
Epoch 8.102: Loss = 0.746262
Epoch 8.103: Loss = 0.699677
Epoch 8.104: Loss = 0.639969
Epoch 8.105: Loss = 0.641739
Epoch 8.106: Loss = 0.794937
Epoch 8.107: Loss = 0.727844
Epoch 8.108: Loss = 0.824463
Epoch 8.109: Loss = 0.753494
Epoch 8.110: Loss = 0.752396
Epoch 8.111: Loss = 0.667587
Epoch 8.112: Loss = 0.670502
Epoch 8.113: Loss = 0.679703
Epoch 8.114: Loss = 0.763687
Epoch 8.115: Loss = 0.686829
Epoch 8.116: Loss = 0.653625
Epoch 8.117: Loss = 0.713852
Epoch 8.118: Loss = 0.641022
Epoch 8.119: Loss = 0.654282
Epoch 8.120: Loss = 0.63678
TRAIN LOSS = 0.702087
TRAIN ACC = 80.423 % (48256/60000)
Loss = 0.599045
Loss = 0.784958
Loss = 0.665146
Loss = 0.58252
Loss = 0.732712
Loss = 0.799484
Loss = 0.942184
Loss = 0.805084
Loss = 0.719467
Loss = 0.66629
Loss = 0.875778
Loss = 0.872864
Loss = 0.767593
Loss = 0.748077
Loss = 0.732132
Loss = 0.737396
Loss = 0.746323
Loss = 0.798553
Loss = 0.739471
Loss = 0.727722
TEST LOSS = 0.75214
TEST ACC = 482.559 % (7956/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.681076
Epoch 9.2: Loss = 0.711884
Epoch 9.3: Loss = 0.695313
Epoch 9.4: Loss = 0.616409
Epoch 9.5: Loss = 0.68219
Epoch 9.6: Loss = 0.825974
Epoch 9.7: Loss = 0.702713
Epoch 9.8: Loss = 0.784836
Epoch 9.9: Loss = 0.603653
Epoch 9.10: Loss = 0.501633
Epoch 9.11: Loss = 0.731415
Epoch 9.12: Loss = 0.740067
Epoch 9.13: Loss = 0.700272
Epoch 9.14: Loss = 0.678467
Epoch 9.15: Loss = 0.72493
Epoch 9.16: Loss = 0.763229
Epoch 9.17: Loss = 0.626984
Epoch 9.18: Loss = 0.718918
Epoch 9.19: Loss = 0.69075
Epoch 9.20: Loss = 0.845398
Epoch 9.21: Loss = 0.68663
Epoch 9.22: Loss = 0.525391
Epoch 9.23: Loss = 0.722733
Epoch 9.24: Loss = 0.831161
Epoch 9.25: Loss = 0.675171
Epoch 9.26: Loss = 0.58252
Epoch 9.27: Loss = 0.711365
Epoch 9.28: Loss = 0.73053
Epoch 9.29: Loss = 0.715576
Epoch 9.30: Loss = 0.699692
Epoch 9.31: Loss = 0.782715
Epoch 9.32: Loss = 0.736053
Epoch 9.33: Loss = 0.553696
Epoch 9.34: Loss = 0.857819
Epoch 9.35: Loss = 0.787216
Epoch 9.36: Loss = 0.798141
Epoch 9.37: Loss = 0.794418
Epoch 9.38: Loss = 0.746643
Epoch 9.39: Loss = 0.772827
Epoch 9.40: Loss = 0.719238
Epoch 9.41: Loss = 0.749924
Epoch 9.42: Loss = 0.690933
Epoch 9.43: Loss = 0.739594
Epoch 9.44: Loss = 0.592239
Epoch 9.45: Loss = 0.733551
Epoch 9.46: Loss = 0.862564
Epoch 9.47: Loss = 0.675934
Epoch 9.48: Loss = 0.64888
Epoch 9.49: Loss = 0.696762
Epoch 9.50: Loss = 0.732803
Epoch 9.51: Loss = 0.599838
Epoch 9.52: Loss = 0.775299
Epoch 9.53: Loss = 0.801422
Epoch 9.54: Loss = 0.569214
Epoch 9.55: Loss = 0.737869
Epoch 9.56: Loss = 0.711929
Epoch 9.57: Loss = 0.786011
Epoch 9.58: Loss = 0.69902
Epoch 9.59: Loss = 0.860764
Epoch 9.60: Loss = 0.781418
Epoch 9.61: Loss = 0.641907
Epoch 9.62: Loss = 0.748856
Epoch 9.63: Loss = 0.647644
Epoch 9.64: Loss = 0.58017
Epoch 9.65: Loss = 0.737015
Epoch 9.66: Loss = 0.62413
Epoch 9.67: Loss = 0.667038
Epoch 9.68: Loss = 0.899704
Epoch 9.69: Loss = 0.770554
Epoch 9.70: Loss = 0.748413
Epoch 9.71: Loss = 0.622803
Epoch 9.72: Loss = 0.719604
Epoch 9.73: Loss = 0.844543
Epoch 9.74: Loss = 0.73024
Epoch 9.75: Loss = 0.689896
Epoch 9.76: Loss = 0.700287
Epoch 9.77: Loss = 0.709518
Epoch 9.78: Loss = 0.682831
Epoch 9.79: Loss = 0.680435
Epoch 9.80: Loss = 0.624298
Epoch 9.81: Loss = 0.695709
Epoch 9.82: Loss = 0.707474
Epoch 9.83: Loss = 0.783737
Epoch 9.84: Loss = 0.651901
Epoch 9.85: Loss = 0.696274
Epoch 9.86: Loss = 0.778488
Epoch 9.87: Loss = 0.675888
Epoch 9.88: Loss = 0.64949
Epoch 9.89: Loss = 0.865875
Epoch 9.90: Loss = 0.725296
Epoch 9.91: Loss = 0.829132
Epoch 9.92: Loss = 0.708649
Epoch 9.93: Loss = 0.703232
Epoch 9.94: Loss = 0.711319
Epoch 9.95: Loss = 0.737534
Epoch 9.96: Loss = 0.648209
Epoch 9.97: Loss = 0.584229
Epoch 9.98: Loss = 0.688202
Epoch 9.99: Loss = 0.723663
Epoch 9.100: Loss = 0.714813
Epoch 9.101: Loss = 0.732132
Epoch 9.102: Loss = 0.792252
Epoch 9.103: Loss = 0.736588
Epoch 9.104: Loss = 0.642746
Epoch 9.105: Loss = 0.64859
Epoch 9.106: Loss = 0.793243
Epoch 9.107: Loss = 0.743607
Epoch 9.108: Loss = 0.850571
Epoch 9.109: Loss = 0.801331
Epoch 9.110: Loss = 0.806107
Epoch 9.111: Loss = 0.713257
Epoch 9.112: Loss = 0.728745
Epoch 9.113: Loss = 0.68927
Epoch 9.114: Loss = 0.819992
Epoch 9.115: Loss = 0.705261
Epoch 9.116: Loss = 0.655716
Epoch 9.117: Loss = 0.772644
Epoch 9.118: Loss = 0.631836
Epoch 9.119: Loss = 0.645218
Epoch 9.120: Loss = 0.66597
TRAIN LOSS = 0.715225
TRAIN ACC = 80.5176 % (48313/60000)
Loss = 0.607712
Loss = 0.804108
Loss = 0.694305
Loss = 0.591782
Loss = 0.762054
Loss = 0.841461
Loss = 0.987061
Loss = 0.795197
Loss = 0.756012
Loss = 0.672028
Loss = 0.917404
Loss = 0.92009
Loss = 0.813034
Loss = 0.803879
Loss = 0.754242
Loss = 0.7565
Loss = 0.763031
Loss = 0.840485
Loss = 0.807877
Loss = 0.739639
TEST LOSS = 0.781395
TEST ACC = 483.13 % (7963/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.717896
Epoch 10.2: Loss = 0.758698
Epoch 10.3: Loss = 0.752014
Epoch 10.4: Loss = 0.60289
Epoch 10.5: Loss = 0.703491
Epoch 10.6: Loss = 0.837341
Epoch 10.7: Loss = 0.732605
Epoch 10.8: Loss = 0.8004
Epoch 10.9: Loss = 0.600693
Epoch 10.10: Loss = 0.496613
Epoch 10.11: Loss = 0.744095
Epoch 10.12: Loss = 0.745361
Epoch 10.13: Loss = 0.744537
Epoch 10.14: Loss = 0.677948
Epoch 10.15: Loss = 0.729538
Epoch 10.16: Loss = 0.785706
Epoch 10.17: Loss = 0.625076
Epoch 10.18: Loss = 0.714645
Epoch 10.19: Loss = 0.664551
Epoch 10.20: Loss = 0.845169
Epoch 10.21: Loss = 0.704514
Epoch 10.22: Loss = 0.522415
Epoch 10.23: Loss = 0.711273
Epoch 10.24: Loss = 0.883316
Epoch 10.25: Loss = 0.688324
Epoch 10.26: Loss = 0.59729
Epoch 10.27: Loss = 0.711761
Epoch 10.28: Loss = 0.734146
Epoch 10.29: Loss = 0.691864
Epoch 10.30: Loss = 0.751083
Epoch 10.31: Loss = 0.774918
Epoch 10.32: Loss = 0.721619
Epoch 10.33: Loss = 0.584503
Epoch 10.34: Loss = 0.859787
Epoch 10.35: Loss = 0.806534
Epoch 10.36: Loss = 0.805527
Epoch 10.37: Loss = 0.769211
Epoch 10.38: Loss = 0.729416
Epoch 10.39: Loss = 0.775589
Epoch 10.40: Loss = 0.728058
Epoch 10.41: Loss = 0.781296
Epoch 10.42: Loss = 0.720566
Epoch 10.43: Loss = 0.739151
Epoch 10.44: Loss = 0.532318
Epoch 10.45: Loss = 0.750031
Epoch 10.46: Loss = 0.842865
Epoch 10.47: Loss = 0.678757
Epoch 10.48: Loss = 0.662445
Epoch 10.49: Loss = 0.682785
Epoch 10.50: Loss = 0.732391
Epoch 10.51: Loss = 0.604187
Epoch 10.52: Loss = 0.812927
Epoch 10.53: Loss = 0.807648
Epoch 10.54: Loss = 0.585938
Epoch 10.55: Loss = 0.789215
Epoch 10.56: Loss = 0.697678
Epoch 10.57: Loss = 0.783264
Epoch 10.58: Loss = 0.715271
Epoch 10.59: Loss = 0.911407
Epoch 10.60: Loss = 0.780884
Epoch 10.61: Loss = 0.622025
Epoch 10.62: Loss = 0.772308
Epoch 10.63: Loss = 0.636032
Epoch 10.64: Loss = 0.599655
Epoch 10.65: Loss = 0.755844
Epoch 10.66: Loss = 0.611298
Epoch 10.67: Loss = 0.649658
Epoch 10.68: Loss = 0.908737
Epoch 10.69: Loss = 0.759705
Epoch 10.70: Loss = 0.75177
Epoch 10.71: Loss = 0.604416
Epoch 10.72: Loss = 0.715637
Epoch 10.73: Loss = 0.857834
Epoch 10.74: Loss = 0.73056
Epoch 10.75: Loss = 0.707138
Epoch 10.76: Loss = 0.700882
Epoch 10.77: Loss = 0.750961
Epoch 10.78: Loss = 0.671692
Epoch 10.79: Loss = 0.67926
Epoch 10.80: Loss = 0.636292
Epoch 10.81: Loss = 0.689087
Epoch 10.82: Loss = 0.701859
Epoch 10.83: Loss = 0.80574
Epoch 10.84: Loss = 0.646362
Epoch 10.85: Loss = 0.705276
Epoch 10.86: Loss = 0.818634
Epoch 10.87: Loss = 0.690918
Epoch 10.88: Loss = 0.683899
Epoch 10.89: Loss = 0.881546
Epoch 10.90: Loss = 0.699585
Epoch 10.91: Loss = 0.825333
Epoch 10.92: Loss = 0.720413
Epoch 10.93: Loss = 0.730728
Epoch 10.94: Loss = 0.763321
Epoch 10.95: Loss = 0.773438
Epoch 10.96: Loss = 0.712341
Epoch 10.97: Loss = 0.59964
Epoch 10.98: Loss = 0.705719
Epoch 10.99: Loss = 0.750549
Epoch 10.100: Loss = 0.701706
Epoch 10.101: Loss = 0.769669
Epoch 10.102: Loss = 0.787292
Epoch 10.103: Loss = 0.752762
Epoch 10.104: Loss = 0.658325
Epoch 10.105: Loss = 0.660782
Epoch 10.106: Loss = 0.821396
Epoch 10.107: Loss = 0.783325
Epoch 10.108: Loss = 0.891403
Epoch 10.109: Loss = 0.843384
Epoch 10.110: Loss = 0.768127
Epoch 10.111: Loss = 0.705688
Epoch 10.112: Loss = 0.737534
Epoch 10.113: Loss = 0.718903
Epoch 10.114: Loss = 0.827057
Epoch 10.115: Loss = 0.696701
Epoch 10.116: Loss = 0.655594
Epoch 10.117: Loss = 0.790009
Epoch 10.118: Loss = 0.647217
Epoch 10.119: Loss = 0.713974
Epoch 10.120: Loss = 0.677246
TRAIN LOSS = 0.725647
TRAIN ACC = 80.8289 % (48499/60000)
Loss = 0.653381
Loss = 0.86145
Loss = 0.71463
Loss = 0.602188
Loss = 0.760178
Loss = 0.83873
Loss = 0.996124
Loss = 0.83989
Loss = 0.771912
Loss = 0.729095
Loss = 0.945587
Loss = 0.938553
Loss = 0.809433
Loss = 0.798386
Loss = 0.768845
Loss = 0.772949
Loss = 0.792938
Loss = 0.853989
Loss = 0.784241
Loss = 0.753983
TEST LOSS = 0.799324
TEST ACC = 484.99 % (7983/10000)
