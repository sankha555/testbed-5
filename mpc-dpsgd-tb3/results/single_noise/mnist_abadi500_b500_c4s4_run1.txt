Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.37102
Epoch 1.2: Loss = 2.30464
Epoch 1.3: Loss = 2.25795
Epoch 1.4: Loss = 2.21861
Epoch 1.5: Loss = 2.18063
Epoch 1.6: Loss = 2.13701
Epoch 1.7: Loss = 2.10382
Epoch 1.8: Loss = 2.0663
Epoch 1.9: Loss = 2.0218
Epoch 1.10: Loss = 1.95958
Epoch 1.11: Loss = 1.91637
Epoch 1.12: Loss = 1.87596
Epoch 1.13: Loss = 1.78519
Epoch 1.14: Loss = 1.79361
Epoch 1.15: Loss = 1.8687
Epoch 1.16: Loss = 1.74651
Epoch 1.17: Loss = 1.69603
Epoch 1.18: Loss = 1.66795
Epoch 1.19: Loss = 1.63768
Epoch 1.20: Loss = 1.59824
Epoch 1.21: Loss = 1.51915
Epoch 1.22: Loss = 1.50838
Epoch 1.23: Loss = 1.44943
Epoch 1.24: Loss = 1.55034
Epoch 1.25: Loss = 1.46088
Epoch 1.26: Loss = 1.50784
Epoch 1.27: Loss = 1.41156
Epoch 1.28: Loss = 1.41826
Epoch 1.29: Loss = 1.41805
Epoch 1.30: Loss = 1.46347
Epoch 1.31: Loss = 1.32549
Epoch 1.32: Loss = 1.35976
Epoch 1.33: Loss = 1.26811
Epoch 1.34: Loss = 1.31676
Epoch 1.35: Loss = 1.22871
Epoch 1.36: Loss = 1.38029
Epoch 1.37: Loss = 1.21957
Epoch 1.38: Loss = 1.14255
Epoch 1.39: Loss = 1.17201
Epoch 1.40: Loss = 1.07262
Epoch 1.41: Loss = 1.12845
Epoch 1.42: Loss = 1.12041
Epoch 1.43: Loss = 1.08459
Epoch 1.44: Loss = 1.00854
Epoch 1.45: Loss = 1.1088
Epoch 1.46: Loss = 1.05823
Epoch 1.47: Loss = 0.983047
Epoch 1.48: Loss = 1.06723
Epoch 1.49: Loss = 1.01076
Epoch 1.50: Loss = 1.0719
Epoch 1.51: Loss = 0.916916
Epoch 1.52: Loss = 0.930206
Epoch 1.53: Loss = 0.973221
Epoch 1.54: Loss = 1.00352
Epoch 1.55: Loss = 0.990295
Epoch 1.56: Loss = 0.919983
Epoch 1.57: Loss = 0.826035
Epoch 1.58: Loss = 0.873932
Epoch 1.59: Loss = 0.905609
Epoch 1.60: Loss = 1.00299
Epoch 1.61: Loss = 0.919876
Epoch 1.62: Loss = 0.958618
Epoch 1.63: Loss = 0.965591
Epoch 1.64: Loss = 0.943436
Epoch 1.65: Loss = 0.976227
Epoch 1.66: Loss = 0.852859
Epoch 1.67: Loss = 0.833282
Epoch 1.68: Loss = 0.708176
Epoch 1.69: Loss = 0.758911
Epoch 1.70: Loss = 0.850082
Epoch 1.71: Loss = 0.773895
Epoch 1.72: Loss = 0.767365
Epoch 1.73: Loss = 0.807602
Epoch 1.74: Loss = 0.662628
Epoch 1.75: Loss = 0.784164
Epoch 1.76: Loss = 0.768097
Epoch 1.77: Loss = 0.725433
Epoch 1.78: Loss = 0.706451
Epoch 1.79: Loss = 0.710541
Epoch 1.80: Loss = 0.804184
Epoch 1.81: Loss = 0.688965
Epoch 1.82: Loss = 0.664398
Epoch 1.83: Loss = 0.811584
Epoch 1.84: Loss = 0.748749
Epoch 1.85: Loss = 0.814224
Epoch 1.86: Loss = 0.708786
Epoch 1.87: Loss = 0.657501
Epoch 1.88: Loss = 0.68837
Epoch 1.89: Loss = 0.775116
Epoch 1.90: Loss = 0.639984
Epoch 1.91: Loss = 0.724426
Epoch 1.92: Loss = 0.712982
Epoch 1.93: Loss = 0.728149
Epoch 1.94: Loss = 0.589874
Epoch 1.95: Loss = 0.698059
Epoch 1.96: Loss = 0.646561
Epoch 1.97: Loss = 0.514236
Epoch 1.98: Loss = 0.626602
Epoch 1.99: Loss = 0.719299
Epoch 1.100: Loss = 0.81076
Epoch 1.101: Loss = 0.700394
Epoch 1.102: Loss = 0.678802
Epoch 1.103: Loss = 0.574585
Epoch 1.104: Loss = 0.557007
Epoch 1.105: Loss = 0.666061
Epoch 1.106: Loss = 0.672287
Epoch 1.107: Loss = 0.555313
Epoch 1.108: Loss = 0.612885
Epoch 1.109: Loss = 0.608566
Epoch 1.110: Loss = 0.616364
Epoch 1.111: Loss = 0.519592
Epoch 1.112: Loss = 0.496414
Epoch 1.113: Loss = 0.578354
Epoch 1.114: Loss = 0.501602
Epoch 1.115: Loss = 0.574081
Epoch 1.116: Loss = 0.570145
Epoch 1.117: Loss = 0.478729
Epoch 1.118: Loss = 0.421616
Epoch 1.119: Loss = 0.41185
Epoch 1.120: Loss = 0.451645
TRAIN LOSS = 1.06566
TRAIN ACC = 71.7743 % (43066/60000)
Loss = 0.622284
Loss = 0.628036
Loss = 0.749603
Loss = 0.687317
Loss = 0.725708
Loss = 0.614532
Loss = 0.599915
Loss = 0.749786
Loss = 0.713104
Loss = 0.671036
Loss = 0.329285
Loss = 0.509628
Loss = 0.332687
Loss = 0.546432
Loss = 0.437607
Loss = 0.407639
Loss = 0.399933
Loss = 0.233978
Loss = 0.417679
Loss = 0.685745
TEST LOSS = 0.553097
TEST ACC = 430.659 % (8414/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.541733
Epoch 2.2: Loss = 0.657166
Epoch 2.3: Loss = 0.63063
Epoch 2.4: Loss = 0.507935
Epoch 2.5: Loss = 0.52063
Epoch 2.6: Loss = 0.519165
Epoch 2.7: Loss = 0.579742
Epoch 2.8: Loss = 0.557983
Epoch 2.9: Loss = 0.528488
Epoch 2.10: Loss = 0.53006
Epoch 2.11: Loss = 0.546875
Epoch 2.12: Loss = 0.507858
Epoch 2.13: Loss = 0.444458
Epoch 2.14: Loss = 0.482712
Epoch 2.15: Loss = 0.622177
Epoch 2.16: Loss = 0.58844
Epoch 2.17: Loss = 0.574219
Epoch 2.18: Loss = 0.636383
Epoch 2.19: Loss = 0.511917
Epoch 2.20: Loss = 0.459366
Epoch 2.21: Loss = 0.441025
Epoch 2.22: Loss = 0.454498
Epoch 2.23: Loss = 0.442276
Epoch 2.24: Loss = 0.658493
Epoch 2.25: Loss = 0.553589
Epoch 2.26: Loss = 0.64003
Epoch 2.27: Loss = 0.590134
Epoch 2.28: Loss = 0.591537
Epoch 2.29: Loss = 0.641159
Epoch 2.30: Loss = 0.686462
Epoch 2.31: Loss = 0.463028
Epoch 2.32: Loss = 0.59787
Epoch 2.33: Loss = 0.496323
Epoch 2.34: Loss = 0.560394
Epoch 2.35: Loss = 0.518372
Epoch 2.36: Loss = 0.626511
Epoch 2.37: Loss = 0.457703
Epoch 2.38: Loss = 0.433502
Epoch 2.39: Loss = 0.507904
Epoch 2.40: Loss = 0.46225
Epoch 2.41: Loss = 0.517822
Epoch 2.42: Loss = 0.580078
Epoch 2.43: Loss = 0.440689
Epoch 2.44: Loss = 0.41954
Epoch 2.45: Loss = 0.532181
Epoch 2.46: Loss = 0.538956
Epoch 2.47: Loss = 0.463104
Epoch 2.48: Loss = 0.523193
Epoch 2.49: Loss = 0.504501
Epoch 2.50: Loss = 0.579834
Epoch 2.51: Loss = 0.444046
Epoch 2.52: Loss = 0.432938
Epoch 2.53: Loss = 0.476685
Epoch 2.54: Loss = 0.570618
Epoch 2.55: Loss = 0.489319
Epoch 2.56: Loss = 0.467545
Epoch 2.57: Loss = 0.449722
Epoch 2.58: Loss = 0.474243
Epoch 2.59: Loss = 0.539566
Epoch 2.60: Loss = 0.58429
Epoch 2.61: Loss = 0.564529
Epoch 2.62: Loss = 0.588623
Epoch 2.63: Loss = 0.616302
Epoch 2.64: Loss = 0.590836
Epoch 2.65: Loss = 0.65271
Epoch 2.66: Loss = 0.504089
Epoch 2.67: Loss = 0.509216
Epoch 2.68: Loss = 0.352646
Epoch 2.69: Loss = 0.422623
Epoch 2.70: Loss = 0.577469
Epoch 2.71: Loss = 0.430115
Epoch 2.72: Loss = 0.430847
Epoch 2.73: Loss = 0.48233
Epoch 2.74: Loss = 0.372986
Epoch 2.75: Loss = 0.575134
Epoch 2.76: Loss = 0.481201
Epoch 2.77: Loss = 0.421906
Epoch 2.78: Loss = 0.451828
Epoch 2.79: Loss = 0.501022
Epoch 2.80: Loss = 0.53537
Epoch 2.81: Loss = 0.420975
Epoch 2.82: Loss = 0.401993
Epoch 2.83: Loss = 0.555389
Epoch 2.84: Loss = 0.470093
Epoch 2.85: Loss = 0.635986
Epoch 2.86: Loss = 0.493729
Epoch 2.87: Loss = 0.399521
Epoch 2.88: Loss = 0.456985
Epoch 2.89: Loss = 0.565414
Epoch 2.90: Loss = 0.407379
Epoch 2.91: Loss = 0.510284
Epoch 2.92: Loss = 0.507584
Epoch 2.93: Loss = 0.553909
Epoch 2.94: Loss = 0.389893
Epoch 2.95: Loss = 0.484573
Epoch 2.96: Loss = 0.49823
Epoch 2.97: Loss = 0.353821
Epoch 2.98: Loss = 0.434952
Epoch 2.99: Loss = 0.536804
Epoch 2.100: Loss = 0.591003
Epoch 2.101: Loss = 0.562653
Epoch 2.102: Loss = 0.450073
Epoch 2.103: Loss = 0.403183
Epoch 2.104: Loss = 0.380554
Epoch 2.105: Loss = 0.522507
Epoch 2.106: Loss = 0.531403
Epoch 2.107: Loss = 0.380783
Epoch 2.108: Loss = 0.472809
Epoch 2.109: Loss = 0.42485
Epoch 2.110: Loss = 0.467133
Epoch 2.111: Loss = 0.374802
Epoch 2.112: Loss = 0.358231
Epoch 2.113: Loss = 0.422653
Epoch 2.114: Loss = 0.377548
Epoch 2.115: Loss = 0.39679
Epoch 2.116: Loss = 0.410156
Epoch 2.117: Loss = 0.31488
Epoch 2.118: Loss = 0.26564
Epoch 2.119: Loss = 0.319427
Epoch 2.120: Loss = 0.34111
TRAIN LOSS = 0.497559
TRAIN ACC = 85.1807 % (51111/60000)
Loss = 0.473526
Loss = 0.488251
Loss = 0.603882
Loss = 0.558456
Loss = 0.624191
Loss = 0.443619
Loss = 0.443344
Loss = 0.629242
Loss = 0.565369
Loss = 0.566025
Loss = 0.212936
Loss = 0.364517
Loss = 0.293518
Loss = 0.423752
Loss = 0.284897
Loss = 0.301666
Loss = 0.27417
Loss = 0.115112
Loss = 0.290176
Loss = 0.571411
TEST LOSS = 0.426403
TEST ACC = 511.108 % (8697/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.41069
Epoch 3.2: Loss = 0.524521
Epoch 3.3: Loss = 0.500656
Epoch 3.4: Loss = 0.376907
Epoch 3.5: Loss = 0.380386
Epoch 3.6: Loss = 0.41095
Epoch 3.7: Loss = 0.433746
Epoch 3.8: Loss = 0.425552
Epoch 3.9: Loss = 0.396179
Epoch 3.10: Loss = 0.440765
Epoch 3.11: Loss = 0.450211
Epoch 3.12: Loss = 0.421463
Epoch 3.13: Loss = 0.335983
Epoch 3.14: Loss = 0.378174
Epoch 3.15: Loss = 0.493118
Epoch 3.16: Loss = 0.481689
Epoch 3.17: Loss = 0.484375
Epoch 3.18: Loss = 0.556686
Epoch 3.19: Loss = 0.420624
Epoch 3.20: Loss = 0.382553
Epoch 3.21: Loss = 0.341583
Epoch 3.22: Loss = 0.365082
Epoch 3.23: Loss = 0.352524
Epoch 3.24: Loss = 0.544495
Epoch 3.25: Loss = 0.458694
Epoch 3.26: Loss = 0.570999
Epoch 3.27: Loss = 0.492004
Epoch 3.28: Loss = 0.512589
Epoch 3.29: Loss = 0.54631
Epoch 3.30: Loss = 0.565506
Epoch 3.31: Loss = 0.374115
Epoch 3.32: Loss = 0.506149
Epoch 3.33: Loss = 0.389908
Epoch 3.34: Loss = 0.463196
Epoch 3.35: Loss = 0.43074
Epoch 3.36: Loss = 0.537415
Epoch 3.37: Loss = 0.374084
Epoch 3.38: Loss = 0.353439
Epoch 3.39: Loss = 0.424026
Epoch 3.40: Loss = 0.37851
Epoch 3.41: Loss = 0.434875
Epoch 3.42: Loss = 0.55101
Epoch 3.43: Loss = 0.363174
Epoch 3.44: Loss = 0.349045
Epoch 3.45: Loss = 0.43364
Epoch 3.46: Loss = 0.473709
Epoch 3.47: Loss = 0.393433
Epoch 3.48: Loss = 0.426819
Epoch 3.49: Loss = 0.432465
Epoch 3.50: Loss = 0.504166
Epoch 3.51: Loss = 0.372543
Epoch 3.52: Loss = 0.332275
Epoch 3.53: Loss = 0.409546
Epoch 3.54: Loss = 0.520721
Epoch 3.55: Loss = 0.419907
Epoch 3.56: Loss = 0.405594
Epoch 3.57: Loss = 0.386292
Epoch 3.58: Loss = 0.412872
Epoch 3.59: Loss = 0.49234
Epoch 3.60: Loss = 0.514084
Epoch 3.61: Loss = 0.475708
Epoch 3.62: Loss = 0.530457
Epoch 3.63: Loss = 0.564209
Epoch 3.64: Loss = 0.510025
Epoch 3.65: Loss = 0.57962
Epoch 3.66: Loss = 0.437469
Epoch 3.67: Loss = 0.429398
Epoch 3.68: Loss = 0.292282
Epoch 3.69: Loss = 0.353149
Epoch 3.70: Loss = 0.527679
Epoch 3.71: Loss = 0.375061
Epoch 3.72: Loss = 0.362747
Epoch 3.73: Loss = 0.424347
Epoch 3.74: Loss = 0.325119
Epoch 3.75: Loss = 0.577209
Epoch 3.76: Loss = 0.429016
Epoch 3.77: Loss = 0.361938
Epoch 3.78: Loss = 0.395889
Epoch 3.79: Loss = 0.455368
Epoch 3.80: Loss = 0.478455
Epoch 3.81: Loss = 0.360275
Epoch 3.82: Loss = 0.352859
Epoch 3.83: Loss = 0.498383
Epoch 3.84: Loss = 0.417831
Epoch 3.85: Loss = 0.567505
Epoch 3.86: Loss = 0.450851
Epoch 3.87: Loss = 0.334641
Epoch 3.88: Loss = 0.411423
Epoch 3.89: Loss = 0.517395
Epoch 3.90: Loss = 0.350998
Epoch 3.91: Loss = 0.46283
Epoch 3.92: Loss = 0.471237
Epoch 3.93: Loss = 0.519699
Epoch 3.94: Loss = 0.344238
Epoch 3.95: Loss = 0.435715
Epoch 3.96: Loss = 0.457367
Epoch 3.97: Loss = 0.329681
Epoch 3.98: Loss = 0.401627
Epoch 3.99: Loss = 0.505127
Epoch 3.100: Loss = 0.570496
Epoch 3.101: Loss = 0.531662
Epoch 3.102: Loss = 0.412659
Epoch 3.103: Loss = 0.348999
Epoch 3.104: Loss = 0.335037
Epoch 3.105: Loss = 0.490677
Epoch 3.106: Loss = 0.489304
Epoch 3.107: Loss = 0.343781
Epoch 3.108: Loss = 0.456802
Epoch 3.109: Loss = 0.369568
Epoch 3.110: Loss = 0.422287
Epoch 3.111: Loss = 0.338943
Epoch 3.112: Loss = 0.328751
Epoch 3.113: Loss = 0.395981
Epoch 3.114: Loss = 0.356094
Epoch 3.115: Loss = 0.342789
Epoch 3.116: Loss = 0.368683
Epoch 3.117: Loss = 0.267059
Epoch 3.118: Loss = 0.226715
Epoch 3.119: Loss = 0.289749
Epoch 3.120: Loss = 0.313828
TRAIN LOSS = 0.428253
TRAIN ACC = 87.0148 % (52212/60000)
Loss = 0.434601
Loss = 0.468124
Loss = 0.552612
Loss = 0.555054
Loss = 0.592636
Loss = 0.399277
Loss = 0.399979
Loss = 0.604218
Loss = 0.52475
Loss = 0.514252
Loss = 0.18605
Loss = 0.326813
Loss = 0.263962
Loss = 0.377426
Loss = 0.245178
Loss = 0.270493
Loss = 0.250656
Loss = 0.0808105
Loss = 0.242599
Loss = 0.53627
TEST LOSS = 0.391288
TEST ACC = 522.119 % (8820/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.380737
Epoch 4.2: Loss = 0.484314
Epoch 4.3: Loss = 0.466293
Epoch 4.4: Loss = 0.335175
Epoch 4.5: Loss = 0.346283
Epoch 4.6: Loss = 0.361313
Epoch 4.7: Loss = 0.375
Epoch 4.8: Loss = 0.389832
Epoch 4.9: Loss = 0.355545
Epoch 4.10: Loss = 0.408203
Epoch 4.11: Loss = 0.424332
Epoch 4.12: Loss = 0.376572
Epoch 4.13: Loss = 0.306839
Epoch 4.14: Loss = 0.355637
Epoch 4.15: Loss = 0.439682
Epoch 4.16: Loss = 0.439209
Epoch 4.17: Loss = 0.475433
Epoch 4.18: Loss = 0.568344
Epoch 4.19: Loss = 0.395935
Epoch 4.20: Loss = 0.351349
Epoch 4.21: Loss = 0.315323
Epoch 4.22: Loss = 0.316849
Epoch 4.23: Loss = 0.319794
Epoch 4.24: Loss = 0.519806
Epoch 4.25: Loss = 0.448517
Epoch 4.26: Loss = 0.549942
Epoch 4.27: Loss = 0.458221
Epoch 4.28: Loss = 0.491241
Epoch 4.29: Loss = 0.510696
Epoch 4.30: Loss = 0.563339
Epoch 4.31: Loss = 0.358475
Epoch 4.32: Loss = 0.461975
Epoch 4.33: Loss = 0.346741
Epoch 4.34: Loss = 0.440201
Epoch 4.35: Loss = 0.395111
Epoch 4.36: Loss = 0.505707
Epoch 4.37: Loss = 0.332901
Epoch 4.38: Loss = 0.335236
Epoch 4.39: Loss = 0.393509
Epoch 4.40: Loss = 0.383621
Epoch 4.41: Loss = 0.426239
Epoch 4.42: Loss = 0.544739
Epoch 4.43: Loss = 0.33049
Epoch 4.44: Loss = 0.318344
Epoch 4.45: Loss = 0.414307
Epoch 4.46: Loss = 0.443909
Epoch 4.47: Loss = 0.3797
Epoch 4.48: Loss = 0.389572
Epoch 4.49: Loss = 0.413681
Epoch 4.50: Loss = 0.482452
Epoch 4.51: Loss = 0.348419
Epoch 4.52: Loss = 0.318359
Epoch 4.53: Loss = 0.379623
Epoch 4.54: Loss = 0.514969
Epoch 4.55: Loss = 0.402115
Epoch 4.56: Loss = 0.371735
Epoch 4.57: Loss = 0.386093
Epoch 4.58: Loss = 0.388474
Epoch 4.59: Loss = 0.462143
Epoch 4.60: Loss = 0.500076
Epoch 4.61: Loss = 0.432297
Epoch 4.62: Loss = 0.509827
Epoch 4.63: Loss = 0.548126
Epoch 4.64: Loss = 0.501389
Epoch 4.65: Loss = 0.583206
Epoch 4.66: Loss = 0.417862
Epoch 4.67: Loss = 0.414993
Epoch 4.68: Loss = 0.270416
Epoch 4.69: Loss = 0.326797
Epoch 4.70: Loss = 0.507248
Epoch 4.71: Loss = 0.350311
Epoch 4.72: Loss = 0.336151
Epoch 4.73: Loss = 0.4039
Epoch 4.74: Loss = 0.305374
Epoch 4.75: Loss = 0.586578
Epoch 4.76: Loss = 0.409454
Epoch 4.77: Loss = 0.348221
Epoch 4.78: Loss = 0.385696
Epoch 4.79: Loss = 0.449875
Epoch 4.80: Loss = 0.473969
Epoch 4.81: Loss = 0.344955
Epoch 4.82: Loss = 0.32695
Epoch 4.83: Loss = 0.487549
Epoch 4.84: Loss = 0.404251
Epoch 4.85: Loss = 0.578156
Epoch 4.86: Loss = 0.452927
Epoch 4.87: Loss = 0.291794
Epoch 4.88: Loss = 0.3927
Epoch 4.89: Loss = 0.48526
Epoch 4.90: Loss = 0.31546
Epoch 4.91: Loss = 0.445923
Epoch 4.92: Loss = 0.461334
Epoch 4.93: Loss = 0.486343
Epoch 4.94: Loss = 0.312347
Epoch 4.95: Loss = 0.412399
Epoch 4.96: Loss = 0.465698
Epoch 4.97: Loss = 0.307724
Epoch 4.98: Loss = 0.369003
Epoch 4.99: Loss = 0.473267
Epoch 4.100: Loss = 0.537537
Epoch 4.101: Loss = 0.522308
Epoch 4.102: Loss = 0.389893
Epoch 4.103: Loss = 0.343857
Epoch 4.104: Loss = 0.309799
Epoch 4.105: Loss = 0.481262
Epoch 4.106: Loss = 0.473389
Epoch 4.107: Loss = 0.317307
Epoch 4.108: Loss = 0.436539
Epoch 4.109: Loss = 0.345169
Epoch 4.110: Loss = 0.415833
Epoch 4.111: Loss = 0.325867
Epoch 4.112: Loss = 0.320114
Epoch 4.113: Loss = 0.36647
Epoch 4.114: Loss = 0.319702
Epoch 4.115: Loss = 0.324631
Epoch 4.116: Loss = 0.356277
Epoch 4.117: Loss = 0.24263
Epoch 4.118: Loss = 0.22847
Epoch 4.119: Loss = 0.263901
Epoch 4.120: Loss = 0.308929
TRAIN LOSS = 0.405869
TRAIN ACC = 87.8479 % (52711/60000)
Loss = 0.392044
Loss = 0.46579
Loss = 0.520859
Loss = 0.547668
Loss = 0.571823
Loss = 0.387405
Loss = 0.371414
Loss = 0.583923
Loss = 0.510742
Loss = 0.495438
Loss = 0.152954
Loss = 0.2957
Loss = 0.254913
Loss = 0.342697
Loss = 0.212875
Loss = 0.252838
Loss = 0.217697
Loss = 0.0609131
Loss = 0.220932
Loss = 0.522598
TEST LOSS = 0.369061
TEST ACC = 527.109 % (8918/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.362717
Epoch 5.2: Loss = 0.460632
Epoch 5.3: Loss = 0.455429
Epoch 5.4: Loss = 0.316895
Epoch 5.5: Loss = 0.335983
Epoch 5.6: Loss = 0.351227
Epoch 5.7: Loss = 0.364853
Epoch 5.8: Loss = 0.36676
Epoch 5.9: Loss = 0.345642
Epoch 5.10: Loss = 0.395279
Epoch 5.11: Loss = 0.39743
Epoch 5.12: Loss = 0.36824
Epoch 5.13: Loss = 0.290817
Epoch 5.14: Loss = 0.32785
Epoch 5.15: Loss = 0.410019
Epoch 5.16: Loss = 0.438065
Epoch 5.17: Loss = 0.455948
Epoch 5.18: Loss = 0.547974
Epoch 5.19: Loss = 0.39241
Epoch 5.20: Loss = 0.336212
Epoch 5.21: Loss = 0.301682
Epoch 5.22: Loss = 0.2995
Epoch 5.23: Loss = 0.296432
Epoch 5.24: Loss = 0.483017
Epoch 5.25: Loss = 0.424911
Epoch 5.26: Loss = 0.545227
Epoch 5.27: Loss = 0.442657
Epoch 5.28: Loss = 0.487625
Epoch 5.29: Loss = 0.498688
Epoch 5.30: Loss = 0.549088
Epoch 5.31: Loss = 0.344543
Epoch 5.32: Loss = 0.428116
Epoch 5.33: Loss = 0.314468
Epoch 5.34: Loss = 0.430374
Epoch 5.35: Loss = 0.373032
Epoch 5.36: Loss = 0.493988
Epoch 5.37: Loss = 0.324661
Epoch 5.38: Loss = 0.325043
Epoch 5.39: Loss = 0.366486
Epoch 5.40: Loss = 0.37886
Epoch 5.41: Loss = 0.390656
Epoch 5.42: Loss = 0.559509
Epoch 5.43: Loss = 0.301361
Epoch 5.44: Loss = 0.298157
Epoch 5.45: Loss = 0.362381
Epoch 5.46: Loss = 0.424728
Epoch 5.47: Loss = 0.357986
Epoch 5.48: Loss = 0.36824
Epoch 5.49: Loss = 0.386047
Epoch 5.50: Loss = 0.453659
Epoch 5.51: Loss = 0.341125
Epoch 5.52: Loss = 0.319641
Epoch 5.53: Loss = 0.371948
Epoch 5.54: Loss = 0.494461
Epoch 5.55: Loss = 0.382446
Epoch 5.56: Loss = 0.374344
Epoch 5.57: Loss = 0.373978
Epoch 5.58: Loss = 0.37532
Epoch 5.59: Loss = 0.436798
Epoch 5.60: Loss = 0.477921
Epoch 5.61: Loss = 0.416382
Epoch 5.62: Loss = 0.502548
Epoch 5.63: Loss = 0.548843
Epoch 5.64: Loss = 0.480072
Epoch 5.65: Loss = 0.539307
Epoch 5.66: Loss = 0.412628
Epoch 5.67: Loss = 0.398605
Epoch 5.68: Loss = 0.244537
Epoch 5.69: Loss = 0.307159
Epoch 5.70: Loss = 0.476563
Epoch 5.71: Loss = 0.340286
Epoch 5.72: Loss = 0.315887
Epoch 5.73: Loss = 0.375427
Epoch 5.74: Loss = 0.296432
Epoch 5.75: Loss = 0.59256
Epoch 5.76: Loss = 0.413971
Epoch 5.77: Loss = 0.332123
Epoch 5.78: Loss = 0.376144
Epoch 5.79: Loss = 0.471619
Epoch 5.80: Loss = 0.438187
Epoch 5.81: Loss = 0.324265
Epoch 5.82: Loss = 0.318481
Epoch 5.83: Loss = 0.487885
Epoch 5.84: Loss = 0.387451
Epoch 5.85: Loss = 0.525681
Epoch 5.86: Loss = 0.445145
Epoch 5.87: Loss = 0.292404
Epoch 5.88: Loss = 0.394974
Epoch 5.89: Loss = 0.459549
Epoch 5.90: Loss = 0.318726
Epoch 5.91: Loss = 0.446762
Epoch 5.92: Loss = 0.456375
Epoch 5.93: Loss = 0.505341
Epoch 5.94: Loss = 0.288666
Epoch 5.95: Loss = 0.398148
Epoch 5.96: Loss = 0.44278
Epoch 5.97: Loss = 0.303391
Epoch 5.98: Loss = 0.356277
Epoch 5.99: Loss = 0.472626
Epoch 5.100: Loss = 0.568008
Epoch 5.101: Loss = 0.519501
Epoch 5.102: Loss = 0.383636
Epoch 5.103: Loss = 0.313217
Epoch 5.104: Loss = 0.315735
Epoch 5.105: Loss = 0.477997
Epoch 5.106: Loss = 0.491898
Epoch 5.107: Loss = 0.304855
Epoch 5.108: Loss = 0.452286
Epoch 5.109: Loss = 0.324142
Epoch 5.110: Loss = 0.41861
Epoch 5.111: Loss = 0.316727
Epoch 5.112: Loss = 0.324249
Epoch 5.113: Loss = 0.35733
Epoch 5.114: Loss = 0.288589
Epoch 5.115: Loss = 0.294861
Epoch 5.116: Loss = 0.339828
Epoch 5.117: Loss = 0.238052
Epoch 5.118: Loss = 0.21521
Epoch 5.119: Loss = 0.270737
Epoch 5.120: Loss = 0.3078
TRAIN LOSS = 0.392548
TRAIN ACC = 88.5498 % (53133/60000)
Loss = 0.387787
Loss = 0.446243
Loss = 0.507629
Loss = 0.550095
Loss = 0.582764
Loss = 0.388367
Loss = 0.364349
Loss = 0.593414
Loss = 0.517502
Loss = 0.491821
Loss = 0.164169
Loss = 0.291183
Loss = 0.284836
Loss = 0.342224
Loss = 0.195374
Loss = 0.242676
Loss = 0.202438
Loss = 0.0501404
Loss = 0.214157
Loss = 0.505341
TEST LOSS = 0.366125
TEST ACC = 531.329 % (8963/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.363586
Epoch 6.2: Loss = 0.468704
Epoch 6.3: Loss = 0.470078
Epoch 6.4: Loss = 0.301178
Epoch 6.5: Loss = 0.328522
Epoch 6.6: Loss = 0.326691
Epoch 6.7: Loss = 0.343262
Epoch 6.8: Loss = 0.35878
Epoch 6.9: Loss = 0.346725
Epoch 6.10: Loss = 0.384323
Epoch 6.11: Loss = 0.389053
Epoch 6.12: Loss = 0.36174
Epoch 6.13: Loss = 0.288116
Epoch 6.14: Loss = 0.32312
Epoch 6.15: Loss = 0.397614
Epoch 6.16: Loss = 0.41568
Epoch 6.17: Loss = 0.447876
Epoch 6.18: Loss = 0.569489
Epoch 6.19: Loss = 0.390228
Epoch 6.20: Loss = 0.345673
Epoch 6.21: Loss = 0.305328
Epoch 6.22: Loss = 0.288101
Epoch 6.23: Loss = 0.320358
Epoch 6.24: Loss = 0.504044
Epoch 6.25: Loss = 0.429855
Epoch 6.26: Loss = 0.509323
Epoch 6.27: Loss = 0.462143
Epoch 6.28: Loss = 0.475189
Epoch 6.29: Loss = 0.493011
Epoch 6.30: Loss = 0.545319
Epoch 6.31: Loss = 0.337997
Epoch 6.32: Loss = 0.424271
Epoch 6.33: Loss = 0.321121
Epoch 6.34: Loss = 0.417084
Epoch 6.35: Loss = 0.369827
Epoch 6.36: Loss = 0.494583
Epoch 6.37: Loss = 0.30191
Epoch 6.38: Loss = 0.316055
Epoch 6.39: Loss = 0.352844
Epoch 6.40: Loss = 0.384048
Epoch 6.41: Loss = 0.388397
Epoch 6.42: Loss = 0.574631
Epoch 6.43: Loss = 0.308624
Epoch 6.44: Loss = 0.29541
Epoch 6.45: Loss = 0.377899
Epoch 6.46: Loss = 0.443741
Epoch 6.47: Loss = 0.353943
Epoch 6.48: Loss = 0.390884
Epoch 6.49: Loss = 0.36937
Epoch 6.50: Loss = 0.456436
Epoch 6.51: Loss = 0.329361
Epoch 6.52: Loss = 0.328995
Epoch 6.53: Loss = 0.350784
Epoch 6.54: Loss = 0.485336
Epoch 6.55: Loss = 0.371567
Epoch 6.56: Loss = 0.366196
Epoch 6.57: Loss = 0.355316
Epoch 6.58: Loss = 0.404205
Epoch 6.59: Loss = 0.428986
Epoch 6.60: Loss = 0.473892
Epoch 6.61: Loss = 0.425522
Epoch 6.62: Loss = 0.480347
Epoch 6.63: Loss = 0.547684
Epoch 6.64: Loss = 0.469116
Epoch 6.65: Loss = 0.53363
Epoch 6.66: Loss = 0.376526
Epoch 6.67: Loss = 0.371887
Epoch 6.68: Loss = 0.217987
Epoch 6.69: Loss = 0.314728
Epoch 6.70: Loss = 0.493515
Epoch 6.71: Loss = 0.335892
Epoch 6.72: Loss = 0.322784
Epoch 6.73: Loss = 0.373199
Epoch 6.74: Loss = 0.315643
Epoch 6.75: Loss = 0.602539
Epoch 6.76: Loss = 0.417725
Epoch 6.77: Loss = 0.331772
Epoch 6.78: Loss = 0.383194
Epoch 6.79: Loss = 0.453476
Epoch 6.80: Loss = 0.449615
Epoch 6.81: Loss = 0.318542
Epoch 6.82: Loss = 0.340195
Epoch 6.83: Loss = 0.486221
Epoch 6.84: Loss = 0.389359
Epoch 6.85: Loss = 0.531357
Epoch 6.86: Loss = 0.446548
Epoch 6.87: Loss = 0.274109
Epoch 6.88: Loss = 0.368851
Epoch 6.89: Loss = 0.484741
Epoch 6.90: Loss = 0.322739
Epoch 6.91: Loss = 0.443863
Epoch 6.92: Loss = 0.455399
Epoch 6.93: Loss = 0.495712
Epoch 6.94: Loss = 0.274689
Epoch 6.95: Loss = 0.3927
Epoch 6.96: Loss = 0.4785
Epoch 6.97: Loss = 0.287338
Epoch 6.98: Loss = 0.351959
Epoch 6.99: Loss = 0.460617
Epoch 6.100: Loss = 0.558884
Epoch 6.101: Loss = 0.526352
Epoch 6.102: Loss = 0.36232
Epoch 6.103: Loss = 0.327271
Epoch 6.104: Loss = 0.332703
Epoch 6.105: Loss = 0.466919
Epoch 6.106: Loss = 0.498535
Epoch 6.107: Loss = 0.305191
Epoch 6.108: Loss = 0.478729
Epoch 6.109: Loss = 0.324097
Epoch 6.110: Loss = 0.417084
Epoch 6.111: Loss = 0.305176
Epoch 6.112: Loss = 0.319901
Epoch 6.113: Loss = 0.364731
Epoch 6.114: Loss = 0.293945
Epoch 6.115: Loss = 0.282806
Epoch 6.116: Loss = 0.319321
Epoch 6.117: Loss = 0.233154
Epoch 6.118: Loss = 0.187149
Epoch 6.119: Loss = 0.255936
Epoch 6.120: Loss = 0.323257
TRAIN LOSS = 0.390244
TRAIN ACC = 88.7833 % (53272/60000)
Loss = 0.377655
Loss = 0.471497
Loss = 0.518173
Loss = 0.518585
Loss = 0.609604
Loss = 0.391602
Loss = 0.367783
Loss = 0.594315
Loss = 0.519974
Loss = 0.492416
Loss = 0.156693
Loss = 0.28952
Loss = 0.294662
Loss = 0.320511
Loss = 0.201248
Loss = 0.252686
Loss = 0.203247
Loss = 0.0471344
Loss = 0.216858
Loss = 0.518723
TEST LOSS = 0.368144
TEST ACC = 532.719 % (8981/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.356049
Epoch 7.2: Loss = 0.466705
Epoch 7.3: Loss = 0.468613
Epoch 7.4: Loss = 0.310654
Epoch 7.5: Loss = 0.324661
Epoch 7.6: Loss = 0.349808
Epoch 7.7: Loss = 0.361618
Epoch 7.8: Loss = 0.361221
Epoch 7.9: Loss = 0.331177
Epoch 7.10: Loss = 0.39444
Epoch 7.11: Loss = 0.385681
Epoch 7.12: Loss = 0.38382
Epoch 7.13: Loss = 0.27951
Epoch 7.14: Loss = 0.315659
Epoch 7.15: Loss = 0.425964
Epoch 7.16: Loss = 0.406723
Epoch 7.17: Loss = 0.457199
Epoch 7.18: Loss = 0.565781
Epoch 7.19: Loss = 0.404877
Epoch 7.20: Loss = 0.330368
Epoch 7.21: Loss = 0.299911
Epoch 7.22: Loss = 0.298721
Epoch 7.23: Loss = 0.320099
Epoch 7.24: Loss = 0.486771
Epoch 7.25: Loss = 0.398254
Epoch 7.26: Loss = 0.514832
Epoch 7.27: Loss = 0.452377
Epoch 7.28: Loss = 0.459457
Epoch 7.29: Loss = 0.50679
Epoch 7.30: Loss = 0.536789
Epoch 7.31: Loss = 0.334503
Epoch 7.32: Loss = 0.436768
Epoch 7.33: Loss = 0.315872
Epoch 7.34: Loss = 0.413284
Epoch 7.35: Loss = 0.378128
Epoch 7.36: Loss = 0.486649
Epoch 7.37: Loss = 0.302567
Epoch 7.38: Loss = 0.318787
Epoch 7.39: Loss = 0.336777
Epoch 7.40: Loss = 0.403244
Epoch 7.41: Loss = 0.382355
Epoch 7.42: Loss = 0.574036
Epoch 7.43: Loss = 0.31131
Epoch 7.44: Loss = 0.29541
Epoch 7.45: Loss = 0.374344
Epoch 7.46: Loss = 0.42778
Epoch 7.47: Loss = 0.355118
Epoch 7.48: Loss = 0.391357
Epoch 7.49: Loss = 0.376236
Epoch 7.50: Loss = 0.459885
Epoch 7.51: Loss = 0.310593
Epoch 7.52: Loss = 0.317886
Epoch 7.53: Loss = 0.350754
Epoch 7.54: Loss = 0.500412
Epoch 7.55: Loss = 0.386963
Epoch 7.56: Loss = 0.373276
Epoch 7.57: Loss = 0.361923
Epoch 7.58: Loss = 0.381989
Epoch 7.59: Loss = 0.440323
Epoch 7.60: Loss = 0.496338
Epoch 7.61: Loss = 0.438553
Epoch 7.62: Loss = 0.484634
Epoch 7.63: Loss = 0.543564
Epoch 7.64: Loss = 0.470596
Epoch 7.65: Loss = 0.526215
Epoch 7.66: Loss = 0.373901
Epoch 7.67: Loss = 0.367416
Epoch 7.68: Loss = 0.212921
Epoch 7.69: Loss = 0.291656
Epoch 7.70: Loss = 0.489929
Epoch 7.71: Loss = 0.33252
Epoch 7.72: Loss = 0.317642
Epoch 7.73: Loss = 0.379776
Epoch 7.74: Loss = 0.289597
Epoch 7.75: Loss = 0.62738
Epoch 7.76: Loss = 0.407501
Epoch 7.77: Loss = 0.346252
Epoch 7.78: Loss = 0.380249
Epoch 7.79: Loss = 0.462021
Epoch 7.80: Loss = 0.405197
Epoch 7.81: Loss = 0.33902
Epoch 7.82: Loss = 0.33461
Epoch 7.83: Loss = 0.461334
Epoch 7.84: Loss = 0.376678
Epoch 7.85: Loss = 0.547699
Epoch 7.86: Loss = 0.441132
Epoch 7.87: Loss = 0.274399
Epoch 7.88: Loss = 0.372543
Epoch 7.89: Loss = 0.483353
Epoch 7.90: Loss = 0.313385
Epoch 7.91: Loss = 0.445007
Epoch 7.92: Loss = 0.431717
Epoch 7.93: Loss = 0.519394
Epoch 7.94: Loss = 0.266922
Epoch 7.95: Loss = 0.383072
Epoch 7.96: Loss = 0.481705
Epoch 7.97: Loss = 0.315353
Epoch 7.98: Loss = 0.363617
Epoch 7.99: Loss = 0.457504
Epoch 7.100: Loss = 0.593185
Epoch 7.101: Loss = 0.52597
Epoch 7.102: Loss = 0.369522
Epoch 7.103: Loss = 0.32402
Epoch 7.104: Loss = 0.324371
Epoch 7.105: Loss = 0.458313
Epoch 7.106: Loss = 0.514404
Epoch 7.107: Loss = 0.294754
Epoch 7.108: Loss = 0.487579
Epoch 7.109: Loss = 0.332352
Epoch 7.110: Loss = 0.410889
Epoch 7.111: Loss = 0.307159
Epoch 7.112: Loss = 0.323761
Epoch 7.113: Loss = 0.36586
Epoch 7.114: Loss = 0.273743
Epoch 7.115: Loss = 0.284286
Epoch 7.116: Loss = 0.315765
Epoch 7.117: Loss = 0.221985
Epoch 7.118: Loss = 0.184235
Epoch 7.119: Loss = 0.26651
Epoch 7.120: Loss = 0.328217
TRAIN LOSS = 0.390091
TRAIN ACC = 89.1418 % (53488/60000)
Loss = 0.37204
Loss = 0.490356
Loss = 0.510498
Loss = 0.552643
Loss = 0.610458
Loss = 0.377289
Loss = 0.367798
Loss = 0.595749
Loss = 0.54245
Loss = 0.481033
Loss = 0.166534
Loss = 0.311234
Loss = 0.30159
Loss = 0.308624
Loss = 0.19075
Loss = 0.244186
Loss = 0.195663
Loss = 0.0471191
Loss = 0.227493
Loss = 0.517044
TEST LOSS = 0.370528
TEST ACC = 534.879 % (8989/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.362946
Epoch 8.2: Loss = 0.457138
Epoch 8.3: Loss = 0.460876
Epoch 8.4: Loss = 0.316635
Epoch 8.5: Loss = 0.319107
Epoch 8.6: Loss = 0.339294
Epoch 8.7: Loss = 0.352844
Epoch 8.8: Loss = 0.355103
Epoch 8.9: Loss = 0.355362
Epoch 8.10: Loss = 0.388367
Epoch 8.11: Loss = 0.388351
Epoch 8.12: Loss = 0.367508
Epoch 8.13: Loss = 0.262924
Epoch 8.14: Loss = 0.311905
Epoch 8.15: Loss = 0.417511
Epoch 8.16: Loss = 0.388153
Epoch 8.17: Loss = 0.43927
Epoch 8.18: Loss = 0.576797
Epoch 8.19: Loss = 0.398361
Epoch 8.20: Loss = 0.32048
Epoch 8.21: Loss = 0.310913
Epoch 8.22: Loss = 0.309174
Epoch 8.23: Loss = 0.324982
Epoch 8.24: Loss = 0.496307
Epoch 8.25: Loss = 0.390274
Epoch 8.26: Loss = 0.51976
Epoch 8.27: Loss = 0.474625
Epoch 8.28: Loss = 0.46907
Epoch 8.29: Loss = 0.513718
Epoch 8.30: Loss = 0.549683
Epoch 8.31: Loss = 0.33316
Epoch 8.32: Loss = 0.403824
Epoch 8.33: Loss = 0.314011
Epoch 8.34: Loss = 0.398483
Epoch 8.35: Loss = 0.375961
Epoch 8.36: Loss = 0.49292
Epoch 8.37: Loss = 0.30751
Epoch 8.38: Loss = 0.329834
Epoch 8.39: Loss = 0.348129
Epoch 8.40: Loss = 0.394501
Epoch 8.41: Loss = 0.375107
Epoch 8.42: Loss = 0.60437
Epoch 8.43: Loss = 0.316986
Epoch 8.44: Loss = 0.295319
Epoch 8.45: Loss = 0.361816
Epoch 8.46: Loss = 0.435608
Epoch 8.47: Loss = 0.36113
Epoch 8.48: Loss = 0.383575
Epoch 8.49: Loss = 0.350128
Epoch 8.50: Loss = 0.459702
Epoch 8.51: Loss = 0.292404
Epoch 8.52: Loss = 0.324448
Epoch 8.53: Loss = 0.348816
Epoch 8.54: Loss = 0.49469
Epoch 8.55: Loss = 0.376511
Epoch 8.56: Loss = 0.372314
Epoch 8.57: Loss = 0.360657
Epoch 8.58: Loss = 0.413696
Epoch 8.59: Loss = 0.455521
Epoch 8.60: Loss = 0.52597
Epoch 8.61: Loss = 0.420181
Epoch 8.62: Loss = 0.495331
Epoch 8.63: Loss = 0.533936
Epoch 8.64: Loss = 0.471313
Epoch 8.65: Loss = 0.546616
Epoch 8.66: Loss = 0.35965
Epoch 8.67: Loss = 0.367569
Epoch 8.68: Loss = 0.202988
Epoch 8.69: Loss = 0.278534
Epoch 8.70: Loss = 0.485291
Epoch 8.71: Loss = 0.347946
Epoch 8.72: Loss = 0.314606
Epoch 8.73: Loss = 0.389343
Epoch 8.74: Loss = 0.280746
Epoch 8.75: Loss = 0.610672
Epoch 8.76: Loss = 0.412796
Epoch 8.77: Loss = 0.358887
Epoch 8.78: Loss = 0.380447
Epoch 8.79: Loss = 0.456772
Epoch 8.80: Loss = 0.424164
Epoch 8.81: Loss = 0.348373
Epoch 8.82: Loss = 0.345322
Epoch 8.83: Loss = 0.464981
Epoch 8.84: Loss = 0.383423
Epoch 8.85: Loss = 0.528931
Epoch 8.86: Loss = 0.463425
Epoch 8.87: Loss = 0.268112
Epoch 8.88: Loss = 0.365372
Epoch 8.89: Loss = 0.481171
Epoch 8.90: Loss = 0.336105
Epoch 8.91: Loss = 0.421906
Epoch 8.92: Loss = 0.432739
Epoch 8.93: Loss = 0.520172
Epoch 8.94: Loss = 0.282852
Epoch 8.95: Loss = 0.384277
Epoch 8.96: Loss = 0.448227
Epoch 8.97: Loss = 0.322403
Epoch 8.98: Loss = 0.369522
Epoch 8.99: Loss = 0.43277
Epoch 8.100: Loss = 0.592072
Epoch 8.101: Loss = 0.522247
Epoch 8.102: Loss = 0.371338
Epoch 8.103: Loss = 0.327042
Epoch 8.104: Loss = 0.324722
Epoch 8.105: Loss = 0.483795
Epoch 8.106: Loss = 0.520157
Epoch 8.107: Loss = 0.307053
Epoch 8.108: Loss = 0.509445
Epoch 8.109: Loss = 0.303818
Epoch 8.110: Loss = 0.42601
Epoch 8.111: Loss = 0.308884
Epoch 8.112: Loss = 0.307983
Epoch 8.113: Loss = 0.346375
Epoch 8.114: Loss = 0.28566
Epoch 8.115: Loss = 0.259216
Epoch 8.116: Loss = 0.318329
Epoch 8.117: Loss = 0.206772
Epoch 8.118: Loss = 0.186356
Epoch 8.119: Loss = 0.245285
Epoch 8.120: Loss = 0.334488
TRAIN LOSS = 0.38974
TRAIN ACC = 89.1922 % (53518/60000)
Loss = 0.370773
Loss = 0.469513
Loss = 0.541611
Loss = 0.542236
Loss = 0.58844
Loss = 0.382248
Loss = 0.361832
Loss = 0.604553
Loss = 0.53154
Loss = 0.474747
Loss = 0.169373
Loss = 0.30928
Loss = 0.2836
Loss = 0.309235
Loss = 0.199326
Loss = 0.267212
Loss = 0.186127
Loss = 0.0437622
Loss = 0.231918
Loss = 0.518997
TEST LOSS = 0.369316
TEST ACC = 535.179 % (8998/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.337387
Epoch 9.2: Loss = 0.470306
Epoch 9.3: Loss = 0.460236
Epoch 9.4: Loss = 0.311462
Epoch 9.5: Loss = 0.296631
Epoch 9.6: Loss = 0.354492
Epoch 9.7: Loss = 0.356125
Epoch 9.8: Loss = 0.355972
Epoch 9.9: Loss = 0.349503
Epoch 9.10: Loss = 0.385269
Epoch 9.11: Loss = 0.400711
Epoch 9.12: Loss = 0.362518
Epoch 9.13: Loss = 0.269501
Epoch 9.14: Loss = 0.317947
Epoch 9.15: Loss = 0.446976
Epoch 9.16: Loss = 0.414642
Epoch 9.17: Loss = 0.47287
Epoch 9.18: Loss = 0.589233
Epoch 9.19: Loss = 0.398895
Epoch 9.20: Loss = 0.314575
Epoch 9.21: Loss = 0.325989
Epoch 9.22: Loss = 0.31778
Epoch 9.23: Loss = 0.313705
Epoch 9.24: Loss = 0.527451
Epoch 9.25: Loss = 0.400894
Epoch 9.26: Loss = 0.514099
Epoch 9.27: Loss = 0.470398
Epoch 9.28: Loss = 0.451752
Epoch 9.29: Loss = 0.51619
Epoch 9.30: Loss = 0.514755
Epoch 9.31: Loss = 0.342819
Epoch 9.32: Loss = 0.43721
Epoch 9.33: Loss = 0.33786
Epoch 9.34: Loss = 0.385574
Epoch 9.35: Loss = 0.383316
Epoch 9.36: Loss = 0.477676
Epoch 9.37: Loss = 0.292679
Epoch 9.38: Loss = 0.340286
Epoch 9.39: Loss = 0.345169
Epoch 9.40: Loss = 0.367416
Epoch 9.41: Loss = 0.373016
Epoch 9.42: Loss = 0.561157
Epoch 9.43: Loss = 0.310318
Epoch 9.44: Loss = 0.318573
Epoch 9.45: Loss = 0.362717
Epoch 9.46: Loss = 0.410294
Epoch 9.47: Loss = 0.360947
Epoch 9.48: Loss = 0.376343
Epoch 9.49: Loss = 0.352875
Epoch 9.50: Loss = 0.469971
Epoch 9.51: Loss = 0.296051
Epoch 9.52: Loss = 0.321747
Epoch 9.53: Loss = 0.348663
Epoch 9.54: Loss = 0.486633
Epoch 9.55: Loss = 0.373886
Epoch 9.56: Loss = 0.363113
Epoch 9.57: Loss = 0.370163
Epoch 9.58: Loss = 0.408524
Epoch 9.59: Loss = 0.450531
Epoch 9.60: Loss = 0.488724
Epoch 9.61: Loss = 0.421066
Epoch 9.62: Loss = 0.493622
Epoch 9.63: Loss = 0.541122
Epoch 9.64: Loss = 0.459503
Epoch 9.65: Loss = 0.538681
Epoch 9.66: Loss = 0.35202
Epoch 9.67: Loss = 0.365891
Epoch 9.68: Loss = 0.19606
Epoch 9.69: Loss = 0.265045
Epoch 9.70: Loss = 0.494873
Epoch 9.71: Loss = 0.352707
Epoch 9.72: Loss = 0.319138
Epoch 9.73: Loss = 0.386856
Epoch 9.74: Loss = 0.289536
Epoch 9.75: Loss = 0.655136
Epoch 9.76: Loss = 0.42421
Epoch 9.77: Loss = 0.349167
Epoch 9.78: Loss = 0.380463
Epoch 9.79: Loss = 0.456268
Epoch 9.80: Loss = 0.431198
Epoch 9.81: Loss = 0.350937
Epoch 9.82: Loss = 0.332794
Epoch 9.83: Loss = 0.467804
Epoch 9.84: Loss = 0.414856
Epoch 9.85: Loss = 0.532104
Epoch 9.86: Loss = 0.440292
Epoch 9.87: Loss = 0.285934
Epoch 9.88: Loss = 0.375702
Epoch 9.89: Loss = 0.503815
Epoch 9.90: Loss = 0.338806
Epoch 9.91: Loss = 0.4039
Epoch 9.92: Loss = 0.4617
Epoch 9.93: Loss = 0.507172
Epoch 9.94: Loss = 0.268372
Epoch 9.95: Loss = 0.378601
Epoch 9.96: Loss = 0.452301
Epoch 9.97: Loss = 0.307816
Epoch 9.98: Loss = 0.382431
Epoch 9.99: Loss = 0.43985
Epoch 9.100: Loss = 0.573364
Epoch 9.101: Loss = 0.536484
Epoch 9.102: Loss = 0.388031
Epoch 9.103: Loss = 0.321014
Epoch 9.104: Loss = 0.330658
Epoch 9.105: Loss = 0.486298
Epoch 9.106: Loss = 0.530579
Epoch 9.107: Loss = 0.30426
Epoch 9.108: Loss = 0.49614
Epoch 9.109: Loss = 0.318817
Epoch 9.110: Loss = 0.409012
Epoch 9.111: Loss = 0.318573
Epoch 9.112: Loss = 0.308517
Epoch 9.113: Loss = 0.349182
Epoch 9.114: Loss = 0.272644
Epoch 9.115: Loss = 0.280426
Epoch 9.116: Loss = 0.352203
Epoch 9.117: Loss = 0.19928
Epoch 9.118: Loss = 0.181473
Epoch 9.119: Loss = 0.249374
Epoch 9.120: Loss = 0.331848
TRAIN LOSS = 0.390732
TRAIN ACC = 89.3219 % (53596/60000)
Loss = 0.384613
Loss = 0.470718
Loss = 0.526367
Loss = 0.561111
Loss = 0.582062
Loss = 0.384506
Loss = 0.353012
Loss = 0.607651
Loss = 0.535385
Loss = 0.465958
Loss = 0.174805
Loss = 0.284775
Loss = 0.297684
Loss = 0.317657
Loss = 0.195648
Loss = 0.282959
Loss = 0.204132
Loss = 0.0436249
Loss = 0.229218
Loss = 0.544968
TEST LOSS = 0.372343
TEST ACC = 535.959 % (8989/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.366486
Epoch 10.2: Loss = 0.485611
Epoch 10.3: Loss = 0.469635
Epoch 10.4: Loss = 0.305283
Epoch 10.5: Loss = 0.300903
Epoch 10.6: Loss = 0.338882
Epoch 10.7: Loss = 0.336243
Epoch 10.8: Loss = 0.346588
Epoch 10.9: Loss = 0.3508
Epoch 10.10: Loss = 0.380951
Epoch 10.11: Loss = 0.40303
Epoch 10.12: Loss = 0.358719
Epoch 10.13: Loss = 0.250839
Epoch 10.14: Loss = 0.321869
Epoch 10.15: Loss = 0.442734
Epoch 10.16: Loss = 0.406418
Epoch 10.17: Loss = 0.457535
Epoch 10.18: Loss = 0.624542
Epoch 10.19: Loss = 0.414642
Epoch 10.20: Loss = 0.32518
Epoch 10.21: Loss = 0.349716
Epoch 10.22: Loss = 0.313034
Epoch 10.23: Loss = 0.333221
Epoch 10.24: Loss = 0.513275
Epoch 10.25: Loss = 0.401978
Epoch 10.26: Loss = 0.516815
Epoch 10.27: Loss = 0.459259
Epoch 10.28: Loss = 0.453949
Epoch 10.29: Loss = 0.521393
Epoch 10.30: Loss = 0.509323
Epoch 10.31: Loss = 0.332718
Epoch 10.32: Loss = 0.44989
Epoch 10.33: Loss = 0.343216
Epoch 10.34: Loss = 0.383606
Epoch 10.35: Loss = 0.402039
Epoch 10.36: Loss = 0.485336
Epoch 10.37: Loss = 0.298126
Epoch 10.38: Loss = 0.356049
Epoch 10.39: Loss = 0.32814
Epoch 10.40: Loss = 0.353928
Epoch 10.41: Loss = 0.369156
Epoch 10.42: Loss = 0.592834
Epoch 10.43: Loss = 0.299408
Epoch 10.44: Loss = 0.334564
Epoch 10.45: Loss = 0.368088
Epoch 10.46: Loss = 0.423477
Epoch 10.47: Loss = 0.385956
Epoch 10.48: Loss = 0.424301
Epoch 10.49: Loss = 0.348648
Epoch 10.50: Loss = 0.494415
Epoch 10.51: Loss = 0.303329
Epoch 10.52: Loss = 0.333664
Epoch 10.53: Loss = 0.364502
Epoch 10.54: Loss = 0.506485
Epoch 10.55: Loss = 0.395966
Epoch 10.56: Loss = 0.369247
Epoch 10.57: Loss = 0.363525
Epoch 10.58: Loss = 0.424103
Epoch 10.59: Loss = 0.454422
Epoch 10.60: Loss = 0.512451
Epoch 10.61: Loss = 0.412979
Epoch 10.62: Loss = 0.484009
Epoch 10.63: Loss = 0.539459
Epoch 10.64: Loss = 0.495895
Epoch 10.65: Loss = 0.539597
Epoch 10.66: Loss = 0.360214
Epoch 10.67: Loss = 0.392303
Epoch 10.68: Loss = 0.203552
Epoch 10.69: Loss = 0.269699
Epoch 10.70: Loss = 0.530655
Epoch 10.71: Loss = 0.346268
Epoch 10.72: Loss = 0.33017
Epoch 10.73: Loss = 0.382217
Epoch 10.74: Loss = 0.301971
Epoch 10.75: Loss = 0.649612
Epoch 10.76: Loss = 0.435867
Epoch 10.77: Loss = 0.345566
Epoch 10.78: Loss = 0.37973
Epoch 10.79: Loss = 0.462997
Epoch 10.80: Loss = 0.44632
Epoch 10.81: Loss = 0.359833
Epoch 10.82: Loss = 0.316071
Epoch 10.83: Loss = 0.457825
Epoch 10.84: Loss = 0.413147
Epoch 10.85: Loss = 0.589035
Epoch 10.86: Loss = 0.473938
Epoch 10.87: Loss = 0.290894
Epoch 10.88: Loss = 0.393494
Epoch 10.89: Loss = 0.514038
Epoch 10.90: Loss = 0.361343
Epoch 10.91: Loss = 0.430176
Epoch 10.92: Loss = 0.479095
Epoch 10.93: Loss = 0.53949
Epoch 10.94: Loss = 0.267136
Epoch 10.95: Loss = 0.383133
Epoch 10.96: Loss = 0.456619
Epoch 10.97: Loss = 0.317184
Epoch 10.98: Loss = 0.409912
Epoch 10.99: Loss = 0.435318
Epoch 10.100: Loss = 0.548981
Epoch 10.101: Loss = 0.572723
Epoch 10.102: Loss = 0.350983
Epoch 10.103: Loss = 0.302597
Epoch 10.104: Loss = 0.318878
Epoch 10.105: Loss = 0.487457
Epoch 10.106: Loss = 0.527725
Epoch 10.107: Loss = 0.295242
Epoch 10.108: Loss = 0.49234
Epoch 10.109: Loss = 0.299698
Epoch 10.110: Loss = 0.413483
Epoch 10.111: Loss = 0.325867
Epoch 10.112: Loss = 0.322021
Epoch 10.113: Loss = 0.340042
Epoch 10.114: Loss = 0.29248
Epoch 10.115: Loss = 0.282074
Epoch 10.116: Loss = 0.361649
Epoch 10.117: Loss = 0.219208
Epoch 10.118: Loss = 0.183197
Epoch 10.119: Loss = 0.23938
Epoch 10.120: Loss = 0.34314
TRAIN LOSS = 0.396469
TRAIN ACC = 89.3951 % (53640/60000)
Loss = 0.403381
Loss = 0.448639
Loss = 0.500427
Loss = 0.557266
Loss = 0.574493
Loss = 0.376587
Loss = 0.352798
Loss = 0.604187
Loss = 0.543839
Loss = 0.460999
Loss = 0.157074
Loss = 0.291962
Loss = 0.343292
Loss = 0.320572
Loss = 0.189774
Loss = 0.288101
Loss = 0.213989
Loss = 0.0408173
Loss = 0.23288
Loss = 0.572693
TEST LOSS = 0.373688
TEST ACC = 536.4 % (9000/10000)
