Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 1000]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 15
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.33403
Epoch 1.2: Loss = 2.30327
Epoch 1.3: Loss = 2.26544
Epoch 1.4: Loss = 2.21811
Epoch 1.5: Loss = 2.16272
Epoch 1.6: Loss = 2.12155
Epoch 1.7: Loss = 2.08221
Epoch 1.8: Loss = 2.01929
Epoch 1.9: Loss = 2.01053
Epoch 1.10: Loss = 1.94868
Epoch 1.11: Loss = 1.92232
Epoch 1.12: Loss = 1.88736
Epoch 1.13: Loss = 1.8508
Epoch 1.14: Loss = 1.80026
Epoch 1.15: Loss = 1.77524
Epoch 1.16: Loss = 1.72943
Epoch 1.17: Loss = 1.68977
Epoch 1.18: Loss = 1.67371
Epoch 1.19: Loss = 1.63033
Epoch 1.20: Loss = 1.60977
Epoch 1.21: Loss = 1.56563
Epoch 1.22: Loss = 1.50294
Epoch 1.23: Loss = 1.52786
Epoch 1.24: Loss = 1.43935
Epoch 1.25: Loss = 1.40146
Epoch 1.26: Loss = 1.42256
Epoch 1.27: Loss = 1.36763
Epoch 1.28: Loss = 1.36868
Epoch 1.29: Loss = 1.31639
Epoch 1.30: Loss = 1.34924
Epoch 1.31: Loss = 1.28377
Epoch 1.32: Loss = 1.28456
Epoch 1.33: Loss = 1.2276
Epoch 1.34: Loss = 1.17827
Epoch 1.35: Loss = 1.23561
Epoch 1.36: Loss = 1.20712
Epoch 1.37: Loss = 1.1378
Epoch 1.38: Loss = 1.05212
Epoch 1.39: Loss = 1.08192
Epoch 1.40: Loss = 1.09406
Epoch 1.41: Loss = 1.03943
Epoch 1.42: Loss = 1.04102
Epoch 1.43: Loss = 1.05302
Epoch 1.44: Loss = 0.978424
Epoch 1.45: Loss = 1.03276
Epoch 1.46: Loss = 0.96904
Epoch 1.47: Loss = 1.04709
Epoch 1.48: Loss = 0.95459
Epoch 1.49: Loss = 0.931366
Epoch 1.50: Loss = 0.937897
Epoch 1.51: Loss = 0.971313
Epoch 1.52: Loss = 0.921829
Epoch 1.53: Loss = 0.840012
Epoch 1.54: Loss = 0.883591
Epoch 1.55: Loss = 0.854248
Epoch 1.56: Loss = 0.864395
Epoch 1.57: Loss = 0.848999
Epoch 1.58: Loss = 0.79689
Epoch 1.59: Loss = 0.886658
Epoch 1.60: Loss = 0.855225
Epoch 1.61: Loss = 0.798294
Epoch 1.62: Loss = 0.804626
Epoch 1.63: Loss = 0.802933
Epoch 1.64: Loss = 0.795609
Epoch 1.65: Loss = 0.792374
Epoch 1.66: Loss = 0.751526
Epoch 1.67: Loss = 0.735489
Epoch 1.68: Loss = 0.774536
Epoch 1.69: Loss = 0.816528
Epoch 1.70: Loss = 0.805542
Epoch 1.71: Loss = 0.686752
Epoch 1.72: Loss = 0.795029
Epoch 1.73: Loss = 0.713364
Epoch 1.74: Loss = 0.743958
Epoch 1.75: Loss = 0.757309
Epoch 1.76: Loss = 0.708023
Epoch 1.77: Loss = 0.714813
Epoch 1.78: Loss = 0.72023
Epoch 1.79: Loss = 0.704239
Epoch 1.80: Loss = 0.729202
Epoch 1.81: Loss = 0.706863
Epoch 1.82: Loss = 0.644836
Epoch 1.83: Loss = 0.644302
Epoch 1.84: Loss = 0.696487
Epoch 1.85: Loss = 0.736679
Epoch 1.86: Loss = 0.66127
Epoch 1.87: Loss = 0.663925
Epoch 1.88: Loss = 0.665497
Epoch 1.89: Loss = 0.62561
Epoch 1.90: Loss = 0.617874
Epoch 1.91: Loss = 0.593445
Epoch 1.92: Loss = 0.636719
Epoch 1.93: Loss = 0.589294
Epoch 1.94: Loss = 0.688599
Epoch 1.95: Loss = 0.650848
Epoch 1.96: Loss = 0.568832
Epoch 1.97: Loss = 0.638245
Epoch 1.98: Loss = 0.630737
Epoch 1.99: Loss = 0.59491
Epoch 1.100: Loss = 0.624207
TRAIN LOSS = 1.11818
TRAIN ACC = 70.6757 % (42408/60000)
Loss = 0.650085
Loss = 0.662827
Loss = 0.790771
Loss = 0.738937
Loss = 0.641632
Loss = 0.644577
Loss = 0.71167
Loss = 0.694443
Loss = 0.52478
Loss = 0.485489
Loss = 0.426666
Loss = 0.522766
Loss = 0.480103
Loss = 0.441849
Loss = 0.285049
Loss = 0.429855
Loss = 0.75351
TEST LOSS = 0.57803
TEST ACC = 424.08 % (8384/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.611984
Epoch 2.2: Loss = 0.651779
Epoch 2.3: Loss = 0.588989
Epoch 2.4: Loss = 0.553879
Epoch 2.5: Loss = 0.616669
Epoch 2.6: Loss = 0.600204
Epoch 2.7: Loss = 0.556061
Epoch 2.8: Loss = 0.554169
Epoch 2.9: Loss = 0.551498
Epoch 2.10: Loss = 0.545685
Epoch 2.11: Loss = 0.565781
Epoch 2.12: Loss = 0.552719
Epoch 2.13: Loss = 0.62381
Epoch 2.14: Loss = 0.561508
Epoch 2.15: Loss = 0.548111
Epoch 2.16: Loss = 0.581223
Epoch 2.17: Loss = 0.534134
Epoch 2.18: Loss = 0.534332
Epoch 2.19: Loss = 0.601883
Epoch 2.20: Loss = 0.536438
Epoch 2.21: Loss = 0.52684
Epoch 2.22: Loss = 0.558243
Epoch 2.23: Loss = 0.561966
Epoch 2.24: Loss = 0.513519
Epoch 2.25: Loss = 0.578674
Epoch 2.26: Loss = 0.607834
Epoch 2.27: Loss = 0.52655
Epoch 2.28: Loss = 0.517609
Epoch 2.29: Loss = 0.527618
Epoch 2.30: Loss = 0.513245
Epoch 2.31: Loss = 0.544189
Epoch 2.32: Loss = 0.591324
Epoch 2.33: Loss = 0.584381
Epoch 2.34: Loss = 0.589371
Epoch 2.35: Loss = 0.515442
Epoch 2.36: Loss = 0.535004
Epoch 2.37: Loss = 0.608124
Epoch 2.38: Loss = 0.553329
Epoch 2.39: Loss = 0.484253
Epoch 2.40: Loss = 0.479599
Epoch 2.41: Loss = 0.568619
Epoch 2.42: Loss = 0.506439
Epoch 2.43: Loss = 0.564926
Epoch 2.44: Loss = 0.504608
Epoch 2.45: Loss = 0.510193
Epoch 2.46: Loss = 0.564575
Epoch 2.47: Loss = 0.515656
Epoch 2.48: Loss = 0.48819
Epoch 2.49: Loss = 0.517731
Epoch 2.50: Loss = 0.531204
Epoch 2.51: Loss = 0.52829
Epoch 2.52: Loss = 0.467133
Epoch 2.53: Loss = 0.479263
Epoch 2.54: Loss = 0.485764
Epoch 2.55: Loss = 0.544601
Epoch 2.56: Loss = 0.489044
Epoch 2.57: Loss = 0.513962
Epoch 2.58: Loss = 0.537643
Epoch 2.59: Loss = 0.507645
Epoch 2.60: Loss = 0.518082
Epoch 2.61: Loss = 0.491409
Epoch 2.62: Loss = 0.501114
Epoch 2.63: Loss = 0.525024
Epoch 2.64: Loss = 0.477203
Epoch 2.65: Loss = 0.513321
Epoch 2.66: Loss = 0.491409
Epoch 2.67: Loss = 0.518753
Epoch 2.68: Loss = 0.450851
Epoch 2.69: Loss = 0.465057
Epoch 2.70: Loss = 0.49147
Epoch 2.71: Loss = 0.495483
Epoch 2.72: Loss = 0.460098
Epoch 2.73: Loss = 0.472214
Epoch 2.74: Loss = 0.472031
Epoch 2.75: Loss = 0.460236
Epoch 2.76: Loss = 0.546066
Epoch 2.77: Loss = 0.505997
Epoch 2.78: Loss = 0.48082
Epoch 2.79: Loss = 0.438324
Epoch 2.80: Loss = 0.425827
Epoch 2.81: Loss = 0.4767
Epoch 2.82: Loss = 0.497452
Epoch 2.83: Loss = 0.496109
Epoch 2.84: Loss = 0.500488
Epoch 2.85: Loss = 0.50882
Epoch 2.86: Loss = 0.445847
Epoch 2.87: Loss = 0.505524
Epoch 2.88: Loss = 0.550644
Epoch 2.89: Loss = 0.422211
Epoch 2.90: Loss = 0.513351
Epoch 2.91: Loss = 0.423019
Epoch 2.92: Loss = 0.48996
Epoch 2.93: Loss = 0.450607
Epoch 2.94: Loss = 0.509079
Epoch 2.95: Loss = 0.444382
Epoch 2.96: Loss = 0.39241
Epoch 2.97: Loss = 0.48233
Epoch 2.98: Loss = 0.426498
Epoch 2.99: Loss = 0.439087
Epoch 2.100: Loss = 0.471268
TRAIN LOSS = 0.518616
TRAIN ACC = 84.874 % (50927/60000)
Loss = 0.489502
Loss = 0.524506
Loss = 0.65181
Loss = 0.6064
Loss = 0.48233
Loss = 0.4944
Loss = 0.579529
Loss = 0.541748
Loss = 0.393173
Loss = 0.335602
Loss = 0.331512
Loss = 0.356583
Loss = 0.314636
Loss = 0.33223
Loss = 0.159576
Loss = 0.296371
Loss = 0.635254
TEST LOSS = 0.438805
TEST ACC = 509.27 % (8709/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.522629
Epoch 3.2: Loss = 0.443939
Epoch 3.3: Loss = 0.472687
Epoch 3.4: Loss = 0.452087
Epoch 3.5: Loss = 0.493378
Epoch 3.6: Loss = 0.453033
Epoch 3.7: Loss = 0.395004
Epoch 3.8: Loss = 0.402328
Epoch 3.9: Loss = 0.425201
Epoch 3.10: Loss = 0.43425
Epoch 3.11: Loss = 0.492386
Epoch 3.12: Loss = 0.489166
Epoch 3.13: Loss = 0.515106
Epoch 3.14: Loss = 0.507446
Epoch 3.15: Loss = 0.436157
Epoch 3.16: Loss = 0.424591
Epoch 3.17: Loss = 0.447556
Epoch 3.18: Loss = 0.46994
Epoch 3.19: Loss = 0.456512
Epoch 3.20: Loss = 0.402634
Epoch 3.21: Loss = 0.486237
Epoch 3.22: Loss = 0.509155
Epoch 3.23: Loss = 0.487061
Epoch 3.24: Loss = 0.436493
Epoch 3.25: Loss = 0.400391
Epoch 3.26: Loss = 0.462189
Epoch 3.27: Loss = 0.3853
Epoch 3.28: Loss = 0.44487
Epoch 3.29: Loss = 0.440872
Epoch 3.30: Loss = 0.394226
Epoch 3.31: Loss = 0.444412
Epoch 3.32: Loss = 0.453979
Epoch 3.33: Loss = 0.473709
Epoch 3.34: Loss = 0.378433
Epoch 3.35: Loss = 0.474594
Epoch 3.36: Loss = 0.404984
Epoch 3.37: Loss = 0.390732
Epoch 3.38: Loss = 0.520844
Epoch 3.39: Loss = 0.455536
Epoch 3.40: Loss = 0.449875
Epoch 3.41: Loss = 0.455795
Epoch 3.42: Loss = 0.494431
Epoch 3.43: Loss = 0.440491
Epoch 3.44: Loss = 0.420135
Epoch 3.45: Loss = 0.408844
Epoch 3.46: Loss = 0.4366
Epoch 3.47: Loss = 0.403381
Epoch 3.48: Loss = 0.425598
Epoch 3.49: Loss = 0.394806
Epoch 3.50: Loss = 0.398834
Epoch 3.51: Loss = 0.372314
Epoch 3.52: Loss = 0.442642
Epoch 3.53: Loss = 0.447113
Epoch 3.54: Loss = 0.438507
Epoch 3.55: Loss = 0.376465
Epoch 3.56: Loss = 0.467606
Epoch 3.57: Loss = 0.417786
Epoch 3.58: Loss = 0.467072
Epoch 3.59: Loss = 0.422989
Epoch 3.60: Loss = 0.450302
Epoch 3.61: Loss = 0.45993
Epoch 3.62: Loss = 0.413483
Epoch 3.63: Loss = 0.469452
Epoch 3.64: Loss = 0.397629
Epoch 3.65: Loss = 0.423492
Epoch 3.66: Loss = 0.413742
Epoch 3.67: Loss = 0.429459
Epoch 3.68: Loss = 0.438461
Epoch 3.69: Loss = 0.459961
Epoch 3.70: Loss = 0.40094
Epoch 3.71: Loss = 0.391739
Epoch 3.72: Loss = 0.481995
Epoch 3.73: Loss = 0.405533
Epoch 3.74: Loss = 0.399734
Epoch 3.75: Loss = 0.412766
Epoch 3.76: Loss = 0.426758
Epoch 3.77: Loss = 0.376068
Epoch 3.78: Loss = 0.40155
Epoch 3.79: Loss = 0.434753
Epoch 3.80: Loss = 0.418015
Epoch 3.81: Loss = 0.347626
Epoch 3.82: Loss = 0.433334
Epoch 3.83: Loss = 0.399139
Epoch 3.84: Loss = 0.406311
Epoch 3.85: Loss = 0.396759
Epoch 3.86: Loss = 0.410019
Epoch 3.87: Loss = 0.364014
Epoch 3.88: Loss = 0.413589
Epoch 3.89: Loss = 0.38382
Epoch 3.90: Loss = 0.345612
Epoch 3.91: Loss = 0.386002
Epoch 3.92: Loss = 0.475281
Epoch 3.93: Loss = 0.446732
Epoch 3.94: Loss = 0.460358
Epoch 3.95: Loss = 0.409744
Epoch 3.96: Loss = 0.387924
Epoch 3.97: Loss = 0.405945
Epoch 3.98: Loss = 0.410904
Epoch 3.99: Loss = 0.445831
Epoch 3.100: Loss = 0.433945
TRAIN LOSS = 0.432312
TRAIN ACC = 87.1292 % (52280/60000)
Loss = 0.439804
Loss = 0.473648
Loss = 0.622086
Loss = 0.568008
Loss = 0.415176
Loss = 0.433685
Loss = 0.552048
Loss = 0.48999
Loss = 0.34964
Loss = 0.300049
Loss = 0.3069
Loss = 0.311722
Loss = 0.260742
Loss = 0.295044
Loss = 0.117981
Loss = 0.241302
Loss = 0.602859
TEST LOSS = 0.394784
TEST ACC = 522.8 % (8816/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.405472
Epoch 4.2: Loss = 0.369171
Epoch 4.3: Loss = 0.444855
Epoch 4.4: Loss = 0.359543
Epoch 4.5: Loss = 0.368469
Epoch 4.6: Loss = 0.504959
Epoch 4.7: Loss = 0.399063
Epoch 4.8: Loss = 0.413895
Epoch 4.9: Loss = 0.42366
Epoch 4.10: Loss = 0.429825
Epoch 4.11: Loss = 0.427994
Epoch 4.12: Loss = 0.436127
Epoch 4.13: Loss = 0.371277
Epoch 4.14: Loss = 0.418503
Epoch 4.15: Loss = 0.386734
Epoch 4.16: Loss = 0.457886
Epoch 4.17: Loss = 0.396317
Epoch 4.18: Loss = 0.37999
Epoch 4.19: Loss = 0.39035
Epoch 4.20: Loss = 0.441406
Epoch 4.21: Loss = 0.437958
Epoch 4.22: Loss = 0.408615
Epoch 4.23: Loss = 0.380493
Epoch 4.24: Loss = 0.418961
Epoch 4.25: Loss = 0.434311
Epoch 4.26: Loss = 0.436951
Epoch 4.27: Loss = 0.384048
Epoch 4.28: Loss = 0.39827
Epoch 4.29: Loss = 0.456512
Epoch 4.30: Loss = 0.354691
Epoch 4.31: Loss = 0.444962
Epoch 4.32: Loss = 0.3479
Epoch 4.33: Loss = 0.392654
Epoch 4.34: Loss = 0.357498
Epoch 4.35: Loss = 0.388947
Epoch 4.36: Loss = 0.362717
Epoch 4.37: Loss = 0.347321
Epoch 4.38: Loss = 0.422134
Epoch 4.39: Loss = 0.394852
Epoch 4.40: Loss = 0.416504
Epoch 4.41: Loss = 0.381607
Epoch 4.42: Loss = 0.401001
Epoch 4.43: Loss = 0.485031
Epoch 4.44: Loss = 0.493713
Epoch 4.45: Loss = 0.397079
Epoch 4.46: Loss = 0.482742
Epoch 4.47: Loss = 0.325409
Epoch 4.48: Loss = 0.386459
Epoch 4.49: Loss = 0.378937
Epoch 4.50: Loss = 0.431671
Epoch 4.51: Loss = 0.316345
Epoch 4.52: Loss = 0.422485
Epoch 4.53: Loss = 0.381699
Epoch 4.54: Loss = 0.392502
Epoch 4.55: Loss = 0.429886
Epoch 4.56: Loss = 0.45105
Epoch 4.57: Loss = 0.339462
Epoch 4.58: Loss = 0.428802
Epoch 4.59: Loss = 0.399734
Epoch 4.60: Loss = 0.446274
Epoch 4.61: Loss = 0.392578
Epoch 4.62: Loss = 0.317505
Epoch 4.63: Loss = 0.334015
Epoch 4.64: Loss = 0.380997
Epoch 4.65: Loss = 0.394821
Epoch 4.66: Loss = 0.410385
Epoch 4.67: Loss = 0.332062
Epoch 4.68: Loss = 0.330704
Epoch 4.69: Loss = 0.420563
Epoch 4.70: Loss = 0.433578
Epoch 4.71: Loss = 0.434982
Epoch 4.72: Loss = 0.411896
Epoch 4.73: Loss = 0.387909
Epoch 4.74: Loss = 0.415207
Epoch 4.75: Loss = 0.451828
Epoch 4.76: Loss = 0.431122
Epoch 4.77: Loss = 0.390503
Epoch 4.78: Loss = 0.439957
Epoch 4.79: Loss = 0.344437
Epoch 4.80: Loss = 0.431061
Epoch 4.81: Loss = 0.365067
Epoch 4.82: Loss = 0.35733
Epoch 4.83: Loss = 0.377975
Epoch 4.84: Loss = 0.366241
Epoch 4.85: Loss = 0.392014
Epoch 4.86: Loss = 0.378311
Epoch 4.87: Loss = 0.419739
Epoch 4.88: Loss = 0.391861
Epoch 4.89: Loss = 0.372787
Epoch 4.90: Loss = 0.433823
Epoch 4.91: Loss = 0.335815
Epoch 4.92: Loss = 0.318604
Epoch 4.93: Loss = 0.388138
Epoch 4.94: Loss = 0.489944
Epoch 4.95: Loss = 0.378174
Epoch 4.96: Loss = 0.40451
Epoch 4.97: Loss = 0.382889
Epoch 4.98: Loss = 0.469299
Epoch 4.99: Loss = 0.477768
Epoch 4.100: Loss = 0.43721
TRAIN LOSS = 0.4021
TRAIN ACC = 87.9486 % (52771/60000)
Loss = 0.416473
Loss = 0.45195
Loss = 0.590195
Loss = 0.5327
Loss = 0.37146
Loss = 0.404587
Loss = 0.5457
Loss = 0.466553
Loss = 0.327393
Loss = 0.265045
Loss = 0.29866
Loss = 0.293854
Loss = 0.228851
Loss = 0.288971
Loss = 0.0951691
Loss = 0.242981
Loss = 0.584305
TEST LOSS = 0.372605
TEST ACC = 527.708 % (8897/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.357559
Epoch 5.2: Loss = 0.449783
Epoch 5.3: Loss = 0.421234
Epoch 5.4: Loss = 0.298019
Epoch 5.5: Loss = 0.4561
Epoch 5.6: Loss = 0.422577
Epoch 5.7: Loss = 0.39006
Epoch 5.8: Loss = 0.375351
Epoch 5.9: Loss = 0.417618
Epoch 5.10: Loss = 0.439514
Epoch 5.11: Loss = 0.396149
Epoch 5.12: Loss = 0.331833
Epoch 5.13: Loss = 0.378555
Epoch 5.14: Loss = 0.358139
Epoch 5.15: Loss = 0.411087
Epoch 5.16: Loss = 0.395035
Epoch 5.17: Loss = 0.396362
Epoch 5.18: Loss = 0.329224
Epoch 5.19: Loss = 0.335144
Epoch 5.20: Loss = 0.347031
Epoch 5.21: Loss = 0.461487
Epoch 5.22: Loss = 0.357178
Epoch 5.23: Loss = 0.490311
Epoch 5.24: Loss = 0.394867
Epoch 5.25: Loss = 0.37413
Epoch 5.26: Loss = 0.422989
Epoch 5.27: Loss = 0.350159
Epoch 5.28: Loss = 0.450333
Epoch 5.29: Loss = 0.368683
Epoch 5.30: Loss = 0.429596
Epoch 5.31: Loss = 0.332825
Epoch 5.32: Loss = 0.395264
Epoch 5.33: Loss = 0.438324
Epoch 5.34: Loss = 0.361588
Epoch 5.35: Loss = 0.325119
Epoch 5.36: Loss = 0.443314
Epoch 5.37: Loss = 0.371185
Epoch 5.38: Loss = 0.438904
Epoch 5.39: Loss = 0.399338
Epoch 5.40: Loss = 0.421036
Epoch 5.41: Loss = 0.375687
Epoch 5.42: Loss = 0.355042
Epoch 5.43: Loss = 0.425491
Epoch 5.44: Loss = 0.361359
Epoch 5.45: Loss = 0.333908
Epoch 5.46: Loss = 0.369797
Epoch 5.47: Loss = 0.412415
Epoch 5.48: Loss = 0.43309
Epoch 5.49: Loss = 0.324188
Epoch 5.50: Loss = 0.435898
Epoch 5.51: Loss = 0.3992
Epoch 5.52: Loss = 0.391937
Epoch 5.53: Loss = 0.360336
Epoch 5.54: Loss = 0.411285
Epoch 5.55: Loss = 0.425003
Epoch 5.56: Loss = 0.363983
Epoch 5.57: Loss = 0.368378
Epoch 5.58: Loss = 0.420334
Epoch 5.59: Loss = 0.371231
Epoch 5.60: Loss = 0.423294
Epoch 5.61: Loss = 0.363159
Epoch 5.62: Loss = 0.409592
Epoch 5.63: Loss = 0.379532
Epoch 5.64: Loss = 0.387024
Epoch 5.65: Loss = 0.322861
Epoch 5.66: Loss = 0.351349
Epoch 5.67: Loss = 0.344437
Epoch 5.68: Loss = 0.442032
Epoch 5.69: Loss = 0.370514
Epoch 5.70: Loss = 0.309052
Epoch 5.71: Loss = 0.324982
Epoch 5.72: Loss = 0.372864
Epoch 5.73: Loss = 0.285004
Epoch 5.74: Loss = 0.403549
Epoch 5.75: Loss = 0.315186
Epoch 5.76: Loss = 0.396133
Epoch 5.77: Loss = 0.375305
Epoch 5.78: Loss = 0.363861
Epoch 5.79: Loss = 0.442886
Epoch 5.80: Loss = 0.390121
Epoch 5.81: Loss = 0.434235
Epoch 5.82: Loss = 0.354507
Epoch 5.83: Loss = 0.414063
Epoch 5.84: Loss = 0.370544
Epoch 5.85: Loss = 0.361389
Epoch 5.86: Loss = 0.4254
Epoch 5.87: Loss = 0.353745
Epoch 5.88: Loss = 0.377335
Epoch 5.89: Loss = 0.480316
Epoch 5.90: Loss = 0.323898
Epoch 5.91: Loss = 0.396011
Epoch 5.92: Loss = 0.432465
Epoch 5.93: Loss = 0.341354
Epoch 5.94: Loss = 0.356476
Epoch 5.95: Loss = 0.367416
Epoch 5.96: Loss = 0.345718
Epoch 5.97: Loss = 0.363937
Epoch 5.98: Loss = 0.438538
Epoch 5.99: Loss = 0.351822
Epoch 5.100: Loss = 0.366638
TRAIN LOSS = 0.38501
TRAIN ACC = 88.5254 % (53117/60000)
Loss = 0.399155
Loss = 0.442032
Loss = 0.582977
Loss = 0.531128
Loss = 0.355515
Loss = 0.388641
Loss = 0.528946
Loss = 0.451721
Loss = 0.312836
Loss = 0.256592
Loss = 0.291443
Loss = 0.261871
Loss = 0.214828
Loss = 0.27446
Loss = 0.0892334
Loss = 0.223999
Loss = 0.570282
TEST LOSS = 0.359134
TEST ACC = 531.169 % (8938/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.365341
Epoch 6.2: Loss = 0.440216
Epoch 6.3: Loss = 0.383392
Epoch 6.4: Loss = 0.391266
Epoch 6.5: Loss = 0.374512
Epoch 6.6: Loss = 0.363907
Epoch 6.7: Loss = 0.363098
Epoch 6.8: Loss = 0.388382
Epoch 6.9: Loss = 0.346237
Epoch 6.10: Loss = 0.333023
Epoch 6.11: Loss = 0.356613
Epoch 6.12: Loss = 0.377823
Epoch 6.13: Loss = 0.424973
Epoch 6.14: Loss = 0.395737
Epoch 6.15: Loss = 0.334503
Epoch 6.16: Loss = 0.406082
Epoch 6.17: Loss = 0.34906
Epoch 6.18: Loss = 0.297287
Epoch 6.19: Loss = 0.339569
Epoch 6.20: Loss = 0.444031
Epoch 6.21: Loss = 0.408463
Epoch 6.22: Loss = 0.343246
Epoch 6.23: Loss = 0.328903
Epoch 6.24: Loss = 0.498505
Epoch 6.25: Loss = 0.412399
Epoch 6.26: Loss = 0.368942
Epoch 6.27: Loss = 0.39888
Epoch 6.28: Loss = 0.322342
Epoch 6.29: Loss = 0.398117
Epoch 6.30: Loss = 0.440292
Epoch 6.31: Loss = 0.381012
Epoch 6.32: Loss = 0.348389
Epoch 6.33: Loss = 0.383759
Epoch 6.34: Loss = 0.387772
Epoch 6.35: Loss = 0.430771
Epoch 6.36: Loss = 0.336639
Epoch 6.37: Loss = 0.262527
Epoch 6.38: Loss = 0.360733
Epoch 6.39: Loss = 0.369492
Epoch 6.40: Loss = 0.344849
Epoch 6.41: Loss = 0.306168
Epoch 6.42: Loss = 0.417953
Epoch 6.43: Loss = 0.387329
Epoch 6.44: Loss = 0.335724
Epoch 6.45: Loss = 0.473816
Epoch 6.46: Loss = 0.399963
Epoch 6.47: Loss = 0.322235
Epoch 6.48: Loss = 0.409332
Epoch 6.49: Loss = 0.411179
Epoch 6.50: Loss = 0.39827
Epoch 6.51: Loss = 0.391464
Epoch 6.52: Loss = 0.369675
Epoch 6.53: Loss = 0.404602
Epoch 6.54: Loss = 0.427444
Epoch 6.55: Loss = 0.446838
Epoch 6.56: Loss = 0.372498
Epoch 6.57: Loss = 0.306412
Epoch 6.58: Loss = 0.412582
Epoch 6.59: Loss = 0.343719
Epoch 6.60: Loss = 0.372879
Epoch 6.61: Loss = 0.396088
Epoch 6.62: Loss = 0.366013
Epoch 6.63: Loss = 0.357285
Epoch 6.64: Loss = 0.342941
Epoch 6.65: Loss = 0.374451
Epoch 6.66: Loss = 0.403351
Epoch 6.67: Loss = 0.368179
Epoch 6.68: Loss = 0.397781
Epoch 6.69: Loss = 0.395889
Epoch 6.70: Loss = 0.395569
Epoch 6.71: Loss = 0.385559
Epoch 6.72: Loss = 0.349869
Epoch 6.73: Loss = 0.445068
Epoch 6.74: Loss = 0.290436
Epoch 6.75: Loss = 0.304321
Epoch 6.76: Loss = 0.343704
Epoch 6.77: Loss = 0.357101
Epoch 6.78: Loss = 0.339172
Epoch 6.79: Loss = 0.34436
Epoch 6.80: Loss = 0.315094
Epoch 6.81: Loss = 0.420135
Epoch 6.82: Loss = 0.419403
Epoch 6.83: Loss = 0.344574
Epoch 6.84: Loss = 0.279755
Epoch 6.85: Loss = 0.383423
Epoch 6.86: Loss = 0.334641
Epoch 6.87: Loss = 0.366257
Epoch 6.88: Loss = 0.378494
Epoch 6.89: Loss = 0.394043
Epoch 6.90: Loss = 0.300507
Epoch 6.91: Loss = 0.385834
Epoch 6.92: Loss = 0.393738
Epoch 6.93: Loss = 0.442154
Epoch 6.94: Loss = 0.413681
Epoch 6.95: Loss = 0.375076
Epoch 6.96: Loss = 0.302536
Epoch 6.97: Loss = 0.358612
Epoch 6.98: Loss = 0.382385
Epoch 6.99: Loss = 0.383392
Epoch 6.100: Loss = 0.337479
TRAIN LOSS = 0.373795
TRAIN ACC = 89.0549 % (53436/60000)
Loss = 0.383499
Loss = 0.431213
Loss = 0.579269
Loss = 0.525314
Loss = 0.361145
Loss = 0.378281
Loss = 0.537003
Loss = 0.443832
Loss = 0.303391
Loss = 0.242126
Loss = 0.302277
Loss = 0.258942
Loss = 0.198334
Loss = 0.277664
Loss = 0.0832672
Loss = 0.217392
Loss = 0.557632
TEST LOSS = 0.353682
TEST ACC = 534.36 % (8969/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.365265
Epoch 7.2: Loss = 0.396439
Epoch 7.3: Loss = 0.359253
Epoch 7.4: Loss = 0.279526
Epoch 7.5: Loss = 0.325241
Epoch 7.6: Loss = 0.427414
Epoch 7.7: Loss = 0.397705
Epoch 7.8: Loss = 0.398605
Epoch 7.9: Loss = 0.361221
Epoch 7.10: Loss = 0.373047
Epoch 7.11: Loss = 0.40535
Epoch 7.12: Loss = 0.380722
Epoch 7.13: Loss = 0.406281
Epoch 7.14: Loss = 0.409668
Epoch 7.15: Loss = 0.361145
Epoch 7.16: Loss = 0.345932
Epoch 7.17: Loss = 0.353882
Epoch 7.18: Loss = 0.333008
Epoch 7.19: Loss = 0.316315
Epoch 7.20: Loss = 0.504684
Epoch 7.21: Loss = 0.423828
Epoch 7.22: Loss = 0.300751
Epoch 7.23: Loss = 0.408081
Epoch 7.24: Loss = 0.283646
Epoch 7.25: Loss = 0.417664
Epoch 7.26: Loss = 0.371887
Epoch 7.27: Loss = 0.340607
Epoch 7.28: Loss = 0.354889
Epoch 7.29: Loss = 0.33902
Epoch 7.30: Loss = 0.323547
Epoch 7.31: Loss = 0.37085
Epoch 7.32: Loss = 0.292236
Epoch 7.33: Loss = 0.430603
Epoch 7.34: Loss = 0.339035
Epoch 7.35: Loss = 0.316299
Epoch 7.36: Loss = 0.33963
Epoch 7.37: Loss = 0.316208
Epoch 7.38: Loss = 0.370712
Epoch 7.39: Loss = 0.430466
Epoch 7.40: Loss = 0.367477
Epoch 7.41: Loss = 0.413498
Epoch 7.42: Loss = 0.380127
Epoch 7.43: Loss = 0.356674
Epoch 7.44: Loss = 0.355545
Epoch 7.45: Loss = 0.473526
Epoch 7.46: Loss = 0.324539
Epoch 7.47: Loss = 0.371155
Epoch 7.48: Loss = 0.369156
Epoch 7.49: Loss = 0.261383
Epoch 7.50: Loss = 0.304642
Epoch 7.51: Loss = 0.382813
Epoch 7.52: Loss = 0.354431
Epoch 7.53: Loss = 0.297089
Epoch 7.54: Loss = 0.356598
Epoch 7.55: Loss = 0.346054
Epoch 7.56: Loss = 0.429474
Epoch 7.57: Loss = 0.458206
Epoch 7.58: Loss = 0.330246
Epoch 7.59: Loss = 0.360901
Epoch 7.60: Loss = 0.330399
Epoch 7.61: Loss = 0.367188
Epoch 7.62: Loss = 0.42131
Epoch 7.63: Loss = 0.335922
Epoch 7.64: Loss = 0.421249
Epoch 7.65: Loss = 0.347992
Epoch 7.66: Loss = 0.352692
Epoch 7.67: Loss = 0.450287
Epoch 7.68: Loss = 0.391693
Epoch 7.69: Loss = 0.337692
Epoch 7.70: Loss = 0.416092
Epoch 7.71: Loss = 0.358078
Epoch 7.72: Loss = 0.35437
Epoch 7.73: Loss = 0.375275
Epoch 7.74: Loss = 0.448349
Epoch 7.75: Loss = 0.356506
Epoch 7.76: Loss = 0.412827
Epoch 7.77: Loss = 0.308182
Epoch 7.78: Loss = 0.350739
Epoch 7.79: Loss = 0.304886
Epoch 7.80: Loss = 0.379272
Epoch 7.81: Loss = 0.330566
Epoch 7.82: Loss = 0.421646
Epoch 7.83: Loss = 0.382736
Epoch 7.84: Loss = 0.430832
Epoch 7.85: Loss = 0.294952
Epoch 7.86: Loss = 0.328461
Epoch 7.87: Loss = 0.379974
Epoch 7.88: Loss = 0.37886
Epoch 7.89: Loss = 0.42131
Epoch 7.90: Loss = 0.340408
Epoch 7.91: Loss = 0.357178
Epoch 7.92: Loss = 0.408554
Epoch 7.93: Loss = 0.389145
Epoch 7.94: Loss = 0.365829
Epoch 7.95: Loss = 0.406372
Epoch 7.96: Loss = 0.303757
Epoch 7.97: Loss = 0.369949
Epoch 7.98: Loss = 0.339035
Epoch 7.99: Loss = 0.390747
Epoch 7.100: Loss = 0.395325
TRAIN LOSS = 0.368225
TRAIN ACC = 89.2944 % (53579/60000)
Loss = 0.368668
Loss = 0.416275
Loss = 0.567307
Loss = 0.522034
Loss = 0.355255
Loss = 0.367889
Loss = 0.526001
Loss = 0.427536
Loss = 0.297745
Loss = 0.2435
Loss = 0.30246
Loss = 0.241837
Loss = 0.185562
Loss = 0.255569
Loss = 0.0820007
Loss = 0.215515
Loss = 0.546143
TEST LOSS = 0.344355
TEST ACC = 535.789 % (9011/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.371384
Epoch 8.2: Loss = 0.363419
Epoch 8.3: Loss = 0.291962
Epoch 8.4: Loss = 0.354004
Epoch 8.5: Loss = 0.373688
Epoch 8.6: Loss = 0.351334
Epoch 8.7: Loss = 0.346893
Epoch 8.8: Loss = 0.333084
Epoch 8.9: Loss = 0.347519
Epoch 8.10: Loss = 0.375015
Epoch 8.11: Loss = 0.334
Epoch 8.12: Loss = 0.423767
Epoch 8.13: Loss = 0.370804
Epoch 8.14: Loss = 0.334091
Epoch 8.15: Loss = 0.334976
Epoch 8.16: Loss = 0.322144
Epoch 8.17: Loss = 0.339966
Epoch 8.18: Loss = 0.324432
Epoch 8.19: Loss = 0.343491
Epoch 8.20: Loss = 0.40416
Epoch 8.21: Loss = 0.366516
Epoch 8.22: Loss = 0.298386
Epoch 8.23: Loss = 0.40007
Epoch 8.24: Loss = 0.336578
Epoch 8.25: Loss = 0.296402
Epoch 8.26: Loss = 0.277069
Epoch 8.27: Loss = 0.343445
Epoch 8.28: Loss = 0.36615
Epoch 8.29: Loss = 0.345062
Epoch 8.30: Loss = 0.438446
Epoch 8.31: Loss = 0.399582
Epoch 8.32: Loss = 0.354965
Epoch 8.33: Loss = 0.406998
Epoch 8.34: Loss = 0.397461
Epoch 8.35: Loss = 0.340347
Epoch 8.36: Loss = 0.341858
Epoch 8.37: Loss = 0.377716
Epoch 8.38: Loss = 0.314728
Epoch 8.39: Loss = 0.400253
Epoch 8.40: Loss = 0.337723
Epoch 8.41: Loss = 0.410065
Epoch 8.42: Loss = 0.396759
Epoch 8.43: Loss = 0.358841
Epoch 8.44: Loss = 0.393677
Epoch 8.45: Loss = 0.30896
Epoch 8.46: Loss = 0.403107
Epoch 8.47: Loss = 0.390488
Epoch 8.48: Loss = 0.352036
Epoch 8.49: Loss = 0.342224
Epoch 8.50: Loss = 0.299759
Epoch 8.51: Loss = 0.42421
Epoch 8.52: Loss = 0.327911
Epoch 8.53: Loss = 0.347458
Epoch 8.54: Loss = 0.408752
Epoch 8.55: Loss = 0.426483
Epoch 8.56: Loss = 0.340576
Epoch 8.57: Loss = 0.339203
Epoch 8.58: Loss = 0.377563
Epoch 8.59: Loss = 0.359146
Epoch 8.60: Loss = 0.35112
Epoch 8.61: Loss = 0.358337
Epoch 8.62: Loss = 0.371368
Epoch 8.63: Loss = 0.360016
Epoch 8.64: Loss = 0.419678
Epoch 8.65: Loss = 0.326569
Epoch 8.66: Loss = 0.395691
Epoch 8.67: Loss = 0.297958
Epoch 8.68: Loss = 0.394058
Epoch 8.69: Loss = 0.430252
Epoch 8.70: Loss = 0.404556
Epoch 8.71: Loss = 0.400452
Epoch 8.72: Loss = 0.320313
Epoch 8.73: Loss = 0.397018
Epoch 8.74: Loss = 0.278137
Epoch 8.75: Loss = 0.455612
Epoch 8.76: Loss = 0.35054
Epoch 8.77: Loss = 0.247986
Epoch 8.78: Loss = 0.414078
Epoch 8.79: Loss = 0.367447
Epoch 8.80: Loss = 0.330902
Epoch 8.81: Loss = 0.307465
Epoch 8.82: Loss = 0.325012
Epoch 8.83: Loss = 0.369904
Epoch 8.84: Loss = 0.293884
Epoch 8.85: Loss = 0.405701
Epoch 8.86: Loss = 0.326294
Epoch 8.87: Loss = 0.391098
Epoch 8.88: Loss = 0.395721
Epoch 8.89: Loss = 0.434204
Epoch 8.90: Loss = 0.369186
Epoch 8.91: Loss = 0.294388
Epoch 8.92: Loss = 0.379364
Epoch 8.93: Loss = 0.372742
Epoch 8.94: Loss = 0.355637
Epoch 8.95: Loss = 0.324814
Epoch 8.96: Loss = 0.401688
Epoch 8.97: Loss = 0.356796
Epoch 8.98: Loss = 0.360321
Epoch 8.99: Loss = 0.382462
Epoch 8.100: Loss = 0.332764
TRAIN LOSS = 0.360382
TRAIN ACC = 89.5248 % (53718/60000)
Loss = 0.363464
Loss = 0.40242
Loss = 0.565826
Loss = 0.514725
Loss = 0.358612
Loss = 0.369644
Loss = 0.522568
Loss = 0.420334
Loss = 0.304184
Loss = 0.243591
Loss = 0.305435
Loss = 0.244934
Loss = 0.18309
Loss = 0.279602
Loss = 0.0786896
Loss = 0.212326
Loss = 0.530991
TEST LOSS = 0.343406
TEST ACC = 537.18 % (9017/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.395798
Epoch 9.2: Loss = 0.415634
Epoch 9.3: Loss = 0.296616
Epoch 9.4: Loss = 0.442764
Epoch 9.5: Loss = 0.331207
Epoch 9.6: Loss = 0.376877
Epoch 9.7: Loss = 0.320511
Epoch 9.8: Loss = 0.334808
Epoch 9.9: Loss = 0.287415
Epoch 9.10: Loss = 0.387955
Epoch 9.11: Loss = 0.45993
Epoch 9.12: Loss = 0.34848
Epoch 9.13: Loss = 0.324875
Epoch 9.14: Loss = 0.352219
Epoch 9.15: Loss = 0.342926
Epoch 9.16: Loss = 0.391052
Epoch 9.17: Loss = 0.361298
Epoch 9.18: Loss = 0.434082
Epoch 9.19: Loss = 0.348785
Epoch 9.20: Loss = 0.348648
Epoch 9.21: Loss = 0.394135
Epoch 9.22: Loss = 0.280869
Epoch 9.23: Loss = 0.355515
Epoch 9.24: Loss = 0.295441
Epoch 9.25: Loss = 0.325073
Epoch 9.26: Loss = 0.342911
Epoch 9.27: Loss = 0.348541
Epoch 9.28: Loss = 0.351151
Epoch 9.29: Loss = 0.421646
Epoch 9.30: Loss = 0.324341
Epoch 9.31: Loss = 0.327423
Epoch 9.32: Loss = 0.397522
Epoch 9.33: Loss = 0.41333
Epoch 9.34: Loss = 0.364517
Epoch 9.35: Loss = 0.291504
Epoch 9.36: Loss = 0.410553
Epoch 9.37: Loss = 0.37674
Epoch 9.38: Loss = 0.396088
Epoch 9.39: Loss = 0.314896
Epoch 9.40: Loss = 0.457245
Epoch 9.41: Loss = 0.396729
Epoch 9.42: Loss = 0.364578
Epoch 9.43: Loss = 0.392578
Epoch 9.44: Loss = 0.37207
Epoch 9.45: Loss = 0.331223
Epoch 9.46: Loss = 0.351868
Epoch 9.47: Loss = 0.302383
Epoch 9.48: Loss = 0.443253
Epoch 9.49: Loss = 0.411362
Epoch 9.50: Loss = 0.340424
Epoch 9.51: Loss = 0.385529
Epoch 9.52: Loss = 0.297577
Epoch 9.53: Loss = 0.339615
Epoch 9.54: Loss = 0.307816
Epoch 9.55: Loss = 0.377426
Epoch 9.56: Loss = 0.307022
Epoch 9.57: Loss = 0.371063
Epoch 9.58: Loss = 0.355484
Epoch 9.59: Loss = 0.312988
Epoch 9.60: Loss = 0.344986
Epoch 9.61: Loss = 0.295364
Epoch 9.62: Loss = 0.364044
Epoch 9.63: Loss = 0.376312
Epoch 9.64: Loss = 0.352219
Epoch 9.65: Loss = 0.351181
Epoch 9.66: Loss = 0.358475
Epoch 9.67: Loss = 0.379791
Epoch 9.68: Loss = 0.354889
Epoch 9.69: Loss = 0.354431
Epoch 9.70: Loss = 0.266373
Epoch 9.71: Loss = 0.326141
Epoch 9.72: Loss = 0.381714
Epoch 9.73: Loss = 0.278915
Epoch 9.74: Loss = 0.381943
Epoch 9.75: Loss = 0.378769
Epoch 9.76: Loss = 0.327576
Epoch 9.77: Loss = 0.336639
Epoch 9.78: Loss = 0.297928
Epoch 9.79: Loss = 0.314621
Epoch 9.80: Loss = 0.36441
Epoch 9.81: Loss = 0.478027
Epoch 9.82: Loss = 0.337097
Epoch 9.83: Loss = 0.314255
Epoch 9.84: Loss = 0.392242
Epoch 9.85: Loss = 0.412888
Epoch 9.86: Loss = 0.316956
Epoch 9.87: Loss = 0.451385
Epoch 9.88: Loss = 0.303085
Epoch 9.89: Loss = 0.41655
Epoch 9.90: Loss = 0.335342
Epoch 9.91: Loss = 0.377579
Epoch 9.92: Loss = 0.328979
Epoch 9.93: Loss = 0.40831
Epoch 9.94: Loss = 0.406418
Epoch 9.95: Loss = 0.290771
Epoch 9.96: Loss = 0.3909
Epoch 9.97: Loss = 0.36615
Epoch 9.98: Loss = 0.347122
Epoch 9.99: Loss = 0.284637
Epoch 9.100: Loss = 0.44664
TRAIN LOSS = 0.358444
TRAIN ACC = 89.6179 % (53773/60000)
Loss = 0.366653
Loss = 0.400375
Loss = 0.558624
Loss = 0.506332
Loss = 0.355667
Loss = 0.360077
Loss = 0.519684
Loss = 0.411163
Loss = 0.295547
Loss = 0.23851
Loss = 0.31546
Loss = 0.236435
Loss = 0.178528
Loss = 0.27681
Loss = 0.0735779
Loss = 0.215286
Loss = 0.524582
TEST LOSS = 0.339507
TEST ACC = 537.729 % (9045/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.337936
Epoch 10.2: Loss = 0.424973
Epoch 10.3: Loss = 0.386063
Epoch 10.4: Loss = 0.369507
Epoch 10.5: Loss = 0.366486
Epoch 10.6: Loss = 0.422928
Epoch 10.7: Loss = 0.398178
Epoch 10.8: Loss = 0.30304
Epoch 10.9: Loss = 0.330536
Epoch 10.10: Loss = 0.389954
Epoch 10.11: Loss = 0.369751
Epoch 10.12: Loss = 0.256927
Epoch 10.13: Loss = 0.397491
Epoch 10.14: Loss = 0.409363
Epoch 10.15: Loss = 0.405716
Epoch 10.16: Loss = 0.366013
Epoch 10.17: Loss = 0.379028
Epoch 10.18: Loss = 0.393799
Epoch 10.19: Loss = 0.383804
Epoch 10.20: Loss = 0.407303
Epoch 10.21: Loss = 0.309982
Epoch 10.22: Loss = 0.291321
Epoch 10.23: Loss = 0.271179
Epoch 10.24: Loss = 0.352493
Epoch 10.25: Loss = 0.298859
Epoch 10.26: Loss = 0.370682
Epoch 10.27: Loss = 0.309052
Epoch 10.28: Loss = 0.329498
Epoch 10.29: Loss = 0.306885
Epoch 10.30: Loss = 0.358765
Epoch 10.31: Loss = 0.287994
Epoch 10.32: Loss = 0.306396
Epoch 10.33: Loss = 0.316711
Epoch 10.34: Loss = 0.428238
Epoch 10.35: Loss = 0.378525
Epoch 10.36: Loss = 0.291916
Epoch 10.37: Loss = 0.246841
Epoch 10.38: Loss = 0.3936
Epoch 10.39: Loss = 0.306931
Epoch 10.40: Loss = 0.368256
Epoch 10.41: Loss = 0.343536
Epoch 10.42: Loss = 0.417908
Epoch 10.43: Loss = 0.429535
Epoch 10.44: Loss = 0.369781
Epoch 10.45: Loss = 0.367889
Epoch 10.46: Loss = 0.341171
Epoch 10.47: Loss = 0.327591
Epoch 10.48: Loss = 0.320465
Epoch 10.49: Loss = 0.326401
Epoch 10.50: Loss = 0.42598
Epoch 10.51: Loss = 0.251389
Epoch 10.52: Loss = 0.373108
Epoch 10.53: Loss = 0.365646
Epoch 10.54: Loss = 0.371719
Epoch 10.55: Loss = 0.443939
Epoch 10.56: Loss = 0.416779
Epoch 10.57: Loss = 0.237381
Epoch 10.58: Loss = 0.34906
Epoch 10.59: Loss = 0.335159
Epoch 10.60: Loss = 0.378281
Epoch 10.61: Loss = 0.398254
Epoch 10.62: Loss = 0.40416
Epoch 10.63: Loss = 0.310318
Epoch 10.64: Loss = 0.319534
Epoch 10.65: Loss = 0.388947
Epoch 10.66: Loss = 0.353775
Epoch 10.67: Loss = 0.433075
Epoch 10.68: Loss = 0.334427
Epoch 10.69: Loss = 0.274384
Epoch 10.70: Loss = 0.371231
Epoch 10.71: Loss = 0.410431
Epoch 10.72: Loss = 0.397369
Epoch 10.73: Loss = 0.312271
Epoch 10.74: Loss = 0.369705
Epoch 10.75: Loss = 0.381302
Epoch 10.76: Loss = 0.4021
Epoch 10.77: Loss = 0.34491
Epoch 10.78: Loss = 0.36734
Epoch 10.79: Loss = 0.41362
Epoch 10.80: Loss = 0.320801
Epoch 10.81: Loss = 0.346024
Epoch 10.82: Loss = 0.33812
Epoch 10.83: Loss = 0.39653
Epoch 10.84: Loss = 0.402084
Epoch 10.85: Loss = 0.429306
Epoch 10.86: Loss = 0.292862
Epoch 10.87: Loss = 0.336487
Epoch 10.88: Loss = 0.268127
Epoch 10.89: Loss = 0.324219
Epoch 10.90: Loss = 0.416962
Epoch 10.91: Loss = 0.296738
Epoch 10.92: Loss = 0.332504
Epoch 10.93: Loss = 0.357101
Epoch 10.94: Loss = 0.35817
Epoch 10.95: Loss = 0.3573
Epoch 10.96: Loss = 0.358032
Epoch 10.97: Loss = 0.344559
Epoch 10.98: Loss = 0.373199
Epoch 10.99: Loss = 0.352493
Epoch 10.100: Loss = 0.314926
TRAIN LOSS = 0.355194
TRAIN ACC = 89.9246 % (53957/60000)
Loss = 0.361176
Loss = 0.396942
Loss = 0.563004
Loss = 0.509628
Loss = 0.356125
Loss = 0.359894
Loss = 0.529556
Loss = 0.413147
Loss = 0.294571
Loss = 0.238205
Loss = 0.311844
Loss = 0.236023
Loss = 0.177902
Loss = 0.271118
Loss = 0.0719452
Loss = 0.217194
Loss = 0.51944
TEST LOSS = 0.339274
TEST ACC = 539.569 % (9069/10000)
terminate called after throwing an instance of 'boost::wrapexcept<boost::system::system_error>'
  what():  read_some: stream truncated [asio.ssl.stream:1]
