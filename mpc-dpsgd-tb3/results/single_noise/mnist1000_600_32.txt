Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 1000]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 15
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 8
***********************************************************
Epoch 1.1: Loss = 2.37505
Epoch 1.2: Loss = 2.32648
Epoch 1.3: Loss = 2.25572
Epoch 1.4: Loss = 2.21837
Epoch 1.5: Loss = 2.18405
Epoch 1.6: Loss = 2.14597
Epoch 1.7: Loss = 2.06268
Epoch 1.8: Loss = 2.04797
Epoch 1.9: Loss = 1.99367
Epoch 1.10: Loss = 1.94969
Epoch 1.11: Loss = 1.91046
Epoch 1.12: Loss = 1.88675
Epoch 1.13: Loss = 1.82785
Epoch 1.14: Loss = 1.80908
Epoch 1.15: Loss = 1.7599
Epoch 1.16: Loss = 1.73436
Epoch 1.17: Loss = 1.69209
Epoch 1.18: Loss = 1.63663
Epoch 1.19: Loss = 1.60802
Epoch 1.20: Loss = 1.6122
Epoch 1.21: Loss = 1.53329
Epoch 1.22: Loss = 1.51964
Epoch 1.23: Loss = 1.52002
Epoch 1.24: Loss = 1.4415
Epoch 1.25: Loss = 1.42818
Epoch 1.26: Loss = 1.3683
Epoch 1.27: Loss = 1.43132
Epoch 1.28: Loss = 1.26936
Epoch 1.29: Loss = 1.33174
Epoch 1.30: Loss = 1.30099
Epoch 1.31: Loss = 1.2511
Epoch 1.32: Loss = 1.22951
Epoch 1.33: Loss = 1.16254
Epoch 1.34: Loss = 1.2547
Epoch 1.35: Loss = 1.16983
Epoch 1.36: Loss = 1.19031
Epoch 1.37: Loss = 1.17339
Epoch 1.38: Loss = 1.09535
Epoch 1.39: Loss = 1.05173
Epoch 1.40: Loss = 1.11571
Epoch 1.41: Loss = 1.09036
Epoch 1.42: Loss = 1.07884
Epoch 1.43: Loss = 1.04263
Epoch 1.44: Loss = 1.041
Epoch 1.45: Loss = 0.998016
Epoch 1.46: Loss = 0.999741
Epoch 1.47: Loss = 1.00479
Epoch 1.48: Loss = 0.960526
Epoch 1.49: Loss = 0.921768
Epoch 1.50: Loss = 0.988678
Epoch 1.51: Loss = 0.951141
Epoch 1.52: Loss = 0.920593
Epoch 1.53: Loss = 0.946716
Epoch 1.54: Loss = 0.861893
Epoch 1.55: Loss = 0.851547
Epoch 1.56: Loss = 0.876144
Epoch 1.57: Loss = 0.87645
Epoch 1.58: Loss = 0.884598
Epoch 1.59: Loss = 0.835159
Epoch 1.60: Loss = 0.872986
Epoch 1.61: Loss = 0.79361
Epoch 1.62: Loss = 0.854874
Epoch 1.63: Loss = 0.837265
Epoch 1.64: Loss = 0.812759
Epoch 1.65: Loss = 0.763824
Epoch 1.66: Loss = 0.80751
Epoch 1.67: Loss = 0.81105
Epoch 1.68: Loss = 0.77417
Epoch 1.69: Loss = 0.803284
Epoch 1.70: Loss = 0.777679
Epoch 1.71: Loss = 0.729065
Epoch 1.72: Loss = 0.727554
Epoch 1.73: Loss = 0.754517
Epoch 1.74: Loss = 0.796295
Epoch 1.75: Loss = 0.78598
Epoch 1.76: Loss = 0.777054
Epoch 1.77: Loss = 0.701828
Epoch 1.78: Loss = 0.671356
Epoch 1.79: Loss = 0.731445
Epoch 1.80: Loss = 0.694962
Epoch 1.81: Loss = 0.680893
Epoch 1.82: Loss = 0.624115
Epoch 1.83: Loss = 0.688416
Epoch 1.84: Loss = 0.698196
Epoch 1.85: Loss = 0.664139
Epoch 1.86: Loss = 0.64299
Epoch 1.87: Loss = 0.676636
Epoch 1.88: Loss = 0.620087
Epoch 1.89: Loss = 0.649048
Epoch 1.90: Loss = 0.675797
Epoch 1.91: Loss = 0.622635
Epoch 1.92: Loss = 0.652863
Epoch 1.93: Loss = 0.654907
Epoch 1.94: Loss = 0.652496
Epoch 1.95: Loss = 0.680283
Epoch 1.96: Loss = 0.626038
Epoch 1.97: Loss = 0.650925
Epoch 1.98: Loss = 0.652969
Epoch 1.99: Loss = 0.548004
Epoch 1.100: Loss = 0.624634
TRAIN LOSS = 1.12274
TRAIN ACC = 68.4677 % (41083/60000)
Loss = 0.663483
Loss = 0.680405
Loss = 0.817169
Loss = 0.742371
Loss = 0.699463
Loss = 0.679108
Loss = 0.742172
Loss = 0.715485
Loss = 0.557373
Loss = 0.493103
Loss = 0.41362
Loss = 0.525787
Loss = 0.469421
Loss = 0.443161
Loss = 0.260178
Loss = 0.441528
Loss = 0.733658
TEST LOSS = 0.589976
TEST ACC = 410.829 % (8193/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.630112
Epoch 2.2: Loss = 0.61467
Epoch 2.3: Loss = 0.57962
Epoch 2.4: Loss = 0.631668
Epoch 2.5: Loss = 0.605331
Epoch 2.6: Loss = 0.606384
Epoch 2.7: Loss = 0.588776
Epoch 2.8: Loss = 0.650864
Epoch 2.9: Loss = 0.590363
Epoch 2.10: Loss = 0.626282
Epoch 2.11: Loss = 0.621796
Epoch 2.12: Loss = 0.512024
Epoch 2.13: Loss = 0.531708
Epoch 2.14: Loss = 0.642273
Epoch 2.15: Loss = 0.553177
Epoch 2.16: Loss = 0.56366
Epoch 2.17: Loss = 0.650604
Epoch 2.18: Loss = 0.518494
Epoch 2.19: Loss = 0.559967
Epoch 2.20: Loss = 0.587677
Epoch 2.21: Loss = 0.602448
Epoch 2.22: Loss = 0.559647
Epoch 2.23: Loss = 0.653122
Epoch 2.24: Loss = 0.534866
Epoch 2.25: Loss = 0.520874
Epoch 2.26: Loss = 0.525101
Epoch 2.27: Loss = 0.528976
Epoch 2.28: Loss = 0.585358
Epoch 2.29: Loss = 0.562988
Epoch 2.30: Loss = 0.578247
Epoch 2.31: Loss = 0.520981
Epoch 2.32: Loss = 0.591965
Epoch 2.33: Loss = 0.562622
Epoch 2.34: Loss = 0.507813
Epoch 2.35: Loss = 0.497498
Epoch 2.36: Loss = 0.546936
Epoch 2.37: Loss = 0.60141
Epoch 2.38: Loss = 0.541168
Epoch 2.39: Loss = 0.546066
Epoch 2.40: Loss = 0.512024
Epoch 2.41: Loss = 0.557007
Epoch 2.42: Loss = 0.493256
Epoch 2.43: Loss = 0.477722
Epoch 2.44: Loss = 0.574417
Epoch 2.45: Loss = 0.555374
Epoch 2.46: Loss = 0.587112
Epoch 2.47: Loss = 0.581848
Epoch 2.48: Loss = 0.554962
Epoch 2.49: Loss = 0.484238
Epoch 2.50: Loss = 0.540329
Epoch 2.51: Loss = 0.519409
Epoch 2.52: Loss = 0.51709
Epoch 2.53: Loss = 0.490997
Epoch 2.54: Loss = 0.502441
Epoch 2.55: Loss = 0.552933
Epoch 2.56: Loss = 0.561172
Epoch 2.57: Loss = 0.537048
Epoch 2.58: Loss = 0.580246
Epoch 2.59: Loss = 0.534546
Epoch 2.60: Loss = 0.52533
Epoch 2.61: Loss = 0.615616
Epoch 2.62: Loss = 0.547684
Epoch 2.63: Loss = 0.54274
Epoch 2.64: Loss = 0.48732
Epoch 2.65: Loss = 0.52153
Epoch 2.66: Loss = 0.502289
Epoch 2.67: Loss = 0.508759
Epoch 2.68: Loss = 0.497467
Epoch 2.69: Loss = 0.566437
Epoch 2.70: Loss = 0.572784
Epoch 2.71: Loss = 0.429077
Epoch 2.72: Loss = 0.496094
Epoch 2.73: Loss = 0.555649
Epoch 2.74: Loss = 0.503708
Epoch 2.75: Loss = 0.474136
Epoch 2.76: Loss = 0.541306
Epoch 2.77: Loss = 0.54567
Epoch 2.78: Loss = 0.462631
Epoch 2.79: Loss = 0.502365
Epoch 2.80: Loss = 0.479294
Epoch 2.81: Loss = 0.461746
Epoch 2.82: Loss = 0.457062
Epoch 2.83: Loss = 0.500137
Epoch 2.84: Loss = 0.461792
Epoch 2.85: Loss = 0.477432
Epoch 2.86: Loss = 0.487671
Epoch 2.87: Loss = 0.538208
Epoch 2.88: Loss = 0.508957
Epoch 2.89: Loss = 0.43779
Epoch 2.90: Loss = 0.50296
Epoch 2.91: Loss = 0.471237
Epoch 2.92: Loss = 0.441986
Epoch 2.93: Loss = 0.551331
Epoch 2.94: Loss = 0.516922
Epoch 2.95: Loss = 0.465042
Epoch 2.96: Loss = 0.591354
Epoch 2.97: Loss = 0.46611
Epoch 2.98: Loss = 0.486221
Epoch 2.99: Loss = 0.444687
Epoch 2.100: Loss = 0.498062
TRAIN LOSS = 0.537933
TRAIN ACC = 83.4824 % (50092/60000)
Loss = 0.529205
Loss = 0.574677
Loss = 0.67981
Loss = 0.646393
Loss = 0.547409
Loss = 0.553558
Loss = 0.649811
Loss = 0.581558
Loss = 0.425278
Loss = 0.364929
Loss = 0.327255
Loss = 0.397964
Loss = 0.336014
Loss = 0.389511
Loss = 0.173431
Loss = 0.32341
Loss = 0.72171
TEST LOSS = 0.478881
TEST ACC = 500.919 % (8540/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.44577
Epoch 3.2: Loss = 0.487778
Epoch 3.3: Loss = 0.501175
Epoch 3.4: Loss = 0.486389
Epoch 3.5: Loss = 0.495789
Epoch 3.6: Loss = 0.469772
Epoch 3.7: Loss = 0.494263
Epoch 3.8: Loss = 0.491837
Epoch 3.9: Loss = 0.539093
Epoch 3.10: Loss = 0.436127
Epoch 3.11: Loss = 0.55867
Epoch 3.12: Loss = 0.44046
Epoch 3.13: Loss = 0.564194
Epoch 3.14: Loss = 0.495087
Epoch 3.15: Loss = 0.486206
Epoch 3.16: Loss = 0.46521
Epoch 3.17: Loss = 0.558167
Epoch 3.18: Loss = 0.489639
Epoch 3.19: Loss = 0.450195
Epoch 3.20: Loss = 0.445496
Epoch 3.21: Loss = 0.449402
Epoch 3.22: Loss = 0.446426
Epoch 3.23: Loss = 0.528473
Epoch 3.24: Loss = 0.425751
Epoch 3.25: Loss = 0.508438
Epoch 3.26: Loss = 0.440826
Epoch 3.27: Loss = 0.492844
Epoch 3.28: Loss = 0.447067
Epoch 3.29: Loss = 0.478348
Epoch 3.30: Loss = 0.44783
Epoch 3.31: Loss = 0.463028
Epoch 3.32: Loss = 0.436066
Epoch 3.33: Loss = 0.579163
Epoch 3.34: Loss = 0.539551
Epoch 3.35: Loss = 0.520691
Epoch 3.36: Loss = 0.489899
Epoch 3.37: Loss = 0.499924
Epoch 3.38: Loss = 0.445175
Epoch 3.39: Loss = 0.455643
Epoch 3.40: Loss = 0.469894
Epoch 3.41: Loss = 0.500168
Epoch 3.42: Loss = 0.499619
Epoch 3.43: Loss = 0.496628
Epoch 3.44: Loss = 0.552811
Epoch 3.45: Loss = 0.474274
Epoch 3.46: Loss = 0.565109
Epoch 3.47: Loss = 0.454437
Epoch 3.48: Loss = 0.499878
Epoch 3.49: Loss = 0.442581
Epoch 3.50: Loss = 0.488861
Epoch 3.51: Loss = 0.467636
Epoch 3.52: Loss = 0.521851
Epoch 3.53: Loss = 0.390717
Epoch 3.54: Loss = 0.552094
Epoch 3.55: Loss = 0.509201
Epoch 3.56: Loss = 0.489105
Epoch 3.57: Loss = 0.433304
Epoch 3.58: Loss = 0.457916
Epoch 3.59: Loss = 0.50145
Epoch 3.60: Loss = 0.417938
Epoch 3.61: Loss = 0.41954
Epoch 3.62: Loss = 0.458755
Epoch 3.63: Loss = 0.498734
Epoch 3.64: Loss = 0.428452
Epoch 3.65: Loss = 0.433945
Epoch 3.66: Loss = 0.464661
Epoch 3.67: Loss = 0.593887
Epoch 3.68: Loss = 0.452286
Epoch 3.69: Loss = 0.478043
Epoch 3.70: Loss = 0.431534
Epoch 3.71: Loss = 0.533615
Epoch 3.72: Loss = 0.510895
Epoch 3.73: Loss = 0.464951
Epoch 3.74: Loss = 0.583862
Epoch 3.75: Loss = 0.506271
Epoch 3.76: Loss = 0.441696
Epoch 3.77: Loss = 0.487167
Epoch 3.78: Loss = 0.519577
Epoch 3.79: Loss = 0.473633
Epoch 3.80: Loss = 0.463837
Epoch 3.81: Loss = 0.440201
Epoch 3.82: Loss = 0.437256
Epoch 3.83: Loss = 0.431259
Epoch 3.84: Loss = 0.452362
Epoch 3.85: Loss = 0.449036
Epoch 3.86: Loss = 0.452454
Epoch 3.87: Loss = 0.444275
Epoch 3.88: Loss = 0.480515
Epoch 3.89: Loss = 0.514725
Epoch 3.90: Loss = 0.456802
Epoch 3.91: Loss = 0.523254
Epoch 3.92: Loss = 0.479553
Epoch 3.93: Loss = 0.440704
Epoch 3.94: Loss = 0.508774
Epoch 3.95: Loss = 0.57016
Epoch 3.96: Loss = 0.476166
Epoch 3.97: Loss = 0.469818
Epoch 3.98: Loss = 0.418839
Epoch 3.99: Loss = 0.416428
Epoch 3.100: Loss = 0.457779
TRAIN LOSS = 0.480469
TRAIN ACC = 85.4355 % (51264/60000)
Loss = 0.491226
Loss = 0.565414
Loss = 0.67421
Loss = 0.643127
Loss = 0.520676
Loss = 0.513153
Loss = 0.64476
Loss = 0.551163
Loss = 0.403717
Loss = 0.334824
Loss = 0.370087
Loss = 0.337402
Loss = 0.291946
Loss = 0.402115
Loss = 0.125122
Loss = 0.317581
Loss = 0.756287
TEST LOSS = 0.461443
TEST ACC = 512.639 % (8616/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.492615
Epoch 4.2: Loss = 0.475845
Epoch 4.3: Loss = 0.562729
Epoch 4.4: Loss = 0.514709
Epoch 4.5: Loss = 0.492096
Epoch 4.6: Loss = 0.541626
Epoch 4.7: Loss = 0.466202
Epoch 4.8: Loss = 0.517059
Epoch 4.9: Loss = 0.527817
Epoch 4.10: Loss = 0.400192
Epoch 4.11: Loss = 0.5009
Epoch 4.12: Loss = 0.505692
Epoch 4.13: Loss = 0.495148
Epoch 4.14: Loss = 0.457077
Epoch 4.15: Loss = 0.460373
Epoch 4.16: Loss = 0.483002
Epoch 4.17: Loss = 0.415939
Epoch 4.18: Loss = 0.46817
Epoch 4.19: Loss = 0.485077
Epoch 4.20: Loss = 0.430679
Epoch 4.21: Loss = 0.51123
Epoch 4.22: Loss = 0.43425
Epoch 4.23: Loss = 0.534958
Epoch 4.24: Loss = 0.552094
Epoch 4.25: Loss = 0.445969
Epoch 4.26: Loss = 0.455093
Epoch 4.27: Loss = 0.446533
Epoch 4.28: Loss = 0.46257
Epoch 4.29: Loss = 0.474182
Epoch 4.30: Loss = 0.483398
Epoch 4.31: Loss = 0.422531
Epoch 4.32: Loss = 0.419708
Epoch 4.33: Loss = 0.500122
Epoch 4.34: Loss = 0.460403
Epoch 4.35: Loss = 0.485916
Epoch 4.36: Loss = 0.526566
Epoch 4.37: Loss = 0.479279
Epoch 4.38: Loss = 0.420273
Epoch 4.39: Loss = 0.481873
Epoch 4.40: Loss = 0.537735
Epoch 4.41: Loss = 0.508881
Epoch 4.42: Loss = 0.466324
Epoch 4.43: Loss = 0.512115
Epoch 4.44: Loss = 0.516327
Epoch 4.45: Loss = 0.550903
Epoch 4.46: Loss = 0.506073
Epoch 4.47: Loss = 0.551178
Epoch 4.48: Loss = 0.504227
Epoch 4.49: Loss = 0.462067
Epoch 4.50: Loss = 0.545959
Epoch 4.51: Loss = 0.456528
Epoch 4.52: Loss = 0.479645
Epoch 4.53: Loss = 0.474121
Epoch 4.54: Loss = 0.536819
Epoch 4.55: Loss = 0.449509
Epoch 4.56: Loss = 0.44519
Epoch 4.57: Loss = 0.493713
Epoch 4.58: Loss = 0.484818
Epoch 4.59: Loss = 0.50473
Epoch 4.60: Loss = 0.460617
Epoch 4.61: Loss = 0.46875
Epoch 4.62: Loss = 0.599167
Epoch 4.63: Loss = 0.42984
Epoch 4.64: Loss = 0.483261
Epoch 4.65: Loss = 0.485489
Epoch 4.66: Loss = 0.458801
Epoch 4.67: Loss = 0.454422
Epoch 4.68: Loss = 0.479645
Epoch 4.69: Loss = 0.471786
Epoch 4.70: Loss = 0.520584
Epoch 4.71: Loss = 0.508133
Epoch 4.72: Loss = 0.517715
Epoch 4.73: Loss = 0.512527
Epoch 4.74: Loss = 0.527161
Epoch 4.75: Loss = 0.407272
Epoch 4.76: Loss = 0.530197
Epoch 4.77: Loss = 0.485199
Epoch 4.78: Loss = 0.406204
Epoch 4.79: Loss = 0.513489
Epoch 4.80: Loss = 0.452377
Epoch 4.81: Loss = 0.501083
Epoch 4.82: Loss = 0.483856
Epoch 4.83: Loss = 0.476318
Epoch 4.84: Loss = 0.442062
Epoch 4.85: Loss = 0.504761
Epoch 4.86: Loss = 0.536591
Epoch 4.87: Loss = 0.556808
Epoch 4.88: Loss = 0.517517
Epoch 4.89: Loss = 0.459869
Epoch 4.90: Loss = 0.391815
Epoch 4.91: Loss = 0.407028
Epoch 4.92: Loss = 0.444504
Epoch 4.93: Loss = 0.438339
Epoch 4.94: Loss = 0.469437
Epoch 4.95: Loss = 0.381027
Epoch 4.96: Loss = 0.480347
Epoch 4.97: Loss = 0.508453
Epoch 4.98: Loss = 0.532333
Epoch 4.99: Loss = 0.537537
Epoch 4.100: Loss = 0.475021
TRAIN LOSS = 0.483963
TRAIN ACC = 85.7437 % (51448/60000)
Loss = 0.557495
Loss = 0.600052
Loss = 0.694778
Loss = 0.658524
Loss = 0.528259
Loss = 0.524368
Loss = 0.666077
Loss = 0.575851
Loss = 0.3853
Loss = 0.325562
Loss = 0.350021
Loss = 0.330017
Loss = 0.271881
Loss = 0.415833
Loss = 0.103653
Loss = 0.313171
Loss = 0.720154
TEST LOSS = 0.466857
TEST ACC = 514.479 % (8638/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.601547
Epoch 5.2: Loss = 0.562927
Epoch 5.3: Loss = 0.460663
Epoch 5.4: Loss = 0.522079
Epoch 5.5: Loss = 0.47934
Epoch 5.6: Loss = 0.440781
Epoch 5.7: Loss = 0.422302
Epoch 5.8: Loss = 0.511551
Epoch 5.9: Loss = 0.470688
Epoch 5.10: Loss = 0.60112
Epoch 5.11: Loss = 0.441467
Epoch 5.12: Loss = 0.382965
Epoch 5.13: Loss = 0.47467
Epoch 5.14: Loss = 0.509003
Epoch 5.15: Loss = 0.540054
Epoch 5.16: Loss = 0.570984
Epoch 5.17: Loss = 0.39743
Epoch 5.18: Loss = 0.508423
Epoch 5.19: Loss = 0.441284
Epoch 5.20: Loss = 0.444016
Epoch 5.21: Loss = 0.463669
Epoch 5.22: Loss = 0.459
Epoch 5.23: Loss = 0.473709
Epoch 5.24: Loss = 0.456314
Epoch 5.25: Loss = 0.46431
Epoch 5.26: Loss = 0.490128
Epoch 5.27: Loss = 0.4785
Epoch 5.28: Loss = 0.443741
Epoch 5.29: Loss = 0.56012
Epoch 5.30: Loss = 0.419083
Epoch 5.31: Loss = 0.454941
Epoch 5.32: Loss = 0.582565
Epoch 5.33: Loss = 0.497604
Epoch 5.34: Loss = 0.431122
Epoch 5.35: Loss = 0.538727
Epoch 5.36: Loss = 0.470337
Epoch 5.37: Loss = 0.47554
Epoch 5.38: Loss = 0.470154
Epoch 5.39: Loss = 0.444366
Epoch 5.40: Loss = 0.479019
Epoch 5.41: Loss = 0.577148
Epoch 5.42: Loss = 0.52858
Epoch 5.43: Loss = 0.500366
Epoch 5.44: Loss = 0.466843
Epoch 5.45: Loss = 0.466629
Epoch 5.46: Loss = 0.439972
Epoch 5.47: Loss = 0.595932
Epoch 5.48: Loss = 0.498871
Epoch 5.49: Loss = 0.511032
Epoch 5.50: Loss = 0.663635
Epoch 5.51: Loss = 0.475967
Epoch 5.52: Loss = 0.482895
Epoch 5.53: Loss = 0.538239
Epoch 5.54: Loss = 0.45929
Epoch 5.55: Loss = 0.40979
Epoch 5.56: Loss = 0.539703
Epoch 5.57: Loss = 0.505981
Epoch 5.58: Loss = 0.451584
Epoch 5.59: Loss = 0.496338
Epoch 5.60: Loss = 0.470901
Epoch 5.61: Loss = 0.50383
Epoch 5.62: Loss = 0.394501
Epoch 5.63: Loss = 0.493591
Epoch 5.64: Loss = 0.470917
Epoch 5.65: Loss = 0.38533
Epoch 5.66: Loss = 0.476257
Epoch 5.67: Loss = 0.47847
Epoch 5.68: Loss = 0.535492
Epoch 5.69: Loss = 0.459305
Epoch 5.70: Loss = 0.446671
Epoch 5.71: Loss = 0.49855
Epoch 5.72: Loss = 0.532242
Epoch 5.73: Loss = 0.512955
Epoch 5.74: Loss = 0.415253
Epoch 5.75: Loss = 0.517227
Epoch 5.76: Loss = 0.431747
Epoch 5.77: Loss = 0.511719
Epoch 5.78: Loss = 0.446411
Epoch 5.79: Loss = 0.5811
Epoch 5.80: Loss = 0.49971
Epoch 5.81: Loss = 0.563004
Epoch 5.82: Loss = 0.483292
Epoch 5.83: Loss = 0.456879
Epoch 5.84: Loss = 0.445053
Epoch 5.85: Loss = 0.454269
Epoch 5.86: Loss = 0.434097
Epoch 5.87: Loss = 0.435196
Epoch 5.88: Loss = 0.466187
Epoch 5.89: Loss = 0.45697
Epoch 5.90: Loss = 0.517365
Epoch 5.91: Loss = 0.426285
Epoch 5.92: Loss = 0.486572
Epoch 5.93: Loss = 0.518188
Epoch 5.94: Loss = 0.475327
Epoch 5.95: Loss = 0.542252
Epoch 5.96: Loss = 0.472687
Epoch 5.97: Loss = 0.438248
Epoch 5.98: Loss = 0.530228
Epoch 5.99: Loss = 0.533981
Epoch 5.100: Loss = 0.518875
TRAIN LOSS = 0.486633
TRAIN ACC = 86.0382 % (51625/60000)
Loss = 0.544128
Loss = 0.5849
Loss = 0.720016
Loss = 0.632004
Loss = 0.558441
Loss = 0.53862
Loss = 0.6931
Loss = 0.558563
Loss = 0.429321
Loss = 0.340912
Loss = 0.385223
Loss = 0.294647
Loss = 0.273758
Loss = 0.411514
Loss = 0.127533
Loss = 0.320236
Loss = 0.728592
TEST LOSS = 0.473919
TEST ACC = 516.249 % (8655/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.482285
Epoch 6.2: Loss = 0.449692
Epoch 6.3: Loss = 0.492081
Epoch 6.4: Loss = 0.478729
Epoch 6.5: Loss = 0.513992
Epoch 6.6: Loss = 0.488907
Epoch 6.7: Loss = 0.51088
Epoch 6.8: Loss = 0.45459
Epoch 6.9: Loss = 0.475327
Epoch 6.10: Loss = 0.455933
Epoch 6.11: Loss = 0.435715
Epoch 6.12: Loss = 0.56546
Epoch 6.13: Loss = 0.556274
Epoch 6.14: Loss = 0.477814
Epoch 6.15: Loss = 0.491684
Epoch 6.16: Loss = 0.44545
Epoch 6.17: Loss = 0.460129
Epoch 6.18: Loss = 0.553986
Epoch 6.19: Loss = 0.417816
Epoch 6.20: Loss = 0.456696
Epoch 6.21: Loss = 0.46666
Epoch 6.22: Loss = 0.425751
Epoch 6.23: Loss = 0.571091
Epoch 6.24: Loss = 0.527634
Epoch 6.25: Loss = 0.531143
Epoch 6.26: Loss = 0.45813
Epoch 6.27: Loss = 0.591827
Epoch 6.28: Loss = 0.393097
Epoch 6.29: Loss = 0.464981
Epoch 6.30: Loss = 0.477158
Epoch 6.31: Loss = 0.46579
Epoch 6.32: Loss = 0.419861
Epoch 6.33: Loss = 0.511536
Epoch 6.34: Loss = 0.486862
Epoch 6.35: Loss = 0.477142
Epoch 6.36: Loss = 0.538788
Epoch 6.37: Loss = 0.558365
Epoch 6.38: Loss = 0.524628
Epoch 6.39: Loss = 0.549255
Epoch 6.40: Loss = 0.533951
Epoch 6.41: Loss = 0.468857
Epoch 6.42: Loss = 0.60968
Epoch 6.43: Loss = 0.451248
Epoch 6.44: Loss = 0.660172
Epoch 6.45: Loss = 0.455063
Epoch 6.46: Loss = 0.490356
Epoch 6.47: Loss = 0.50145
Epoch 6.48: Loss = 0.578949
Epoch 6.49: Loss = 0.518188
Epoch 6.50: Loss = 0.462387
Epoch 6.51: Loss = 0.467316
Epoch 6.52: Loss = 0.491989
Epoch 6.53: Loss = 0.594971
Epoch 6.54: Loss = 0.504272
Epoch 6.55: Loss = 0.351334
Epoch 6.56: Loss = 0.502914
Epoch 6.57: Loss = 0.601151
Epoch 6.58: Loss = 0.445328
Epoch 6.59: Loss = 0.55983
Epoch 6.60: Loss = 0.520126
Epoch 6.61: Loss = 0.525116
Epoch 6.62: Loss = 0.41774
Epoch 6.63: Loss = 0.555023
Epoch 6.64: Loss = 0.410446
Epoch 6.65: Loss = 0.359985
Epoch 6.66: Loss = 0.528641
Epoch 6.67: Loss = 0.571884
Epoch 6.68: Loss = 0.398361
Epoch 6.69: Loss = 0.502426
Epoch 6.70: Loss = 0.502991
Epoch 6.71: Loss = 0.456467
Epoch 6.72: Loss = 0.469437
Epoch 6.73: Loss = 0.507172
Epoch 6.74: Loss = 0.518265
Epoch 6.75: Loss = 0.479446
Epoch 6.76: Loss = 0.530731
Epoch 6.77: Loss = 0.583038
Epoch 6.78: Loss = 0.425751
Epoch 6.79: Loss = 0.510712
Epoch 6.80: Loss = 0.542343
Epoch 6.81: Loss = 0.539261
Epoch 6.82: Loss = 0.493698
Epoch 6.83: Loss = 0.440689
Epoch 6.84: Loss = 0.41832
Epoch 6.85: Loss = 0.481903
Epoch 6.86: Loss = 0.579773
Epoch 6.87: Loss = 0.501358
Epoch 6.88: Loss = 0.500305
Epoch 6.89: Loss = 0.400604
Epoch 6.90: Loss = 0.416733
Epoch 6.91: Loss = 0.497986
Epoch 6.92: Loss = 0.463425
Epoch 6.93: Loss = 0.357574
Epoch 6.94: Loss = 0.404572
Epoch 6.95: Loss = 0.469391
Epoch 6.96: Loss = 0.524384
Epoch 6.97: Loss = 0.618607
Epoch 6.98: Loss = 0.408279
Epoch 6.99: Loss = 0.517944
Epoch 6.100: Loss = 0.522659
TRAIN LOSS = 0.492264
TRAIN ACC = 86.2366 % (51744/60000)
Loss = 0.555862
Loss = 0.623672
Loss = 0.720505
Loss = 0.663773
Loss = 0.552536
Loss = 0.536011
Loss = 0.663223
Loss = 0.545914
Loss = 0.396194
Loss = 0.322525
Loss = 0.45845
Loss = 0.315582
Loss = 0.259979
Loss = 0.426071
Loss = 0.0864868
Loss = 0.289856
Loss = 0.73175
TEST LOSS = 0.474268
TEST ACC = 517.439 % (8693/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.430038
Epoch 7.2: Loss = 0.48764
Epoch 7.3: Loss = 0.559601
Epoch 7.4: Loss = 0.477051
Epoch 7.5: Loss = 0.44957
Epoch 7.6: Loss = 0.509323
Epoch 7.7: Loss = 0.503357
Epoch 7.8: Loss = 0.439926
Epoch 7.9: Loss = 0.569092
Epoch 7.10: Loss = 0.570496
Epoch 7.11: Loss = 0.413361
Epoch 7.12: Loss = 0.449631
Epoch 7.13: Loss = 0.392029
Epoch 7.14: Loss = 0.571609
Epoch 7.15: Loss = 0.577347
Epoch 7.16: Loss = 0.512283
Epoch 7.17: Loss = 0.494263
Epoch 7.18: Loss = 0.544373
Epoch 7.19: Loss = 0.508972
Epoch 7.20: Loss = 0.487518
Epoch 7.21: Loss = 0.489655
Epoch 7.22: Loss = 0.579269
Epoch 7.23: Loss = 0.535156
Epoch 7.24: Loss = 0.420959
Epoch 7.25: Loss = 0.468063
Epoch 7.26: Loss = 0.473206
Epoch 7.27: Loss = 0.534805
Epoch 7.28: Loss = 0.434082
Epoch 7.29: Loss = 0.513184
Epoch 7.30: Loss = 0.550598
Epoch 7.31: Loss = 0.404343
Epoch 7.32: Loss = 0.501862
Epoch 7.33: Loss = 0.537003
Epoch 7.34: Loss = 0.452698
Epoch 7.35: Loss = 0.448517
Epoch 7.36: Loss = 0.526016
Epoch 7.37: Loss = 0.402069
Epoch 7.38: Loss = 0.424316
Epoch 7.39: Loss = 0.427963
Epoch 7.40: Loss = 0.524155
Epoch 7.41: Loss = 0.420181
Epoch 7.42: Loss = 0.436447
Epoch 7.43: Loss = 0.556671
Epoch 7.44: Loss = 0.521713
Epoch 7.45: Loss = 0.521927
Epoch 7.46: Loss = 0.453735
Epoch 7.47: Loss = 0.516113
Epoch 7.48: Loss = 0.624405
Epoch 7.49: Loss = 0.420959
Epoch 7.50: Loss = 0.534683
Epoch 7.51: Loss = 0.454697
Epoch 7.52: Loss = 0.461563
Epoch 7.53: Loss = 0.51416
Epoch 7.54: Loss = 0.474503
Epoch 7.55: Loss = 0.575882
Epoch 7.56: Loss = 0.538361
Epoch 7.57: Loss = 0.47551
Epoch 7.58: Loss = 0.496323
Epoch 7.59: Loss = 0.489731
Epoch 7.60: Loss = 0.511276
Epoch 7.61: Loss = 0.553864
Epoch 7.62: Loss = 0.541656
Epoch 7.63: Loss = 0.506485
Epoch 7.64: Loss = 0.626144
Epoch 7.65: Loss = 0.535843
Epoch 7.66: Loss = 0.527649
Epoch 7.67: Loss = 0.522461
Epoch 7.68: Loss = 0.481583
Epoch 7.69: Loss = 0.558899
Epoch 7.70: Loss = 0.388306
Epoch 7.71: Loss = 0.525085
Epoch 7.72: Loss = 0.505051
Epoch 7.73: Loss = 0.465332
Epoch 7.74: Loss = 0.451752
Epoch 7.75: Loss = 0.49556
Epoch 7.76: Loss = 0.395081
Epoch 7.77: Loss = 0.563171
Epoch 7.78: Loss = 0.552856
Epoch 7.79: Loss = 0.486298
Epoch 7.80: Loss = 0.486832
Epoch 7.81: Loss = 0.498947
Epoch 7.82: Loss = 0.460831
Epoch 7.83: Loss = 0.430237
Epoch 7.84: Loss = 0.463562
Epoch 7.85: Loss = 0.601318
Epoch 7.86: Loss = 0.498093
Epoch 7.87: Loss = 0.712158
Epoch 7.88: Loss = 0.440216
Epoch 7.89: Loss = 0.463501
Epoch 7.90: Loss = 0.471008
Epoch 7.91: Loss = 0.54335
Epoch 7.92: Loss = 0.653595
Epoch 7.93: Loss = 0.500168
Epoch 7.94: Loss = 0.561462
Epoch 7.95: Loss = 0.498337
Epoch 7.96: Loss = 0.486069
Epoch 7.97: Loss = 0.54454
Epoch 7.98: Loss = 0.601639
Epoch 7.99: Loss = 0.489395
Epoch 7.100: Loss = 0.436798
TRAIN LOSS = 0.50119
TRAIN ACC = 86.2762 % (51769/60000)
Loss = 0.577698
Loss = 0.623489
Loss = 0.735657
Loss = 0.701553
Loss = 0.552582
Loss = 0.563965
Loss = 0.718826
Loss = 0.576889
Loss = 0.437943
Loss = 0.325226
Loss = 0.438049
Loss = 0.331589
Loss = 0.26886
Loss = 0.412674
Loss = 0.0906219
Loss = 0.286407
Loss = 0.761276
TEST LOSS = 0.488973
TEST ACC = 517.69 % (8682/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.615326
Epoch 8.2: Loss = 0.471802
Epoch 8.3: Loss = 0.456055
Epoch 8.4: Loss = 0.489517
Epoch 8.5: Loss = 0.541809
Epoch 8.6: Loss = 0.470642
Epoch 8.7: Loss = 0.395462
Epoch 8.8: Loss = 0.480881
Epoch 8.9: Loss = 0.44603
Epoch 8.10: Loss = 0.424805
Epoch 8.11: Loss = 0.506042
Epoch 8.12: Loss = 0.489075
Epoch 8.13: Loss = 0.631622
Epoch 8.14: Loss = 0.486145
Epoch 8.15: Loss = 0.43512
Epoch 8.16: Loss = 0.440201
Epoch 8.17: Loss = 0.556381
Epoch 8.18: Loss = 0.616791
Epoch 8.19: Loss = 0.51207
Epoch 8.20: Loss = 0.460068
Epoch 8.21: Loss = 0.464066
Epoch 8.22: Loss = 0.480087
Epoch 8.23: Loss = 0.536224
Epoch 8.24: Loss = 0.587448
Epoch 8.25: Loss = 0.378754
Epoch 8.26: Loss = 0.478485
Epoch 8.27: Loss = 0.466965
Epoch 8.28: Loss = 0.621078
Epoch 8.29: Loss = 0.531342
Epoch 8.30: Loss = 0.55687
Epoch 8.31: Loss = 0.489517
Epoch 8.32: Loss = 0.480148
Epoch 8.33: Loss = 0.49884
Epoch 8.34: Loss = 0.490814
Epoch 8.35: Loss = 0.545471
Epoch 8.36: Loss = 0.561569
Epoch 8.37: Loss = 0.46109
Epoch 8.38: Loss = 0.576904
Epoch 8.39: Loss = 0.535599
Epoch 8.40: Loss = 0.422089
Epoch 8.41: Loss = 0.504822
Epoch 8.42: Loss = 0.57991
Epoch 8.43: Loss = 0.648071
Epoch 8.44: Loss = 0.595322
Epoch 8.45: Loss = 0.544205
Epoch 8.46: Loss = 0.43161
Epoch 8.47: Loss = 0.452469
Epoch 8.48: Loss = 0.530975
Epoch 8.49: Loss = 0.677124
Epoch 8.50: Loss = 0.563141
Epoch 8.51: Loss = 0.513565
Epoch 8.52: Loss = 0.55751
Epoch 8.53: Loss = 0.543701
Epoch 8.54: Loss = 0.471786
Epoch 8.55: Loss = 0.463409
Epoch 8.56: Loss = 0.559753
Epoch 8.57: Loss = 0.448898
Epoch 8.58: Loss = 0.559601
Epoch 8.59: Loss = 0.662476
Epoch 8.60: Loss = 0.459015
Epoch 8.61: Loss = 0.512146
Epoch 8.62: Loss = 0.482239
Epoch 8.63: Loss = 0.53067
Epoch 8.64: Loss = 0.524353
Epoch 8.65: Loss = 0.428223
Epoch 8.66: Loss = 0.653275
Epoch 8.67: Loss = 0.505295
Epoch 8.68: Loss = 0.54248
Epoch 8.69: Loss = 0.451797
Epoch 8.70: Loss = 0.514099
Epoch 8.71: Loss = 0.446976
Epoch 8.72: Loss = 0.529205
Epoch 8.73: Loss = 0.47699
Epoch 8.74: Loss = 0.516281
Epoch 8.75: Loss = 0.505112
Epoch 8.76: Loss = 0.482651
Epoch 8.77: Loss = 0.582275
Epoch 8.78: Loss = 0.440475
Epoch 8.79: Loss = 0.458099
Epoch 8.80: Loss = 0.53537
Epoch 8.81: Loss = 0.513077
Epoch 8.82: Loss = 0.504425
Epoch 8.83: Loss = 0.533447
Epoch 8.84: Loss = 0.648178
Epoch 8.85: Loss = 0.480347
Epoch 8.86: Loss = 0.543777
Epoch 8.87: Loss = 0.602234
Epoch 8.88: Loss = 0.52182
Epoch 8.89: Loss = 0.481857
Epoch 8.90: Loss = 0.466263
Epoch 8.91: Loss = 0.574966
Epoch 8.92: Loss = 0.546249
Epoch 8.93: Loss = 0.439911
Epoch 8.94: Loss = 0.482025
Epoch 8.95: Loss = 0.556198
Epoch 8.96: Loss = 0.538635
Epoch 8.97: Loss = 0.44104
Epoch 8.98: Loss = 0.459412
Epoch 8.99: Loss = 0.424774
Epoch 8.100: Loss = 0.360153
TRAIN LOSS = 0.510941
TRAIN ACC = 86.5921 % (51957/60000)
Loss = 0.600296
Loss = 0.619705
Loss = 0.715134
Loss = 0.688995
Loss = 0.563721
Loss = 0.552582
Loss = 0.708099
Loss = 0.564346
Loss = 0.415146
Loss = 0.311676
Loss = 0.392578
Loss = 0.31102
Loss = 0.269424
Loss = 0.451736
Loss = 0.103333
Loss = 0.277191
Loss = 0.767731
TEST LOSS = 0.483408
TEST ACC = 519.569 % (8755/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.527863
Epoch 9.2: Loss = 0.507126
Epoch 9.3: Loss = 0.529037
Epoch 9.4: Loss = 0.460068
Epoch 9.5: Loss = 0.537628
Epoch 9.6: Loss = 0.531326
Epoch 9.7: Loss = 0.494705
Epoch 9.8: Loss = 0.505692
Epoch 9.9: Loss = 0.467941
Epoch 9.10: Loss = 0.621445
Epoch 9.11: Loss = 0.556366
Epoch 9.12: Loss = 0.552963
Epoch 9.13: Loss = 0.503815
Epoch 9.14: Loss = 0.505844
Epoch 9.15: Loss = 0.565231
Epoch 9.16: Loss = 0.577728
Epoch 9.17: Loss = 0.489975
Epoch 9.18: Loss = 0.519424
Epoch 9.19: Loss = 0.428284
Epoch 9.20: Loss = 0.50914
Epoch 9.21: Loss = 0.484497
Epoch 9.22: Loss = 0.552719
Epoch 9.23: Loss = 0.566833
Epoch 9.24: Loss = 0.537216
Epoch 9.25: Loss = 0.611084
Epoch 9.26: Loss = 0.548599
Epoch 9.27: Loss = 0.49173
Epoch 9.28: Loss = 0.464478
Epoch 9.29: Loss = 0.462112
Epoch 9.30: Loss = 0.468765
Epoch 9.31: Loss = 0.508469
Epoch 9.32: Loss = 0.558624
Epoch 9.33: Loss = 0.423019
Epoch 9.34: Loss = 0.55899
Epoch 9.35: Loss = 0.416931
Epoch 9.36: Loss = 0.556198
Epoch 9.37: Loss = 0.423477
Epoch 9.38: Loss = 0.531433
Epoch 9.39: Loss = 0.452225
Epoch 9.40: Loss = 0.556244
Epoch 9.41: Loss = 0.568542
Epoch 9.42: Loss = 0.569595
Epoch 9.43: Loss = 0.48764
Epoch 9.44: Loss = 0.48259
Epoch 9.45: Loss = 0.464569
Epoch 9.46: Loss = 0.671692
Epoch 9.47: Loss = 0.560989
Epoch 9.48: Loss = 0.493103
Epoch 9.49: Loss = 0.546509
Epoch 9.50: Loss = 0.538193
Epoch 9.51: Loss = 0.38028
Epoch 9.52: Loss = 0.500565
Epoch 9.53: Loss = 0.455307
Epoch 9.54: Loss = 0.448334
Epoch 9.55: Loss = 0.400253
Epoch 9.56: Loss = 0.507156
Epoch 9.57: Loss = 0.522568
Epoch 9.58: Loss = 0.541702
Epoch 9.59: Loss = 0.438782
Epoch 9.60: Loss = 0.603271
Epoch 9.61: Loss = 0.564651
Epoch 9.62: Loss = 0.570847
Epoch 9.63: Loss = 0.420349
Epoch 9.64: Loss = 0.562088
Epoch 9.65: Loss = 0.538803
Epoch 9.66: Loss = 0.594757
Epoch 9.67: Loss = 0.625229
Epoch 9.68: Loss = 0.4617
Epoch 9.69: Loss = 0.476791
Epoch 9.70: Loss = 0.518356
Epoch 9.71: Loss = 0.633652
Epoch 9.72: Loss = 0.530106
Epoch 9.73: Loss = 0.60051
Epoch 9.74: Loss = 0.482208
Epoch 9.75: Loss = 0.436813
Epoch 9.76: Loss = 0.580261
Epoch 9.77: Loss = 0.441208
Epoch 9.78: Loss = 0.50737
Epoch 9.79: Loss = 0.454636
Epoch 9.80: Loss = 0.447464
Epoch 9.81: Loss = 0.500595
Epoch 9.82: Loss = 0.408508
Epoch 9.83: Loss = 0.508942
Epoch 9.84: Loss = 0.506943
Epoch 9.85: Loss = 0.508301
Epoch 9.86: Loss = 0.582504
Epoch 9.87: Loss = 0.572479
Epoch 9.88: Loss = 0.611359
Epoch 9.89: Loss = 0.53302
Epoch 9.90: Loss = 0.551636
Epoch 9.91: Loss = 0.658936
Epoch 9.92: Loss = 0.484543
Epoch 9.93: Loss = 0.544434
Epoch 9.94: Loss = 0.579239
Epoch 9.95: Loss = 0.474487
Epoch 9.96: Loss = 0.539963
Epoch 9.97: Loss = 0.528305
Epoch 9.98: Loss = 0.526627
Epoch 9.99: Loss = 0.512848
Epoch 9.100: Loss = 0.48909
TRAIN LOSS = 0.518188
TRAIN ACC = 86.9278 % (52159/60000)
Loss = 0.557968
Loss = 0.60611
Loss = 0.753296
Loss = 0.664902
Loss = 0.558258
Loss = 0.526306
Loss = 0.691925
Loss = 0.559464
Loss = 0.418076
Loss = 0.327057
Loss = 0.380447
Loss = 0.350327
Loss = 0.268524
Loss = 0.417618
Loss = 0.109818
Loss = 0.295593
Loss = 0.787231
TEST LOSS = 0.48063
TEST ACC = 521.59 % (8787/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.510498
Epoch 10.2: Loss = 0.562836
Epoch 10.3: Loss = 0.475281
Epoch 10.4: Loss = 0.567703
Epoch 10.5: Loss = 0.472305
Epoch 10.6: Loss = 0.463394
Epoch 10.7: Loss = 0.394455
Epoch 10.8: Loss = 0.637161
Epoch 10.9: Loss = 0.513748
Epoch 10.10: Loss = 0.729675
Epoch 10.11: Loss = 0.567291
Epoch 10.12: Loss = 0.353149
Epoch 10.13: Loss = 0.536011
Epoch 10.14: Loss = 0.405441
Epoch 10.15: Loss = 0.539429
Epoch 10.16: Loss = 0.647705
Epoch 10.17: Loss = 0.560608
Epoch 10.18: Loss = 0.53035
Epoch 10.19: Loss = 0.463852
Epoch 10.20: Loss = 0.584595
Epoch 10.21: Loss = 0.562668
Epoch 10.22: Loss = 0.524216
Epoch 10.23: Loss = 0.519241
Epoch 10.24: Loss = 0.546448
Epoch 10.25: Loss = 0.441483
Epoch 10.26: Loss = 0.455292
Epoch 10.27: Loss = 0.426468
Epoch 10.28: Loss = 0.485764
Epoch 10.29: Loss = 0.523224
Epoch 10.30: Loss = 0.510086
Epoch 10.31: Loss = 0.570755
Epoch 10.32: Loss = 0.418518
Epoch 10.33: Loss = 0.579147
Epoch 10.34: Loss = 0.571625
Epoch 10.35: Loss = 0.513031
Epoch 10.36: Loss = 0.453384
Epoch 10.37: Loss = 0.433685
Epoch 10.38: Loss = 0.56044
Epoch 10.39: Loss = 0.514359
Epoch 10.40: Loss = 0.448715
Epoch 10.41: Loss = 0.474701
Epoch 10.42: Loss = 0.60585
Epoch 10.43: Loss = 0.437683
Epoch 10.44: Loss = 0.592743
Epoch 10.45: Loss = 0.350693
Epoch 10.46: Loss = 0.407776
Epoch 10.47: Loss = 0.494873
Epoch 10.48: Loss = 0.554825
Epoch 10.49: Loss = 0.535614
Epoch 10.50: Loss = 0.470764
Epoch 10.51: Loss = 0.578278
Epoch 10.52: Loss = 0.451721
Epoch 10.53: Loss = 0.57547
Epoch 10.54: Loss = 0.516586
Epoch 10.55: Loss = 0.585846
Epoch 10.56: Loss = 0.485168
Epoch 10.57: Loss = 0.49826
Epoch 10.58: Loss = 0.497742
Epoch 10.59: Loss = 0.523026
Epoch 10.60: Loss = 0.607452
Epoch 10.61: Loss = 0.48877
Epoch 10.62: Loss = 0.447052
Epoch 10.63: Loss = 0.393311
Epoch 10.64: Loss = 0.459229
Epoch 10.65: Loss = 0.485977
Epoch 10.66: Loss = 0.471436
Epoch 10.67: Loss = 0.522354
Epoch 10.68: Loss = 0.470703
Epoch 10.69: Loss = 0.442078
Epoch 10.70: Loss = 0.54155
Epoch 10.71: Loss = 0.569824
Epoch 10.72: Loss = 0.541397
Epoch 10.73: Loss = 0.31012
Epoch 10.74: Loss = 0.495987
Epoch 10.75: Loss = 0.609665
Epoch 10.76: Loss = 0.518372
Epoch 10.77: Loss = 0.397614
Epoch 10.78: Loss = 0.545837
Epoch 10.79: Loss = 0.525665
Epoch 10.80: Loss = 0.450317
Epoch 10.81: Loss = 0.490738
Epoch 10.82: Loss = 0.472061
Epoch 10.83: Loss = 0.391327
Epoch 10.84: Loss = 0.473312
Epoch 10.85: Loss = 0.545578
Epoch 10.86: Loss = 0.591812
Epoch 10.87: Loss = 0.557556
Epoch 10.88: Loss = 0.530609
Epoch 10.89: Loss = 0.504868
Epoch 10.90: Loss = 0.500351
Epoch 10.91: Loss = 0.537216
Epoch 10.92: Loss = 0.46167
Epoch 10.93: Loss = 0.469925
Epoch 10.94: Loss = 0.505188
Epoch 10.95: Loss = 0.577316
Epoch 10.96: Loss = 0.625549
Epoch 10.97: Loss = 0.51326
Epoch 10.98: Loss = 0.713196
Epoch 10.99: Loss = 0.531006
Epoch 10.100: Loss = 0.597305
TRAIN LOSS = 0.510971
TRAIN ACC = 87.1674 % (52302/60000)
Loss = 0.530609
Loss = 0.622192
Loss = 0.726959
Loss = 0.69693
Loss = 0.544373
Loss = 0.518814
Loss = 0.695648
Loss = 0.565842
Loss = 0.401001
Loss = 0.335052
Loss = 0.354782
Loss = 0.296219
Loss = 0.245941
Loss = 0.390594
Loss = 0.107468
Loss = 0.278671
Loss = 0.786758
TEST LOSS = 0.470136
TEST ACC = 523.019 % (8798/10000)
Epoch 11.1: Loss = 0.527985
Epoch 11.2: Loss = 0.540085
Epoch 11.3: Loss = 0.596207
Epoch 11.4: Loss = 0.601212
Epoch 11.5: Loss = 0.482834
Epoch 11.6: Loss = 0.579346
Epoch 11.7: Loss = 0.511795
Epoch 11.8: Loss = 0.450226
Epoch 11.9: Loss = 0.505508
Epoch 11.10: Loss = 0.465012
Epoch 11.11: Loss = 0.493774
Epoch 11.12: Loss = 0.572479
Epoch 11.13: Loss = 0.47435
Epoch 11.14: Loss = 0.527954
Epoch 11.15: Loss = 0.574631
Epoch 11.16: Loss = 0.509445
Epoch 11.17: Loss = 0.448868
Epoch 11.18: Loss = 0.484161
Epoch 11.19: Loss = 0.552887
Epoch 11.20: Loss = 0.639984
Epoch 11.21: Loss = 0.624969
Epoch 11.22: Loss = 0.48999
Epoch 11.23: Loss = 0.484619
Epoch 11.24: Loss = 0.428177
Epoch 11.25: Loss = 0.666107
Epoch 11.26: Loss = 0.558563
Epoch 11.27: Loss = 0.50119
Epoch 11.28: Loss = 0.617981
Epoch 11.29: Loss = 0.522461
Epoch 11.30: Loss = 0.434006
Epoch 11.31: Loss = 0.579544
Epoch 11.32: Loss = 0.511108
Epoch 11.33: Loss = 0.435303
Epoch 11.34: Loss = 0.500565
Epoch 11.35: Loss = 0.569839
Epoch 11.36: Loss = 0.570023
Epoch 11.37: Loss = 0.423111
Epoch 11.38: Loss = 0.464691
Epoch 11.39: Loss = 0.510712
Epoch 11.40: Loss = 0.514847
Epoch 11.41: Loss = 0.515213
Epoch 11.42: Loss = 0.46109
Epoch 11.43: Loss = 0.480423
Epoch 11.44: Loss = 0.536163
Epoch 11.45: Loss = 0.606689
Epoch 11.46: Loss = 0.487228
Epoch 11.47: Loss = 0.530182
Epoch 11.48: Loss = 0.454575
Epoch 11.49: Loss = 0.40062
Epoch 11.50: Loss = 0.564423
Epoch 11.51: Loss = 0.538254
Epoch 11.52: Loss = 0.420502
Epoch 11.53: Loss = 0.4935
Epoch 11.54: Loss = 0.46846
Epoch 11.55: Loss = 0.524109
Epoch 11.56: Loss = 0.461441
Epoch 11.57: Loss = 0.429031
Epoch 11.58: Loss = 0.611084
Epoch 11.59: Loss = 0.50209
Epoch 11.60: Loss = 0.586502
Epoch 11.61: Loss = 0.482025
Epoch 11.62: Loss = 0.570374
Epoch 11.63: Loss = 0.519165
Epoch 11.64: Loss = 0.50499
Epoch 11.65: Loss = 0.462173
Epoch 11.66: Loss = 0.501663
Epoch 11.67: Loss = 0.472015
Epoch 11.68: Loss = 0.54924
Epoch 11.69: Loss = 0.502289
Epoch 11.70: Loss = 0.637146
Epoch 11.71: Loss = 0.475952
Epoch 11.72: Loss = 0.484558
Epoch 11.73: Loss = 0.464111
Epoch 11.74: Loss = 0.61171
Epoch 11.75: Loss = 0.470963
Epoch 11.76: Loss = 0.577209
Epoch 11.77: Loss = 0.52298
Epoch 11.78: Loss = 0.506729
Epoch 11.79: Loss = 0.476089
Epoch 11.80: Loss = 0.65126
Epoch 11.81: Loss = 0.566025
Epoch 11.82: Loss = 0.400101
Epoch 11.83: Loss = 0.498154
Epoch 11.84: Loss = 0.521286
Epoch 11.85: Loss = 0.509048
Epoch 11.86: Loss = 0.486755
Epoch 11.87: Loss = 0.410248
Epoch 11.88: Loss = 0.493271
Epoch 11.89: Loss = 0.456039
Epoch 11.90: Loss = 0.52153
Epoch 11.91: Loss = 0.551666
Epoch 11.92: Loss = 0.470398
Epoch 11.93: Loss = 0.519913
Epoch 11.94: Loss = 0.427414
Epoch 11.95: Loss = 0.478271
Epoch 11.96: Loss = 0.41188
Epoch 11.97: Loss = 0.519806
Epoch 11.98: Loss = 0.439301
Epoch 11.99: Loss = 0.505066
Epoch 11.100: Loss = 0.495804
TRAIN LOSS = 0.511429
TRAIN ACC = 87.1704 % (52305/60000)
Loss = 0.526627
Loss = 0.593735
Loss = 0.744263
Loss = 0.685028
Loss = 0.522614
Loss = 0.520462
Loss = 0.705795
Loss = 0.563309
Loss = 0.421356
Loss = 0.335999
Loss = 0.417892
Loss = 0.316422
Loss = 0.260773
Loss = 0.389877
Loss = 0.107315
Loss = 0.30069
Loss = 0.799545
TEST LOSS = 0.476711
TEST ACC = 523.048 % (8781/10000)
Epoch 12.1: Loss = 0.444672
Epoch 12.2: Loss = 0.50354
Epoch 12.3: Loss = 0.416534
Epoch 12.4: Loss = 0.577271
Epoch 12.5: Loss = 0.553726
Epoch 12.6: Loss = 0.438797
Epoch 12.7: Loss = 0.502731
Epoch 12.8: Loss = 0.329575
Epoch 12.9: Loss = 0.580933
Epoch 12.10: Loss = 0.473679
Epoch 12.11: Loss = 0.467453
Epoch 12.12: Loss = 0.587692
Epoch 12.13: Loss = 0.454697
Epoch 12.14: Loss = 0.43576
Epoch 12.15: Loss = 0.433563
Epoch 12.16: Loss = 0.412842
Epoch 12.17: Loss = 0.452957
Epoch 12.18: Loss = 0.625488
Epoch 12.19: Loss = 0.463852
Epoch 12.20: Loss = 0.526047
Epoch 12.21: Loss = 0.415558
Epoch 12.22: Loss = 0.4646
Epoch 12.23: Loss = 0.415955
Epoch 12.24: Loss = 0.370087
Epoch 12.25: Loss = 0.511292
Epoch 12.26: Loss = 0.412979
Epoch 12.27: Loss = 0.518051
Epoch 12.28: Loss = 0.468109
Epoch 12.29: Loss = 0.595673
Epoch 12.30: Loss = 0.480698
Epoch 12.31: Loss = 0.540268
Epoch 12.32: Loss = 0.532639
Epoch 12.33: Loss = 0.597824
Epoch 12.34: Loss = 0.457474
Epoch 12.35: Loss = 0.545715
Epoch 12.36: Loss = 0.466629
Epoch 12.37: Loss = 0.667053
Epoch 12.38: Loss = 0.492355
Epoch 12.39: Loss = 0.445053
Epoch 12.40: Loss = 0.446716
Epoch 12.41: Loss = 0.444534
Epoch 12.42: Loss = 0.603104
Epoch 12.43: Loss = 0.515198
Epoch 12.44: Loss = 0.497528
Epoch 12.45: Loss = 0.577438
Epoch 12.46: Loss = 0.57016
Epoch 12.47: Loss = 0.534256
Epoch 12.48: Loss = 0.510056
Epoch 12.49: Loss = 0.480667
Epoch 12.50: Loss = 0.552338
Epoch 12.51: Loss = 0.564026
Epoch 12.52: Loss = 0.493835
Epoch 12.53: Loss = 0.454727
Epoch 12.54: Loss = 0.575562
Epoch 12.55: Loss = 0.538864
Epoch 12.56: Loss = 0.511566
Epoch 12.57: Loss = 0.515656
Epoch 12.58: Loss = 0.571869
Epoch 12.59: Loss = 0.454651
Epoch 12.60: Loss = 0.485855
Epoch 12.61: Loss = 0.402328
Epoch 12.62: Loss = 0.489624
Epoch 12.63: Loss = 0.553467
Epoch 12.64: Loss = 0.658813
Epoch 12.65: Loss = 0.465454
Epoch 12.66: Loss = 0.504776
Epoch 12.67: Loss = 0.474396
Epoch 12.68: Loss = 0.64679
Epoch 12.69: Loss = 0.42247
Epoch 12.70: Loss = 0.445267
Epoch 12.71: Loss = 0.667969
Epoch 12.72: Loss = 0.57872
Epoch 12.73: Loss = 0.458817
Epoch 12.74: Loss = 0.499268
Epoch 12.75: Loss = 0.55127
Epoch 12.76: Loss = 0.590088
Epoch 12.77: Loss = 0.543045
Epoch 12.78: Loss = 0.590408
Epoch 12.79: Loss = 0.467148
Epoch 12.80: Loss = 0.427628
Epoch 12.81: Loss = 0.624069
Epoch 12.82: Loss = 0.447678
Epoch 12.83: Loss = 0.418274
Epoch 12.84: Loss = 0.567566
Epoch 12.85: Loss = 0.565598
Epoch 12.86: Loss = 0.458557
Epoch 12.87: Loss = 0.480408
Epoch 12.88: Loss = 0.613831
Epoch 12.89: Loss = 0.521881
Epoch 12.90: Loss = 0.429199
Epoch 12.91: Loss = 0.478195
Epoch 12.92: Loss = 0.438614
Epoch 12.93: Loss = 0.567917
Epoch 12.94: Loss = 0.574265
Epoch 12.95: Loss = 0.538788
Epoch 12.96: Loss = 0.550461
Epoch 12.97: Loss = 0.709229
Epoch 12.98: Loss = 0.533737
Epoch 12.99: Loss = 0.486038
Epoch 12.100: Loss = 0.448639
TRAIN LOSS = 0.508698
TRAIN ACC = 87.3337 % (52402/60000)
Loss = 0.51918
Loss = 0.595139
Loss = 0.768463
Loss = 0.683853
Loss = 0.519638
Loss = 0.530273
Loss = 0.683029
Loss = 0.561813
Loss = 0.399002
Loss = 0.355179
Loss = 0.454269
Loss = 0.322144
Loss = 0.269104
Loss = 0.396606
Loss = 0.107605
Loss = 0.307465
Loss = 0.811111
TEST LOSS = 0.48081
TEST ACC = 524.019 % (8806/10000)
Epoch 13.1: Loss = 0.617859
Epoch 13.2: Loss = 0.455063
Epoch 13.3: Loss = 0.525162
Epoch 13.4: Loss = 0.477798
Epoch 13.5: Loss = 0.501297
Epoch 13.6: Loss = 0.490356
Epoch 13.7: Loss = 0.514557
Epoch 13.8: Loss = 0.519073
Epoch 13.9: Loss = 0.522064
Epoch 13.10: Loss = 0.401016
Epoch 13.11: Loss = 0.522827
Epoch 13.12: Loss = 0.451248
Epoch 13.13: Loss = 0.457489
Epoch 13.14: Loss = 0.435928
Epoch 13.15: Loss = 0.524734
Epoch 13.16: Loss = 0.479721
Epoch 13.17: Loss = 0.487427
Epoch 13.18: Loss = 0.52533
Epoch 13.19: Loss = 0.589462
Epoch 13.20: Loss = 0.509781
Epoch 13.21: Loss = 0.685425
Epoch 13.22: Loss = 0.493011
Epoch 13.23: Loss = 0.497528
Epoch 13.24: Loss = 0.412277
Epoch 13.25: Loss = 0.524384
Epoch 13.26: Loss = 0.48381
Epoch 13.27: Loss = 0.497879
Epoch 13.28: Loss = 0.495667
Epoch 13.29: Loss = 0.395935
Epoch 13.30: Loss = 0.37648
Epoch 13.31: Loss = 0.393677
Epoch 13.32: Loss = 0.44455
Epoch 13.33: Loss = 0.41806
Epoch 13.34: Loss = 0.484726
Epoch 13.35: Loss = 0.532089
Epoch 13.36: Loss = 0.588593
Epoch 13.37: Loss = 0.454391
Epoch 13.38: Loss = 0.513275
Epoch 13.39: Loss = 0.480301
Epoch 13.40: Loss = 0.531525
Epoch 13.41: Loss = 0.504898
Epoch 13.42: Loss = 0.445389
Epoch 13.43: Loss = 0.551193
Epoch 13.44: Loss = 0.410461
Epoch 13.45: Loss = 0.470871
Epoch 13.46: Loss = 0.432281
Epoch 13.47: Loss = 0.553574
Epoch 13.48: Loss = 0.433609
Epoch 13.49: Loss = 0.624588
Epoch 13.50: Loss = 0.450836
Epoch 13.51: Loss = 0.465637
Epoch 13.52: Loss = 0.592621
Epoch 13.53: Loss = 0.537918
Epoch 13.54: Loss = 0.528671
Epoch 13.55: Loss = 0.596802
Epoch 13.56: Loss = 0.478287
Epoch 13.57: Loss = 0.605484
Epoch 13.58: Loss = 0.519592
Epoch 13.59: Loss = 0.46315
Epoch 13.60: Loss = 0.520111
Epoch 13.61: Loss = 0.591522
Epoch 13.62: Loss = 0.650391
Epoch 13.63: Loss = 0.642197
Epoch 13.64: Loss = 0.595413
Epoch 13.65: Loss = 0.459518
Epoch 13.66: Loss = 0.468124
Epoch 13.67: Loss = 0.402725
Epoch 13.68: Loss = 0.476379
Epoch 13.69: Loss = 0.498337
Epoch 13.70: Loss = 0.523407
Epoch 13.71: Loss = 0.389526
Epoch 13.72: Loss = 0.405762
Epoch 13.73: Loss = 0.457703
Epoch 13.74: Loss = 0.521225
Epoch 13.75: Loss = 0.537491
Epoch 13.76: Loss = 0.541763
Epoch 13.77: Loss = 0.575607
Epoch 13.78: Loss = 0.449051
Epoch 13.79: Loss = 0.372986
Epoch 13.80: Loss = 0.567429
Epoch 13.81: Loss = 0.666992
Epoch 13.82: Loss = 0.505234
Epoch 13.83: Loss = 0.578506
Epoch 13.84: Loss = 0.503021
Epoch 13.85: Loss = 0.508453
Epoch 13.86: Loss = 0.517914
Epoch 13.87: Loss = 0.595947
Epoch 13.88: Loss = 0.527267
Epoch 13.89: Loss = 0.502762
Epoch 13.90: Loss = 0.561203
Epoch 13.91: Loss = 0.56694
Epoch 13.92: Loss = 0.554184
Epoch 13.93: Loss = 0.582413
Epoch 13.94: Loss = 0.438705
Epoch 13.95: Loss = 0.496033
Epoch 13.96: Loss = 0.542099
Epoch 13.97: Loss = 0.437683
Epoch 13.98: Loss = 0.531921
Epoch 13.99: Loss = 0.460419
Epoch 13.100: Loss = 0.532089
TRAIN LOSS = 0.506332
TRAIN ACC = 87.6129 % (52570/60000)
Loss = 0.510605
Loss = 0.598419
Loss = 0.80304
Loss = 0.700897
Loss = 0.513901
Loss = 0.541977
Loss = 0.723206
Loss = 0.56369
Loss = 0.401062
Loss = 0.338394
Loss = 0.428574
Loss = 0.303314
Loss = 0.265854
Loss = 0.372147
Loss = 0.114853
Loss = 0.270126
Loss = 0.794174
TEST LOSS = 0.47877
TEST ACC = 525.699 % (8827/10000)
Epoch 14.1: Loss = 0.530792
Epoch 14.2: Loss = 0.58287
Epoch 14.3: Loss = 0.40892
Epoch 14.4: Loss = 0.556992
Epoch 14.5: Loss = 0.420303
Epoch 14.6: Loss = 0.553833
Epoch 14.7: Loss = 0.680649
Epoch 14.8: Loss = 0.572388
Epoch 14.9: Loss = 0.457535
Epoch 14.10: Loss = 0.570175
Epoch 14.11: Loss = 0.609787
Epoch 14.12: Loss = 0.506836
Epoch 14.13: Loss = 0.408432
Epoch 14.14: Loss = 0.464813
Epoch 14.15: Loss = 0.569977
Epoch 14.16: Loss = 0.454803
Epoch 14.17: Loss = 0.427673
Epoch 14.18: Loss = 0.420624
Epoch 14.19: Loss = 0.55278
Epoch 14.20: Loss = 0.547424
Epoch 14.21: Loss = 0.492676
Epoch 14.22: Loss = 0.473846
Epoch 14.23: Loss = 0.528549
Epoch 14.24: Loss = 0.527939
Epoch 14.25: Loss = 0.431381
Epoch 14.26: Loss = 0.438034
Epoch 14.27: Loss = 0.550583
Epoch 14.28: Loss = 0.601166
Epoch 14.29: Loss = 0.509537
Epoch 14.30: Loss = 0.443481
Epoch 14.31: Loss = 0.424591
Epoch 14.32: Loss = 0.531723
Epoch 14.33: Loss = 0.514267
Epoch 14.34: Loss = 0.499481
Epoch 14.35: Loss = 0.561707
Epoch 14.36: Loss = 0.550476
Epoch 14.37: Loss = 0.434799
Epoch 14.38: Loss = 0.601501
Epoch 14.39: Loss = 0.535309
Epoch 14.40: Loss = 0.569992
Epoch 14.41: Loss = 0.466019
Epoch 14.42: Loss = 0.665848
Epoch 14.43: Loss = 0.583694
Epoch 14.44: Loss = 0.533478
Epoch 14.45: Loss = 0.605942
Epoch 14.46: Loss = 0.510345
Epoch 14.47: Loss = 0.524567
Epoch 14.48: Loss = 0.481888
Epoch 14.49: Loss = 0.489334
Epoch 14.50: Loss = 0.55336
Epoch 14.51: Loss = 0.444046
Epoch 14.52: Loss = 0.611206
Epoch 14.53: Loss = 0.408524
Epoch 14.54: Loss = 0.434464
Epoch 14.55: Loss = 0.450684
Epoch 14.56: Loss = 0.540039
Epoch 14.57: Loss = 0.655151
Epoch 14.58: Loss = 0.613327
Epoch 14.59: Loss = 0.375732
Epoch 14.60: Loss = 0.527893
Epoch 14.61: Loss = 0.536163
Epoch 14.62: Loss = 0.45314
Epoch 14.63: Loss = 0.411758
Epoch 14.64: Loss = 0.445724
Epoch 14.65: Loss = 0.487106
Epoch 14.66: Loss = 0.501999
Epoch 14.67: Loss = 0.54509
Epoch 14.68: Loss = 0.450958
Epoch 14.69: Loss = 0.477554
Epoch 14.70: Loss = 0.526352
Epoch 14.71: Loss = 0.454544
Epoch 14.72: Loss = 0.470291
Epoch 14.73: Loss = 0.459396
Epoch 14.74: Loss = 0.432022
Epoch 14.75: Loss = 0.523285
Epoch 14.76: Loss = 0.529648
Epoch 14.77: Loss = 0.566299
Epoch 14.78: Loss = 0.479385
Epoch 14.79: Loss = 0.524673
Epoch 14.80: Loss = 0.509995
Epoch 14.81: Loss = 0.479431
Epoch 14.82: Loss = 0.444534
Epoch 14.83: Loss = 0.441574
Epoch 14.84: Loss = 0.508209
Epoch 14.85: Loss = 0.418533
Epoch 14.86: Loss = 0.6194
Epoch 14.87: Loss = 0.441345
Epoch 14.88: Loss = 0.467819
Epoch 14.89: Loss = 0.594315
Epoch 14.90: Loss = 0.558136
Epoch 14.91: Loss = 0.561584
Epoch 14.92: Loss = 0.497559
Epoch 14.93: Loss = 0.489166
Epoch 14.94: Loss = 0.616913
Epoch 14.95: Loss = 0.619293
Epoch 14.96: Loss = 0.44931
Epoch 14.97: Loss = 0.473221
Epoch 14.98: Loss = 0.348892
Epoch 14.99: Loss = 0.427582
Epoch 14.100: Loss = 0.4048
TRAIN LOSS = 0.506424
TRAIN ACC = 87.9196 % (52754/60000)
Loss = 0.488937
Loss = 0.600784
Loss = 0.78714
Loss = 0.687592
Loss = 0.516006
Loss = 0.509918
Loss = 0.68869
Loss = 0.552155
Loss = 0.408234
Loss = 0.327637
Loss = 0.490662
Loss = 0.281113
Loss = 0.247513
Loss = 0.414169
Loss = 0.113419
Loss = 0.32753
Loss = 0.845734
TEST LOSS = 0.480319
TEST ACC = 527.539 % (8840/10000)
Epoch 15.1: Loss = 0.55452
Epoch 15.2: Loss = 0.52916
Epoch 15.3: Loss = 0.575851
Epoch 15.4: Loss = 0.606918
Epoch 15.5: Loss = 0.513779
Epoch 15.6: Loss = 0.558304
Epoch 15.7: Loss = 0.522781
Epoch 15.8: Loss = 0.613281
Epoch 15.9: Loss = 0.762054
Epoch 15.10: Loss = 0.45665
Epoch 15.11: Loss = 0.472687
Epoch 15.12: Loss = 0.507492
Epoch 15.13: Loss = 0.4151
Epoch 15.14: Loss = 0.478348
Epoch 15.15: Loss = 0.539932
Epoch 15.16: Loss = 0.586868
Epoch 15.17: Loss = 0.499237
Epoch 15.18: Loss = 0.509521
Epoch 15.19: Loss = 0.560272
Epoch 15.20: Loss = 0.587112
Epoch 15.21: Loss = 0.466507
Epoch 15.22: Loss = 0.457657
Epoch 15.23: Loss = 0.459824
Epoch 15.24: Loss = 0.395828
Epoch 15.25: Loss = 0.46785
Epoch 15.26: Loss = 0.489441
Epoch 15.27: Loss = 0.615997
Epoch 15.28: Loss = 0.40451
Epoch 15.29: Loss = 0.520554
Epoch 15.30: Loss = 0.521271
Epoch 15.31: Loss = 0.556152
Epoch 15.32: Loss = 0.439743
Epoch 15.33: Loss = 0.478607
Epoch 15.34: Loss = 0.530807
Epoch 15.35: Loss = 0.517136
Epoch 15.36: Loss = 0.4758
Epoch 15.37: Loss = 0.392929
Epoch 15.38: Loss = 0.605362
Epoch 15.39: Loss = 0.574051
Epoch 15.40: Loss = 0.482361
Epoch 15.41: Loss = 0.585419
Epoch 15.42: Loss = 0.585052
Epoch 15.43: Loss = 0.416504
Epoch 15.44: Loss = 0.532425
Epoch 15.45: Loss = 0.527771
Epoch 15.46: Loss = 0.466553
Epoch 15.47: Loss = 0.537186
Epoch 15.48: Loss = 0.486542
Epoch 15.49: Loss = 0.361679
Epoch 15.50: Loss = 0.396591
Epoch 15.51: Loss = 0.564041
Epoch 15.52: Loss = 0.489197
Epoch 15.53: Loss = 0.475876
Epoch 15.54: Loss = 0.391113
Epoch 15.55: Loss = 0.656784
Epoch 15.56: Loss = 0.445984
Epoch 15.57: Loss = 0.459961
Epoch 15.58: Loss = 0.667084
Epoch 15.59: Loss = 0.351639
Epoch 15.60: Loss = 0.490555
Epoch 15.61: Loss = 0.529144
Epoch 15.62: Loss = 0.506302
Epoch 15.63: Loss = 0.496918
Epoch 15.64: Loss = 0.505615
Epoch 15.65: Loss = 0.542252
Epoch 15.66: Loss = 0.579117
Epoch 15.67: Loss = 0.452454
Epoch 15.68: Loss = 0.593872
Epoch 15.69: Loss = 0.545776
Epoch 15.70: Loss = 0.550995
Epoch 15.71: Loss = 0.516388
Epoch 15.72: Loss = 0.424393
Epoch 15.73: Loss = 0.380112
Epoch 15.74: Loss = 0.525238
Epoch 15.75: Loss = 0.419052
Epoch 15.76: Loss = 0.586441
Epoch 15.77: Loss = 0.440903
Epoch 15.78: Loss = 0.537872
Epoch 15.79: Loss = 0.421402
Epoch 15.80: Loss = 0.400345
Epoch 15.81: Loss = 0.720245
Epoch 15.82: Loss = 0.383713
Epoch 15.83: Loss = 0.540421
Epoch 15.84: Loss = 0.631836
Epoch 15.85: Loss = 0.514069
Epoch 15.86: Loss = 0.486298
Epoch 15.87: Loss = 0.549896
Epoch 15.88: Loss = 0.499619
Epoch 15.89: Loss = 0.612961
Epoch 15.90: Loss = 0.58522
Epoch 15.91: Loss = 0.469711
Epoch 15.92: Loss = 0.423721
Epoch 15.93: Loss = 0.564026
Epoch 15.94: Loss = 0.714264
Epoch 15.95: Loss = 0.470337
Epoch 15.96: Loss = 0.512009
Epoch 15.97: Loss = 0.575836
Epoch 15.98: Loss = 0.489838
Epoch 15.99: Loss = 0.541611
Epoch 15.100: Loss = 0.493103
TRAIN LOSS = 0.513229
TRAIN ACC = 87.912 % (52750/60000)
Loss = 0.498016
Loss = 0.606415
Loss = 0.792618
Loss = 0.721344
Loss = 0.475616
Loss = 0.512436
Loss = 0.711258
Loss = 0.576004
Loss = 0.421127
Loss = 0.322952
Loss = 0.531357
Loss = 0.314926
Loss = 0.264313
Loss = 0.436844
Loss = 0.121536
Loss = 0.348404
Loss = 0.880112
TEST LOSS = 0.494514
TEST ACC = 527.499 % (8845/10000)
The following benchmarks are including preprocessing (offline phase).
Time = 59842.3 seconds 
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
