Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.32011
Epoch 1.2: Loss = 2.29245
Epoch 1.3: Loss = 2.26884
Epoch 1.4: Loss = 2.24124
Epoch 1.5: Loss = 2.22125
Epoch 1.6: Loss = 2.18184
Epoch 1.7: Loss = 2.17709
Epoch 1.8: Loss = 2.12888
Epoch 1.9: Loss = 2.08826
Epoch 1.10: Loss = 2.03931
Epoch 1.11: Loss = 1.99786
Epoch 1.12: Loss = 1.99916
Epoch 1.13: Loss = 1.91783
Epoch 1.14: Loss = 1.93494
Epoch 1.15: Loss = 1.99039
Epoch 1.16: Loss = 1.8837
Epoch 1.17: Loss = 1.85385
Epoch 1.18: Loss = 1.83969
Epoch 1.19: Loss = 1.80539
Epoch 1.20: Loss = 1.75278
Epoch 1.21: Loss = 1.68932
Epoch 1.22: Loss = 1.69771
Epoch 1.23: Loss = 1.62964
Epoch 1.24: Loss = 1.70097
Epoch 1.25: Loss = 1.62544
Epoch 1.26: Loss = 1.64325
Epoch 1.27: Loss = 1.56345
Epoch 1.28: Loss = 1.56664
Epoch 1.29: Loss = 1.56284
Epoch 1.30: Loss = 1.57379
Epoch 1.31: Loss = 1.48355
Epoch 1.32: Loss = 1.49217
Epoch 1.33: Loss = 1.40773
Epoch 1.34: Loss = 1.42569
Epoch 1.35: Loss = 1.34068
Epoch 1.36: Loss = 1.46037
Epoch 1.37: Loss = 1.32146
Epoch 1.38: Loss = 1.27396
Epoch 1.39: Loss = 1.26248
Epoch 1.40: Loss = 1.17644
Epoch 1.41: Loss = 1.23132
Epoch 1.42: Loss = 1.22707
Epoch 1.43: Loss = 1.16647
Epoch 1.44: Loss = 1.0737
Epoch 1.45: Loss = 1.21016
Epoch 1.46: Loss = 1.1481
Epoch 1.47: Loss = 1.06252
Epoch 1.48: Loss = 1.12169
Epoch 1.49: Loss = 1.06444
Epoch 1.50: Loss = 1.11578
Epoch 1.51: Loss = 0.972046
Epoch 1.52: Loss = 0.983643
Epoch 1.53: Loss = 1.00592
Epoch 1.54: Loss = 1.01768
Epoch 1.55: Loss = 0.996185
Epoch 1.56: Loss = 0.932922
Epoch 1.57: Loss = 0.870087
Epoch 1.58: Loss = 0.904709
Epoch 1.59: Loss = 0.924744
Epoch 1.60: Loss = 1.04704
Epoch 1.61: Loss = 0.948074
Epoch 1.62: Loss = 0.973801
Epoch 1.63: Loss = 0.991714
Epoch 1.64: Loss = 0.934662
Epoch 1.65: Loss = 1.0011
Epoch 1.66: Loss = 0.852509
Epoch 1.67: Loss = 0.868347
Epoch 1.68: Loss = 0.709137
Epoch 1.69: Loss = 0.808487
Epoch 1.70: Loss = 0.855865
Epoch 1.71: Loss = 0.787506
Epoch 1.72: Loss = 0.803879
Epoch 1.73: Loss = 0.815491
Epoch 1.74: Loss = 0.671478
Epoch 1.75: Loss = 0.835999
Epoch 1.76: Loss = 0.787903
Epoch 1.77: Loss = 0.738876
Epoch 1.78: Loss = 0.733948
Epoch 1.79: Loss = 0.711014
Epoch 1.80: Loss = 0.807785
Epoch 1.81: Loss = 0.673798
Epoch 1.82: Loss = 0.653976
Epoch 1.83: Loss = 0.797867
Epoch 1.84: Loss = 0.725098
Epoch 1.85: Loss = 0.825653
Epoch 1.86: Loss = 0.714172
Epoch 1.87: Loss = 0.64267
Epoch 1.88: Loss = 0.677826
Epoch 1.89: Loss = 0.740356
Epoch 1.90: Loss = 0.634888
Epoch 1.91: Loss = 0.718735
Epoch 1.92: Loss = 0.678909
Epoch 1.93: Loss = 0.711868
Epoch 1.94: Loss = 0.571838
Epoch 1.95: Loss = 0.687027
Epoch 1.96: Loss = 0.667114
Epoch 1.97: Loss = 0.510162
Epoch 1.98: Loss = 0.603806
Epoch 1.99: Loss = 0.677597
Epoch 1.100: Loss = 0.768829
Epoch 1.101: Loss = 0.716064
Epoch 1.102: Loss = 0.598297
Epoch 1.103: Loss = 0.57666
Epoch 1.104: Loss = 0.542664
Epoch 1.105: Loss = 0.651657
Epoch 1.106: Loss = 0.655991
Epoch 1.107: Loss = 0.531631
Epoch 1.108: Loss = 0.585129
Epoch 1.109: Loss = 0.526962
Epoch 1.110: Loss = 0.589844
Epoch 1.111: Loss = 0.502991
Epoch 1.112: Loss = 0.47287
Epoch 1.113: Loss = 0.53595
Epoch 1.114: Loss = 0.477509
Epoch 1.115: Loss = 0.554703
Epoch 1.116: Loss = 0.544418
Epoch 1.117: Loss = 0.44693
Epoch 1.118: Loss = 0.386795
Epoch 1.119: Loss = 0.412994
Epoch 1.120: Loss = 0.422165
TRAIN LOSS = 1.10803
TRAIN ACC = 69.7845 % (41873/60000)
Loss = 0.55867
Loss = 0.566925
Loss = 0.701904
Loss = 0.627182
Loss = 0.681152
Loss = 0.572067
Loss = 0.528015
Loss = 0.712021
Loss = 0.666031
Loss = 0.614105
Loss = 0.335251
Loss = 0.427887
Loss = 0.349518
Loss = 0.508438
Loss = 0.405731
Loss = 0.428421
Loss = 0.385391
Loss = 0.211853
Loss = 0.386002
Loss = 0.650345
TEST LOSS = 0.515845
TEST ACC = 418.729 % (8501/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.504211
Epoch 2.2: Loss = 0.638687
Epoch 2.3: Loss = 0.622925
Epoch 2.4: Loss = 0.488632
Epoch 2.5: Loss = 0.479462
Epoch 2.6: Loss = 0.500381
Epoch 2.7: Loss = 0.553314
Epoch 2.8: Loss = 0.505417
Epoch 2.9: Loss = 0.502563
Epoch 2.10: Loss = 0.49472
Epoch 2.11: Loss = 0.498062
Epoch 2.12: Loss = 0.495361
Epoch 2.13: Loss = 0.417084
Epoch 2.14: Loss = 0.465424
Epoch 2.15: Loss = 0.600815
Epoch 2.16: Loss = 0.557404
Epoch 2.17: Loss = 0.587173
Epoch 2.18: Loss = 0.644516
Epoch 2.19: Loss = 0.493408
Epoch 2.20: Loss = 0.439117
Epoch 2.21: Loss = 0.422501
Epoch 2.22: Loss = 0.437943
Epoch 2.23: Loss = 0.429337
Epoch 2.24: Loss = 0.670776
Epoch 2.25: Loss = 0.511093
Epoch 2.26: Loss = 0.587418
Epoch 2.27: Loss = 0.561356
Epoch 2.28: Loss = 0.544479
Epoch 2.29: Loss = 0.606186
Epoch 2.30: Loss = 0.626266
Epoch 2.31: Loss = 0.459808
Epoch 2.32: Loss = 0.575958
Epoch 2.33: Loss = 0.474915
Epoch 2.34: Loss = 0.521225
Epoch 2.35: Loss = 0.49501
Epoch 2.36: Loss = 0.592041
Epoch 2.37: Loss = 0.414291
Epoch 2.38: Loss = 0.421799
Epoch 2.39: Loss = 0.482071
Epoch 2.40: Loss = 0.406387
Epoch 2.41: Loss = 0.482193
Epoch 2.42: Loss = 0.596695
Epoch 2.43: Loss = 0.424194
Epoch 2.44: Loss = 0.37381
Epoch 2.45: Loss = 0.485184
Epoch 2.46: Loss = 0.516525
Epoch 2.47: Loss = 0.422211
Epoch 2.48: Loss = 0.498886
Epoch 2.49: Loss = 0.477371
Epoch 2.50: Loss = 0.562958
Epoch 2.51: Loss = 0.42099
Epoch 2.52: Loss = 0.399078
Epoch 2.53: Loss = 0.451797
Epoch 2.54: Loss = 0.527344
Epoch 2.55: Loss = 0.452423
Epoch 2.56: Loss = 0.427399
Epoch 2.57: Loss = 0.418228
Epoch 2.58: Loss = 0.475662
Epoch 2.59: Loss = 0.526794
Epoch 2.60: Loss = 0.586121
Epoch 2.61: Loss = 0.521423
Epoch 2.62: Loss = 0.54538
Epoch 2.63: Loss = 0.605591
Epoch 2.64: Loss = 0.520767
Epoch 2.65: Loss = 0.656372
Epoch 2.66: Loss = 0.45137
Epoch 2.67: Loss = 0.50354
Epoch 2.68: Loss = 0.305756
Epoch 2.69: Loss = 0.40715
Epoch 2.70: Loss = 0.550613
Epoch 2.71: Loss = 0.398163
Epoch 2.72: Loss = 0.415527
Epoch 2.73: Loss = 0.450714
Epoch 2.74: Loss = 0.34108
Epoch 2.75: Loss = 0.594696
Epoch 2.76: Loss = 0.487366
Epoch 2.77: Loss = 0.406769
Epoch 2.78: Loss = 0.451157
Epoch 2.79: Loss = 0.506882
Epoch 2.80: Loss = 0.513992
Epoch 2.81: Loss = 0.413925
Epoch 2.82: Loss = 0.360168
Epoch 2.83: Loss = 0.545624
Epoch 2.84: Loss = 0.448822
Epoch 2.85: Loss = 0.618256
Epoch 2.86: Loss = 0.499237
Epoch 2.87: Loss = 0.377899
Epoch 2.88: Loss = 0.426865
Epoch 2.89: Loss = 0.501328
Epoch 2.90: Loss = 0.382278
Epoch 2.91: Loss = 0.488922
Epoch 2.92: Loss = 0.483871
Epoch 2.93: Loss = 0.525848
Epoch 2.94: Loss = 0.358734
Epoch 2.95: Loss = 0.474365
Epoch 2.96: Loss = 0.509918
Epoch 2.97: Loss = 0.355087
Epoch 2.98: Loss = 0.406906
Epoch 2.99: Loss = 0.517654
Epoch 2.100: Loss = 0.608429
Epoch 2.101: Loss = 0.572586
Epoch 2.102: Loss = 0.439911
Epoch 2.103: Loss = 0.413345
Epoch 2.104: Loss = 0.355103
Epoch 2.105: Loss = 0.507324
Epoch 2.106: Loss = 0.52182
Epoch 2.107: Loss = 0.362091
Epoch 2.108: Loss = 0.453827
Epoch 2.109: Loss = 0.363892
Epoch 2.110: Loss = 0.453888
Epoch 2.111: Loss = 0.350159
Epoch 2.112: Loss = 0.3461
Epoch 2.113: Loss = 0.395523
Epoch 2.114: Loss = 0.348343
Epoch 2.115: Loss = 0.378983
Epoch 2.116: Loss = 0.40715
Epoch 2.117: Loss = 0.281052
Epoch 2.118: Loss = 0.234619
Epoch 2.119: Loss = 0.326172
Epoch 2.120: Loss = 0.324905
TRAIN LOSS = 0.475266
TRAIN ACC = 85.7651 % (51461/60000)
Loss = 0.420166
Loss = 0.471619
Loss = 0.580765
Loss = 0.526398
Loss = 0.583191
Loss = 0.442017
Loss = 0.397247
Loss = 0.605698
Loss = 0.555603
Loss = 0.505768
Loss = 0.227524
Loss = 0.333069
Loss = 0.304001
Loss = 0.396545
Loss = 0.268723
Loss = 0.333115
Loss = 0.259201
Loss = 0.101257
Loss = 0.263687
Loss = 0.531845
TEST LOSS = 0.405372
TEST ACC = 514.609 % (8796/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.395294
Epoch 3.2: Loss = 0.535385
Epoch 3.3: Loss = 0.509338
Epoch 3.4: Loss = 0.351364
Epoch 3.5: Loss = 0.374512
Epoch 3.6: Loss = 0.398239
Epoch 3.7: Loss = 0.397858
Epoch 3.8: Loss = 0.402237
Epoch 3.9: Loss = 0.39566
Epoch 3.10: Loss = 0.405273
Epoch 3.11: Loss = 0.419678
Epoch 3.12: Loss = 0.408279
Epoch 3.13: Loss = 0.318588
Epoch 3.14: Loss = 0.374634
Epoch 3.15: Loss = 0.463913
Epoch 3.16: Loss = 0.463638
Epoch 3.17: Loss = 0.510941
Epoch 3.18: Loss = 0.600052
Epoch 3.19: Loss = 0.427216
Epoch 3.20: Loss = 0.36171
Epoch 3.21: Loss = 0.341415
Epoch 3.22: Loss = 0.341888
Epoch 3.23: Loss = 0.350281
Epoch 3.24: Loss = 0.578751
Epoch 3.25: Loss = 0.416519
Epoch 3.26: Loss = 0.52005
Epoch 3.27: Loss = 0.508163
Epoch 3.28: Loss = 0.47467
Epoch 3.29: Loss = 0.539948
Epoch 3.30: Loss = 0.558182
Epoch 3.31: Loss = 0.371536
Epoch 3.32: Loss = 0.493225
Epoch 3.33: Loss = 0.383041
Epoch 3.34: Loss = 0.45752
Epoch 3.35: Loss = 0.414078
Epoch 3.36: Loss = 0.516968
Epoch 3.37: Loss = 0.32074
Epoch 3.38: Loss = 0.361588
Epoch 3.39: Loss = 0.401871
Epoch 3.40: Loss = 0.353897
Epoch 3.41: Loss = 0.404938
Epoch 3.42: Loss = 0.559799
Epoch 3.43: Loss = 0.340286
Epoch 3.44: Loss = 0.325577
Epoch 3.45: Loss = 0.41272
Epoch 3.46: Loss = 0.456726
Epoch 3.47: Loss = 0.373322
Epoch 3.48: Loss = 0.432465
Epoch 3.49: Loss = 0.419754
Epoch 3.50: Loss = 0.510468
Epoch 3.51: Loss = 0.36058
Epoch 3.52: Loss = 0.331192
Epoch 3.53: Loss = 0.392212
Epoch 3.54: Loss = 0.481903
Epoch 3.55: Loss = 0.393066
Epoch 3.56: Loss = 0.374435
Epoch 3.57: Loss = 0.382248
Epoch 3.58: Loss = 0.422882
Epoch 3.59: Loss = 0.488586
Epoch 3.60: Loss = 0.519089
Epoch 3.61: Loss = 0.453186
Epoch 3.62: Loss = 0.490616
Epoch 3.63: Loss = 0.57103
Epoch 3.64: Loss = 0.490952
Epoch 3.65: Loss = 0.614319
Epoch 3.66: Loss = 0.410416
Epoch 3.67: Loss = 0.450714
Epoch 3.68: Loss = 0.25058
Epoch 3.69: Loss = 0.356461
Epoch 3.70: Loss = 0.52858
Epoch 3.71: Loss = 0.351135
Epoch 3.72: Loss = 0.358749
Epoch 3.73: Loss = 0.392899
Epoch 3.74: Loss = 0.307663
Epoch 3.75: Loss = 0.585709
Epoch 3.76: Loss = 0.442383
Epoch 3.77: Loss = 0.350281
Epoch 3.78: Loss = 0.412384
Epoch 3.79: Loss = 0.486038
Epoch 3.80: Loss = 0.46347
Epoch 3.81: Loss = 0.360229
Epoch 3.82: Loss = 0.310593
Epoch 3.83: Loss = 0.483337
Epoch 3.84: Loss = 0.403473
Epoch 3.85: Loss = 0.582733
Epoch 3.86: Loss = 0.485382
Epoch 3.87: Loss = 0.320786
Epoch 3.88: Loss = 0.380463
Epoch 3.89: Loss = 0.472961
Epoch 3.90: Loss = 0.333588
Epoch 3.91: Loss = 0.463882
Epoch 3.92: Loss = 0.466995
Epoch 3.93: Loss = 0.498886
Epoch 3.94: Loss = 0.31395
Epoch 3.95: Loss = 0.422272
Epoch 3.96: Loss = 0.475861
Epoch 3.97: Loss = 0.328186
Epoch 3.98: Loss = 0.370438
Epoch 3.99: Loss = 0.477585
Epoch 3.100: Loss = 0.576828
Epoch 3.101: Loss = 0.555573
Epoch 3.102: Loss = 0.398773
Epoch 3.103: Loss = 0.3629
Epoch 3.104: Loss = 0.32515
Epoch 3.105: Loss = 0.495926
Epoch 3.106: Loss = 0.506149
Epoch 3.107: Loss = 0.323837
Epoch 3.108: Loss = 0.433411
Epoch 3.109: Loss = 0.338303
Epoch 3.110: Loss = 0.412933
Epoch 3.111: Loss = 0.326797
Epoch 3.112: Loss = 0.334732
Epoch 3.113: Loss = 0.370178
Epoch 3.114: Loss = 0.31308
Epoch 3.115: Loss = 0.326248
Epoch 3.116: Loss = 0.383102
Epoch 3.117: Loss = 0.23494
Epoch 3.118: Loss = 0.196945
Epoch 3.119: Loss = 0.301804
Epoch 3.120: Loss = 0.311859
TRAIN LOSS = 0.417908
TRAIN ACC = 87.7106 % (52628/60000)
Loss = 0.383102
Loss = 0.44313
Loss = 0.533905
Loss = 0.507889
Loss = 0.545654
Loss = 0.407944
Loss = 0.361679
Loss = 0.598999
Loss = 0.531265
Loss = 0.473877
Loss = 0.198181
Loss = 0.294052
Loss = 0.304474
Loss = 0.356873
Loss = 0.219269
Loss = 0.293961
Loss = 0.207596
Loss = 0.0732727
Loss = 0.230637
Loss = 0.502274
TEST LOSS = 0.373402
TEST ACC = 526.279 % (8932/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.354141
Epoch 4.2: Loss = 0.492599
Epoch 4.3: Loss = 0.476791
Epoch 4.4: Loss = 0.316986
Epoch 4.5: Loss = 0.337006
Epoch 4.6: Loss = 0.368637
Epoch 4.7: Loss = 0.344452
Epoch 4.8: Loss = 0.363983
Epoch 4.9: Loss = 0.357208
Epoch 4.10: Loss = 0.372665
Epoch 4.11: Loss = 0.395309
Epoch 4.12: Loss = 0.386429
Epoch 4.13: Loss = 0.284088
Epoch 4.14: Loss = 0.335403
Epoch 4.15: Loss = 0.417572
Epoch 4.16: Loss = 0.415237
Epoch 4.17: Loss = 0.466766
Epoch 4.18: Loss = 0.592209
Epoch 4.19: Loss = 0.411377
Epoch 4.20: Loss = 0.327744
Epoch 4.21: Loss = 0.32576
Epoch 4.22: Loss = 0.301346
Epoch 4.23: Loss = 0.314255
Epoch 4.24: Loss = 0.554779
Epoch 4.25: Loss = 0.397369
Epoch 4.26: Loss = 0.511993
Epoch 4.27: Loss = 0.483307
Epoch 4.28: Loss = 0.438614
Epoch 4.29: Loss = 0.517029
Epoch 4.30: Loss = 0.527054
Epoch 4.31: Loss = 0.344452
Epoch 4.32: Loss = 0.468094
Epoch 4.33: Loss = 0.358963
Epoch 4.34: Loss = 0.425323
Epoch 4.35: Loss = 0.388046
Epoch 4.36: Loss = 0.482925
Epoch 4.37: Loss = 0.279968
Epoch 4.38: Loss = 0.351334
Epoch 4.39: Loss = 0.385178
Epoch 4.40: Loss = 0.344101
Epoch 4.41: Loss = 0.366089
Epoch 4.42: Loss = 0.561066
Epoch 4.43: Loss = 0.305145
Epoch 4.44: Loss = 0.30661
Epoch 4.45: Loss = 0.391174
Epoch 4.46: Loss = 0.42421
Epoch 4.47: Loss = 0.363495
Epoch 4.48: Loss = 0.416031
Epoch 4.49: Loss = 0.399185
Epoch 4.50: Loss = 0.488052
Epoch 4.51: Loss = 0.346695
Epoch 4.52: Loss = 0.310242
Epoch 4.53: Loss = 0.37326
Epoch 4.54: Loss = 0.470398
Epoch 4.55: Loss = 0.369156
Epoch 4.56: Loss = 0.353607
Epoch 4.57: Loss = 0.375015
Epoch 4.58: Loss = 0.400848
Epoch 4.59: Loss = 0.470322
Epoch 4.60: Loss = 0.489731
Epoch 4.61: Loss = 0.416092
Epoch 4.62: Loss = 0.456512
Epoch 4.63: Loss = 0.553528
Epoch 4.64: Loss = 0.476685
Epoch 4.65: Loss = 0.596603
Epoch 4.66: Loss = 0.388443
Epoch 4.67: Loss = 0.408005
Epoch 4.68: Loss = 0.236404
Epoch 4.69: Loss = 0.338837
Epoch 4.70: Loss = 0.506088
Epoch 4.71: Loss = 0.331329
Epoch 4.72: Loss = 0.330261
Epoch 4.73: Loss = 0.365204
Epoch 4.74: Loss = 0.30159
Epoch 4.75: Loss = 0.601685
Epoch 4.76: Loss = 0.433319
Epoch 4.77: Loss = 0.31546
Epoch 4.78: Loss = 0.388092
Epoch 4.79: Loss = 0.487793
Epoch 4.80: Loss = 0.435913
Epoch 4.81: Loss = 0.343658
Epoch 4.82: Loss = 0.296661
Epoch 4.83: Loss = 0.455261
Epoch 4.84: Loss = 0.3853
Epoch 4.85: Loss = 0.574402
Epoch 4.86: Loss = 0.484116
Epoch 4.87: Loss = 0.30484
Epoch 4.88: Loss = 0.363022
Epoch 4.89: Loss = 0.460678
Epoch 4.90: Loss = 0.316681
Epoch 4.91: Loss = 0.443451
Epoch 4.92: Loss = 0.460999
Epoch 4.93: Loss = 0.487518
Epoch 4.94: Loss = 0.296402
Epoch 4.95: Loss = 0.409821
Epoch 4.96: Loss = 0.456528
Epoch 4.97: Loss = 0.329575
Epoch 4.98: Loss = 0.345047
Epoch 4.99: Loss = 0.454056
Epoch 4.100: Loss = 0.57724
Epoch 4.101: Loss = 0.55896
Epoch 4.102: Loss = 0.383148
Epoch 4.103: Loss = 0.336914
Epoch 4.104: Loss = 0.304901
Epoch 4.105: Loss = 0.481369
Epoch 4.106: Loss = 0.493011
Epoch 4.107: Loss = 0.304443
Epoch 4.108: Loss = 0.416931
Epoch 4.109: Loss = 0.326569
Epoch 4.110: Loss = 0.390823
Epoch 4.111: Loss = 0.313202
Epoch 4.112: Loss = 0.311401
Epoch 4.113: Loss = 0.359833
Epoch 4.114: Loss = 0.290771
Epoch 4.115: Loss = 0.294159
Epoch 4.116: Loss = 0.380981
Epoch 4.117: Loss = 0.211273
Epoch 4.118: Loss = 0.193466
Epoch 4.119: Loss = 0.283997
Epoch 4.120: Loss = 0.31131
TRAIN LOSS = 0.396301
TRAIN ACC = 88.8336 % (53302/60000)
Loss = 0.354355
Loss = 0.431335
Loss = 0.513718
Loss = 0.512558
Loss = 0.551254
Loss = 0.379639
Loss = 0.333298
Loss = 0.595627
Loss = 0.51091
Loss = 0.463089
Loss = 0.18869
Loss = 0.270645
Loss = 0.31813
Loss = 0.346466
Loss = 0.195328
Loss = 0.282318
Loss = 0.194626
Loss = 0.0646057
Loss = 0.218201
Loss = 0.489975
TEST LOSS = 0.360738
TEST ACC = 533.018 % (8976/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.343307
Epoch 5.2: Loss = 0.479141
Epoch 5.3: Loss = 0.470154
Epoch 5.4: Loss = 0.292328
Epoch 5.5: Loss = 0.323105
Epoch 5.6: Loss = 0.358475
Epoch 5.7: Loss = 0.32283
Epoch 5.8: Loss = 0.350388
Epoch 5.9: Loss = 0.344604
Epoch 5.10: Loss = 0.360764
Epoch 5.11: Loss = 0.385025
Epoch 5.12: Loss = 0.38063
Epoch 5.13: Loss = 0.265488
Epoch 5.14: Loss = 0.331757
Epoch 5.15: Loss = 0.393127
Epoch 5.16: Loss = 0.395752
Epoch 5.17: Loss = 0.45816
Epoch 5.18: Loss = 0.611359
Epoch 5.19: Loss = 0.409134
Epoch 5.20: Loss = 0.324005
Epoch 5.21: Loss = 0.313568
Epoch 5.22: Loss = 0.284714
Epoch 5.23: Loss = 0.301453
Epoch 5.24: Loss = 0.546844
Epoch 5.25: Loss = 0.39621
Epoch 5.26: Loss = 0.516235
Epoch 5.27: Loss = 0.474518
Epoch 5.28: Loss = 0.435425
Epoch 5.29: Loss = 0.505798
Epoch 5.30: Loss = 0.519867
Epoch 5.31: Loss = 0.331848
Epoch 5.32: Loss = 0.449173
Epoch 5.33: Loss = 0.330246
Epoch 5.34: Loss = 0.407379
Epoch 5.35: Loss = 0.375092
Epoch 5.36: Loss = 0.469543
Epoch 5.37: Loss = 0.264374
Epoch 5.38: Loss = 0.335983
Epoch 5.39: Loss = 0.370819
Epoch 5.40: Loss = 0.327881
Epoch 5.41: Loss = 0.352249
Epoch 5.42: Loss = 0.570709
Epoch 5.43: Loss = 0.302673
Epoch 5.44: Loss = 0.304276
Epoch 5.45: Loss = 0.372986
Epoch 5.46: Loss = 0.400635
Epoch 5.47: Loss = 0.353867
Epoch 5.48: Loss = 0.4021
Epoch 5.49: Loss = 0.382599
Epoch 5.50: Loss = 0.482559
Epoch 5.51: Loss = 0.329208
Epoch 5.52: Loss = 0.291245
Epoch 5.53: Loss = 0.364166
Epoch 5.54: Loss = 0.469452
Epoch 5.55: Loss = 0.37233
Epoch 5.56: Loss = 0.347839
Epoch 5.57: Loss = 0.366547
Epoch 5.58: Loss = 0.380234
Epoch 5.59: Loss = 0.458374
Epoch 5.60: Loss = 0.476517
Epoch 5.61: Loss = 0.407364
Epoch 5.62: Loss = 0.447128
Epoch 5.63: Loss = 0.551697
Epoch 5.64: Loss = 0.4823
Epoch 5.65: Loss = 0.582611
Epoch 5.66: Loss = 0.386154
Epoch 5.67: Loss = 0.394135
Epoch 5.68: Loss = 0.234726
Epoch 5.69: Loss = 0.322083
Epoch 5.70: Loss = 0.488342
Epoch 5.71: Loss = 0.327774
Epoch 5.72: Loss = 0.310501
Epoch 5.73: Loss = 0.368164
Epoch 5.74: Loss = 0.2948
Epoch 5.75: Loss = 0.622437
Epoch 5.76: Loss = 0.414749
Epoch 5.77: Loss = 0.308212
Epoch 5.78: Loss = 0.385162
Epoch 5.79: Loss = 0.478363
Epoch 5.80: Loss = 0.426804
Epoch 5.81: Loss = 0.334183
Epoch 5.82: Loss = 0.289322
Epoch 5.83: Loss = 0.445374
Epoch 5.84: Loss = 0.375168
Epoch 5.85: Loss = 0.550171
Epoch 5.86: Loss = 0.48381
Epoch 5.87: Loss = 0.291199
Epoch 5.88: Loss = 0.350983
Epoch 5.89: Loss = 0.456635
Epoch 5.90: Loss = 0.311371
Epoch 5.91: Loss = 0.428833
Epoch 5.92: Loss = 0.463013
Epoch 5.93: Loss = 0.491852
Epoch 5.94: Loss = 0.285675
Epoch 5.95: Loss = 0.400711
Epoch 5.96: Loss = 0.446732
Epoch 5.97: Loss = 0.324142
Epoch 5.98: Loss = 0.324966
Epoch 5.99: Loss = 0.445999
Epoch 5.100: Loss = 0.575775
Epoch 5.101: Loss = 0.556442
Epoch 5.102: Loss = 0.369522
Epoch 5.103: Loss = 0.330887
Epoch 5.104: Loss = 0.300079
Epoch 5.105: Loss = 0.481064
Epoch 5.106: Loss = 0.502975
Epoch 5.107: Loss = 0.306824
Epoch 5.108: Loss = 0.415985
Epoch 5.109: Loss = 0.323334
Epoch 5.110: Loss = 0.393143
Epoch 5.111: Loss = 0.314163
Epoch 5.112: Loss = 0.310791
Epoch 5.113: Loss = 0.352249
Epoch 5.114: Loss = 0.269623
Epoch 5.115: Loss = 0.268326
Epoch 5.116: Loss = 0.372696
Epoch 5.117: Loss = 0.205933
Epoch 5.118: Loss = 0.176163
Epoch 5.119: Loss = 0.285934
Epoch 5.120: Loss = 0.319199
TRAIN LOSS = 0.387512
TRAIN ACC = 89.2609 % (53559/60000)
Loss = 0.341553
Loss = 0.445541
Loss = 0.52681
Loss = 0.513596
Loss = 0.543304
Loss = 0.38443
Loss = 0.321899
Loss = 0.611496
Loss = 0.508713
Loss = 0.449814
Loss = 0.173553
Loss = 0.285721
Loss = 0.301575
Loss = 0.322159
Loss = 0.181061
Loss = 0.271149
Loss = 0.180359
Loss = 0.0569763
Loss = 0.212738
Loss = 0.499161
TEST LOSS = 0.35658
TEST ACC = 535.59 % (9001/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.336838
Epoch 6.2: Loss = 0.466278
Epoch 6.3: Loss = 0.450684
Epoch 6.4: Loss = 0.279831
Epoch 6.5: Loss = 0.315094
Epoch 6.6: Loss = 0.341904
Epoch 6.7: Loss = 0.303314
Epoch 6.8: Loss = 0.360413
Epoch 6.9: Loss = 0.347763
Epoch 6.10: Loss = 0.347778
Epoch 6.11: Loss = 0.384933
Epoch 6.12: Loss = 0.368881
Epoch 6.13: Loss = 0.25206
Epoch 6.14: Loss = 0.321198
Epoch 6.15: Loss = 0.381775
Epoch 6.16: Loss = 0.386185
Epoch 6.17: Loss = 0.454102
Epoch 6.18: Loss = 0.619492
Epoch 6.19: Loss = 0.409317
Epoch 6.20: Loss = 0.293625
Epoch 6.21: Loss = 0.31131
Epoch 6.22: Loss = 0.269989
Epoch 6.23: Loss = 0.288864
Epoch 6.24: Loss = 0.529419
Epoch 6.25: Loss = 0.380463
Epoch 6.26: Loss = 0.51828
Epoch 6.27: Loss = 0.466156
Epoch 6.28: Loss = 0.434982
Epoch 6.29: Loss = 0.487854
Epoch 6.30: Loss = 0.516815
Epoch 6.31: Loss = 0.319931
Epoch 6.32: Loss = 0.426804
Epoch 6.33: Loss = 0.322922
Epoch 6.34: Loss = 0.394501
Epoch 6.35: Loss = 0.370422
Epoch 6.36: Loss = 0.449005
Epoch 6.37: Loss = 0.255127
Epoch 6.38: Loss = 0.333435
Epoch 6.39: Loss = 0.33786
Epoch 6.40: Loss = 0.328552
Epoch 6.41: Loss = 0.349487
Epoch 6.42: Loss = 0.567719
Epoch 6.43: Loss = 0.295914
Epoch 6.44: Loss = 0.297318
Epoch 6.45: Loss = 0.35788
Epoch 6.46: Loss = 0.390366
Epoch 6.47: Loss = 0.341293
Epoch 6.48: Loss = 0.408829
Epoch 6.49: Loss = 0.362869
Epoch 6.50: Loss = 0.469147
Epoch 6.51: Loss = 0.313248
Epoch 6.52: Loss = 0.286621
Epoch 6.53: Loss = 0.355087
Epoch 6.54: Loss = 0.479019
Epoch 6.55: Loss = 0.364914
Epoch 6.56: Loss = 0.340927
Epoch 6.57: Loss = 0.362808
Epoch 6.58: Loss = 0.359009
Epoch 6.59: Loss = 0.455551
Epoch 6.60: Loss = 0.472412
Epoch 6.61: Loss = 0.377029
Epoch 6.62: Loss = 0.441223
Epoch 6.63: Loss = 0.546722
Epoch 6.64: Loss = 0.495499
Epoch 6.65: Loss = 0.572784
Epoch 6.66: Loss = 0.377167
Epoch 6.67: Loss = 0.383774
Epoch 6.68: Loss = 0.232986
Epoch 6.69: Loss = 0.304642
Epoch 6.70: Loss = 0.483276
Epoch 6.71: Loss = 0.319992
Epoch 6.72: Loss = 0.294846
Epoch 6.73: Loss = 0.355072
Epoch 6.74: Loss = 0.287811
Epoch 6.75: Loss = 0.62085
Epoch 6.76: Loss = 0.419891
Epoch 6.77: Loss = 0.293777
Epoch 6.78: Loss = 0.383041
Epoch 6.79: Loss = 0.473755
Epoch 6.80: Loss = 0.407593
Epoch 6.81: Loss = 0.318893
Epoch 6.82: Loss = 0.277924
Epoch 6.83: Loss = 0.427795
Epoch 6.84: Loss = 0.358871
Epoch 6.85: Loss = 0.537918
Epoch 6.86: Loss = 0.475693
Epoch 6.87: Loss = 0.266968
Epoch 6.88: Loss = 0.34906
Epoch 6.89: Loss = 0.445663
Epoch 6.90: Loss = 0.312302
Epoch 6.91: Loss = 0.42868
Epoch 6.92: Loss = 0.450119
Epoch 6.93: Loss = 0.495621
Epoch 6.94: Loss = 0.271393
Epoch 6.95: Loss = 0.395493
Epoch 6.96: Loss = 0.452194
Epoch 6.97: Loss = 0.311462
Epoch 6.98: Loss = 0.316666
Epoch 6.99: Loss = 0.434891
Epoch 6.100: Loss = 0.563995
Epoch 6.101: Loss = 0.554199
Epoch 6.102: Loss = 0.368988
Epoch 6.103: Loss = 0.3116
Epoch 6.104: Loss = 0.301514
Epoch 6.105: Loss = 0.474442
Epoch 6.106: Loss = 0.499359
Epoch 6.107: Loss = 0.294006
Epoch 6.108: Loss = 0.405151
Epoch 6.109: Loss = 0.310867
Epoch 6.110: Loss = 0.38266
Epoch 6.111: Loss = 0.313278
Epoch 6.112: Loss = 0.288788
Epoch 6.113: Loss = 0.343674
Epoch 6.114: Loss = 0.258301
Epoch 6.115: Loss = 0.244705
Epoch 6.116: Loss = 0.348679
Epoch 6.117: Loss = 0.192673
Epoch 6.118: Loss = 0.165451
Epoch 6.119: Loss = 0.292038
Epoch 6.120: Loss = 0.31842
TRAIN LOSS = 0.378555
TRAIN ACC = 89.7461 % (53850/60000)
Loss = 0.337845
Loss = 0.437454
Loss = 0.502426
Loss = 0.503387
Loss = 0.531815
Loss = 0.372025
Loss = 0.307846
Loss = 0.597275
Loss = 0.501495
Loss = 0.430832
Loss = 0.175003
Loss = 0.270142
Loss = 0.316467
Loss = 0.313644
Loss = 0.172928
Loss = 0.280197
Loss = 0.17569
Loss = 0.054184
Loss = 0.205902
Loss = 0.501556
TEST LOSS = 0.349406
TEST ACC = 538.499 % (9068/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.325836
Epoch 7.2: Loss = 0.452515
Epoch 7.3: Loss = 0.451614
Epoch 7.4: Loss = 0.282547
Epoch 7.5: Loss = 0.29158
Epoch 7.6: Loss = 0.34082
Epoch 7.7: Loss = 0.293869
Epoch 7.8: Loss = 0.345016
Epoch 7.9: Loss = 0.334427
Epoch 7.10: Loss = 0.335251
Epoch 7.11: Loss = 0.388794
Epoch 7.12: Loss = 0.359375
Epoch 7.13: Loss = 0.242386
Epoch 7.14: Loss = 0.313141
Epoch 7.15: Loss = 0.363632
Epoch 7.16: Loss = 0.376099
Epoch 7.17: Loss = 0.456284
Epoch 7.18: Loss = 0.599899
Epoch 7.19: Loss = 0.411224
Epoch 7.20: Loss = 0.277817
Epoch 7.21: Loss = 0.307663
Epoch 7.22: Loss = 0.269424
Epoch 7.23: Loss = 0.299667
Epoch 7.24: Loss = 0.539108
Epoch 7.25: Loss = 0.370132
Epoch 7.26: Loss = 0.513992
Epoch 7.27: Loss = 0.475815
Epoch 7.28: Loss = 0.436539
Epoch 7.29: Loss = 0.495529
Epoch 7.30: Loss = 0.528992
Epoch 7.31: Loss = 0.312531
Epoch 7.32: Loss = 0.420288
Epoch 7.33: Loss = 0.310974
Epoch 7.34: Loss = 0.369873
Epoch 7.35: Loss = 0.355621
Epoch 7.36: Loss = 0.4319
Epoch 7.37: Loss = 0.238251
Epoch 7.38: Loss = 0.329422
Epoch 7.39: Loss = 0.328354
Epoch 7.40: Loss = 0.328857
Epoch 7.41: Loss = 0.340225
Epoch 7.42: Loss = 0.571091
Epoch 7.43: Loss = 0.297745
Epoch 7.44: Loss = 0.270203
Epoch 7.45: Loss = 0.334946
Epoch 7.46: Loss = 0.385681
Epoch 7.47: Loss = 0.339157
Epoch 7.48: Loss = 0.390106
Epoch 7.49: Loss = 0.352966
Epoch 7.50: Loss = 0.45607
Epoch 7.51: Loss = 0.29483
Epoch 7.52: Loss = 0.288406
Epoch 7.53: Loss = 0.344086
Epoch 7.54: Loss = 0.46582
Epoch 7.55: Loss = 0.371368
Epoch 7.56: Loss = 0.331955
Epoch 7.57: Loss = 0.348755
Epoch 7.58: Loss = 0.343292
Epoch 7.59: Loss = 0.448639
Epoch 7.60: Loss = 0.456726
Epoch 7.61: Loss = 0.352539
Epoch 7.62: Loss = 0.419159
Epoch 7.63: Loss = 0.531906
Epoch 7.64: Loss = 0.46376
Epoch 7.65: Loss = 0.56749
Epoch 7.66: Loss = 0.365753
Epoch 7.67: Loss = 0.365082
Epoch 7.68: Loss = 0.227341
Epoch 7.69: Loss = 0.289719
Epoch 7.70: Loss = 0.478195
Epoch 7.71: Loss = 0.308914
Epoch 7.72: Loss = 0.274292
Epoch 7.73: Loss = 0.339767
Epoch 7.74: Loss = 0.278351
Epoch 7.75: Loss = 0.632965
Epoch 7.76: Loss = 0.404282
Epoch 7.77: Loss = 0.277008
Epoch 7.78: Loss = 0.368668
Epoch 7.79: Loss = 0.472565
Epoch 7.80: Loss = 0.384918
Epoch 7.81: Loss = 0.314377
Epoch 7.82: Loss = 0.277802
Epoch 7.83: Loss = 0.415131
Epoch 7.84: Loss = 0.358597
Epoch 7.85: Loss = 0.517334
Epoch 7.86: Loss = 0.460907
Epoch 7.87: Loss = 0.263611
Epoch 7.88: Loss = 0.338211
Epoch 7.89: Loss = 0.43869
Epoch 7.90: Loss = 0.301712
Epoch 7.91: Loss = 0.425064
Epoch 7.92: Loss = 0.444794
Epoch 7.93: Loss = 0.497818
Epoch 7.94: Loss = 0.254837
Epoch 7.95: Loss = 0.384506
Epoch 7.96: Loss = 0.436096
Epoch 7.97: Loss = 0.292542
Epoch 7.98: Loss = 0.314896
Epoch 7.99: Loss = 0.423386
Epoch 7.100: Loss = 0.558685
Epoch 7.101: Loss = 0.555054
Epoch 7.102: Loss = 0.348038
Epoch 7.103: Loss = 0.305603
Epoch 7.104: Loss = 0.295502
Epoch 7.105: Loss = 0.468048
Epoch 7.106: Loss = 0.500305
Epoch 7.107: Loss = 0.288834
Epoch 7.108: Loss = 0.40451
Epoch 7.109: Loss = 0.314209
Epoch 7.110: Loss = 0.371246
Epoch 7.111: Loss = 0.310577
Epoch 7.112: Loss = 0.279175
Epoch 7.113: Loss = 0.341125
Epoch 7.114: Loss = 0.252502
Epoch 7.115: Loss = 0.246231
Epoch 7.116: Loss = 0.34613
Epoch 7.117: Loss = 0.19516
Epoch 7.118: Loss = 0.170151
Epoch 7.119: Loss = 0.276199
Epoch 7.120: Loss = 0.328186
TRAIN LOSS = 0.370468
TRAIN ACC = 90.1398 % (54086/60000)
Loss = 0.323624
Loss = 0.428802
Loss = 0.497269
Loss = 0.493835
Loss = 0.518661
Loss = 0.375305
Loss = 0.313858
Loss = 0.587372
Loss = 0.495178
Loss = 0.423874
Loss = 0.175705
Loss = 0.276291
Loss = 0.302429
Loss = 0.305832
Loss = 0.17749
Loss = 0.26416
Loss = 0.167374
Loss = 0.0510254
Loss = 0.204208
Loss = 0.515213
TEST LOSS = 0.344875
TEST ACC = 540.858 % (9067/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.312561
Epoch 8.2: Loss = 0.461853
Epoch 8.3: Loss = 0.446381
Epoch 8.4: Loss = 0.275467
Epoch 8.5: Loss = 0.29454
Epoch 8.6: Loss = 0.331741
Epoch 8.7: Loss = 0.279114
Epoch 8.8: Loss = 0.336609
Epoch 8.9: Loss = 0.330658
Epoch 8.10: Loss = 0.344864
Epoch 8.11: Loss = 0.373901
Epoch 8.12: Loss = 0.355652
Epoch 8.13: Loss = 0.235535
Epoch 8.14: Loss = 0.312012
Epoch 8.15: Loss = 0.360214
Epoch 8.16: Loss = 0.373596
Epoch 8.17: Loss = 0.452591
Epoch 8.18: Loss = 0.5979
Epoch 8.19: Loss = 0.415985
Epoch 8.20: Loss = 0.277435
Epoch 8.21: Loss = 0.296005
Epoch 8.22: Loss = 0.263184
Epoch 8.23: Loss = 0.287552
Epoch 8.24: Loss = 0.565659
Epoch 8.25: Loss = 0.379456
Epoch 8.26: Loss = 0.514771
Epoch 8.27: Loss = 0.458725
Epoch 8.28: Loss = 0.434357
Epoch 8.29: Loss = 0.484497
Epoch 8.30: Loss = 0.523376
Epoch 8.31: Loss = 0.321457
Epoch 8.32: Loss = 0.407166
Epoch 8.33: Loss = 0.2966
Epoch 8.34: Loss = 0.371674
Epoch 8.35: Loss = 0.347382
Epoch 8.36: Loss = 0.420822
Epoch 8.37: Loss = 0.234894
Epoch 8.38: Loss = 0.322906
Epoch 8.39: Loss = 0.320435
Epoch 8.40: Loss = 0.328918
Epoch 8.41: Loss = 0.33046
Epoch 8.42: Loss = 0.575745
Epoch 8.43: Loss = 0.285019
Epoch 8.44: Loss = 0.264664
Epoch 8.45: Loss = 0.314224
Epoch 8.46: Loss = 0.37822
Epoch 8.47: Loss = 0.325119
Epoch 8.48: Loss = 0.369156
Epoch 8.49: Loss = 0.344574
Epoch 8.50: Loss = 0.465729
Epoch 8.51: Loss = 0.290161
Epoch 8.52: Loss = 0.29216
Epoch 8.53: Loss = 0.34111
Epoch 8.54: Loss = 0.471939
Epoch 8.55: Loss = 0.366074
Epoch 8.56: Loss = 0.314117
Epoch 8.57: Loss = 0.355606
Epoch 8.58: Loss = 0.339096
Epoch 8.59: Loss = 0.433212
Epoch 8.60: Loss = 0.450439
Epoch 8.61: Loss = 0.353165
Epoch 8.62: Loss = 0.41153
Epoch 8.63: Loss = 0.535507
Epoch 8.64: Loss = 0.449463
Epoch 8.65: Loss = 0.555405
Epoch 8.66: Loss = 0.360886
Epoch 8.67: Loss = 0.368042
Epoch 8.68: Loss = 0.225296
Epoch 8.69: Loss = 0.27655
Epoch 8.70: Loss = 0.456238
Epoch 8.71: Loss = 0.306793
Epoch 8.72: Loss = 0.2668
Epoch 8.73: Loss = 0.335236
Epoch 8.74: Loss = 0.276779
Epoch 8.75: Loss = 0.624313
Epoch 8.76: Loss = 0.391953
Epoch 8.77: Loss = 0.273285
Epoch 8.78: Loss = 0.366592
Epoch 8.79: Loss = 0.452469
Epoch 8.80: Loss = 0.378311
Epoch 8.81: Loss = 0.312943
Epoch 8.82: Loss = 0.272751
Epoch 8.83: Loss = 0.431122
Epoch 8.84: Loss = 0.358612
Epoch 8.85: Loss = 0.507111
Epoch 8.86: Loss = 0.455978
Epoch 8.87: Loss = 0.257446
Epoch 8.88: Loss = 0.328918
Epoch 8.89: Loss = 0.420792
Epoch 8.90: Loss = 0.298965
Epoch 8.91: Loss = 0.423279
Epoch 8.92: Loss = 0.455307
Epoch 8.93: Loss = 0.489334
Epoch 8.94: Loss = 0.257385
Epoch 8.95: Loss = 0.381424
Epoch 8.96: Loss = 0.436417
Epoch 8.97: Loss = 0.291092
Epoch 8.98: Loss = 0.307541
Epoch 8.99: Loss = 0.411179
Epoch 8.100: Loss = 0.54715
Epoch 8.101: Loss = 0.556046
Epoch 8.102: Loss = 0.348053
Epoch 8.103: Loss = 0.314255
Epoch 8.104: Loss = 0.300415
Epoch 8.105: Loss = 0.46489
Epoch 8.106: Loss = 0.501221
Epoch 8.107: Loss = 0.279007
Epoch 8.108: Loss = 0.408737
Epoch 8.109: Loss = 0.305695
Epoch 8.110: Loss = 0.371964
Epoch 8.111: Loss = 0.303909
Epoch 8.112: Loss = 0.287018
Epoch 8.113: Loss = 0.343506
Epoch 8.114: Loss = 0.24353
Epoch 8.115: Loss = 0.244934
Epoch 8.116: Loss = 0.327667
Epoch 8.117: Loss = 0.186661
Epoch 8.118: Loss = 0.171936
Epoch 8.119: Loss = 0.278397
Epoch 8.120: Loss = 0.345398
TRAIN LOSS = 0.366241
TRAIN ACC = 90.3595 % (54218/60000)
Loss = 0.310394
Loss = 0.436554
Loss = 0.496307
Loss = 0.492111
Loss = 0.531845
Loss = 0.368454
Loss = 0.305649
Loss = 0.606125
Loss = 0.486008
Loss = 0.442596
Loss = 0.181396
Loss = 0.293091
Loss = 0.317871
Loss = 0.298172
Loss = 0.164627
Loss = 0.2612
Loss = 0.173569
Loss = 0.0486145
Loss = 0.196533
Loss = 0.502991
TEST LOSS = 0.345705
TEST ACC = 542.178 % (9088/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.316177
Epoch 9.2: Loss = 0.462204
Epoch 9.3: Loss = 0.44751
Epoch 9.4: Loss = 0.275391
Epoch 9.5: Loss = 0.2995
Epoch 9.6: Loss = 0.352127
Epoch 9.7: Loss = 0.274536
Epoch 9.8: Loss = 0.338745
Epoch 9.9: Loss = 0.327774
Epoch 9.10: Loss = 0.359283
Epoch 9.11: Loss = 0.400253
Epoch 9.12: Loss = 0.355225
Epoch 9.13: Loss = 0.234085
Epoch 9.14: Loss = 0.301956
Epoch 9.15: Loss = 0.351685
Epoch 9.16: Loss = 0.381058
Epoch 9.17: Loss = 0.455811
Epoch 9.18: Loss = 0.585464
Epoch 9.19: Loss = 0.405731
Epoch 9.20: Loss = 0.2724
Epoch 9.21: Loss = 0.294662
Epoch 9.22: Loss = 0.267227
Epoch 9.23: Loss = 0.284683
Epoch 9.24: Loss = 0.565506
Epoch 9.25: Loss = 0.368668
Epoch 9.26: Loss = 0.504822
Epoch 9.27: Loss = 0.471313
Epoch 9.28: Loss = 0.4319
Epoch 9.29: Loss = 0.484192
Epoch 9.30: Loss = 0.509552
Epoch 9.31: Loss = 0.312408
Epoch 9.32: Loss = 0.409882
Epoch 9.33: Loss = 0.302246
Epoch 9.34: Loss = 0.377487
Epoch 9.35: Loss = 0.343857
Epoch 9.36: Loss = 0.419022
Epoch 9.37: Loss = 0.24292
Epoch 9.38: Loss = 0.324768
Epoch 9.39: Loss = 0.323059
Epoch 9.40: Loss = 0.322556
Epoch 9.41: Loss = 0.319199
Epoch 9.42: Loss = 0.588242
Epoch 9.43: Loss = 0.286118
Epoch 9.44: Loss = 0.264572
Epoch 9.45: Loss = 0.322098
Epoch 9.46: Loss = 0.371994
Epoch 9.47: Loss = 0.329224
Epoch 9.48: Loss = 0.358582
Epoch 9.49: Loss = 0.33316
Epoch 9.50: Loss = 0.458282
Epoch 9.51: Loss = 0.27449
Epoch 9.52: Loss = 0.281891
Epoch 9.53: Loss = 0.354645
Epoch 9.54: Loss = 0.46257
Epoch 9.55: Loss = 0.383698
Epoch 9.56: Loss = 0.31958
Epoch 9.57: Loss = 0.359726
Epoch 9.58: Loss = 0.339386
Epoch 9.59: Loss = 0.433167
Epoch 9.60: Loss = 0.438568
Epoch 9.61: Loss = 0.348877
Epoch 9.62: Loss = 0.422592
Epoch 9.63: Loss = 0.532516
Epoch 9.64: Loss = 0.460114
Epoch 9.65: Loss = 0.542252
Epoch 9.66: Loss = 0.355347
Epoch 9.67: Loss = 0.363861
Epoch 9.68: Loss = 0.218735
Epoch 9.69: Loss = 0.277786
Epoch 9.70: Loss = 0.452911
Epoch 9.71: Loss = 0.315735
Epoch 9.72: Loss = 0.260437
Epoch 9.73: Loss = 0.326706
Epoch 9.74: Loss = 0.275146
Epoch 9.75: Loss = 0.602066
Epoch 9.76: Loss = 0.403244
Epoch 9.77: Loss = 0.270233
Epoch 9.78: Loss = 0.378311
Epoch 9.79: Loss = 0.473755
Epoch 9.80: Loss = 0.381668
Epoch 9.81: Loss = 0.318787
Epoch 9.82: Loss = 0.273483
Epoch 9.83: Loss = 0.423508
Epoch 9.84: Loss = 0.364059
Epoch 9.85: Loss = 0.517136
Epoch 9.86: Loss = 0.452225
Epoch 9.87: Loss = 0.255096
Epoch 9.88: Loss = 0.359268
Epoch 9.89: Loss = 0.4142
Epoch 9.90: Loss = 0.292847
Epoch 9.91: Loss = 0.417953
Epoch 9.92: Loss = 0.444031
Epoch 9.93: Loss = 0.508041
Epoch 9.94: Loss = 0.249313
Epoch 9.95: Loss = 0.367294
Epoch 9.96: Loss = 0.422577
Epoch 9.97: Loss = 0.301163
Epoch 9.98: Loss = 0.309128
Epoch 9.99: Loss = 0.413086
Epoch 9.100: Loss = 0.526627
Epoch 9.101: Loss = 0.578064
Epoch 9.102: Loss = 0.348145
Epoch 9.103: Loss = 0.299072
Epoch 9.104: Loss = 0.294662
Epoch 9.105: Loss = 0.464111
Epoch 9.106: Loss = 0.50914
Epoch 9.107: Loss = 0.275543
Epoch 9.108: Loss = 0.402344
Epoch 9.109: Loss = 0.314224
Epoch 9.110: Loss = 0.378738
Epoch 9.111: Loss = 0.309799
Epoch 9.112: Loss = 0.281174
Epoch 9.113: Loss = 0.352631
Epoch 9.114: Loss = 0.236816
Epoch 9.115: Loss = 0.252579
Epoch 9.116: Loss = 0.332993
Epoch 9.117: Loss = 0.186996
Epoch 9.118: Loss = 0.18457
Epoch 9.119: Loss = 0.282684
Epoch 9.120: Loss = 0.351089
TRAIN LOSS = 0.366379
TRAIN ACC = 90.5396 % (54326/60000)
Loss = 0.313217
Loss = 0.434921
Loss = 0.504379
Loss = 0.501144
Loss = 0.544128
Loss = 0.360565
Loss = 0.306366
Loss = 0.614563
Loss = 0.491333
Loss = 0.437195
Loss = 0.17218
Loss = 0.290588
Loss = 0.346237
Loss = 0.288177
Loss = 0.15921
Loss = 0.242386
Loss = 0.169861
Loss = 0.0500183
Loss = 0.199875
Loss = 0.496552
TEST LOSS = 0.346145
TEST ACC = 543.259 % (9099/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.322571
Epoch 10.2: Loss = 0.465836
Epoch 10.3: Loss = 0.461243
Epoch 10.4: Loss = 0.266052
Epoch 10.5: Loss = 0.306168
Epoch 10.6: Loss = 0.361191
Epoch 10.7: Loss = 0.282654
Epoch 10.8: Loss = 0.338409
Epoch 10.9: Loss = 0.342255
Epoch 10.10: Loss = 0.366669
Epoch 10.11: Loss = 0.405273
Epoch 10.12: Loss = 0.353134
Epoch 10.13: Loss = 0.232178
Epoch 10.14: Loss = 0.296814
Epoch 10.15: Loss = 0.34874
Epoch 10.16: Loss = 0.361496
Epoch 10.17: Loss = 0.451569
Epoch 10.18: Loss = 0.590591
Epoch 10.19: Loss = 0.425583
Epoch 10.20: Loss = 0.268936
Epoch 10.21: Loss = 0.301804
Epoch 10.22: Loss = 0.264053
Epoch 10.23: Loss = 0.278198
Epoch 10.24: Loss = 0.561371
Epoch 10.25: Loss = 0.378433
Epoch 10.26: Loss = 0.508087
Epoch 10.27: Loss = 0.46579
Epoch 10.28: Loss = 0.42218
Epoch 10.29: Loss = 0.471069
Epoch 10.30: Loss = 0.504913
Epoch 10.31: Loss = 0.31723
Epoch 10.32: Loss = 0.399002
Epoch 10.33: Loss = 0.309845
Epoch 10.34: Loss = 0.374451
Epoch 10.35: Loss = 0.322647
Epoch 10.36: Loss = 0.40715
Epoch 10.37: Loss = 0.242004
Epoch 10.38: Loss = 0.317154
Epoch 10.39: Loss = 0.315704
Epoch 10.40: Loss = 0.344315
Epoch 10.41: Loss = 0.32991
Epoch 10.42: Loss = 0.602234
Epoch 10.43: Loss = 0.289536
Epoch 10.44: Loss = 0.266983
Epoch 10.45: Loss = 0.312256
Epoch 10.46: Loss = 0.379471
Epoch 10.47: Loss = 0.334808
Epoch 10.48: Loss = 0.364456
Epoch 10.49: Loss = 0.326279
Epoch 10.50: Loss = 0.464401
Epoch 10.51: Loss = 0.271439
Epoch 10.52: Loss = 0.288177
Epoch 10.53: Loss = 0.367859
Epoch 10.54: Loss = 0.467819
Epoch 10.55: Loss = 0.390198
Epoch 10.56: Loss = 0.324966
Epoch 10.57: Loss = 0.357651
Epoch 10.58: Loss = 0.344238
Epoch 10.59: Loss = 0.434174
Epoch 10.60: Loss = 0.435791
Epoch 10.61: Loss = 0.348099
Epoch 10.62: Loss = 0.402435
Epoch 10.63: Loss = 0.528473
Epoch 10.64: Loss = 0.456528
Epoch 10.65: Loss = 0.547699
Epoch 10.66: Loss = 0.360336
Epoch 10.67: Loss = 0.342361
Epoch 10.68: Loss = 0.220947
Epoch 10.69: Loss = 0.261948
Epoch 10.70: Loss = 0.436844
Epoch 10.71: Loss = 0.309586
Epoch 10.72: Loss = 0.257294
Epoch 10.73: Loss = 0.314087
Epoch 10.74: Loss = 0.28125
Epoch 10.75: Loss = 0.639343
Epoch 10.76: Loss = 0.40007
Epoch 10.77: Loss = 0.265091
Epoch 10.78: Loss = 0.368561
Epoch 10.79: Loss = 0.477829
Epoch 10.80: Loss = 0.373398
Epoch 10.81: Loss = 0.317291
Epoch 10.82: Loss = 0.262772
Epoch 10.83: Loss = 0.43222
Epoch 10.84: Loss = 0.35701
Epoch 10.85: Loss = 0.505188
Epoch 10.86: Loss = 0.461685
Epoch 10.87: Loss = 0.248474
Epoch 10.88: Loss = 0.349258
Epoch 10.89: Loss = 0.409134
Epoch 10.90: Loss = 0.284042
Epoch 10.91: Loss = 0.431305
Epoch 10.92: Loss = 0.435486
Epoch 10.93: Loss = 0.511841
Epoch 10.94: Loss = 0.234695
Epoch 10.95: Loss = 0.358627
Epoch 10.96: Loss = 0.423126
Epoch 10.97: Loss = 0.295212
Epoch 10.98: Loss = 0.303528
Epoch 10.99: Loss = 0.418732
Epoch 10.100: Loss = 0.534821
Epoch 10.101: Loss = 0.574203
Epoch 10.102: Loss = 0.346695
Epoch 10.103: Loss = 0.291367
Epoch 10.104: Loss = 0.298325
Epoch 10.105: Loss = 0.462875
Epoch 10.106: Loss = 0.501328
Epoch 10.107: Loss = 0.267426
Epoch 10.108: Loss = 0.409805
Epoch 10.109: Loss = 0.307434
Epoch 10.110: Loss = 0.38208
Epoch 10.111: Loss = 0.307388
Epoch 10.112: Loss = 0.274536
Epoch 10.113: Loss = 0.348236
Epoch 10.114: Loss = 0.240326
Epoch 10.115: Loss = 0.247879
Epoch 10.116: Loss = 0.339249
Epoch 10.117: Loss = 0.181351
Epoch 10.118: Loss = 0.185013
Epoch 10.119: Loss = 0.265564
Epoch 10.120: Loss = 0.352356
TRAIN LOSS = 0.365463
TRAIN ACC = 90.8295 % (54500/60000)
Loss = 0.291122
Loss = 0.423782
Loss = 0.48764
Loss = 0.480499
Loss = 0.525391
Loss = 0.347809
Loss = 0.300507
Loss = 0.610489
Loss = 0.478851
Loss = 0.435638
Loss = 0.161835
Loss = 0.283752
Loss = 0.349731
Loss = 0.296494
Loss = 0.151077
Loss = 0.25267
Loss = 0.172638
Loss = 0.0508575
Loss = 0.201599
Loss = 0.475235
TEST LOSS = 0.338881
TEST ACC = 545 % (9132/10000)
