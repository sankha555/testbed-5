Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.32103
Epoch 1.2: Loss = 2.29318
Epoch 1.3: Loss = 2.26514
Epoch 1.4: Loss = 2.23616
Epoch 1.5: Loss = 2.20879
Epoch 1.6: Loss = 2.18718
Epoch 1.7: Loss = 2.1593
Epoch 1.8: Loss = 2.10449
Epoch 1.9: Loss = 2.08076
Epoch 1.10: Loss = 2.05905
Epoch 1.11: Loss = 2.02864
Epoch 1.12: Loss = 2.00691
Epoch 1.13: Loss = 1.94406
Epoch 1.14: Loss = 1.94374
Epoch 1.15: Loss = 1.97633
Epoch 1.16: Loss = 1.8829
Epoch 1.17: Loss = 1.839
Epoch 1.18: Loss = 1.82324
Epoch 1.19: Loss = 1.78719
Epoch 1.20: Loss = 1.76105
Epoch 1.21: Loss = 1.68216
Epoch 1.22: Loss = 1.67557
Epoch 1.23: Loss = 1.60156
Epoch 1.24: Loss = 1.69075
Epoch 1.25: Loss = 1.60776
Epoch 1.26: Loss = 1.64116
Epoch 1.27: Loss = 1.58919
Epoch 1.28: Loss = 1.55936
Epoch 1.29: Loss = 1.52649
Epoch 1.30: Loss = 1.57228
Epoch 1.31: Loss = 1.46396
Epoch 1.32: Loss = 1.47568
Epoch 1.33: Loss = 1.38084
Epoch 1.34: Loss = 1.39937
Epoch 1.35: Loss = 1.35594
Epoch 1.36: Loss = 1.42052
Epoch 1.37: Loss = 1.29498
Epoch 1.38: Loss = 1.2421
Epoch 1.39: Loss = 1.2271
Epoch 1.40: Loss = 1.1416
Epoch 1.41: Loss = 1.20886
Epoch 1.42: Loss = 1.17819
Epoch 1.43: Loss = 1.11417
Epoch 1.44: Loss = 1.03758
Epoch 1.45: Loss = 1.17818
Epoch 1.46: Loss = 1.12721
Epoch 1.47: Loss = 1.0349
Epoch 1.48: Loss = 1.0824
Epoch 1.49: Loss = 1.02367
Epoch 1.50: Loss = 1.10747
Epoch 1.51: Loss = 0.896469
Epoch 1.52: Loss = 0.931885
Epoch 1.53: Loss = 0.956696
Epoch 1.54: Loss = 0.988083
Epoch 1.55: Loss = 0.960999
Epoch 1.56: Loss = 0.914688
Epoch 1.57: Loss = 0.825439
Epoch 1.58: Loss = 0.875443
Epoch 1.59: Loss = 0.902817
Epoch 1.60: Loss = 1.00168
Epoch 1.61: Loss = 0.945053
Epoch 1.62: Loss = 0.964661
Epoch 1.63: Loss = 0.968262
Epoch 1.64: Loss = 0.94838
Epoch 1.65: Loss = 0.975601
Epoch 1.66: Loss = 0.81105
Epoch 1.67: Loss = 0.810867
Epoch 1.68: Loss = 0.674728
Epoch 1.69: Loss = 0.774689
Epoch 1.70: Loss = 0.831619
Epoch 1.71: Loss = 0.751556
Epoch 1.72: Loss = 0.74324
Epoch 1.73: Loss = 0.784286
Epoch 1.74: Loss = 0.643753
Epoch 1.75: Loss = 0.802231
Epoch 1.76: Loss = 0.758743
Epoch 1.77: Loss = 0.701813
Epoch 1.78: Loss = 0.68782
Epoch 1.79: Loss = 0.705032
Epoch 1.80: Loss = 0.803146
Epoch 1.81: Loss = 0.671753
Epoch 1.82: Loss = 0.632629
Epoch 1.83: Loss = 0.796158
Epoch 1.84: Loss = 0.729813
Epoch 1.85: Loss = 0.782486
Epoch 1.86: Loss = 0.694077
Epoch 1.87: Loss = 0.632584
Epoch 1.88: Loss = 0.66301
Epoch 1.89: Loss = 0.748688
Epoch 1.90: Loss = 0.615738
Epoch 1.91: Loss = 0.692032
Epoch 1.92: Loss = 0.66684
Epoch 1.93: Loss = 0.71373
Epoch 1.94: Loss = 0.544296
Epoch 1.95: Loss = 0.670532
Epoch 1.96: Loss = 0.660843
Epoch 1.97: Loss = 0.495087
Epoch 1.98: Loss = 0.616806
Epoch 1.99: Loss = 0.693924
Epoch 1.100: Loss = 0.781219
Epoch 1.101: Loss = 0.687805
Epoch 1.102: Loss = 0.626205
Epoch 1.103: Loss = 0.574677
Epoch 1.104: Loss = 0.551819
Epoch 1.105: Loss = 0.641388
Epoch 1.106: Loss = 0.662186
Epoch 1.107: Loss = 0.539215
Epoch 1.108: Loss = 0.599594
Epoch 1.109: Loss = 0.547546
Epoch 1.110: Loss = 0.588318
Epoch 1.111: Loss = 0.49704
Epoch 1.112: Loss = 0.467834
Epoch 1.113: Loss = 0.552963
Epoch 1.114: Loss = 0.464188
Epoch 1.115: Loss = 0.547943
Epoch 1.116: Loss = 0.547989
Epoch 1.117: Loss = 0.44104
Epoch 1.118: Loss = 0.381409
Epoch 1.119: Loss = 0.394363
Epoch 1.120: Loss = 0.414825
TRAIN LOSS = 1.09268
TRAIN ACC = 70.4697 % (42284/60000)
Loss = 0.585373
Loss = 0.612549
Loss = 0.71788
Loss = 0.688187
Loss = 0.693787
Loss = 0.586044
Loss = 0.552948
Loss = 0.722183
Loss = 0.679596
Loss = 0.646072
Loss = 0.318268
Loss = 0.474289
Loss = 0.33609
Loss = 0.523453
Loss = 0.4077
Loss = 0.41246
Loss = 0.39978
Loss = 0.211517
Loss = 0.377991
Loss = 0.647095
TEST LOSS = 0.529663
TEST ACC = 422.839 % (8379/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.500504
Epoch 2.2: Loss = 0.633179
Epoch 2.3: Loss = 0.607941
Epoch 2.4: Loss = 0.46904
Epoch 2.5: Loss = 0.488388
Epoch 2.6: Loss = 0.500153
Epoch 2.7: Loss = 0.543564
Epoch 2.8: Loss = 0.523636
Epoch 2.9: Loss = 0.509567
Epoch 2.10: Loss = 0.494202
Epoch 2.11: Loss = 0.522903
Epoch 2.12: Loss = 0.506668
Epoch 2.13: Loss = 0.428833
Epoch 2.14: Loss = 0.479462
Epoch 2.15: Loss = 0.619568
Epoch 2.16: Loss = 0.562271
Epoch 2.17: Loss = 0.578674
Epoch 2.18: Loss = 0.660034
Epoch 2.19: Loss = 0.495453
Epoch 2.20: Loss = 0.462036
Epoch 2.21: Loss = 0.43309
Epoch 2.22: Loss = 0.429749
Epoch 2.23: Loss = 0.44223
Epoch 2.24: Loss = 0.632965
Epoch 2.25: Loss = 0.518188
Epoch 2.26: Loss = 0.627335
Epoch 2.27: Loss = 0.588547
Epoch 2.28: Loss = 0.553726
Epoch 2.29: Loss = 0.606506
Epoch 2.30: Loss = 0.676636
Epoch 2.31: Loss = 0.456741
Epoch 2.32: Loss = 0.597977
Epoch 2.33: Loss = 0.491135
Epoch 2.34: Loss = 0.539597
Epoch 2.35: Loss = 0.521561
Epoch 2.36: Loss = 0.602356
Epoch 2.37: Loss = 0.440079
Epoch 2.38: Loss = 0.418335
Epoch 2.39: Loss = 0.492401
Epoch 2.40: Loss = 0.437576
Epoch 2.41: Loss = 0.52803
Epoch 2.42: Loss = 0.579086
Epoch 2.43: Loss = 0.434006
Epoch 2.44: Loss = 0.390396
Epoch 2.45: Loss = 0.525375
Epoch 2.46: Loss = 0.54216
Epoch 2.47: Loss = 0.463577
Epoch 2.48: Loss = 0.494431
Epoch 2.49: Loss = 0.493622
Epoch 2.50: Loss = 0.581024
Epoch 2.51: Loss = 0.399078
Epoch 2.52: Loss = 0.402237
Epoch 2.53: Loss = 0.460159
Epoch 2.54: Loss = 0.553879
Epoch 2.55: Loss = 0.476425
Epoch 2.56: Loss = 0.459549
Epoch 2.57: Loss = 0.404266
Epoch 2.58: Loss = 0.457947
Epoch 2.59: Loss = 0.506256
Epoch 2.60: Loss = 0.591904
Epoch 2.61: Loss = 0.551956
Epoch 2.62: Loss = 0.581131
Epoch 2.63: Loss = 0.598404
Epoch 2.64: Loss = 0.598312
Epoch 2.65: Loss = 0.636902
Epoch 2.66: Loss = 0.472702
Epoch 2.67: Loss = 0.47908
Epoch 2.68: Loss = 0.315842
Epoch 2.69: Loss = 0.406082
Epoch 2.70: Loss = 0.546249
Epoch 2.71: Loss = 0.397003
Epoch 2.72: Loss = 0.412415
Epoch 2.73: Loss = 0.46817
Epoch 2.74: Loss = 0.355011
Epoch 2.75: Loss = 0.577591
Epoch 2.76: Loss = 0.498077
Epoch 2.77: Loss = 0.410751
Epoch 2.78: Loss = 0.42868
Epoch 2.79: Loss = 0.501053
Epoch 2.80: Loss = 0.527573
Epoch 2.81: Loss = 0.418121
Epoch 2.82: Loss = 0.360596
Epoch 2.83: Loss = 0.524277
Epoch 2.84: Loss = 0.448792
Epoch 2.85: Loss = 0.610321
Epoch 2.86: Loss = 0.497375
Epoch 2.87: Loss = 0.387466
Epoch 2.88: Loss = 0.439484
Epoch 2.89: Loss = 0.533646
Epoch 2.90: Loss = 0.381851
Epoch 2.91: Loss = 0.490295
Epoch 2.92: Loss = 0.479691
Epoch 2.93: Loss = 0.545792
Epoch 2.94: Loss = 0.354767
Epoch 2.95: Loss = 0.453903
Epoch 2.96: Loss = 0.50676
Epoch 2.97: Loss = 0.337158
Epoch 2.98: Loss = 0.432907
Epoch 2.99: Loss = 0.52951
Epoch 2.100: Loss = 0.612808
Epoch 2.101: Loss = 0.569565
Epoch 2.102: Loss = 0.467667
Epoch 2.103: Loss = 0.390533
Epoch 2.104: Loss = 0.373291
Epoch 2.105: Loss = 0.510284
Epoch 2.106: Loss = 0.524094
Epoch 2.107: Loss = 0.368271
Epoch 2.108: Loss = 0.472092
Epoch 2.109: Loss = 0.40184
Epoch 2.110: Loss = 0.453079
Epoch 2.111: Loss = 0.335678
Epoch 2.112: Loss = 0.335037
Epoch 2.113: Loss = 0.408035
Epoch 2.114: Loss = 0.350754
Epoch 2.115: Loss = 0.376465
Epoch 2.116: Loss = 0.406586
Epoch 2.117: Loss = 0.279434
Epoch 2.118: Loss = 0.243134
Epoch 2.119: Loss = 0.300369
Epoch 2.120: Loss = 0.305511
TRAIN LOSS = 0.481812
TRAIN ACC = 85.228 % (51139/60000)
Loss = 0.442932
Loss = 0.50592
Loss = 0.599106
Loss = 0.572281
Loss = 0.607727
Loss = 0.427032
Loss = 0.412933
Loss = 0.619064
Loss = 0.569824
Loss = 0.537796
Loss = 0.199448
Loss = 0.353699
Loss = 0.285492
Loss = 0.396835
Loss = 0.251205
Loss = 0.296524
Loss = 0.266205
Loss = 0.105469
Loss = 0.242126
Loss = 0.545898
TEST LOSS = 0.411876
TEST ACC = 511.389 % (8746/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.392242
Epoch 3.2: Loss = 0.504684
Epoch 3.3: Loss = 0.48172
Epoch 3.4: Loss = 0.3479
Epoch 3.5: Loss = 0.37674
Epoch 3.6: Loss = 0.394943
Epoch 3.7: Loss = 0.390198
Epoch 3.8: Loss = 0.412888
Epoch 3.9: Loss = 0.386688
Epoch 3.10: Loss = 0.391525
Epoch 3.11: Loss = 0.440231
Epoch 3.12: Loss = 0.394104
Epoch 3.13: Loss = 0.326874
Epoch 3.14: Loss = 0.378494
Epoch 3.15: Loss = 0.482635
Epoch 3.16: Loss = 0.450104
Epoch 3.17: Loss = 0.478287
Epoch 3.18: Loss = 0.610275
Epoch 3.19: Loss = 0.408569
Epoch 3.20: Loss = 0.373795
Epoch 3.21: Loss = 0.348053
Epoch 3.22: Loss = 0.319351
Epoch 3.23: Loss = 0.359772
Epoch 3.24: Loss = 0.5392
Epoch 3.25: Loss = 0.431732
Epoch 3.26: Loss = 0.546677
Epoch 3.27: Loss = 0.503204
Epoch 3.28: Loss = 0.467255
Epoch 3.29: Loss = 0.515915
Epoch 3.30: Loss = 0.568573
Epoch 3.31: Loss = 0.369324
Epoch 3.32: Loss = 0.49324
Epoch 3.33: Loss = 0.388199
Epoch 3.34: Loss = 0.441986
Epoch 3.35: Loss = 0.435211
Epoch 3.36: Loss = 0.506683
Epoch 3.37: Loss = 0.334763
Epoch 3.38: Loss = 0.351044
Epoch 3.39: Loss = 0.396545
Epoch 3.40: Loss = 0.364487
Epoch 3.41: Loss = 0.428635
Epoch 3.42: Loss = 0.545166
Epoch 3.43: Loss = 0.347183
Epoch 3.44: Loss = 0.337357
Epoch 3.45: Loss = 0.430527
Epoch 3.46: Loss = 0.459259
Epoch 3.47: Loss = 0.392487
Epoch 3.48: Loss = 0.419418
Epoch 3.49: Loss = 0.421555
Epoch 3.50: Loss = 0.511688
Epoch 3.51: Loss = 0.333176
Epoch 3.52: Loss = 0.325714
Epoch 3.53: Loss = 0.390411
Epoch 3.54: Loss = 0.498917
Epoch 3.55: Loss = 0.404007
Epoch 3.56: Loss = 0.408142
Epoch 3.57: Loss = 0.367737
Epoch 3.58: Loss = 0.40097
Epoch 3.59: Loss = 0.459747
Epoch 3.60: Loss = 0.543564
Epoch 3.61: Loss = 0.46936
Epoch 3.62: Loss = 0.514847
Epoch 3.63: Loss = 0.546906
Epoch 3.64: Loss = 0.545685
Epoch 3.65: Loss = 0.585785
Epoch 3.66: Loss = 0.41684
Epoch 3.67: Loss = 0.406708
Epoch 3.68: Loss = 0.261551
Epoch 3.69: Loss = 0.348236
Epoch 3.70: Loss = 0.502472
Epoch 3.71: Loss = 0.342224
Epoch 3.72: Loss = 0.341095
Epoch 3.73: Loss = 0.412308
Epoch 3.74: Loss = 0.303543
Epoch 3.75: Loss = 0.571274
Epoch 3.76: Loss = 0.443069
Epoch 3.77: Loss = 0.340195
Epoch 3.78: Loss = 0.387115
Epoch 3.79: Loss = 0.478363
Epoch 3.80: Loss = 0.468903
Epoch 3.81: Loss = 0.379517
Epoch 3.82: Loss = 0.309723
Epoch 3.83: Loss = 0.467743
Epoch 3.84: Loss = 0.397034
Epoch 3.85: Loss = 0.580215
Epoch 3.86: Loss = 0.463043
Epoch 3.87: Loss = 0.333557
Epoch 3.88: Loss = 0.392212
Epoch 3.89: Loss = 0.478394
Epoch 3.90: Loss = 0.335159
Epoch 3.91: Loss = 0.453949
Epoch 3.92: Loss = 0.448685
Epoch 3.93: Loss = 0.532028
Epoch 3.94: Loss = 0.313782
Epoch 3.95: Loss = 0.412109
Epoch 3.96: Loss = 0.475052
Epoch 3.97: Loss = 0.31015
Epoch 3.98: Loss = 0.395386
Epoch 3.99: Loss = 0.477722
Epoch 3.100: Loss = 0.567902
Epoch 3.101: Loss = 0.553833
Epoch 3.102: Loss = 0.40892
Epoch 3.103: Loss = 0.357742
Epoch 3.104: Loss = 0.336426
Epoch 3.105: Loss = 0.480469
Epoch 3.106: Loss = 0.49173
Epoch 3.107: Loss = 0.315186
Epoch 3.108: Loss = 0.437424
Epoch 3.109: Loss = 0.368332
Epoch 3.110: Loss = 0.42308
Epoch 3.111: Loss = 0.316177
Epoch 3.112: Loss = 0.307114
Epoch 3.113: Loss = 0.37735
Epoch 3.114: Loss = 0.313248
Epoch 3.115: Loss = 0.325897
Epoch 3.116: Loss = 0.374542
Epoch 3.117: Loss = 0.231888
Epoch 3.118: Loss = 0.210236
Epoch 3.119: Loss = 0.266785
Epoch 3.120: Loss = 0.303207
TRAIN LOSS = 0.416168
TRAIN ACC = 87.6297 % (52580/60000)
Loss = 0.403946
Loss = 0.460007
Loss = 0.555283
Loss = 0.5401
Loss = 0.565964
Loss = 0.390198
Loss = 0.367432
Loss = 0.588348
Loss = 0.540421
Loss = 0.489044
Loss = 0.174774
Loss = 0.300125
Loss = 0.273727
Loss = 0.353607
Loss = 0.205063
Loss = 0.255951
Loss = 0.223206
Loss = 0.0745544
Loss = 0.213715
Loss = 0.526382
TEST LOSS = 0.375092
TEST ACC = 525.8 % (8898/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.364349
Epoch 4.2: Loss = 0.466354
Epoch 4.3: Loss = 0.454712
Epoch 4.4: Loss = 0.308853
Epoch 4.5: Loss = 0.349213
Epoch 4.6: Loss = 0.367203
Epoch 4.7: Loss = 0.342499
Epoch 4.8: Loss = 0.375305
Epoch 4.9: Loss = 0.354218
Epoch 4.10: Loss = 0.344254
Epoch 4.11: Loss = 0.424515
Epoch 4.12: Loss = 0.375366
Epoch 4.13: Loss = 0.300919
Epoch 4.14: Loss = 0.351257
Epoch 4.15: Loss = 0.425354
Epoch 4.16: Loss = 0.417831
Epoch 4.17: Loss = 0.457764
Epoch 4.18: Loss = 0.601868
Epoch 4.19: Loss = 0.397537
Epoch 4.20: Loss = 0.334915
Epoch 4.21: Loss = 0.331573
Epoch 4.22: Loss = 0.288406
Epoch 4.23: Loss = 0.3293
Epoch 4.24: Loss = 0.5289
Epoch 4.25: Loss = 0.402267
Epoch 4.26: Loss = 0.534958
Epoch 4.27: Loss = 0.482941
Epoch 4.28: Loss = 0.445282
Epoch 4.29: Loss = 0.490875
Epoch 4.30: Loss = 0.54187
Epoch 4.31: Loss = 0.343002
Epoch 4.32: Loss = 0.458496
Epoch 4.33: Loss = 0.357132
Epoch 4.34: Loss = 0.42012
Epoch 4.35: Loss = 0.400772
Epoch 4.36: Loss = 0.473373
Epoch 4.37: Loss = 0.294693
Epoch 4.38: Loss = 0.32933
Epoch 4.39: Loss = 0.362717
Epoch 4.40: Loss = 0.344238
Epoch 4.41: Loss = 0.385773
Epoch 4.42: Loss = 0.553329
Epoch 4.43: Loss = 0.326065
Epoch 4.44: Loss = 0.319672
Epoch 4.45: Loss = 0.382599
Epoch 4.46: Loss = 0.430695
Epoch 4.47: Loss = 0.376358
Epoch 4.48: Loss = 0.406311
Epoch 4.49: Loss = 0.382339
Epoch 4.50: Loss = 0.479858
Epoch 4.51: Loss = 0.310577
Epoch 4.52: Loss = 0.317291
Epoch 4.53: Loss = 0.363968
Epoch 4.54: Loss = 0.484848
Epoch 4.55: Loss = 0.399384
Epoch 4.56: Loss = 0.396042
Epoch 4.57: Loss = 0.347137
Epoch 4.58: Loss = 0.378937
Epoch 4.59: Loss = 0.438614
Epoch 4.60: Loss = 0.51445
Epoch 4.61: Loss = 0.413818
Epoch 4.62: Loss = 0.484665
Epoch 4.63: Loss = 0.530716
Epoch 4.64: Loss = 0.522308
Epoch 4.65: Loss = 0.555908
Epoch 4.66: Loss = 0.396057
Epoch 4.67: Loss = 0.381516
Epoch 4.68: Loss = 0.244171
Epoch 4.69: Loss = 0.323334
Epoch 4.70: Loss = 0.483398
Epoch 4.71: Loss = 0.32103
Epoch 4.72: Loss = 0.313904
Epoch 4.73: Loss = 0.379501
Epoch 4.74: Loss = 0.280563
Epoch 4.75: Loss = 0.571503
Epoch 4.76: Loss = 0.411819
Epoch 4.77: Loss = 0.312668
Epoch 4.78: Loss = 0.376144
Epoch 4.79: Loss = 0.465302
Epoch 4.80: Loss = 0.412735
Epoch 4.81: Loss = 0.357025
Epoch 4.82: Loss = 0.292038
Epoch 4.83: Loss = 0.442963
Epoch 4.84: Loss = 0.365021
Epoch 4.85: Loss = 0.576813
Epoch 4.86: Loss = 0.457932
Epoch 4.87: Loss = 0.303055
Epoch 4.88: Loss = 0.378113
Epoch 4.89: Loss = 0.450058
Epoch 4.90: Loss = 0.315155
Epoch 4.91: Loss = 0.440979
Epoch 4.92: Loss = 0.435089
Epoch 4.93: Loss = 0.506836
Epoch 4.94: Loss = 0.301636
Epoch 4.95: Loss = 0.387527
Epoch 4.96: Loss = 0.451782
Epoch 4.97: Loss = 0.297379
Epoch 4.98: Loss = 0.38118
Epoch 4.99: Loss = 0.463043
Epoch 4.100: Loss = 0.529602
Epoch 4.101: Loss = 0.551422
Epoch 4.102: Loss = 0.378708
Epoch 4.103: Loss = 0.331314
Epoch 4.104: Loss = 0.316895
Epoch 4.105: Loss = 0.450638
Epoch 4.106: Loss = 0.480209
Epoch 4.107: Loss = 0.287247
Epoch 4.108: Loss = 0.428177
Epoch 4.109: Loss = 0.34967
Epoch 4.110: Loss = 0.416565
Epoch 4.111: Loss = 0.299622
Epoch 4.112: Loss = 0.29364
Epoch 4.113: Loss = 0.350845
Epoch 4.114: Loss = 0.29863
Epoch 4.115: Loss = 0.292633
Epoch 4.116: Loss = 0.35173
Epoch 4.117: Loss = 0.201141
Epoch 4.118: Loss = 0.189468
Epoch 4.119: Loss = 0.253693
Epoch 4.120: Loss = 0.3004
TRAIN LOSS = 0.392548
TRAIN ACC = 88.7527 % (53254/60000)
Loss = 0.380966
Loss = 0.44809
Loss = 0.528458
Loss = 0.536072
Loss = 0.565567
Loss = 0.35643
Loss = 0.337036
Loss = 0.596054
Loss = 0.538116
Loss = 0.46907
Loss = 0.156769
Loss = 0.278229
Loss = 0.280731
Loss = 0.338654
Loss = 0.188889
Loss = 0.245697
Loss = 0.209763
Loss = 0.0643921
Loss = 0.210129
Loss = 0.508591
TEST LOSS = 0.361885
TEST ACC = 532.539 % (8974/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.357147
Epoch 5.2: Loss = 0.464966
Epoch 5.3: Loss = 0.451904
Epoch 5.4: Loss = 0.304184
Epoch 5.5: Loss = 0.331482
Epoch 5.6: Loss = 0.351944
Epoch 5.7: Loss = 0.319931
Epoch 5.8: Loss = 0.348816
Epoch 5.9: Loss = 0.345551
Epoch 5.10: Loss = 0.32933
Epoch 5.11: Loss = 0.411301
Epoch 5.12: Loss = 0.362534
Epoch 5.13: Loss = 0.276321
Epoch 5.14: Loss = 0.343124
Epoch 5.15: Loss = 0.401398
Epoch 5.16: Loss = 0.413208
Epoch 5.17: Loss = 0.443054
Epoch 5.18: Loss = 0.599701
Epoch 5.19: Loss = 0.396317
Epoch 5.20: Loss = 0.311432
Epoch 5.21: Loss = 0.325638
Epoch 5.22: Loss = 0.275772
Epoch 5.23: Loss = 0.309998
Epoch 5.24: Loss = 0.541397
Epoch 5.25: Loss = 0.399002
Epoch 5.26: Loss = 0.53862
Epoch 5.27: Loss = 0.485901
Epoch 5.28: Loss = 0.422913
Epoch 5.29: Loss = 0.484222
Epoch 5.30: Loss = 0.524689
Epoch 5.31: Loss = 0.345459
Epoch 5.32: Loss = 0.440048
Epoch 5.33: Loss = 0.342026
Epoch 5.34: Loss = 0.394043
Epoch 5.35: Loss = 0.397339
Epoch 5.36: Loss = 0.430634
Epoch 5.37: Loss = 0.257675
Epoch 5.38: Loss = 0.314758
Epoch 5.39: Loss = 0.345306
Epoch 5.40: Loss = 0.323914
Epoch 5.41: Loss = 0.367874
Epoch 5.42: Loss = 0.542938
Epoch 5.43: Loss = 0.313187
Epoch 5.44: Loss = 0.288391
Epoch 5.45: Loss = 0.364853
Epoch 5.46: Loss = 0.432129
Epoch 5.47: Loss = 0.38533
Epoch 5.48: Loss = 0.385452
Epoch 5.49: Loss = 0.37822
Epoch 5.50: Loss = 0.479492
Epoch 5.51: Loss = 0.302521
Epoch 5.52: Loss = 0.287506
Epoch 5.53: Loss = 0.344955
Epoch 5.54: Loss = 0.482834
Epoch 5.55: Loss = 0.387222
Epoch 5.56: Loss = 0.387115
Epoch 5.57: Loss = 0.352173
Epoch 5.58: Loss = 0.368469
Epoch 5.59: Loss = 0.429749
Epoch 5.60: Loss = 0.501526
Epoch 5.61: Loss = 0.403351
Epoch 5.62: Loss = 0.459534
Epoch 5.63: Loss = 0.522827
Epoch 5.64: Loss = 0.503113
Epoch 5.65: Loss = 0.567612
Epoch 5.66: Loss = 0.387634
Epoch 5.67: Loss = 0.354919
Epoch 5.68: Loss = 0.23027
Epoch 5.69: Loss = 0.316666
Epoch 5.70: Loss = 0.468231
Epoch 5.71: Loss = 0.320557
Epoch 5.72: Loss = 0.294235
Epoch 5.73: Loss = 0.382584
Epoch 5.74: Loss = 0.2827
Epoch 5.75: Loss = 0.57605
Epoch 5.76: Loss = 0.410309
Epoch 5.77: Loss = 0.29393
Epoch 5.78: Loss = 0.360214
Epoch 5.79: Loss = 0.46582
Epoch 5.80: Loss = 0.401184
Epoch 5.81: Loss = 0.357788
Epoch 5.82: Loss = 0.286026
Epoch 5.83: Loss = 0.449615
Epoch 5.84: Loss = 0.360611
Epoch 5.85: Loss = 0.55191
Epoch 5.86: Loss = 0.46907
Epoch 5.87: Loss = 0.288315
Epoch 5.88: Loss = 0.369965
Epoch 5.89: Loss = 0.450714
Epoch 5.90: Loss = 0.307205
Epoch 5.91: Loss = 0.428009
Epoch 5.92: Loss = 0.440979
Epoch 5.93: Loss = 0.502777
Epoch 5.94: Loss = 0.282318
Epoch 5.95: Loss = 0.372955
Epoch 5.96: Loss = 0.451096
Epoch 5.97: Loss = 0.294724
Epoch 5.98: Loss = 0.356735
Epoch 5.99: Loss = 0.431732
Epoch 5.100: Loss = 0.537048
Epoch 5.101: Loss = 0.549973
Epoch 5.102: Loss = 0.372147
Epoch 5.103: Loss = 0.322342
Epoch 5.104: Loss = 0.31987
Epoch 5.105: Loss = 0.462769
Epoch 5.106: Loss = 0.484268
Epoch 5.107: Loss = 0.277939
Epoch 5.108: Loss = 0.422119
Epoch 5.109: Loss = 0.343338
Epoch 5.110: Loss = 0.403214
Epoch 5.111: Loss = 0.304474
Epoch 5.112: Loss = 0.304306
Epoch 5.113: Loss = 0.337158
Epoch 5.114: Loss = 0.283005
Epoch 5.115: Loss = 0.295334
Epoch 5.116: Loss = 0.355377
Epoch 5.117: Loss = 0.193741
Epoch 5.118: Loss = 0.183014
Epoch 5.119: Loss = 0.258469
Epoch 5.120: Loss = 0.321259
TRAIN LOSS = 0.383591
TRAIN ACC = 89.3204 % (53595/60000)
Loss = 0.363708
Loss = 0.439362
Loss = 0.516418
Loss = 0.528915
Loss = 0.554779
Loss = 0.342438
Loss = 0.324249
Loss = 0.593384
Loss = 0.518143
Loss = 0.45784
Loss = 0.149872
Loss = 0.276276
Loss = 0.30162
Loss = 0.33252
Loss = 0.180511
Loss = 0.240311
Loss = 0.205292
Loss = 0.0509949
Loss = 0.19754
Loss = 0.513153
TEST LOSS = 0.354366
TEST ACC = 535.95 % (9013/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.356232
Epoch 6.2: Loss = 0.452591
Epoch 6.3: Loss = 0.463776
Epoch 6.4: Loss = 0.302368
Epoch 6.5: Loss = 0.311676
Epoch 6.6: Loss = 0.340637
Epoch 6.7: Loss = 0.312424
Epoch 6.8: Loss = 0.332443
Epoch 6.9: Loss = 0.343521
Epoch 6.10: Loss = 0.322296
Epoch 6.11: Loss = 0.398682
Epoch 6.12: Loss = 0.354126
Epoch 6.13: Loss = 0.268723
Epoch 6.14: Loss = 0.339218
Epoch 6.15: Loss = 0.379868
Epoch 6.16: Loss = 0.391129
Epoch 6.17: Loss = 0.447403
Epoch 6.18: Loss = 0.609177
Epoch 6.19: Loss = 0.407074
Epoch 6.20: Loss = 0.301529
Epoch 6.21: Loss = 0.315414
Epoch 6.22: Loss = 0.275757
Epoch 6.23: Loss = 0.296097
Epoch 6.24: Loss = 0.536057
Epoch 6.25: Loss = 0.393448
Epoch 6.26: Loss = 0.547287
Epoch 6.27: Loss = 0.475037
Epoch 6.28: Loss = 0.427536
Epoch 6.29: Loss = 0.4814
Epoch 6.30: Loss = 0.534485
Epoch 6.31: Loss = 0.335739
Epoch 6.32: Loss = 0.422318
Epoch 6.33: Loss = 0.323746
Epoch 6.34: Loss = 0.391357
Epoch 6.35: Loss = 0.39711
Epoch 6.36: Loss = 0.437256
Epoch 6.37: Loss = 0.249069
Epoch 6.38: Loss = 0.307816
Epoch 6.39: Loss = 0.327942
Epoch 6.40: Loss = 0.29895
Epoch 6.41: Loss = 0.360275
Epoch 6.42: Loss = 0.548096
Epoch 6.43: Loss = 0.318756
Epoch 6.44: Loss = 0.28479
Epoch 6.45: Loss = 0.345367
Epoch 6.46: Loss = 0.408173
Epoch 6.47: Loss = 0.373108
Epoch 6.48: Loss = 0.396347
Epoch 6.49: Loss = 0.362137
Epoch 6.50: Loss = 0.485992
Epoch 6.51: Loss = 0.289398
Epoch 6.52: Loss = 0.281128
Epoch 6.53: Loss = 0.345627
Epoch 6.54: Loss = 0.488037
Epoch 6.55: Loss = 0.384872
Epoch 6.56: Loss = 0.386841
Epoch 6.57: Loss = 0.3517
Epoch 6.58: Loss = 0.349731
Epoch 6.59: Loss = 0.417801
Epoch 6.60: Loss = 0.50415
Epoch 6.61: Loss = 0.375351
Epoch 6.62: Loss = 0.435776
Epoch 6.63: Loss = 0.52005
Epoch 6.64: Loss = 0.492264
Epoch 6.65: Loss = 0.556091
Epoch 6.66: Loss = 0.373062
Epoch 6.67: Loss = 0.338669
Epoch 6.68: Loss = 0.228149
Epoch 6.69: Loss = 0.309784
Epoch 6.70: Loss = 0.470596
Epoch 6.71: Loss = 0.326767
Epoch 6.72: Loss = 0.273087
Epoch 6.73: Loss = 0.356781
Epoch 6.74: Loss = 0.271805
Epoch 6.75: Loss = 0.575409
Epoch 6.76: Loss = 0.412262
Epoch 6.77: Loss = 0.279037
Epoch 6.78: Loss = 0.366928
Epoch 6.79: Loss = 0.46904
Epoch 6.80: Loss = 0.388306
Epoch 6.81: Loss = 0.327667
Epoch 6.82: Loss = 0.278854
Epoch 6.83: Loss = 0.440796
Epoch 6.84: Loss = 0.346771
Epoch 6.85: Loss = 0.521255
Epoch 6.86: Loss = 0.48056
Epoch 6.87: Loss = 0.290237
Epoch 6.88: Loss = 0.361328
Epoch 6.89: Loss = 0.430634
Epoch 6.90: Loss = 0.291397
Epoch 6.91: Loss = 0.427643
Epoch 6.92: Loss = 0.444611
Epoch 6.93: Loss = 0.505859
Epoch 6.94: Loss = 0.279907
Epoch 6.95: Loss = 0.375381
Epoch 6.96: Loss = 0.431442
Epoch 6.97: Loss = 0.307114
Epoch 6.98: Loss = 0.349365
Epoch 6.99: Loss = 0.412262
Epoch 6.100: Loss = 0.541595
Epoch 6.101: Loss = 0.558533
Epoch 6.102: Loss = 0.367752
Epoch 6.103: Loss = 0.316971
Epoch 6.104: Loss = 0.304688
Epoch 6.105: Loss = 0.457703
Epoch 6.106: Loss = 0.482162
Epoch 6.107: Loss = 0.261993
Epoch 6.108: Loss = 0.424118
Epoch 6.109: Loss = 0.332626
Epoch 6.110: Loss = 0.387375
Epoch 6.111: Loss = 0.287598
Epoch 6.112: Loss = 0.294739
Epoch 6.113: Loss = 0.337097
Epoch 6.114: Loss = 0.271454
Epoch 6.115: Loss = 0.276596
Epoch 6.116: Loss = 0.351639
Epoch 6.117: Loss = 0.183395
Epoch 6.118: Loss = 0.181442
Epoch 6.119: Loss = 0.264343
Epoch 6.120: Loss = 0.311356
TRAIN LOSS = 0.37674
TRAIN ACC = 89.8331 % (53903/60000)
Loss = 0.346634
Loss = 0.448929
Loss = 0.509476
Loss = 0.53418
Loss = 0.548767
Loss = 0.340408
Loss = 0.310715
Loss = 0.58577
Loss = 0.512238
Loss = 0.454422
Loss = 0.132584
Loss = 0.27388
Loss = 0.312164
Loss = 0.333832
Loss = 0.176285
Loss = 0.223434
Loss = 0.196259
Loss = 0.0475464
Loss = 0.177826
Loss = 0.508652
TEST LOSS = 0.3487
TEST ACC = 539.029 % (9066/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.354965
Epoch 7.2: Loss = 0.437988
Epoch 7.3: Loss = 0.458115
Epoch 7.4: Loss = 0.304352
Epoch 7.5: Loss = 0.307465
Epoch 7.6: Loss = 0.328445
Epoch 7.7: Loss = 0.30777
Epoch 7.8: Loss = 0.340622
Epoch 7.9: Loss = 0.338989
Epoch 7.10: Loss = 0.312317
Epoch 7.11: Loss = 0.378448
Epoch 7.12: Loss = 0.340668
Epoch 7.13: Loss = 0.26088
Epoch 7.14: Loss = 0.334198
Epoch 7.15: Loss = 0.38472
Epoch 7.16: Loss = 0.382767
Epoch 7.17: Loss = 0.427933
Epoch 7.18: Loss = 0.61026
Epoch 7.19: Loss = 0.411469
Epoch 7.20: Loss = 0.295044
Epoch 7.21: Loss = 0.310226
Epoch 7.22: Loss = 0.268936
Epoch 7.23: Loss = 0.287094
Epoch 7.24: Loss = 0.531403
Epoch 7.25: Loss = 0.381134
Epoch 7.26: Loss = 0.538071
Epoch 7.27: Loss = 0.46936
Epoch 7.28: Loss = 0.417648
Epoch 7.29: Loss = 0.483856
Epoch 7.30: Loss = 0.504578
Epoch 7.31: Loss = 0.328827
Epoch 7.32: Loss = 0.393967
Epoch 7.33: Loss = 0.318512
Epoch 7.34: Loss = 0.395096
Epoch 7.35: Loss = 0.384109
Epoch 7.36: Loss = 0.42894
Epoch 7.37: Loss = 0.244232
Epoch 7.38: Loss = 0.309219
Epoch 7.39: Loss = 0.332367
Epoch 7.40: Loss = 0.306366
Epoch 7.41: Loss = 0.349152
Epoch 7.42: Loss = 0.562607
Epoch 7.43: Loss = 0.307755
Epoch 7.44: Loss = 0.274948
Epoch 7.45: Loss = 0.331009
Epoch 7.46: Loss = 0.414734
Epoch 7.47: Loss = 0.372955
Epoch 7.48: Loss = 0.391556
Epoch 7.49: Loss = 0.358765
Epoch 7.50: Loss = 0.478561
Epoch 7.51: Loss = 0.279373
Epoch 7.52: Loss = 0.266373
Epoch 7.53: Loss = 0.330841
Epoch 7.54: Loss = 0.488678
Epoch 7.55: Loss = 0.362869
Epoch 7.56: Loss = 0.378159
Epoch 7.57: Loss = 0.353378
Epoch 7.58: Loss = 0.334213
Epoch 7.59: Loss = 0.405502
Epoch 7.60: Loss = 0.496613
Epoch 7.61: Loss = 0.383713
Epoch 7.62: Loss = 0.427856
Epoch 7.63: Loss = 0.517288
Epoch 7.64: Loss = 0.470108
Epoch 7.65: Loss = 0.539322
Epoch 7.66: Loss = 0.361252
Epoch 7.67: Loss = 0.330627
Epoch 7.68: Loss = 0.21582
Epoch 7.69: Loss = 0.295181
Epoch 7.70: Loss = 0.460022
Epoch 7.71: Loss = 0.326294
Epoch 7.72: Loss = 0.253372
Epoch 7.73: Loss = 0.353439
Epoch 7.74: Loss = 0.266357
Epoch 7.75: Loss = 0.560715
Epoch 7.76: Loss = 0.396881
Epoch 7.77: Loss = 0.28244
Epoch 7.78: Loss = 0.348358
Epoch 7.79: Loss = 0.477753
Epoch 7.80: Loss = 0.377945
Epoch 7.81: Loss = 0.327484
Epoch 7.82: Loss = 0.281647
Epoch 7.83: Loss = 0.431961
Epoch 7.84: Loss = 0.338165
Epoch 7.85: Loss = 0.504486
Epoch 7.86: Loss = 0.492737
Epoch 7.87: Loss = 0.276398
Epoch 7.88: Loss = 0.352066
Epoch 7.89: Loss = 0.429291
Epoch 7.90: Loss = 0.27272
Epoch 7.91: Loss = 0.422928
Epoch 7.92: Loss = 0.453125
Epoch 7.93: Loss = 0.499985
Epoch 7.94: Loss = 0.268539
Epoch 7.95: Loss = 0.366455
Epoch 7.96: Loss = 0.418625
Epoch 7.97: Loss = 0.304977
Epoch 7.98: Loss = 0.335968
Epoch 7.99: Loss = 0.417511
Epoch 7.100: Loss = 0.545502
Epoch 7.101: Loss = 0.568558
Epoch 7.102: Loss = 0.37384
Epoch 7.103: Loss = 0.302063
Epoch 7.104: Loss = 0.300186
Epoch 7.105: Loss = 0.451385
Epoch 7.106: Loss = 0.472565
Epoch 7.107: Loss = 0.244019
Epoch 7.108: Loss = 0.415909
Epoch 7.109: Loss = 0.349915
Epoch 7.110: Loss = 0.373672
Epoch 7.111: Loss = 0.281891
Epoch 7.112: Loss = 0.287964
Epoch 7.113: Loss = 0.336975
Epoch 7.114: Loss = 0.260345
Epoch 7.115: Loss = 0.260056
Epoch 7.116: Loss = 0.341324
Epoch 7.117: Loss = 0.182098
Epoch 7.118: Loss = 0.175674
Epoch 7.119: Loss = 0.255447
Epoch 7.120: Loss = 0.321671
TRAIN LOSS = 0.370255
TRAIN ACC = 90.2298 % (54141/60000)
Loss = 0.343079
Loss = 0.436508
Loss = 0.494705
Loss = 0.528473
Loss = 0.544876
Loss = 0.336975
Loss = 0.31369
Loss = 0.589981
Loss = 0.517883
Loss = 0.450394
Loss = 0.125015
Loss = 0.254501
Loss = 0.300095
Loss = 0.328659
Loss = 0.157623
Loss = 0.211838
Loss = 0.179825
Loss = 0.0406647
Loss = 0.181992
Loss = 0.489532
TEST LOSS = 0.341315
TEST ACC = 541.409 % (9106/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.343292
Epoch 8.2: Loss = 0.438644
Epoch 8.3: Loss = 0.458694
Epoch 8.4: Loss = 0.297913
Epoch 8.5: Loss = 0.294022
Epoch 8.6: Loss = 0.314133
Epoch 8.7: Loss = 0.300598
Epoch 8.8: Loss = 0.331848
Epoch 8.9: Loss = 0.325516
Epoch 8.10: Loss = 0.317917
Epoch 8.11: Loss = 0.370529
Epoch 8.12: Loss = 0.323044
Epoch 8.13: Loss = 0.246613
Epoch 8.14: Loss = 0.307129
Epoch 8.15: Loss = 0.373734
Epoch 8.16: Loss = 0.377197
Epoch 8.17: Loss = 0.435242
Epoch 8.18: Loss = 0.617294
Epoch 8.19: Loss = 0.412613
Epoch 8.20: Loss = 0.28595
Epoch 8.21: Loss = 0.302841
Epoch 8.22: Loss = 0.255432
Epoch 8.23: Loss = 0.270599
Epoch 8.24: Loss = 0.525085
Epoch 8.25: Loss = 0.378708
Epoch 8.26: Loss = 0.534912
Epoch 8.27: Loss = 0.470078
Epoch 8.28: Loss = 0.411438
Epoch 8.29: Loss = 0.485641
Epoch 8.30: Loss = 0.510895
Epoch 8.31: Loss = 0.337067
Epoch 8.32: Loss = 0.399414
Epoch 8.33: Loss = 0.327103
Epoch 8.34: Loss = 0.38562
Epoch 8.35: Loss = 0.377945
Epoch 8.36: Loss = 0.409561
Epoch 8.37: Loss = 0.234421
Epoch 8.38: Loss = 0.304077
Epoch 8.39: Loss = 0.325027
Epoch 8.40: Loss = 0.313416
Epoch 8.41: Loss = 0.338333
Epoch 8.42: Loss = 0.550476
Epoch 8.43: Loss = 0.31868
Epoch 8.44: Loss = 0.272964
Epoch 8.45: Loss = 0.318192
Epoch 8.46: Loss = 0.404663
Epoch 8.47: Loss = 0.343796
Epoch 8.48: Loss = 0.3806
Epoch 8.49: Loss = 0.344604
Epoch 8.50: Loss = 0.480179
Epoch 8.51: Loss = 0.268097
Epoch 8.52: Loss = 0.262573
Epoch 8.53: Loss = 0.325119
Epoch 8.54: Loss = 0.48172
Epoch 8.55: Loss = 0.36441
Epoch 8.56: Loss = 0.353287
Epoch 8.57: Loss = 0.359818
Epoch 8.58: Loss = 0.338196
Epoch 8.59: Loss = 0.392624
Epoch 8.60: Loss = 0.483658
Epoch 8.61: Loss = 0.367447
Epoch 8.62: Loss = 0.424103
Epoch 8.63: Loss = 0.51387
Epoch 8.64: Loss = 0.456116
Epoch 8.65: Loss = 0.521286
Epoch 8.66: Loss = 0.363785
Epoch 8.67: Loss = 0.337067
Epoch 8.68: Loss = 0.202637
Epoch 8.69: Loss = 0.293686
Epoch 8.70: Loss = 0.46199
Epoch 8.71: Loss = 0.326981
Epoch 8.72: Loss = 0.251099
Epoch 8.73: Loss = 0.341812
Epoch 8.74: Loss = 0.257767
Epoch 8.75: Loss = 0.569016
Epoch 8.76: Loss = 0.398972
Epoch 8.77: Loss = 0.266998
Epoch 8.78: Loss = 0.34346
Epoch 8.79: Loss = 0.490051
Epoch 8.80: Loss = 0.370102
Epoch 8.81: Loss = 0.32283
Epoch 8.82: Loss = 0.278061
Epoch 8.83: Loss = 0.425034
Epoch 8.84: Loss = 0.330719
Epoch 8.85: Loss = 0.513885
Epoch 8.86: Loss = 0.486328
Epoch 8.87: Loss = 0.268585
Epoch 8.88: Loss = 0.354019
Epoch 8.89: Loss = 0.434143
Epoch 8.90: Loss = 0.26741
Epoch 8.91: Loss = 0.438583
Epoch 8.92: Loss = 0.447098
Epoch 8.93: Loss = 0.504227
Epoch 8.94: Loss = 0.253891
Epoch 8.95: Loss = 0.373032
Epoch 8.96: Loss = 0.40062
Epoch 8.97: Loss = 0.28714
Epoch 8.98: Loss = 0.319092
Epoch 8.99: Loss = 0.400757
Epoch 8.100: Loss = 0.531982
Epoch 8.101: Loss = 0.556305
Epoch 8.102: Loss = 0.350891
Epoch 8.103: Loss = 0.296112
Epoch 8.104: Loss = 0.298004
Epoch 8.105: Loss = 0.428192
Epoch 8.106: Loss = 0.483124
Epoch 8.107: Loss = 0.24704
Epoch 8.108: Loss = 0.417221
Epoch 8.109: Loss = 0.346481
Epoch 8.110: Loss = 0.369797
Epoch 8.111: Loss = 0.278168
Epoch 8.112: Loss = 0.283447
Epoch 8.113: Loss = 0.328491
Epoch 8.114: Loss = 0.255768
Epoch 8.115: Loss = 0.259399
Epoch 8.116: Loss = 0.336411
Epoch 8.117: Loss = 0.176804
Epoch 8.118: Loss = 0.173996
Epoch 8.119: Loss = 0.242798
Epoch 8.120: Loss = 0.335037
TRAIN LOSS = 0.364777
TRAIN ACC = 90.4572 % (54277/60000)
Loss = 0.333969
Loss = 0.427719
Loss = 0.484177
Loss = 0.533859
Loss = 0.543472
Loss = 0.331543
Loss = 0.308716
Loss = 0.596161
Loss = 0.512054
Loss = 0.44873
Loss = 0.123932
Loss = 0.263626
Loss = 0.302719
Loss = 0.32341
Loss = 0.161682
Loss = 0.218796
Loss = 0.183273
Loss = 0.0411987
Loss = 0.185379
Loss = 0.496124
TEST LOSS = 0.341027
TEST ACC = 542.769 % (9107/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.355515
Epoch 9.2: Loss = 0.439743
Epoch 9.3: Loss = 0.453201
Epoch 9.4: Loss = 0.296555
Epoch 9.5: Loss = 0.29924
Epoch 9.6: Loss = 0.314987
Epoch 9.7: Loss = 0.301834
Epoch 9.8: Loss = 0.331696
Epoch 9.9: Loss = 0.338928
Epoch 9.10: Loss = 0.333481
Epoch 9.11: Loss = 0.370926
Epoch 9.12: Loss = 0.322189
Epoch 9.13: Loss = 0.247528
Epoch 9.14: Loss = 0.310303
Epoch 9.15: Loss = 0.368546
Epoch 9.16: Loss = 0.372345
Epoch 9.17: Loss = 0.434601
Epoch 9.18: Loss = 0.61026
Epoch 9.19: Loss = 0.404099
Epoch 9.20: Loss = 0.290436
Epoch 9.21: Loss = 0.278564
Epoch 9.22: Loss = 0.253723
Epoch 9.23: Loss = 0.266312
Epoch 9.24: Loss = 0.516708
Epoch 9.25: Loss = 0.364731
Epoch 9.26: Loss = 0.551437
Epoch 9.27: Loss = 0.459991
Epoch 9.28: Loss = 0.412018
Epoch 9.29: Loss = 0.486206
Epoch 9.30: Loss = 0.494812
Epoch 9.31: Loss = 0.33725
Epoch 9.32: Loss = 0.399002
Epoch 9.33: Loss = 0.318954
Epoch 9.34: Loss = 0.384445
Epoch 9.35: Loss = 0.361237
Epoch 9.36: Loss = 0.409698
Epoch 9.37: Loss = 0.225098
Epoch 9.38: Loss = 0.306778
Epoch 9.39: Loss = 0.31752
Epoch 9.40: Loss = 0.296936
Epoch 9.41: Loss = 0.333237
Epoch 9.42: Loss = 0.568298
Epoch 9.43: Loss = 0.318634
Epoch 9.44: Loss = 0.278687
Epoch 9.45: Loss = 0.313583
Epoch 9.46: Loss = 0.393234
Epoch 9.47: Loss = 0.337662
Epoch 9.48: Loss = 0.380402
Epoch 9.49: Loss = 0.346146
Epoch 9.50: Loss = 0.476227
Epoch 9.51: Loss = 0.268402
Epoch 9.52: Loss = 0.262161
Epoch 9.53: Loss = 0.324066
Epoch 9.54: Loss = 0.487152
Epoch 9.55: Loss = 0.351624
Epoch 9.56: Loss = 0.35495
Epoch 9.57: Loss = 0.353287
Epoch 9.58: Loss = 0.327667
Epoch 9.59: Loss = 0.386734
Epoch 9.60: Loss = 0.485001
Epoch 9.61: Loss = 0.362411
Epoch 9.62: Loss = 0.421646
Epoch 9.63: Loss = 0.519806
Epoch 9.64: Loss = 0.447449
Epoch 9.65: Loss = 0.484528
Epoch 9.66: Loss = 0.353714
Epoch 9.67: Loss = 0.325714
Epoch 9.68: Loss = 0.206894
Epoch 9.69: Loss = 0.296341
Epoch 9.70: Loss = 0.447784
Epoch 9.71: Loss = 0.324509
Epoch 9.72: Loss = 0.237549
Epoch 9.73: Loss = 0.333893
Epoch 9.74: Loss = 0.258102
Epoch 9.75: Loss = 0.565079
Epoch 9.76: Loss = 0.405197
Epoch 9.77: Loss = 0.260727
Epoch 9.78: Loss = 0.344574
Epoch 9.79: Loss = 0.481979
Epoch 9.80: Loss = 0.349106
Epoch 9.81: Loss = 0.313141
Epoch 9.82: Loss = 0.265427
Epoch 9.83: Loss = 0.420624
Epoch 9.84: Loss = 0.330994
Epoch 9.85: Loss = 0.496002
Epoch 9.86: Loss = 0.480042
Epoch 9.87: Loss = 0.256302
Epoch 9.88: Loss = 0.346603
Epoch 9.89: Loss = 0.434021
Epoch 9.90: Loss = 0.264328
Epoch 9.91: Loss = 0.42981
Epoch 9.92: Loss = 0.429047
Epoch 9.93: Loss = 0.517349
Epoch 9.94: Loss = 0.249481
Epoch 9.95: Loss = 0.384781
Epoch 9.96: Loss = 0.410812
Epoch 9.97: Loss = 0.294281
Epoch 9.98: Loss = 0.315384
Epoch 9.99: Loss = 0.388046
Epoch 9.100: Loss = 0.550995
Epoch 9.101: Loss = 0.569366
Epoch 9.102: Loss = 0.356659
Epoch 9.103: Loss = 0.293747
Epoch 9.104: Loss = 0.296875
Epoch 9.105: Loss = 0.430313
Epoch 9.106: Loss = 0.497086
Epoch 9.107: Loss = 0.243057
Epoch 9.108: Loss = 0.421219
Epoch 9.109: Loss = 0.335388
Epoch 9.110: Loss = 0.362259
Epoch 9.111: Loss = 0.277802
Epoch 9.112: Loss = 0.287338
Epoch 9.113: Loss = 0.318665
Epoch 9.114: Loss = 0.273544
Epoch 9.115: Loss = 0.23468
Epoch 9.116: Loss = 0.328384
Epoch 9.117: Loss = 0.163101
Epoch 9.118: Loss = 0.163666
Epoch 9.119: Loss = 0.236465
Epoch 9.120: Loss = 0.324585
TRAIN LOSS = 0.361694
TRAIN ACC = 90.7318 % (54441/60000)
Loss = 0.314117
Loss = 0.423386
Loss = 0.48912
Loss = 0.516357
Loss = 0.544846
Loss = 0.32193
Loss = 0.305161
Loss = 0.599655
Loss = 0.504776
Loss = 0.460556
Loss = 0.129318
Loss = 0.244278
Loss = 0.310944
Loss = 0.326935
Loss = 0.145508
Loss = 0.219757
Loss = 0.182739
Loss = 0.0406952
Loss = 0.186935
Loss = 0.484665
TEST LOSS = 0.337584
TEST ACC = 544.409 % (9134/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.351181
Epoch 10.2: Loss = 0.430206
Epoch 10.3: Loss = 0.454346
Epoch 10.4: Loss = 0.294067
Epoch 10.5: Loss = 0.28447
Epoch 10.6: Loss = 0.33429
Epoch 10.7: Loss = 0.290192
Epoch 10.8: Loss = 0.324371
Epoch 10.9: Loss = 0.337372
Epoch 10.10: Loss = 0.324127
Epoch 10.11: Loss = 0.375595
Epoch 10.12: Loss = 0.31868
Epoch 10.13: Loss = 0.239792
Epoch 10.14: Loss = 0.301208
Epoch 10.15: Loss = 0.360062
Epoch 10.16: Loss = 0.363373
Epoch 10.17: Loss = 0.414078
Epoch 10.18: Loss = 0.582947
Epoch 10.19: Loss = 0.397598
Epoch 10.20: Loss = 0.282379
Epoch 10.21: Loss = 0.283936
Epoch 10.22: Loss = 0.251617
Epoch 10.23: Loss = 0.264389
Epoch 10.24: Loss = 0.519806
Epoch 10.25: Loss = 0.362961
Epoch 10.26: Loss = 0.545303
Epoch 10.27: Loss = 0.45256
Epoch 10.28: Loss = 0.407837
Epoch 10.29: Loss = 0.486084
Epoch 10.30: Loss = 0.500626
Epoch 10.31: Loss = 0.335968
Epoch 10.32: Loss = 0.398285
Epoch 10.33: Loss = 0.308838
Epoch 10.34: Loss = 0.396683
Epoch 10.35: Loss = 0.357071
Epoch 10.36: Loss = 0.4198
Epoch 10.37: Loss = 0.222946
Epoch 10.38: Loss = 0.301483
Epoch 10.39: Loss = 0.317596
Epoch 10.40: Loss = 0.300674
Epoch 10.41: Loss = 0.324158
Epoch 10.42: Loss = 0.56694
Epoch 10.43: Loss = 0.318726
Epoch 10.44: Loss = 0.280777
Epoch 10.45: Loss = 0.317719
Epoch 10.46: Loss = 0.400757
Epoch 10.47: Loss = 0.335754
Epoch 10.48: Loss = 0.390259
Epoch 10.49: Loss = 0.336731
Epoch 10.50: Loss = 0.480637
Epoch 10.51: Loss = 0.273819
Epoch 10.52: Loss = 0.264893
Epoch 10.53: Loss = 0.317734
Epoch 10.54: Loss = 0.472565
Epoch 10.55: Loss = 0.356934
Epoch 10.56: Loss = 0.349899
Epoch 10.57: Loss = 0.345993
Epoch 10.58: Loss = 0.327515
Epoch 10.59: Loss = 0.386063
Epoch 10.60: Loss = 0.464111
Epoch 10.61: Loss = 0.339508
Epoch 10.62: Loss = 0.419464
Epoch 10.63: Loss = 0.510956
Epoch 10.64: Loss = 0.438675
Epoch 10.65: Loss = 0.500458
Epoch 10.66: Loss = 0.368134
Epoch 10.67: Loss = 0.314255
Epoch 10.68: Loss = 0.20192
Epoch 10.69: Loss = 0.289978
Epoch 10.70: Loss = 0.431458
Epoch 10.71: Loss = 0.331589
Epoch 10.72: Loss = 0.243134
Epoch 10.73: Loss = 0.334946
Epoch 10.74: Loss = 0.261292
Epoch 10.75: Loss = 0.585541
Epoch 10.76: Loss = 0.408981
Epoch 10.77: Loss = 0.269241
Epoch 10.78: Loss = 0.343994
Epoch 10.79: Loss = 0.480026
Epoch 10.80: Loss = 0.323685
Epoch 10.81: Loss = 0.308517
Epoch 10.82: Loss = 0.246811
Epoch 10.83: Loss = 0.414444
Epoch 10.84: Loss = 0.321335
Epoch 10.85: Loss = 0.481796
Epoch 10.86: Loss = 0.473907
Epoch 10.87: Loss = 0.239456
Epoch 10.88: Loss = 0.338669
Epoch 10.89: Loss = 0.446854
Epoch 10.90: Loss = 0.270645
Epoch 10.91: Loss = 0.423767
Epoch 10.92: Loss = 0.440994
Epoch 10.93: Loss = 0.512772
Epoch 10.94: Loss = 0.235229
Epoch 10.95: Loss = 0.363174
Epoch 10.96: Loss = 0.402222
Epoch 10.97: Loss = 0.276199
Epoch 10.98: Loss = 0.30481
Epoch 10.99: Loss = 0.389618
Epoch 10.100: Loss = 0.549728
Epoch 10.101: Loss = 0.561646
Epoch 10.102: Loss = 0.337448
Epoch 10.103: Loss = 0.287445
Epoch 10.104: Loss = 0.304993
Epoch 10.105: Loss = 0.429489
Epoch 10.106: Loss = 0.501404
Epoch 10.107: Loss = 0.243652
Epoch 10.108: Loss = 0.403168
Epoch 10.109: Loss = 0.32431
Epoch 10.110: Loss = 0.353439
Epoch 10.111: Loss = 0.284042
Epoch 10.112: Loss = 0.284836
Epoch 10.113: Loss = 0.304245
Epoch 10.114: Loss = 0.282761
Epoch 10.115: Loss = 0.220413
Epoch 10.116: Loss = 0.315659
Epoch 10.117: Loss = 0.156113
Epoch 10.118: Loss = 0.163208
Epoch 10.119: Loss = 0.224228
Epoch 10.120: Loss = 0.340775
TRAIN LOSS = 0.358078
TRAIN ACC = 90.8768 % (54529/60000)
Loss = 0.322891
Loss = 0.430161
Loss = 0.476578
Loss = 0.506424
Loss = 0.555328
Loss = 0.322861
Loss = 0.308075
Loss = 0.607468
Loss = 0.499039
Loss = 0.464249
Loss = 0.127502
Loss = 0.245209
Loss = 0.287094
Loss = 0.314468
Loss = 0.146469
Loss = 0.207733
Loss = 0.181595
Loss = 0.0404205
Loss = 0.198685
Loss = 0.489578
TEST LOSS = 0.336591
TEST ACC = 545.29 % (9147/10000)
