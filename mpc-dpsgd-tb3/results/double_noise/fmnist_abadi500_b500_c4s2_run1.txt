Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.42001
Epoch 1.2: Loss = 2.31133
Epoch 1.3: Loss = 2.25493
Epoch 1.4: Loss = 2.13628
Epoch 1.5: Loss = 2.09262
Epoch 1.6: Loss = 2.02841
Epoch 1.7: Loss = 1.96477
Epoch 1.8: Loss = 1.88063
Epoch 1.9: Loss = 1.8159
Epoch 1.10: Loss = 1.73866
Epoch 1.11: Loss = 1.73576
Epoch 1.12: Loss = 1.67854
Epoch 1.13: Loss = 1.61601
Epoch 1.14: Loss = 1.58821
Epoch 1.15: Loss = 1.52414
Epoch 1.16: Loss = 1.54196
Epoch 1.17: Loss = 1.49608
Epoch 1.18: Loss = 1.44461
Epoch 1.19: Loss = 1.41936
Epoch 1.20: Loss = 1.43185
Epoch 1.21: Loss = 1.38281
Epoch 1.22: Loss = 1.3438
Epoch 1.23: Loss = 1.30614
Epoch 1.24: Loss = 1.37447
Epoch 1.25: Loss = 1.3026
Epoch 1.26: Loss = 1.24318
Epoch 1.27: Loss = 1.22374
Epoch 1.28: Loss = 1.22949
Epoch 1.29: Loss = 1.19414
Epoch 1.30: Loss = 1.1671
Epoch 1.31: Loss = 1.20184
Epoch 1.32: Loss = 1.18263
Epoch 1.33: Loss = 1.06531
Epoch 1.34: Loss = 1.14169
Epoch 1.35: Loss = 1.19905
Epoch 1.36: Loss = 1.16927
Epoch 1.37: Loss = 1.11855
Epoch 1.38: Loss = 1.08244
Epoch 1.39: Loss = 1.05025
Epoch 1.40: Loss = 1.05717
Epoch 1.41: Loss = 1.0932
Epoch 1.42: Loss = 1.02829
Epoch 1.43: Loss = 0.994324
Epoch 1.44: Loss = 0.969711
Epoch 1.45: Loss = 1.0208
Epoch 1.46: Loss = 1.00583
Epoch 1.47: Loss = 0.973495
Epoch 1.48: Loss = 0.937805
Epoch 1.49: Loss = 0.984665
Epoch 1.50: Loss = 0.944962
Epoch 1.51: Loss = 0.913086
Epoch 1.52: Loss = 0.98587
Epoch 1.53: Loss = 0.983307
Epoch 1.54: Loss = 0.852737
Epoch 1.55: Loss = 0.945953
Epoch 1.56: Loss = 0.95546
Epoch 1.57: Loss = 0.952011
Epoch 1.58: Loss = 0.922211
Epoch 1.59: Loss = 0.91713
Epoch 1.60: Loss = 0.92865
Epoch 1.61: Loss = 0.838211
Epoch 1.62: Loss = 0.942535
Epoch 1.63: Loss = 0.80127
Epoch 1.64: Loss = 0.825302
Epoch 1.65: Loss = 0.845123
Epoch 1.66: Loss = 0.86824
Epoch 1.67: Loss = 0.79744
Epoch 1.68: Loss = 0.908737
Epoch 1.69: Loss = 0.860474
Epoch 1.70: Loss = 0.843414
Epoch 1.71: Loss = 0.777771
Epoch 1.72: Loss = 0.803589
Epoch 1.73: Loss = 0.89035
Epoch 1.74: Loss = 0.869476
Epoch 1.75: Loss = 0.807053
Epoch 1.76: Loss = 0.822418
Epoch 1.77: Loss = 0.795013
Epoch 1.78: Loss = 0.811066
Epoch 1.79: Loss = 0.737335
Epoch 1.80: Loss = 0.785568
Epoch 1.81: Loss = 0.758575
Epoch 1.82: Loss = 0.7854
Epoch 1.83: Loss = 0.817947
Epoch 1.84: Loss = 0.794235
Epoch 1.85: Loss = 0.758347
Epoch 1.86: Loss = 0.840942
Epoch 1.87: Loss = 0.821106
Epoch 1.88: Loss = 0.708237
Epoch 1.89: Loss = 0.831894
Epoch 1.90: Loss = 0.768829
Epoch 1.91: Loss = 0.812103
Epoch 1.92: Loss = 0.796188
Epoch 1.93: Loss = 0.807404
Epoch 1.94: Loss = 0.75264
Epoch 1.95: Loss = 0.79599
Epoch 1.96: Loss = 0.745789
Epoch 1.97: Loss = 0.657425
Epoch 1.98: Loss = 0.764114
Epoch 1.99: Loss = 0.752701
Epoch 1.100: Loss = 0.737534
Epoch 1.101: Loss = 0.792999
Epoch 1.102: Loss = 0.775909
Epoch 1.103: Loss = 0.777557
Epoch 1.104: Loss = 0.729507
Epoch 1.105: Loss = 0.680969
Epoch 1.106: Loss = 0.839615
Epoch 1.107: Loss = 0.772171
Epoch 1.108: Loss = 0.768829
Epoch 1.109: Loss = 0.749924
Epoch 1.110: Loss = 0.767456
Epoch 1.111: Loss = 0.685883
Epoch 1.112: Loss = 0.688446
Epoch 1.113: Loss = 0.73526
Epoch 1.114: Loss = 0.751083
Epoch 1.115: Loss = 0.733627
Epoch 1.116: Loss = 0.661499
Epoch 1.117: Loss = 0.787415
Epoch 1.118: Loss = 0.684311
Epoch 1.119: Loss = 0.706818
Epoch 1.120: Loss = 0.70433
TRAIN LOSS = 1.05333
TRAIN ACC = 65.4068 % (39246/60000)
Loss = 0.669205
Loss = 0.772995
Loss = 0.751816
Loss = 0.691711
Loss = 0.677155
Loss = 0.827911
Loss = 0.841782
Loss = 0.794876
Loss = 0.721405
Loss = 0.692337
Loss = 0.788773
Loss = 0.751862
Loss = 0.749908
Loss = 0.766327
Loss = 0.732819
Loss = 0.788818
Loss = 0.701843
Loss = 0.739944
Loss = 0.806091
Loss = 0.735321
TEST LOSS = 0.750145
TEST ACC = 392.459 % (7412/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.725372
Epoch 2.2: Loss = 0.704269
Epoch 2.3: Loss = 0.776642
Epoch 2.4: Loss = 0.660034
Epoch 2.5: Loss = 0.721909
Epoch 2.6: Loss = 0.78743
Epoch 2.7: Loss = 0.722549
Epoch 2.8: Loss = 0.786865
Epoch 2.9: Loss = 0.629074
Epoch 2.10: Loss = 0.583496
Epoch 2.11: Loss = 0.7883
Epoch 2.12: Loss = 0.726791
Epoch 2.13: Loss = 0.707581
Epoch 2.14: Loss = 0.696701
Epoch 2.15: Loss = 0.70726
Epoch 2.16: Loss = 0.742798
Epoch 2.17: Loss = 0.688599
Epoch 2.18: Loss = 0.714539
Epoch 2.19: Loss = 0.695007
Epoch 2.20: Loss = 0.781311
Epoch 2.21: Loss = 0.674393
Epoch 2.22: Loss = 0.620697
Epoch 2.23: Loss = 0.698441
Epoch 2.24: Loss = 0.801788
Epoch 2.25: Loss = 0.700562
Epoch 2.26: Loss = 0.633972
Epoch 2.27: Loss = 0.679764
Epoch 2.28: Loss = 0.714935
Epoch 2.29: Loss = 0.694519
Epoch 2.30: Loss = 0.672363
Epoch 2.31: Loss = 0.733856
Epoch 2.32: Loss = 0.676041
Epoch 2.33: Loss = 0.596313
Epoch 2.34: Loss = 0.723572
Epoch 2.35: Loss = 0.73349
Epoch 2.36: Loss = 0.737015
Epoch 2.37: Loss = 0.720261
Epoch 2.38: Loss = 0.67897
Epoch 2.39: Loss = 0.74588
Epoch 2.40: Loss = 0.700699
Epoch 2.41: Loss = 0.734146
Epoch 2.42: Loss = 0.704987
Epoch 2.43: Loss = 0.664139
Epoch 2.44: Loss = 0.623108
Epoch 2.45: Loss = 0.708221
Epoch 2.46: Loss = 0.755875
Epoch 2.47: Loss = 0.651047
Epoch 2.48: Loss = 0.628235
Epoch 2.49: Loss = 0.722107
Epoch 2.50: Loss = 0.684967
Epoch 2.51: Loss = 0.600296
Epoch 2.52: Loss = 0.709076
Epoch 2.53: Loss = 0.762817
Epoch 2.54: Loss = 0.579773
Epoch 2.55: Loss = 0.694138
Epoch 2.56: Loss = 0.72789
Epoch 2.57: Loss = 0.739441
Epoch 2.58: Loss = 0.695877
Epoch 2.59: Loss = 0.699951
Epoch 2.60: Loss = 0.690994
Epoch 2.61: Loss = 0.616486
Epoch 2.62: Loss = 0.731644
Epoch 2.63: Loss = 0.588333
Epoch 2.64: Loss = 0.600891
Epoch 2.65: Loss = 0.667252
Epoch 2.66: Loss = 0.666992
Epoch 2.67: Loss = 0.631119
Epoch 2.68: Loss = 0.748871
Epoch 2.69: Loss = 0.691086
Epoch 2.70: Loss = 0.694839
Epoch 2.71: Loss = 0.596741
Epoch 2.72: Loss = 0.657822
Epoch 2.73: Loss = 0.744263
Epoch 2.74: Loss = 0.719162
Epoch 2.75: Loss = 0.621704
Epoch 2.76: Loss = 0.649475
Epoch 2.77: Loss = 0.645401
Epoch 2.78: Loss = 0.678391
Epoch 2.79: Loss = 0.626892
Epoch 2.80: Loss = 0.619965
Epoch 2.81: Loss = 0.615219
Epoch 2.82: Loss = 0.608078
Epoch 2.83: Loss = 0.680603
Epoch 2.84: Loss = 0.634888
Epoch 2.85: Loss = 0.64357
Epoch 2.86: Loss = 0.702637
Epoch 2.87: Loss = 0.670776
Epoch 2.88: Loss = 0.594345
Epoch 2.89: Loss = 0.723206
Epoch 2.90: Loss = 0.65657
Epoch 2.91: Loss = 0.712952
Epoch 2.92: Loss = 0.679901
Epoch 2.93: Loss = 0.704819
Epoch 2.94: Loss = 0.635651
Epoch 2.95: Loss = 0.679932
Epoch 2.96: Loss = 0.641327
Epoch 2.97: Loss = 0.549927
Epoch 2.98: Loss = 0.648315
Epoch 2.99: Loss = 0.654373
Epoch 2.100: Loss = 0.636108
Epoch 2.101: Loss = 0.703827
Epoch 2.102: Loss = 0.665863
Epoch 2.103: Loss = 0.6642
Epoch 2.104: Loss = 0.616547
Epoch 2.105: Loss = 0.586685
Epoch 2.106: Loss = 0.748795
Epoch 2.107: Loss = 0.707489
Epoch 2.108: Loss = 0.696243
Epoch 2.109: Loss = 0.680084
Epoch 2.110: Loss = 0.675201
Epoch 2.111: Loss = 0.596588
Epoch 2.112: Loss = 0.609711
Epoch 2.113: Loss = 0.645844
Epoch 2.114: Loss = 0.667542
Epoch 2.115: Loss = 0.650284
Epoch 2.116: Loss = 0.587387
Epoch 2.117: Loss = 0.702682
Epoch 2.118: Loss = 0.598892
Epoch 2.119: Loss = 0.635223
Epoch 2.120: Loss = 0.634674
TRAIN LOSS = 0.678391
TRAIN ACC = 76.9119 % (46150/60000)
Loss = 0.592789
Loss = 0.716156
Loss = 0.65799
Loss = 0.601166
Loss = 0.601242
Loss = 0.771851
Loss = 0.781082
Loss = 0.740967
Loss = 0.660477
Loss = 0.612885
Loss = 0.73761
Loss = 0.698364
Loss = 0.661697
Loss = 0.695175
Loss = 0.659424
Loss = 0.715134
Loss = 0.629745
Loss = 0.681244
Loss = 0.741791
Loss = 0.672897
TEST LOSS = 0.681484
TEST ACC = 461.499 % (7702/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.659302
Epoch 3.2: Loss = 0.655823
Epoch 3.3: Loss = 0.694534
Epoch 3.4: Loss = 0.587051
Epoch 3.5: Loss = 0.641632
Epoch 3.6: Loss = 0.733551
Epoch 3.7: Loss = 0.662979
Epoch 3.8: Loss = 0.744598
Epoch 3.9: Loss = 0.531189
Epoch 3.10: Loss = 0.512161
Epoch 3.11: Loss = 0.738922
Epoch 3.12: Loss = 0.663025
Epoch 3.13: Loss = 0.675858
Epoch 3.14: Loss = 0.638977
Epoch 3.15: Loss = 0.661346
Epoch 3.16: Loss = 0.698044
Epoch 3.17: Loss = 0.613022
Epoch 3.18: Loss = 0.656586
Epoch 3.19: Loss = 0.632904
Epoch 3.20: Loss = 0.73288
Epoch 3.21: Loss = 0.597717
Epoch 3.22: Loss = 0.534851
Epoch 3.23: Loss = 0.654846
Epoch 3.24: Loss = 0.761719
Epoch 3.25: Loss = 0.643158
Epoch 3.26: Loss = 0.573578
Epoch 3.27: Loss = 0.649918
Epoch 3.28: Loss = 0.661713
Epoch 3.29: Loss = 0.64856
Epoch 3.30: Loss = 0.634094
Epoch 3.31: Loss = 0.709274
Epoch 3.32: Loss = 0.62204
Epoch 3.33: Loss = 0.544312
Epoch 3.34: Loss = 0.694061
Epoch 3.35: Loss = 0.684937
Epoch 3.36: Loss = 0.70546
Epoch 3.37: Loss = 0.692841
Epoch 3.38: Loss = 0.652054
Epoch 3.39: Loss = 0.71138
Epoch 3.40: Loss = 0.64003
Epoch 3.41: Loss = 0.689758
Epoch 3.42: Loss = 0.659409
Epoch 3.43: Loss = 0.632126
Epoch 3.44: Loss = 0.577698
Epoch 3.45: Loss = 0.687088
Epoch 3.46: Loss = 0.728683
Epoch 3.47: Loss = 0.614273
Epoch 3.48: Loss = 0.595016
Epoch 3.49: Loss = 0.683243
Epoch 3.50: Loss = 0.644592
Epoch 3.51: Loss = 0.550018
Epoch 3.52: Loss = 0.682388
Epoch 3.53: Loss = 0.730347
Epoch 3.54: Loss = 0.537689
Epoch 3.55: Loss = 0.666931
Epoch 3.56: Loss = 0.681488
Epoch 3.57: Loss = 0.704803
Epoch 3.58: Loss = 0.649445
Epoch 3.59: Loss = 0.668259
Epoch 3.60: Loss = 0.661545
Epoch 3.61: Loss = 0.584106
Epoch 3.62: Loss = 0.695816
Epoch 3.63: Loss = 0.558762
Epoch 3.64: Loss = 0.541855
Epoch 3.65: Loss = 0.625244
Epoch 3.66: Loss = 0.614517
Epoch 3.67: Loss = 0.599899
Epoch 3.68: Loss = 0.749054
Epoch 3.69: Loss = 0.649628
Epoch 3.70: Loss = 0.662598
Epoch 3.71: Loss = 0.564941
Epoch 3.72: Loss = 0.632889
Epoch 3.73: Loss = 0.730759
Epoch 3.74: Loss = 0.680817
Epoch 3.75: Loss = 0.597443
Epoch 3.76: Loss = 0.615845
Epoch 3.77: Loss = 0.623047
Epoch 3.78: Loss = 0.64119
Epoch 3.79: Loss = 0.60289
Epoch 3.80: Loss = 0.56691
Epoch 3.81: Loss = 0.5867
Epoch 3.82: Loss = 0.581512
Epoch 3.83: Loss = 0.677643
Epoch 3.84: Loss = 0.606918
Epoch 3.85: Loss = 0.622681
Epoch 3.86: Loss = 0.67189
Epoch 3.87: Loss = 0.6418
Epoch 3.88: Loss = 0.577698
Epoch 3.89: Loss = 0.727386
Epoch 3.90: Loss = 0.633041
Epoch 3.91: Loss = 0.685745
Epoch 3.92: Loss = 0.656876
Epoch 3.93: Loss = 0.670471
Epoch 3.94: Loss = 0.617264
Epoch 3.95: Loss = 0.648163
Epoch 3.96: Loss = 0.610428
Epoch 3.97: Loss = 0.514404
Epoch 3.98: Loss = 0.632828
Epoch 3.99: Loss = 0.627411
Epoch 3.100: Loss = 0.611343
Epoch 3.101: Loss = 0.696838
Epoch 3.102: Loss = 0.635849
Epoch 3.103: Loss = 0.638412
Epoch 3.104: Loss = 0.590118
Epoch 3.105: Loss = 0.562683
Epoch 3.106: Loss = 0.731018
Epoch 3.107: Loss = 0.688828
Epoch 3.108: Loss = 0.691162
Epoch 3.109: Loss = 0.669846
Epoch 3.110: Loss = 0.656479
Epoch 3.111: Loss = 0.585007
Epoch 3.112: Loss = 0.602341
Epoch 3.113: Loss = 0.614044
Epoch 3.114: Loss = 0.647858
Epoch 3.115: Loss = 0.617722
Epoch 3.116: Loss = 0.564758
Epoch 3.117: Loss = 0.69252
Epoch 3.118: Loss = 0.571411
Epoch 3.119: Loss = 0.620529
Epoch 3.120: Loss = 0.603394
TRAIN LOSS = 0.641815
TRAIN ACC = 78.7857 % (47274/60000)
Loss = 0.577881
Loss = 0.713516
Loss = 0.630112
Loss = 0.56842
Loss = 0.588562
Loss = 0.75441
Loss = 0.78717
Loss = 0.718964
Loss = 0.648849
Loss = 0.589813
Loss = 0.754395
Loss = 0.696167
Loss = 0.654221
Loss = 0.678558
Loss = 0.641312
Loss = 0.697327
Loss = 0.618057
Loss = 0.676025
Loss = 0.730377
Loss = 0.647842
TEST LOSS = 0.668599
TEST ACC = 472.739 % (7827/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.642059
Epoch 4.2: Loss = 0.631546
Epoch 4.3: Loss = 0.666962
Epoch 4.4: Loss = 0.571884
Epoch 4.5: Loss = 0.625473
Epoch 4.6: Loss = 0.70256
Epoch 4.7: Loss = 0.644714
Epoch 4.8: Loss = 0.738617
Epoch 4.9: Loss = 0.495544
Epoch 4.10: Loss = 0.497681
Epoch 4.11: Loss = 0.727142
Epoch 4.12: Loss = 0.655807
Epoch 4.13: Loss = 0.660385
Epoch 4.14: Loss = 0.615952
Epoch 4.15: Loss = 0.649017
Epoch 4.16: Loss = 0.685577
Epoch 4.17: Loss = 0.58461
Epoch 4.18: Loss = 0.648514
Epoch 4.19: Loss = 0.604141
Epoch 4.20: Loss = 0.712723
Epoch 4.21: Loss = 0.570068
Epoch 4.22: Loss = 0.503708
Epoch 4.23: Loss = 0.626724
Epoch 4.24: Loss = 0.74855
Epoch 4.25: Loss = 0.603775
Epoch 4.26: Loss = 0.545151
Epoch 4.27: Loss = 0.627853
Epoch 4.28: Loss = 0.640564
Epoch 4.29: Loss = 0.634171
Epoch 4.30: Loss = 0.625732
Epoch 4.31: Loss = 0.679367
Epoch 4.32: Loss = 0.599701
Epoch 4.33: Loss = 0.524063
Epoch 4.34: Loss = 0.671463
Epoch 4.35: Loss = 0.652725
Epoch 4.36: Loss = 0.680084
Epoch 4.37: Loss = 0.68071
Epoch 4.38: Loss = 0.638718
Epoch 4.39: Loss = 0.692749
Epoch 4.40: Loss = 0.610947
Epoch 4.41: Loss = 0.67424
Epoch 4.42: Loss = 0.63176
Epoch 4.43: Loss = 0.62764
Epoch 4.44: Loss = 0.546143
Epoch 4.45: Loss = 0.67041
Epoch 4.46: Loss = 0.727325
Epoch 4.47: Loss = 0.592758
Epoch 4.48: Loss = 0.565994
Epoch 4.49: Loss = 0.644165
Epoch 4.50: Loss = 0.633606
Epoch 4.51: Loss = 0.519455
Epoch 4.52: Loss = 0.66449
Epoch 4.53: Loss = 0.728745
Epoch 4.54: Loss = 0.513535
Epoch 4.55: Loss = 0.651337
Epoch 4.56: Loss = 0.663437
Epoch 4.57: Loss = 0.693314
Epoch 4.58: Loss = 0.622925
Epoch 4.59: Loss = 0.66011
Epoch 4.60: Loss = 0.628143
Epoch 4.61: Loss = 0.563171
Epoch 4.62: Loss = 0.664597
Epoch 4.63: Loss = 0.547699
Epoch 4.64: Loss = 0.519623
Epoch 4.65: Loss = 0.623245
Epoch 4.66: Loss = 0.586685
Epoch 4.67: Loss = 0.594223
Epoch 4.68: Loss = 0.762207
Epoch 4.69: Loss = 0.631699
Epoch 4.70: Loss = 0.63623
Epoch 4.71: Loss = 0.539978
Epoch 4.72: Loss = 0.620422
Epoch 4.73: Loss = 0.711731
Epoch 4.74: Loss = 0.660492
Epoch 4.75: Loss = 0.577774
Epoch 4.76: Loss = 0.610992
Epoch 4.77: Loss = 0.61824
Epoch 4.78: Loss = 0.634308
Epoch 4.79: Loss = 0.586884
Epoch 4.80: Loss = 0.554962
Epoch 4.81: Loss = 0.566452
Epoch 4.82: Loss = 0.57814
Epoch 4.83: Loss = 0.654877
Epoch 4.84: Loss = 0.588776
Epoch 4.85: Loss = 0.595154
Epoch 4.86: Loss = 0.656845
Epoch 4.87: Loss = 0.610672
Epoch 4.88: Loss = 0.566574
Epoch 4.89: Loss = 0.699799
Epoch 4.90: Loss = 0.636475
Epoch 4.91: Loss = 0.668488
Epoch 4.92: Loss = 0.639175
Epoch 4.93: Loss = 0.657822
Epoch 4.94: Loss = 0.608887
Epoch 4.95: Loss = 0.637375
Epoch 4.96: Loss = 0.605164
Epoch 4.97: Loss = 0.50499
Epoch 4.98: Loss = 0.60936
Epoch 4.99: Loss = 0.623444
Epoch 4.100: Loss = 0.6026
Epoch 4.101: Loss = 0.683609
Epoch 4.102: Loss = 0.64386
Epoch 4.103: Loss = 0.61972
Epoch 4.104: Loss = 0.564117
Epoch 4.105: Loss = 0.545792
Epoch 4.106: Loss = 0.710342
Epoch 4.107: Loss = 0.656982
Epoch 4.108: Loss = 0.703384
Epoch 4.109: Loss = 0.675919
Epoch 4.110: Loss = 0.657944
Epoch 4.111: Loss = 0.578232
Epoch 4.112: Loss = 0.592346
Epoch 4.113: Loss = 0.603012
Epoch 4.114: Loss = 0.652283
Epoch 4.115: Loss = 0.599655
Epoch 4.116: Loss = 0.56813
Epoch 4.117: Loss = 0.68013
Epoch 4.118: Loss = 0.560318
Epoch 4.119: Loss = 0.621368
Epoch 4.120: Loss = 0.598343
TRAIN LOSS = 0.625427
TRAIN ACC = 79.8309 % (47901/60000)
Loss = 0.562698
Loss = 0.705276
Loss = 0.604675
Loss = 0.554886
Loss = 0.587555
Loss = 0.744202
Loss = 0.778564
Loss = 0.707596
Loss = 0.637985
Loss = 0.582657
Loss = 0.758377
Loss = 0.695557
Loss = 0.650803
Loss = 0.658127
Loss = 0.630219
Loss = 0.684448
Loss = 0.606186
Loss = 0.684296
Loss = 0.720139
Loss = 0.647568
TEST LOSS = 0.66009
TEST ACC = 479.008 % (7943/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.619125
Epoch 5.2: Loss = 0.631241
Epoch 5.3: Loss = 0.676682
Epoch 5.4: Loss = 0.568436
Epoch 5.5: Loss = 0.616837
Epoch 5.6: Loss = 0.702179
Epoch 5.7: Loss = 0.62912
Epoch 5.8: Loss = 0.740555
Epoch 5.9: Loss = 0.490677
Epoch 5.10: Loss = 0.483276
Epoch 5.11: Loss = 0.72467
Epoch 5.12: Loss = 0.636383
Epoch 5.13: Loss = 0.654572
Epoch 5.14: Loss = 0.625961
Epoch 5.15: Loss = 0.646393
Epoch 5.16: Loss = 0.697159
Epoch 5.17: Loss = 0.570267
Epoch 5.18: Loss = 0.651215
Epoch 5.19: Loss = 0.593964
Epoch 5.20: Loss = 0.713882
Epoch 5.21: Loss = 0.556641
Epoch 5.22: Loss = 0.482864
Epoch 5.23: Loss = 0.628586
Epoch 5.24: Loss = 0.736145
Epoch 5.25: Loss = 0.599869
Epoch 5.26: Loss = 0.52681
Epoch 5.27: Loss = 0.614502
Epoch 5.28: Loss = 0.629288
Epoch 5.29: Loss = 0.617889
Epoch 5.30: Loss = 0.620071
Epoch 5.31: Loss = 0.683273
Epoch 5.32: Loss = 0.59346
Epoch 5.33: Loss = 0.505997
Epoch 5.34: Loss = 0.678772
Epoch 5.35: Loss = 0.665619
Epoch 5.36: Loss = 0.667206
Epoch 5.37: Loss = 0.669754
Epoch 5.38: Loss = 0.629944
Epoch 5.39: Loss = 0.683441
Epoch 5.40: Loss = 0.604233
Epoch 5.41: Loss = 0.66478
Epoch 5.42: Loss = 0.616806
Epoch 5.43: Loss = 0.616669
Epoch 5.44: Loss = 0.532471
Epoch 5.45: Loss = 0.654236
Epoch 5.46: Loss = 0.719315
Epoch 5.47: Loss = 0.577911
Epoch 5.48: Loss = 0.543884
Epoch 5.49: Loss = 0.622437
Epoch 5.50: Loss = 0.62381
Epoch 5.51: Loss = 0.504074
Epoch 5.52: Loss = 0.672745
Epoch 5.53: Loss = 0.704071
Epoch 5.54: Loss = 0.501785
Epoch 5.55: Loss = 0.633453
Epoch 5.56: Loss = 0.666397
Epoch 5.57: Loss = 0.685959
Epoch 5.58: Loss = 0.609756
Epoch 5.59: Loss = 0.666809
Epoch 5.60: Loss = 0.613754
Epoch 5.61: Loss = 0.574356
Epoch 5.62: Loss = 0.667267
Epoch 5.63: Loss = 0.534775
Epoch 5.64: Loss = 0.513641
Epoch 5.65: Loss = 0.612656
Epoch 5.66: Loss = 0.568954
Epoch 5.67: Loss = 0.5858
Epoch 5.68: Loss = 0.7742
Epoch 5.69: Loss = 0.604385
Epoch 5.70: Loss = 0.625473
Epoch 5.71: Loss = 0.531189
Epoch 5.72: Loss = 0.615509
Epoch 5.73: Loss = 0.686142
Epoch 5.74: Loss = 0.649582
Epoch 5.75: Loss = 0.5616
Epoch 5.76: Loss = 0.597702
Epoch 5.77: Loss = 0.611038
Epoch 5.78: Loss = 0.615906
Epoch 5.79: Loss = 0.578262
Epoch 5.80: Loss = 0.54538
Epoch 5.81: Loss = 0.567032
Epoch 5.82: Loss = 0.578613
Epoch 5.83: Loss = 0.64769
Epoch 5.84: Loss = 0.571045
Epoch 5.85: Loss = 0.591431
Epoch 5.86: Loss = 0.664688
Epoch 5.87: Loss = 0.600037
Epoch 5.88: Loss = 0.553741
Epoch 5.89: Loss = 0.700302
Epoch 5.90: Loss = 0.618652
Epoch 5.91: Loss = 0.670044
Epoch 5.92: Loss = 0.627029
Epoch 5.93: Loss = 0.646423
Epoch 5.94: Loss = 0.609741
Epoch 5.95: Loss = 0.637329
Epoch 5.96: Loss = 0.591583
Epoch 5.97: Loss = 0.497055
Epoch 5.98: Loss = 0.588623
Epoch 5.99: Loss = 0.616867
Epoch 5.100: Loss = 0.605957
Epoch 5.101: Loss = 0.668076
Epoch 5.102: Loss = 0.646194
Epoch 5.103: Loss = 0.599503
Epoch 5.104: Loss = 0.553375
Epoch 5.105: Loss = 0.526749
Epoch 5.106: Loss = 0.717377
Epoch 5.107: Loss = 0.647064
Epoch 5.108: Loss = 0.717194
Epoch 5.109: Loss = 0.670029
Epoch 5.110: Loss = 0.646286
Epoch 5.111: Loss = 0.566895
Epoch 5.112: Loss = 0.578476
Epoch 5.113: Loss = 0.588272
Epoch 5.114: Loss = 0.634262
Epoch 5.115: Loss = 0.583633
Epoch 5.116: Loss = 0.542725
Epoch 5.117: Loss = 0.664429
Epoch 5.118: Loss = 0.560989
Epoch 5.119: Loss = 0.592972
Epoch 5.120: Loss = 0.59552
TRAIN LOSS = 0.616989
TRAIN ACC = 80.4886 % (48295/60000)
Loss = 0.546555
Loss = 0.698074
Loss = 0.583664
Loss = 0.542023
Loss = 0.585129
Loss = 0.73735
Loss = 0.767029
Loss = 0.686127
Loss = 0.634109
Loss = 0.575363
Loss = 0.755432
Loss = 0.708862
Loss = 0.643463
Loss = 0.651184
Loss = 0.63855
Loss = 0.676941
Loss = 0.593414
Loss = 0.677582
Loss = 0.716629
Loss = 0.622192
TEST LOSS = 0.651983
TEST ACC = 482.95 % (8033/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.609741
Epoch 6.2: Loss = 0.614441
Epoch 6.3: Loss = 0.663177
Epoch 6.4: Loss = 0.549805
Epoch 6.5: Loss = 0.616135
Epoch 6.6: Loss = 0.699127
Epoch 6.7: Loss = 0.614288
Epoch 6.8: Loss = 0.731918
Epoch 6.9: Loss = 0.482407
Epoch 6.10: Loss = 0.459625
Epoch 6.11: Loss = 0.7229
Epoch 6.12: Loss = 0.616806
Epoch 6.13: Loss = 0.643417
Epoch 6.14: Loss = 0.605103
Epoch 6.15: Loss = 0.643463
Epoch 6.16: Loss = 0.672363
Epoch 6.17: Loss = 0.550262
Epoch 6.18: Loss = 0.637085
Epoch 6.19: Loss = 0.572449
Epoch 6.20: Loss = 0.704956
Epoch 6.21: Loss = 0.543411
Epoch 6.22: Loss = 0.475052
Epoch 6.23: Loss = 0.60553
Epoch 6.24: Loss = 0.728424
Epoch 6.25: Loss = 0.580765
Epoch 6.26: Loss = 0.521652
Epoch 6.27: Loss = 0.586807
Epoch 6.28: Loss = 0.627716
Epoch 6.29: Loss = 0.603287
Epoch 6.30: Loss = 0.620087
Epoch 6.31: Loss = 0.671738
Epoch 6.32: Loss = 0.566284
Epoch 6.33: Loss = 0.503403
Epoch 6.34: Loss = 0.675156
Epoch 6.35: Loss = 0.659851
Epoch 6.36: Loss = 0.673859
Epoch 6.37: Loss = 0.68399
Epoch 6.38: Loss = 0.630936
Epoch 6.39: Loss = 0.680161
Epoch 6.40: Loss = 0.604111
Epoch 6.41: Loss = 0.642303
Epoch 6.42: Loss = 0.611664
Epoch 6.43: Loss = 0.612778
Epoch 6.44: Loss = 0.520844
Epoch 6.45: Loss = 0.65004
Epoch 6.46: Loss = 0.720688
Epoch 6.47: Loss = 0.57869
Epoch 6.48: Loss = 0.53064
Epoch 6.49: Loss = 0.615585
Epoch 6.50: Loss = 0.630722
Epoch 6.51: Loss = 0.4888
Epoch 6.52: Loss = 0.657578
Epoch 6.53: Loss = 0.687225
Epoch 6.54: Loss = 0.497864
Epoch 6.55: Loss = 0.618454
Epoch 6.56: Loss = 0.662766
Epoch 6.57: Loss = 0.671707
Epoch 6.58: Loss = 0.590805
Epoch 6.59: Loss = 0.662079
Epoch 6.60: Loss = 0.624023
Epoch 6.61: Loss = 0.555283
Epoch 6.62: Loss = 0.649399
Epoch 6.63: Loss = 0.529266
Epoch 6.64: Loss = 0.506836
Epoch 6.65: Loss = 0.603607
Epoch 6.66: Loss = 0.56369
Epoch 6.67: Loss = 0.560028
Epoch 6.68: Loss = 0.751144
Epoch 6.69: Loss = 0.594101
Epoch 6.70: Loss = 0.605835
Epoch 6.71: Loss = 0.532043
Epoch 6.72: Loss = 0.611053
Epoch 6.73: Loss = 0.680634
Epoch 6.74: Loss = 0.647232
Epoch 6.75: Loss = 0.557968
Epoch 6.76: Loss = 0.588593
Epoch 6.77: Loss = 0.619919
Epoch 6.78: Loss = 0.613678
Epoch 6.79: Loss = 0.581131
Epoch 6.80: Loss = 0.542664
Epoch 6.81: Loss = 0.562485
Epoch 6.82: Loss = 0.569931
Epoch 6.83: Loss = 0.658432
Epoch 6.84: Loss = 0.572403
Epoch 6.85: Loss = 0.589859
Epoch 6.86: Loss = 0.659607
Epoch 6.87: Loss = 0.592636
Epoch 6.88: Loss = 0.549301
Epoch 6.89: Loss = 0.697983
Epoch 6.90: Loss = 0.613647
Epoch 6.91: Loss = 0.6716
Epoch 6.92: Loss = 0.645599
Epoch 6.93: Loss = 0.63649
Epoch 6.94: Loss = 0.610306
Epoch 6.95: Loss = 0.637589
Epoch 6.96: Loss = 0.580551
Epoch 6.97: Loss = 0.484451
Epoch 6.98: Loss = 0.586609
Epoch 6.99: Loss = 0.600204
Epoch 6.100: Loss = 0.600983
Epoch 6.101: Loss = 0.663727
Epoch 6.102: Loss = 0.641327
Epoch 6.103: Loss = 0.606049
Epoch 6.104: Loss = 0.548477
Epoch 6.105: Loss = 0.523972
Epoch 6.106: Loss = 0.721985
Epoch 6.107: Loss = 0.640594
Epoch 6.108: Loss = 0.713577
Epoch 6.109: Loss = 0.668808
Epoch 6.110: Loss = 0.635437
Epoch 6.111: Loss = 0.564743
Epoch 6.112: Loss = 0.585983
Epoch 6.113: Loss = 0.584656
Epoch 6.114: Loss = 0.632904
Epoch 6.115: Loss = 0.566757
Epoch 6.116: Loss = 0.538116
Epoch 6.117: Loss = 0.67807
Epoch 6.118: Loss = 0.563751
Epoch 6.119: Loss = 0.598129
Epoch 6.120: Loss = 0.578369
TRAIN LOSS = 0.610001
TRAIN ACC = 80.9219 % (48555/60000)
Loss = 0.543427
Loss = 0.692047
Loss = 0.594681
Loss = 0.536331
Loss = 0.578064
Loss = 0.725525
Loss = 0.771576
Loss = 0.689392
Loss = 0.630051
Loss = 0.581299
Loss = 0.761658
Loss = 0.707977
Loss = 0.633331
Loss = 0.64592
Loss = 0.630646
Loss = 0.66272
Loss = 0.5961
Loss = 0.67215
Loss = 0.703629
Loss = 0.631653
TEST LOSS = 0.649409
TEST ACC = 485.55 % (8035/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.591156
Epoch 7.2: Loss = 0.600235
Epoch 7.3: Loss = 0.637955
Epoch 7.4: Loss = 0.548218
Epoch 7.5: Loss = 0.619507
Epoch 7.6: Loss = 0.69249
Epoch 7.7: Loss = 0.606308
Epoch 7.8: Loss = 0.740128
Epoch 7.9: Loss = 0.47316
Epoch 7.10: Loss = 0.4422
Epoch 7.11: Loss = 0.721375
Epoch 7.12: Loss = 0.613861
Epoch 7.13: Loss = 0.636749
Epoch 7.14: Loss = 0.604813
Epoch 7.15: Loss = 0.634888
Epoch 7.16: Loss = 0.696945
Epoch 7.17: Loss = 0.54361
Epoch 7.18: Loss = 0.636276
Epoch 7.19: Loss = 0.576324
Epoch 7.20: Loss = 0.711807
Epoch 7.21: Loss = 0.541534
Epoch 7.22: Loss = 0.46376
Epoch 7.23: Loss = 0.616287
Epoch 7.24: Loss = 0.727692
Epoch 7.25: Loss = 0.561264
Epoch 7.26: Loss = 0.517212
Epoch 7.27: Loss = 0.591583
Epoch 7.28: Loss = 0.631027
Epoch 7.29: Loss = 0.599747
Epoch 7.30: Loss = 0.619949
Epoch 7.31: Loss = 0.655853
Epoch 7.32: Loss = 0.582916
Epoch 7.33: Loss = 0.498093
Epoch 7.34: Loss = 0.676666
Epoch 7.35: Loss = 0.663681
Epoch 7.36: Loss = 0.687012
Epoch 7.37: Loss = 0.696457
Epoch 7.38: Loss = 0.628494
Epoch 7.39: Loss = 0.680832
Epoch 7.40: Loss = 0.60437
Epoch 7.41: Loss = 0.649933
Epoch 7.42: Loss = 0.616409
Epoch 7.43: Loss = 0.611237
Epoch 7.44: Loss = 0.516281
Epoch 7.45: Loss = 0.666946
Epoch 7.46: Loss = 0.734741
Epoch 7.47: Loss = 0.593201
Epoch 7.48: Loss = 0.530487
Epoch 7.49: Loss = 0.63942
Epoch 7.50: Loss = 0.633194
Epoch 7.51: Loss = 0.49353
Epoch 7.52: Loss = 0.670364
Epoch 7.53: Loss = 0.704971
Epoch 7.54: Loss = 0.497864
Epoch 7.55: Loss = 0.621429
Epoch 7.56: Loss = 0.66275
Epoch 7.57: Loss = 0.677399
Epoch 7.58: Loss = 0.602631
Epoch 7.59: Loss = 0.681793
Epoch 7.60: Loss = 0.633316
Epoch 7.61: Loss = 0.563766
Epoch 7.62: Loss = 0.661011
Epoch 7.63: Loss = 0.540222
Epoch 7.64: Loss = 0.514267
Epoch 7.65: Loss = 0.619263
Epoch 7.66: Loss = 0.562958
Epoch 7.67: Loss = 0.576675
Epoch 7.68: Loss = 0.782776
Epoch 7.69: Loss = 0.60556
Epoch 7.70: Loss = 0.61557
Epoch 7.71: Loss = 0.536133
Epoch 7.72: Loss = 0.617325
Epoch 7.73: Loss = 0.678207
Epoch 7.74: Loss = 0.646912
Epoch 7.75: Loss = 0.560028
Epoch 7.76: Loss = 0.604553
Epoch 7.77: Loss = 0.627899
Epoch 7.78: Loss = 0.627426
Epoch 7.79: Loss = 0.575378
Epoch 7.80: Loss = 0.560379
Epoch 7.81: Loss = 0.577194
Epoch 7.82: Loss = 0.573013
Epoch 7.83: Loss = 0.675781
Epoch 7.84: Loss = 0.58345
Epoch 7.85: Loss = 0.578049
Epoch 7.86: Loss = 0.65274
Epoch 7.87: Loss = 0.603409
Epoch 7.88: Loss = 0.561691
Epoch 7.89: Loss = 0.709824
Epoch 7.90: Loss = 0.617065
Epoch 7.91: Loss = 0.689545
Epoch 7.92: Loss = 0.629471
Epoch 7.93: Loss = 0.635773
Epoch 7.94: Loss = 0.611511
Epoch 7.95: Loss = 0.641479
Epoch 7.96: Loss = 0.580231
Epoch 7.97: Loss = 0.472473
Epoch 7.98: Loss = 0.579269
Epoch 7.99: Loss = 0.60408
Epoch 7.100: Loss = 0.606689
Epoch 7.101: Loss = 0.683121
Epoch 7.102: Loss = 0.658783
Epoch 7.103: Loss = 0.610703
Epoch 7.104: Loss = 0.528275
Epoch 7.105: Loss = 0.526871
Epoch 7.106: Loss = 0.705795
Epoch 7.107: Loss = 0.647171
Epoch 7.108: Loss = 0.742188
Epoch 7.109: Loss = 0.673416
Epoch 7.110: Loss = 0.644653
Epoch 7.111: Loss = 0.578125
Epoch 7.112: Loss = 0.611725
Epoch 7.113: Loss = 0.593964
Epoch 7.114: Loss = 0.65715
Epoch 7.115: Loss = 0.581711
Epoch 7.116: Loss = 0.553406
Epoch 7.117: Loss = 0.683807
Epoch 7.118: Loss = 0.556396
Epoch 7.119: Loss = 0.624359
Epoch 7.120: Loss = 0.593597
TRAIN LOSS = 0.614395
TRAIN ACC = 81.282 % (48771/60000)
Loss = 0.552139
Loss = 0.705826
Loss = 0.606995
Loss = 0.534943
Loss = 0.574753
Loss = 0.745728
Loss = 0.805298
Loss = 0.716324
Loss = 0.638306
Loss = 0.591812
Loss = 0.798889
Loss = 0.732437
Loss = 0.656036
Loss = 0.663574
Loss = 0.649826
Loss = 0.677322
Loss = 0.621948
Loss = 0.690186
Loss = 0.740097
Loss = 0.65799
TEST LOSS = 0.668021
TEST ACC = 487.709 % (8059/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.599869
Epoch 8.2: Loss = 0.616379
Epoch 8.3: Loss = 0.634949
Epoch 8.4: Loss = 0.541519
Epoch 8.5: Loss = 0.632233
Epoch 8.6: Loss = 0.70137
Epoch 8.7: Loss = 0.621353
Epoch 8.8: Loss = 0.751877
Epoch 8.9: Loss = 0.471695
Epoch 8.10: Loss = 0.449509
Epoch 8.11: Loss = 0.724731
Epoch 8.12: Loss = 0.615097
Epoch 8.13: Loss = 0.653519
Epoch 8.14: Loss = 0.62114
Epoch 8.15: Loss = 0.647598
Epoch 8.16: Loss = 0.676529
Epoch 8.17: Loss = 0.547363
Epoch 8.18: Loss = 0.621262
Epoch 8.19: Loss = 0.56517
Epoch 8.20: Loss = 0.714325
Epoch 8.21: Loss = 0.53862
Epoch 8.22: Loss = 0.463867
Epoch 8.23: Loss = 0.615189
Epoch 8.24: Loss = 0.716049
Epoch 8.25: Loss = 0.559494
Epoch 8.26: Loss = 0.512909
Epoch 8.27: Loss = 0.582138
Epoch 8.28: Loss = 0.629135
Epoch 8.29: Loss = 0.607513
Epoch 8.30: Loss = 0.623566
Epoch 8.31: Loss = 0.662735
Epoch 8.32: Loss = 0.566299
Epoch 8.33: Loss = 0.49617
Epoch 8.34: Loss = 0.675583
Epoch 8.35: Loss = 0.665955
Epoch 8.36: Loss = 0.664749
Epoch 8.37: Loss = 0.695862
Epoch 8.38: Loss = 0.601395
Epoch 8.39: Loss = 0.673309
Epoch 8.40: Loss = 0.600388
Epoch 8.41: Loss = 0.623657
Epoch 8.42: Loss = 0.612289
Epoch 8.43: Loss = 0.603912
Epoch 8.44: Loss = 0.503021
Epoch 8.45: Loss = 0.680801
Epoch 8.46: Loss = 0.732605
Epoch 8.47: Loss = 0.59726
Epoch 8.48: Loss = 0.525787
Epoch 8.49: Loss = 0.619629
Epoch 8.50: Loss = 0.636749
Epoch 8.51: Loss = 0.473373
Epoch 8.52: Loss = 0.656219
Epoch 8.53: Loss = 0.683594
Epoch 8.54: Loss = 0.485718
Epoch 8.55: Loss = 0.616226
Epoch 8.56: Loss = 0.661057
Epoch 8.57: Loss = 0.682007
Epoch 8.58: Loss = 0.585388
Epoch 8.59: Loss = 0.677582
Epoch 8.60: Loss = 0.629013
Epoch 8.61: Loss = 0.55777
Epoch 8.62: Loss = 0.633331
Epoch 8.63: Loss = 0.537903
Epoch 8.64: Loss = 0.487839
Epoch 8.65: Loss = 0.6185
Epoch 8.66: Loss = 0.536469
Epoch 8.67: Loss = 0.555878
Epoch 8.68: Loss = 0.782455
Epoch 8.69: Loss = 0.587341
Epoch 8.70: Loss = 0.592209
Epoch 8.71: Loss = 0.528564
Epoch 8.72: Loss = 0.600571
Epoch 8.73: Loss = 0.674911
Epoch 8.74: Loss = 0.630127
Epoch 8.75: Loss = 0.548615
Epoch 8.76: Loss = 0.576248
Epoch 8.77: Loss = 0.601257
Epoch 8.78: Loss = 0.607681
Epoch 8.79: Loss = 0.555252
Epoch 8.80: Loss = 0.547241
Epoch 8.81: Loss = 0.552444
Epoch 8.82: Loss = 0.558975
Epoch 8.83: Loss = 0.660736
Epoch 8.84: Loss = 0.564484
Epoch 8.85: Loss = 0.569534
Epoch 8.86: Loss = 0.653641
Epoch 8.87: Loss = 0.604904
Epoch 8.88: Loss = 0.545242
Epoch 8.89: Loss = 0.687119
Epoch 8.90: Loss = 0.613693
Epoch 8.91: Loss = 0.665237
Epoch 8.92: Loss = 0.600616
Epoch 8.93: Loss = 0.624359
Epoch 8.94: Loss = 0.602829
Epoch 8.95: Loss = 0.621262
Epoch 8.96: Loss = 0.562912
Epoch 8.97: Loss = 0.469101
Epoch 8.98: Loss = 0.584946
Epoch 8.99: Loss = 0.595169
Epoch 8.100: Loss = 0.60112
Epoch 8.101: Loss = 0.66864
Epoch 8.102: Loss = 0.65538
Epoch 8.103: Loss = 0.615723
Epoch 8.104: Loss = 0.534271
Epoch 8.105: Loss = 0.517227
Epoch 8.106: Loss = 0.694336
Epoch 8.107: Loss = 0.641586
Epoch 8.108: Loss = 0.741196
Epoch 8.109: Loss = 0.661575
Epoch 8.110: Loss = 0.639557
Epoch 8.111: Loss = 0.57254
Epoch 8.112: Loss = 0.60173
Epoch 8.113: Loss = 0.584106
Epoch 8.114: Loss = 0.656372
Epoch 8.115: Loss = 0.570831
Epoch 8.116: Loss = 0.533813
Epoch 8.117: Loss = 0.680862
Epoch 8.118: Loss = 0.551941
Epoch 8.119: Loss = 0.607986
Epoch 8.120: Loss = 0.582489
TRAIN LOSS = 0.607147
TRAIN ACC = 81.7123 % (49029/60000)
Loss = 0.536499
Loss = 0.680954
Loss = 0.601089
Loss = 0.522552
Loss = 0.571671
Loss = 0.721298
Loss = 0.808533
Loss = 0.690201
Loss = 0.63031
Loss = 0.574051
Loss = 0.789017
Loss = 0.726364
Loss = 0.644943
Loss = 0.662842
Loss = 0.645355
Loss = 0.66246
Loss = 0.602341
Loss = 0.681366
Loss = 0.714233
Loss = 0.640717
TEST LOSS = 0.65534
TEST ACC = 490.289 % (8075/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.578842
Epoch 9.2: Loss = 0.598541
Epoch 9.3: Loss = 0.625671
Epoch 9.4: Loss = 0.538086
Epoch 9.5: Loss = 0.613983
Epoch 9.6: Loss = 0.698288
Epoch 9.7: Loss = 0.581558
Epoch 9.8: Loss = 0.729095
Epoch 9.9: Loss = 0.465698
Epoch 9.10: Loss = 0.446091
Epoch 9.11: Loss = 0.716232
Epoch 9.12: Loss = 0.59726
Epoch 9.13: Loss = 0.625275
Epoch 9.14: Loss = 0.611847
Epoch 9.15: Loss = 0.648178
Epoch 9.16: Loss = 0.683105
Epoch 9.17: Loss = 0.548141
Epoch 9.18: Loss = 0.615387
Epoch 9.19: Loss = 0.553772
Epoch 9.20: Loss = 0.696152
Epoch 9.21: Loss = 0.535873
Epoch 9.22: Loss = 0.44957
Epoch 9.23: Loss = 0.606232
Epoch 9.24: Loss = 0.712692
Epoch 9.25: Loss = 0.580887
Epoch 9.26: Loss = 0.499557
Epoch 9.27: Loss = 0.578613
Epoch 9.28: Loss = 0.613098
Epoch 9.29: Loss = 0.603378
Epoch 9.30: Loss = 0.605072
Epoch 9.31: Loss = 0.658218
Epoch 9.32: Loss = 0.554703
Epoch 9.33: Loss = 0.49942
Epoch 9.34: Loss = 0.682388
Epoch 9.35: Loss = 0.668579
Epoch 9.36: Loss = 0.673386
Epoch 9.37: Loss = 0.687149
Epoch 9.38: Loss = 0.602066
Epoch 9.39: Loss = 0.651901
Epoch 9.40: Loss = 0.596603
Epoch 9.41: Loss = 0.618652
Epoch 9.42: Loss = 0.605759
Epoch 9.43: Loss = 0.602356
Epoch 9.44: Loss = 0.510559
Epoch 9.45: Loss = 0.67244
Epoch 9.46: Loss = 0.754013
Epoch 9.47: Loss = 0.601089
Epoch 9.48: Loss = 0.522247
Epoch 9.49: Loss = 0.616028
Epoch 9.50: Loss = 0.653336
Epoch 9.51: Loss = 0.475479
Epoch 9.52: Loss = 0.66011
Epoch 9.53: Loss = 0.675919
Epoch 9.54: Loss = 0.480804
Epoch 9.55: Loss = 0.641281
Epoch 9.56: Loss = 0.659409
Epoch 9.57: Loss = 0.690262
Epoch 9.58: Loss = 0.597321
Epoch 9.59: Loss = 0.6828
Epoch 9.60: Loss = 0.65036
Epoch 9.61: Loss = 0.547455
Epoch 9.62: Loss = 0.653976
Epoch 9.63: Loss = 0.528717
Epoch 9.64: Loss = 0.489471
Epoch 9.65: Loss = 0.621185
Epoch 9.66: Loss = 0.536163
Epoch 9.67: Loss = 0.550598
Epoch 9.68: Loss = 0.805069
Epoch 9.69: Loss = 0.591537
Epoch 9.70: Loss = 0.584564
Epoch 9.71: Loss = 0.520203
Epoch 9.72: Loss = 0.638977
Epoch 9.73: Loss = 0.667892
Epoch 9.74: Loss = 0.642639
Epoch 9.75: Loss = 0.551804
Epoch 9.76: Loss = 0.577271
Epoch 9.77: Loss = 0.605682
Epoch 9.78: Loss = 0.613739
Epoch 9.79: Loss = 0.569519
Epoch 9.80: Loss = 0.56076
Epoch 9.81: Loss = 0.566437
Epoch 9.82: Loss = 0.566742
Epoch 9.83: Loss = 0.672913
Epoch 9.84: Loss = 0.576385
Epoch 9.85: Loss = 0.581451
Epoch 9.86: Loss = 0.66304
Epoch 9.87: Loss = 0.596634
Epoch 9.88: Loss = 0.543091
Epoch 9.89: Loss = 0.680771
Epoch 9.90: Loss = 0.637497
Epoch 9.91: Loss = 0.67038
Epoch 9.92: Loss = 0.612595
Epoch 9.93: Loss = 0.610001
Epoch 9.94: Loss = 0.59903
Epoch 9.95: Loss = 0.629547
Epoch 9.96: Loss = 0.550598
Epoch 9.97: Loss = 0.469513
Epoch 9.98: Loss = 0.595139
Epoch 9.99: Loss = 0.59996
Epoch 9.100: Loss = 0.62085
Epoch 9.101: Loss = 0.649261
Epoch 9.102: Loss = 0.670822
Epoch 9.103: Loss = 0.603271
Epoch 9.104: Loss = 0.543289
Epoch 9.105: Loss = 0.516174
Epoch 9.106: Loss = 0.685013
Epoch 9.107: Loss = 0.638382
Epoch 9.108: Loss = 0.770157
Epoch 9.109: Loss = 0.649414
Epoch 9.110: Loss = 0.661652
Epoch 9.111: Loss = 0.590225
Epoch 9.112: Loss = 0.60672
Epoch 9.113: Loss = 0.596329
Epoch 9.114: Loss = 0.663452
Epoch 9.115: Loss = 0.574188
Epoch 9.116: Loss = 0.543091
Epoch 9.117: Loss = 0.662369
Epoch 9.118: Loss = 0.54454
Epoch 9.119: Loss = 0.614349
Epoch 9.120: Loss = 0.570831
TRAIN LOSS = 0.607132
TRAIN ACC = 81.9534 % (49174/60000)
Loss = 0.542709
Loss = 0.681442
Loss = 0.595749
Loss = 0.517578
Loss = 0.575821
Loss = 0.734985
Loss = 0.80661
Loss = 0.70311
Loss = 0.626526
Loss = 0.580414
Loss = 0.811142
Loss = 0.740524
Loss = 0.652603
Loss = 0.650864
Loss = 0.64032
Loss = 0.663269
Loss = 0.630081
Loss = 0.678833
Loss = 0.710175
Loss = 0.638336
TEST LOSS = 0.659054
TEST ACC = 491.739 % (8089/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.592224
Epoch 10.2: Loss = 0.624008
Epoch 10.3: Loss = 0.639313
Epoch 10.4: Loss = 0.546112
Epoch 10.5: Loss = 0.628326
Epoch 10.6: Loss = 0.698166
Epoch 10.7: Loss = 0.586731
Epoch 10.8: Loss = 0.731995
Epoch 10.9: Loss = 0.466522
Epoch 10.10: Loss = 0.432648
Epoch 10.11: Loss = 0.72998
Epoch 10.12: Loss = 0.601288
Epoch 10.13: Loss = 0.631073
Epoch 10.14: Loss = 0.602936
Epoch 10.15: Loss = 0.622131
Epoch 10.16: Loss = 0.665405
Epoch 10.17: Loss = 0.544678
Epoch 10.18: Loss = 0.621048
Epoch 10.19: Loss = 0.546967
Epoch 10.20: Loss = 0.716263
Epoch 10.21: Loss = 0.541992
Epoch 10.22: Loss = 0.467377
Epoch 10.23: Loss = 0.614914
Epoch 10.24: Loss = 0.736923
Epoch 10.25: Loss = 0.590088
Epoch 10.26: Loss = 0.493805
Epoch 10.27: Loss = 0.584473
Epoch 10.28: Loss = 0.619263
Epoch 10.29: Loss = 0.585739
Epoch 10.30: Loss = 0.606766
Epoch 10.31: Loss = 0.657761
Epoch 10.32: Loss = 0.535156
Epoch 10.33: Loss = 0.50705
Epoch 10.34: Loss = 0.697418
Epoch 10.35: Loss = 0.670532
Epoch 10.36: Loss = 0.686584
Epoch 10.37: Loss = 0.706116
Epoch 10.38: Loss = 0.609421
Epoch 10.39: Loss = 0.654694
Epoch 10.40: Loss = 0.590393
Epoch 10.41: Loss = 0.626633
Epoch 10.42: Loss = 0.607132
Epoch 10.43: Loss = 0.590775
Epoch 10.44: Loss = 0.514877
Epoch 10.45: Loss = 0.690201
Epoch 10.46: Loss = 0.741196
Epoch 10.47: Loss = 0.614685
Epoch 10.48: Loss = 0.505432
Epoch 10.49: Loss = 0.620667
Epoch 10.50: Loss = 0.654175
Epoch 10.51: Loss = 0.478516
Epoch 10.52: Loss = 0.658615
Epoch 10.53: Loss = 0.67984
Epoch 10.54: Loss = 0.482071
Epoch 10.55: Loss = 0.620377
Epoch 10.56: Loss = 0.656403
Epoch 10.57: Loss = 0.678757
Epoch 10.58: Loss = 0.582825
Epoch 10.59: Loss = 0.687073
Epoch 10.60: Loss = 0.641266
Epoch 10.61: Loss = 0.567413
Epoch 10.62: Loss = 0.661514
Epoch 10.63: Loss = 0.528656
Epoch 10.64: Loss = 0.481537
Epoch 10.65: Loss = 0.623489
Epoch 10.66: Loss = 0.533966
Epoch 10.67: Loss = 0.560333
Epoch 10.68: Loss = 0.826935
Epoch 10.69: Loss = 0.589691
Epoch 10.70: Loss = 0.593887
Epoch 10.71: Loss = 0.528488
Epoch 10.72: Loss = 0.616989
Epoch 10.73: Loss = 0.677551
Epoch 10.74: Loss = 0.639832
Epoch 10.75: Loss = 0.568268
Epoch 10.76: Loss = 0.574188
Epoch 10.77: Loss = 0.617798
Epoch 10.78: Loss = 0.618317
Epoch 10.79: Loss = 0.57901
Epoch 10.80: Loss = 0.549347
Epoch 10.81: Loss = 0.571365
Epoch 10.82: Loss = 0.569138
Epoch 10.83: Loss = 0.660706
Epoch 10.84: Loss = 0.571548
Epoch 10.85: Loss = 0.554581
Epoch 10.86: Loss = 0.659622
Epoch 10.87: Loss = 0.603348
Epoch 10.88: Loss = 0.568008
Epoch 10.89: Loss = 0.676697
Epoch 10.90: Loss = 0.643433
Epoch 10.91: Loss = 0.680496
Epoch 10.92: Loss = 0.619354
Epoch 10.93: Loss = 0.604309
Epoch 10.94: Loss = 0.605972
Epoch 10.95: Loss = 0.63797
Epoch 10.96: Loss = 0.567719
Epoch 10.97: Loss = 0.465622
Epoch 10.98: Loss = 0.587097
Epoch 10.99: Loss = 0.612579
Epoch 10.100: Loss = 0.612289
Epoch 10.101: Loss = 0.641373
Epoch 10.102: Loss = 0.679077
Epoch 10.103: Loss = 0.597473
Epoch 10.104: Loss = 0.531555
Epoch 10.105: Loss = 0.511047
Epoch 10.106: Loss = 0.691711
Epoch 10.107: Loss = 0.65358
Epoch 10.108: Loss = 0.766281
Epoch 10.109: Loss = 0.671219
Epoch 10.110: Loss = 0.637482
Epoch 10.111: Loss = 0.590744
Epoch 10.112: Loss = 0.589966
Epoch 10.113: Loss = 0.590683
Epoch 10.114: Loss = 0.668976
Epoch 10.115: Loss = 0.57019
Epoch 10.116: Loss = 0.533844
Epoch 10.117: Loss = 0.656479
Epoch 10.118: Loss = 0.546188
Epoch 10.119: Loss = 0.612457
Epoch 10.120: Loss = 0.568954
TRAIN LOSS = 0.608643
TRAIN ACC = 82.0801 % (49251/60000)
Loss = 0.554642
Loss = 0.682556
Loss = 0.595551
Loss = 0.522537
Loss = 0.576096
Loss = 0.737671
Loss = 0.816818
Loss = 0.68924
Loss = 0.626389
Loss = 0.570786
Loss = 0.814941
Loss = 0.739609
Loss = 0.658401
Loss = 0.65625
Loss = 0.648468
Loss = 0.649445
Loss = 0.637482
Loss = 0.68837
Loss = 0.700165
Loss = 0.631546
TEST LOSS = 0.659848
TEST ACC = 492.509 % (8125/10000)
