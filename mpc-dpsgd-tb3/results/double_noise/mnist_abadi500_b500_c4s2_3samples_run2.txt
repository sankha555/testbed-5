Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.40724
Epoch 1.2: Loss = 2.33217
Epoch 1.3: Loss = 2.31717
Epoch 1.4: Loss = 2.25238
Epoch 1.5: Loss = 2.20653
Epoch 1.6: Loss = 2.18858
Epoch 1.7: Loss = 2.15324
Epoch 1.8: Loss = 2.09789
Epoch 1.9: Loss = 2.04439
Epoch 1.10: Loss = 1.99063
Epoch 1.11: Loss = 1.92375
Epoch 1.12: Loss = 1.91751
Epoch 1.13: Loss = 1.85114
Epoch 1.14: Loss = 1.82269
Epoch 1.15: Loss = 1.89217
Epoch 1.16: Loss = 1.7794
Epoch 1.17: Loss = 1.72433
Epoch 1.18: Loss = 1.70712
Epoch 1.19: Loss = 1.65909
Epoch 1.20: Loss = 1.62315
Epoch 1.21: Loss = 1.53758
Epoch 1.22: Loss = 1.52274
Epoch 1.23: Loss = 1.47664
Epoch 1.24: Loss = 1.52898
Epoch 1.25: Loss = 1.46205
Epoch 1.26: Loss = 1.49344
Epoch 1.27: Loss = 1.4182
Epoch 1.28: Loss = 1.41576
Epoch 1.29: Loss = 1.39848
Epoch 1.30: Loss = 1.47594
Epoch 1.31: Loss = 1.31493
Epoch 1.32: Loss = 1.3511
Epoch 1.33: Loss = 1.26453
Epoch 1.34: Loss = 1.31314
Epoch 1.35: Loss = 1.23013
Epoch 1.36: Loss = 1.33746
Epoch 1.37: Loss = 1.17313
Epoch 1.38: Loss = 1.16776
Epoch 1.39: Loss = 1.13048
Epoch 1.40: Loss = 1.08238
Epoch 1.41: Loss = 1.13127
Epoch 1.42: Loss = 1.08699
Epoch 1.43: Loss = 1.04121
Epoch 1.44: Loss = 0.990982
Epoch 1.45: Loss = 1.14578
Epoch 1.46: Loss = 1.0374
Epoch 1.47: Loss = 0.977844
Epoch 1.48: Loss = 1.0448
Epoch 1.49: Loss = 0.955063
Epoch 1.50: Loss = 1.04102
Epoch 1.51: Loss = 0.881256
Epoch 1.52: Loss = 0.899246
Epoch 1.53: Loss = 0.931122
Epoch 1.54: Loss = 0.970734
Epoch 1.55: Loss = 0.952255
Epoch 1.56: Loss = 0.875931
Epoch 1.57: Loss = 0.820724
Epoch 1.58: Loss = 0.835037
Epoch 1.59: Loss = 0.876053
Epoch 1.60: Loss = 0.988922
Epoch 1.61: Loss = 0.904099
Epoch 1.62: Loss = 0.941101
Epoch 1.63: Loss = 0.95163
Epoch 1.64: Loss = 0.922791
Epoch 1.65: Loss = 0.963654
Epoch 1.66: Loss = 0.809158
Epoch 1.67: Loss = 0.828568
Epoch 1.68: Loss = 0.678116
Epoch 1.69: Loss = 0.757736
Epoch 1.70: Loss = 0.833191
Epoch 1.71: Loss = 0.741089
Epoch 1.72: Loss = 0.763168
Epoch 1.73: Loss = 0.772797
Epoch 1.74: Loss = 0.649887
Epoch 1.75: Loss = 0.787979
Epoch 1.76: Loss = 0.75943
Epoch 1.77: Loss = 0.722031
Epoch 1.78: Loss = 0.697632
Epoch 1.79: Loss = 0.710114
Epoch 1.80: Loss = 0.799377
Epoch 1.81: Loss = 0.680511
Epoch 1.82: Loss = 0.62648
Epoch 1.83: Loss = 0.823334
Epoch 1.84: Loss = 0.735992
Epoch 1.85: Loss = 0.778793
Epoch 1.86: Loss = 0.686432
Epoch 1.87: Loss = 0.627228
Epoch 1.88: Loss = 0.654694
Epoch 1.89: Loss = 0.761658
Epoch 1.90: Loss = 0.61702
Epoch 1.91: Loss = 0.674896
Epoch 1.92: Loss = 0.688858
Epoch 1.93: Loss = 0.712234
Epoch 1.94: Loss = 0.553345
Epoch 1.95: Loss = 0.671371
Epoch 1.96: Loss = 0.642853
Epoch 1.97: Loss = 0.491882
Epoch 1.98: Loss = 0.60083
Epoch 1.99: Loss = 0.700912
Epoch 1.100: Loss = 0.775375
Epoch 1.101: Loss = 0.67662
Epoch 1.102: Loss = 0.613113
Epoch 1.103: Loss = 0.554504
Epoch 1.104: Loss = 0.553162
Epoch 1.105: Loss = 0.658081
Epoch 1.106: Loss = 0.651718
Epoch 1.107: Loss = 0.529556
Epoch 1.108: Loss = 0.607086
Epoch 1.109: Loss = 0.555511
Epoch 1.110: Loss = 0.587296
Epoch 1.111: Loss = 0.487061
Epoch 1.112: Loss = 0.473221
Epoch 1.113: Loss = 0.542099
Epoch 1.114: Loss = 0.483719
Epoch 1.115: Loss = 0.548828
Epoch 1.116: Loss = 0.563141
Epoch 1.117: Loss = 0.432755
Epoch 1.118: Loss = 0.398056
Epoch 1.119: Loss = 0.390747
Epoch 1.120: Loss = 0.433365
TRAIN LOSS = 1.05591
TRAIN ACC = 71.9788 % (43190/60000)
Loss = 0.573853
Loss = 0.595688
Loss = 0.705933
Loss = 0.654068
Loss = 0.690582
Loss = 0.585159
Loss = 0.557205
Loss = 0.722946
Loss = 0.675766
Loss = 0.64093
Loss = 0.334213
Loss = 0.477661
Loss = 0.328552
Loss = 0.53624
Loss = 0.4328
Loss = 0.406403
Loss = 0.380035
Loss = 0.212875
Loss = 0.403534
Loss = 0.654282
TEST LOSS = 0.528436
TEST ACC = 431.9 % (8495/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.499878
Epoch 2.2: Loss = 0.631424
Epoch 2.3: Loss = 0.599197
Epoch 2.4: Loss = 0.477844
Epoch 2.5: Loss = 0.48111
Epoch 2.6: Loss = 0.482712
Epoch 2.7: Loss = 0.553696
Epoch 2.8: Loss = 0.49556
Epoch 2.9: Loss = 0.493362
Epoch 2.10: Loss = 0.496338
Epoch 2.11: Loss = 0.49585
Epoch 2.12: Loss = 0.492813
Epoch 2.13: Loss = 0.43721
Epoch 2.14: Loss = 0.463333
Epoch 2.15: Loss = 0.612091
Epoch 2.16: Loss = 0.564575
Epoch 2.17: Loss = 0.560501
Epoch 2.18: Loss = 0.627304
Epoch 2.19: Loss = 0.493317
Epoch 2.20: Loss = 0.454239
Epoch 2.21: Loss = 0.425781
Epoch 2.22: Loss = 0.429947
Epoch 2.23: Loss = 0.431915
Epoch 2.24: Loss = 0.586884
Epoch 2.25: Loss = 0.521164
Epoch 2.26: Loss = 0.594498
Epoch 2.27: Loss = 0.567963
Epoch 2.28: Loss = 0.565872
Epoch 2.29: Loss = 0.609436
Epoch 2.30: Loss = 0.677338
Epoch 2.31: Loss = 0.461899
Epoch 2.32: Loss = 0.584152
Epoch 2.33: Loss = 0.466171
Epoch 2.34: Loss = 0.542206
Epoch 2.35: Loss = 0.513718
Epoch 2.36: Loss = 0.591873
Epoch 2.37: Loss = 0.413773
Epoch 2.38: Loss = 0.419205
Epoch 2.39: Loss = 0.487747
Epoch 2.40: Loss = 0.435471
Epoch 2.41: Loss = 0.485382
Epoch 2.42: Loss = 0.571182
Epoch 2.43: Loss = 0.419067
Epoch 2.44: Loss = 0.379089
Epoch 2.45: Loss = 0.499863
Epoch 2.46: Loss = 0.523773
Epoch 2.47: Loss = 0.430588
Epoch 2.48: Loss = 0.510818
Epoch 2.49: Loss = 0.450836
Epoch 2.50: Loss = 0.538223
Epoch 2.51: Loss = 0.412003
Epoch 2.52: Loss = 0.40979
Epoch 2.53: Loss = 0.456619
Epoch 2.54: Loss = 0.54306
Epoch 2.55: Loss = 0.47261
Epoch 2.56: Loss = 0.438995
Epoch 2.57: Loss = 0.426971
Epoch 2.58: Loss = 0.459091
Epoch 2.59: Loss = 0.514282
Epoch 2.60: Loss = 0.589294
Epoch 2.61: Loss = 0.529938
Epoch 2.62: Loss = 0.563751
Epoch 2.63: Loss = 0.607391
Epoch 2.64: Loss = 0.551697
Epoch 2.65: Loss = 0.661011
Epoch 2.66: Loss = 0.460602
Epoch 2.67: Loss = 0.502991
Epoch 2.68: Loss = 0.325607
Epoch 2.69: Loss = 0.408493
Epoch 2.70: Loss = 0.542206
Epoch 2.71: Loss = 0.418991
Epoch 2.72: Loss = 0.426819
Epoch 2.73: Loss = 0.472534
Epoch 2.74: Loss = 0.349014
Epoch 2.75: Loss = 0.587372
Epoch 2.76: Loss = 0.474075
Epoch 2.77: Loss = 0.41626
Epoch 2.78: Loss = 0.426315
Epoch 2.79: Loss = 0.504639
Epoch 2.80: Loss = 0.528519
Epoch 2.81: Loss = 0.407135
Epoch 2.82: Loss = 0.370255
Epoch 2.83: Loss = 0.561157
Epoch 2.84: Loss = 0.471542
Epoch 2.85: Loss = 0.574463
Epoch 2.86: Loss = 0.490204
Epoch 2.87: Loss = 0.387619
Epoch 2.88: Loss = 0.449493
Epoch 2.89: Loss = 0.528671
Epoch 2.90: Loss = 0.390701
Epoch 2.91: Loss = 0.483246
Epoch 2.92: Loss = 0.502914
Epoch 2.93: Loss = 0.543564
Epoch 2.94: Loss = 0.353134
Epoch 2.95: Loss = 0.47673
Epoch 2.96: Loss = 0.480576
Epoch 2.97: Loss = 0.337784
Epoch 2.98: Loss = 0.429916
Epoch 2.99: Loss = 0.520691
Epoch 2.100: Loss = 0.576309
Epoch 2.101: Loss = 0.528839
Epoch 2.102: Loss = 0.457214
Epoch 2.103: Loss = 0.395935
Epoch 2.104: Loss = 0.370377
Epoch 2.105: Loss = 0.522034
Epoch 2.106: Loss = 0.519516
Epoch 2.107: Loss = 0.369034
Epoch 2.108: Loss = 0.474335
Epoch 2.109: Loss = 0.394165
Epoch 2.110: Loss = 0.44371
Epoch 2.111: Loss = 0.345871
Epoch 2.112: Loss = 0.353226
Epoch 2.113: Loss = 0.40184
Epoch 2.114: Loss = 0.337585
Epoch 2.115: Loss = 0.376755
Epoch 2.116: Loss = 0.419037
Epoch 2.117: Loss = 0.290649
Epoch 2.118: Loss = 0.251602
Epoch 2.119: Loss = 0.293106
Epoch 2.120: Loss = 0.328415
TRAIN LOSS = 0.47699
TRAIN ACC = 85.997 % (51600/60000)
Loss = 0.42099
Loss = 0.475082
Loss = 0.572998
Loss = 0.533127
Loss = 0.580429
Loss = 0.449524
Loss = 0.41394
Loss = 0.616409
Loss = 0.539352
Loss = 0.512482
Loss = 0.22406
Loss = 0.341125
Loss = 0.272385
Loss = 0.406754
Loss = 0.282761
Loss = 0.32254
Loss = 0.261612
Loss = 0.102066
Loss = 0.268188
Loss = 0.543854
TEST LOSS = 0.406984
TEST ACC = 515.999 % (8800/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.376144
Epoch 3.2: Loss = 0.509598
Epoch 3.3: Loss = 0.48558
Epoch 3.4: Loss = 0.356949
Epoch 3.5: Loss = 0.368118
Epoch 3.6: Loss = 0.371887
Epoch 3.7: Loss = 0.42012
Epoch 3.8: Loss = 0.39743
Epoch 3.9: Loss = 0.383942
Epoch 3.10: Loss = 0.390442
Epoch 3.11: Loss = 0.414215
Epoch 3.12: Loss = 0.396851
Epoch 3.13: Loss = 0.346954
Epoch 3.14: Loss = 0.349792
Epoch 3.15: Loss = 0.47052
Epoch 3.16: Loss = 0.461395
Epoch 3.17: Loss = 0.488663
Epoch 3.18: Loss = 0.55455
Epoch 3.19: Loss = 0.406403
Epoch 3.20: Loss = 0.365677
Epoch 3.21: Loss = 0.344269
Epoch 3.22: Loss = 0.327332
Epoch 3.23: Loss = 0.350815
Epoch 3.24: Loss = 0.510147
Epoch 3.25: Loss = 0.456955
Epoch 3.26: Loss = 0.533951
Epoch 3.27: Loss = 0.503448
Epoch 3.28: Loss = 0.484665
Epoch 3.29: Loss = 0.559769
Epoch 3.30: Loss = 0.567917
Epoch 3.31: Loss = 0.384995
Epoch 3.32: Loss = 0.494949
Epoch 3.33: Loss = 0.390945
Epoch 3.34: Loss = 0.468048
Epoch 3.35: Loss = 0.443054
Epoch 3.36: Loss = 0.507309
Epoch 3.37: Loss = 0.322418
Epoch 3.38: Loss = 0.364639
Epoch 3.39: Loss = 0.404694
Epoch 3.40: Loss = 0.377335
Epoch 3.41: Loss = 0.41275
Epoch 3.42: Loss = 0.548981
Epoch 3.43: Loss = 0.349503
Epoch 3.44: Loss = 0.312836
Epoch 3.45: Loss = 0.418198
Epoch 3.46: Loss = 0.467026
Epoch 3.47: Loss = 0.374619
Epoch 3.48: Loss = 0.432434
Epoch 3.49: Loss = 0.395767
Epoch 3.50: Loss = 0.475937
Epoch 3.51: Loss = 0.354279
Epoch 3.52: Loss = 0.343796
Epoch 3.53: Loss = 0.390869
Epoch 3.54: Loss = 0.488678
Epoch 3.55: Loss = 0.404984
Epoch 3.56: Loss = 0.384567
Epoch 3.57: Loss = 0.375717
Epoch 3.58: Loss = 0.413528
Epoch 3.59: Loss = 0.469238
Epoch 3.60: Loss = 0.53035
Epoch 3.61: Loss = 0.456924
Epoch 3.62: Loss = 0.497375
Epoch 3.63: Loss = 0.558243
Epoch 3.64: Loss = 0.490204
Epoch 3.65: Loss = 0.610291
Epoch 3.66: Loss = 0.405411
Epoch 3.67: Loss = 0.449173
Epoch 3.68: Loss = 0.268677
Epoch 3.69: Loss = 0.344757
Epoch 3.70: Loss = 0.503525
Epoch 3.71: Loss = 0.358414
Epoch 3.72: Loss = 0.352661
Epoch 3.73: Loss = 0.411301
Epoch 3.74: Loss = 0.315048
Epoch 3.75: Loss = 0.577148
Epoch 3.76: Loss = 0.424973
Epoch 3.77: Loss = 0.345322
Epoch 3.78: Loss = 0.386719
Epoch 3.79: Loss = 0.465866
Epoch 3.80: Loss = 0.471619
Epoch 3.81: Loss = 0.352722
Epoch 3.82: Loss = 0.317841
Epoch 3.83: Loss = 0.49556
Epoch 3.84: Loss = 0.42038
Epoch 3.85: Loss = 0.554108
Epoch 3.86: Loss = 0.463303
Epoch 3.87: Loss = 0.339386
Epoch 3.88: Loss = 0.406128
Epoch 3.89: Loss = 0.496353
Epoch 3.90: Loss = 0.339722
Epoch 3.91: Loss = 0.465729
Epoch 3.92: Loss = 0.460846
Epoch 3.93: Loss = 0.505875
Epoch 3.94: Loss = 0.32338
Epoch 3.95: Loss = 0.423691
Epoch 3.96: Loss = 0.45105
Epoch 3.97: Loss = 0.32016
Epoch 3.98: Loss = 0.372086
Epoch 3.99: Loss = 0.475082
Epoch 3.100: Loss = 0.569489
Epoch 3.101: Loss = 0.509766
Epoch 3.102: Loss = 0.409714
Epoch 3.103: Loss = 0.359558
Epoch 3.104: Loss = 0.333374
Epoch 3.105: Loss = 0.507141
Epoch 3.106: Loss = 0.495346
Epoch 3.107: Loss = 0.319519
Epoch 3.108: Loss = 0.456146
Epoch 3.109: Loss = 0.354446
Epoch 3.110: Loss = 0.417877
Epoch 3.111: Loss = 0.308395
Epoch 3.112: Loss = 0.318649
Epoch 3.113: Loss = 0.370834
Epoch 3.114: Loss = 0.304916
Epoch 3.115: Loss = 0.326477
Epoch 3.116: Loss = 0.389267
Epoch 3.117: Loss = 0.255386
Epoch 3.118: Loss = 0.229538
Epoch 3.119: Loss = 0.231628
Epoch 3.120: Loss = 0.308563
TRAIN LOSS = 0.415924
TRAIN ACC = 87.5427 % (52528/60000)
Loss = 0.38237
Loss = 0.429138
Loss = 0.532578
Loss = 0.500458
Loss = 0.542847
Loss = 0.420822
Loss = 0.36261
Loss = 0.594513
Loss = 0.499985
Loss = 0.474319
Loss = 0.214706
Loss = 0.306992
Loss = 0.238892
Loss = 0.370438
Loss = 0.237305
Loss = 0.305252
Loss = 0.216919
Loss = 0.0740051
Loss = 0.237442
Loss = 0.519409
TEST LOSS = 0.37305
TEST ACC = 525.279 % (8901/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.354202
Epoch 4.2: Loss = 0.503326
Epoch 4.3: Loss = 0.47493
Epoch 4.4: Loss = 0.326981
Epoch 4.5: Loss = 0.328156
Epoch 4.6: Loss = 0.343903
Epoch 4.7: Loss = 0.384308
Epoch 4.8: Loss = 0.35527
Epoch 4.9: Loss = 0.35762
Epoch 4.10: Loss = 0.371704
Epoch 4.11: Loss = 0.389465
Epoch 4.12: Loss = 0.363342
Epoch 4.13: Loss = 0.315842
Epoch 4.14: Loss = 0.321747
Epoch 4.15: Loss = 0.416901
Epoch 4.16: Loss = 0.429871
Epoch 4.17: Loss = 0.470169
Epoch 4.18: Loss = 0.553162
Epoch 4.19: Loss = 0.393341
Epoch 4.20: Loss = 0.322891
Epoch 4.21: Loss = 0.331085
Epoch 4.22: Loss = 0.297577
Epoch 4.23: Loss = 0.316467
Epoch 4.24: Loss = 0.51535
Epoch 4.25: Loss = 0.436691
Epoch 4.26: Loss = 0.517624
Epoch 4.27: Loss = 0.484955
Epoch 4.28: Loss = 0.443008
Epoch 4.29: Loss = 0.53064
Epoch 4.30: Loss = 0.536606
Epoch 4.31: Loss = 0.343674
Epoch 4.32: Loss = 0.469574
Epoch 4.33: Loss = 0.358231
Epoch 4.34: Loss = 0.432068
Epoch 4.35: Loss = 0.417175
Epoch 4.36: Loss = 0.465408
Epoch 4.37: Loss = 0.293259
Epoch 4.38: Loss = 0.344009
Epoch 4.39: Loss = 0.372925
Epoch 4.40: Loss = 0.367416
Epoch 4.41: Loss = 0.378265
Epoch 4.42: Loss = 0.525345
Epoch 4.43: Loss = 0.313751
Epoch 4.44: Loss = 0.28746
Epoch 4.45: Loss = 0.389236
Epoch 4.46: Loss = 0.456589
Epoch 4.47: Loss = 0.349716
Epoch 4.48: Loss = 0.407349
Epoch 4.49: Loss = 0.364838
Epoch 4.50: Loss = 0.450012
Epoch 4.51: Loss = 0.331055
Epoch 4.52: Loss = 0.331909
Epoch 4.53: Loss = 0.351578
Epoch 4.54: Loss = 0.479645
Epoch 4.55: Loss = 0.38588
Epoch 4.56: Loss = 0.371918
Epoch 4.57: Loss = 0.363434
Epoch 4.58: Loss = 0.394104
Epoch 4.59: Loss = 0.443832
Epoch 4.60: Loss = 0.510468
Epoch 4.61: Loss = 0.43515
Epoch 4.62: Loss = 0.476395
Epoch 4.63: Loss = 0.54216
Epoch 4.64: Loss = 0.49733
Epoch 4.65: Loss = 0.581726
Epoch 4.66: Loss = 0.383728
Epoch 4.67: Loss = 0.429626
Epoch 4.68: Loss = 0.253799
Epoch 4.69: Loss = 0.340347
Epoch 4.70: Loss = 0.48909
Epoch 4.71: Loss = 0.332291
Epoch 4.72: Loss = 0.343781
Epoch 4.73: Loss = 0.398788
Epoch 4.74: Loss = 0.300842
Epoch 4.75: Loss = 0.5914
Epoch 4.76: Loss = 0.428284
Epoch 4.77: Loss = 0.338928
Epoch 4.78: Loss = 0.372772
Epoch 4.79: Loss = 0.454529
Epoch 4.80: Loss = 0.439163
Epoch 4.81: Loss = 0.343842
Epoch 4.82: Loss = 0.28688
Epoch 4.83: Loss = 0.479706
Epoch 4.84: Loss = 0.399353
Epoch 4.85: Loss = 0.542328
Epoch 4.86: Loss = 0.450394
Epoch 4.87: Loss = 0.308868
Epoch 4.88: Loss = 0.402466
Epoch 4.89: Loss = 0.489349
Epoch 4.90: Loss = 0.332108
Epoch 4.91: Loss = 0.451309
Epoch 4.92: Loss = 0.453217
Epoch 4.93: Loss = 0.512589
Epoch 4.94: Loss = 0.310959
Epoch 4.95: Loss = 0.411163
Epoch 4.96: Loss = 0.431183
Epoch 4.97: Loss = 0.310577
Epoch 4.98: Loss = 0.355515
Epoch 4.99: Loss = 0.459259
Epoch 4.100: Loss = 0.567551
Epoch 4.101: Loss = 0.511536
Epoch 4.102: Loss = 0.389343
Epoch 4.103: Loss = 0.350769
Epoch 4.104: Loss = 0.323288
Epoch 4.105: Loss = 0.493713
Epoch 4.106: Loss = 0.49762
Epoch 4.107: Loss = 0.323975
Epoch 4.108: Loss = 0.460297
Epoch 4.109: Loss = 0.333954
Epoch 4.110: Loss = 0.411057
Epoch 4.111: Loss = 0.306915
Epoch 4.112: Loss = 0.313995
Epoch 4.113: Loss = 0.352066
Epoch 4.114: Loss = 0.288895
Epoch 4.115: Loss = 0.291977
Epoch 4.116: Loss = 0.350616
Epoch 4.117: Loss = 0.215866
Epoch 4.118: Loss = 0.196777
Epoch 4.119: Loss = 0.245361
Epoch 4.120: Loss = 0.299622
TRAIN LOSS = 0.396866
TRAIN ACC = 88.2462 % (52950/60000)
Loss = 0.355637
Loss = 0.437897
Loss = 0.525299
Loss = 0.503693
Loss = 0.532211
Loss = 0.395584
Loss = 0.340897
Loss = 0.584946
Loss = 0.497345
Loss = 0.466995
Loss = 0.178024
Loss = 0.282913
Loss = 0.241516
Loss = 0.336853
Loss = 0.210068
Loss = 0.299377
Loss = 0.20932
Loss = 0.056015
Loss = 0.229523
Loss = 0.501053
TEST LOSS = 0.359258
TEST ACC = 529.5 % (8923/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.351227
Epoch 5.2: Loss = 0.488159
Epoch 5.3: Loss = 0.473648
Epoch 5.4: Loss = 0.303375
Epoch 5.5: Loss = 0.310516
Epoch 5.6: Loss = 0.322052
Epoch 5.7: Loss = 0.352371
Epoch 5.8: Loss = 0.343781
Epoch 5.9: Loss = 0.355164
Epoch 5.10: Loss = 0.344147
Epoch 5.11: Loss = 0.390549
Epoch 5.12: Loss = 0.350708
Epoch 5.13: Loss = 0.300568
Epoch 5.14: Loss = 0.321091
Epoch 5.15: Loss = 0.383133
Epoch 5.16: Loss = 0.405838
Epoch 5.17: Loss = 0.452316
Epoch 5.18: Loss = 0.565887
Epoch 5.19: Loss = 0.380905
Epoch 5.20: Loss = 0.320419
Epoch 5.21: Loss = 0.332977
Epoch 5.22: Loss = 0.281052
Epoch 5.23: Loss = 0.304123
Epoch 5.24: Loss = 0.496445
Epoch 5.25: Loss = 0.430481
Epoch 5.26: Loss = 0.501343
Epoch 5.27: Loss = 0.475784
Epoch 5.28: Loss = 0.423019
Epoch 5.29: Loss = 0.519577
Epoch 5.30: Loss = 0.511459
Epoch 5.31: Loss = 0.354721
Epoch 5.32: Loss = 0.432709
Epoch 5.33: Loss = 0.330627
Epoch 5.34: Loss = 0.423752
Epoch 5.35: Loss = 0.408188
Epoch 5.36: Loss = 0.44931
Epoch 5.37: Loss = 0.282471
Epoch 5.38: Loss = 0.33136
Epoch 5.39: Loss = 0.340652
Epoch 5.40: Loss = 0.342896
Epoch 5.41: Loss = 0.36615
Epoch 5.42: Loss = 0.515305
Epoch 5.43: Loss = 0.309357
Epoch 5.44: Loss = 0.278992
Epoch 5.45: Loss = 0.373749
Epoch 5.46: Loss = 0.446091
Epoch 5.47: Loss = 0.341125
Epoch 5.48: Loss = 0.40123
Epoch 5.49: Loss = 0.348816
Epoch 5.50: Loss = 0.445923
Epoch 5.51: Loss = 0.313889
Epoch 5.52: Loss = 0.304688
Epoch 5.53: Loss = 0.354202
Epoch 5.54: Loss = 0.470596
Epoch 5.55: Loss = 0.372177
Epoch 5.56: Loss = 0.33728
Epoch 5.57: Loss = 0.360611
Epoch 5.58: Loss = 0.371353
Epoch 5.59: Loss = 0.418228
Epoch 5.60: Loss = 0.496429
Epoch 5.61: Loss = 0.399506
Epoch 5.62: Loss = 0.460281
Epoch 5.63: Loss = 0.528519
Epoch 5.64: Loss = 0.492172
Epoch 5.65: Loss = 0.559647
Epoch 5.66: Loss = 0.376221
Epoch 5.67: Loss = 0.405685
Epoch 5.68: Loss = 0.248245
Epoch 5.69: Loss = 0.317184
Epoch 5.70: Loss = 0.483902
Epoch 5.71: Loss = 0.334595
Epoch 5.72: Loss = 0.332962
Epoch 5.73: Loss = 0.377625
Epoch 5.74: Loss = 0.300034
Epoch 5.75: Loss = 0.575562
Epoch 5.76: Loss = 0.418533
Epoch 5.77: Loss = 0.315552
Epoch 5.78: Loss = 0.349472
Epoch 5.79: Loss = 0.449173
Epoch 5.80: Loss = 0.415726
Epoch 5.81: Loss = 0.317795
Epoch 5.82: Loss = 0.286575
Epoch 5.83: Loss = 0.456223
Epoch 5.84: Loss = 0.376389
Epoch 5.85: Loss = 0.529404
Epoch 5.86: Loss = 0.468597
Epoch 5.87: Loss = 0.276184
Epoch 5.88: Loss = 0.373535
Epoch 5.89: Loss = 0.468231
Epoch 5.90: Loss = 0.311096
Epoch 5.91: Loss = 0.438126
Epoch 5.92: Loss = 0.429062
Epoch 5.93: Loss = 0.513092
Epoch 5.94: Loss = 0.30069
Epoch 5.95: Loss = 0.406128
Epoch 5.96: Loss = 0.43399
Epoch 5.97: Loss = 0.312103
Epoch 5.98: Loss = 0.335663
Epoch 5.99: Loss = 0.435287
Epoch 5.100: Loss = 0.587723
Epoch 5.101: Loss = 0.517517
Epoch 5.102: Loss = 0.364807
Epoch 5.103: Loss = 0.346207
Epoch 5.104: Loss = 0.313385
Epoch 5.105: Loss = 0.476044
Epoch 5.106: Loss = 0.51535
Epoch 5.107: Loss = 0.308228
Epoch 5.108: Loss = 0.451996
Epoch 5.109: Loss = 0.336121
Epoch 5.110: Loss = 0.400467
Epoch 5.111: Loss = 0.295181
Epoch 5.112: Loss = 0.308395
Epoch 5.113: Loss = 0.327499
Epoch 5.114: Loss = 0.282135
Epoch 5.115: Loss = 0.278961
Epoch 5.116: Loss = 0.314804
Epoch 5.117: Loss = 0.216675
Epoch 5.118: Loss = 0.193405
Epoch 5.119: Loss = 0.244278
Epoch 5.120: Loss = 0.284927
TRAIN LOSS = 0.384064
TRAIN ACC = 88.7939 % (53278/60000)
Loss = 0.349731
Loss = 0.446838
Loss = 0.529861
Loss = 0.52179
Loss = 0.545288
Loss = 0.384369
Loss = 0.326431
Loss = 0.596741
Loss = 0.480865
Loss = 0.476807
Loss = 0.166122
Loss = 0.268784
Loss = 0.246979
Loss = 0.32164
Loss = 0.189926
Loss = 0.28334
Loss = 0.199875
Loss = 0.0565796
Loss = 0.227066
Loss = 0.496521
TEST LOSS = 0.355778
TEST ACC = 532.779 % (8951/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.340439
Epoch 6.2: Loss = 0.477173
Epoch 6.3: Loss = 0.46785
Epoch 6.4: Loss = 0.295654
Epoch 6.5: Loss = 0.296616
Epoch 6.6: Loss = 0.32634
Epoch 6.7: Loss = 0.332962
Epoch 6.8: Loss = 0.340378
Epoch 6.9: Loss = 0.347961
Epoch 6.10: Loss = 0.339661
Epoch 6.11: Loss = 0.393051
Epoch 6.12: Loss = 0.351288
Epoch 6.13: Loss = 0.300171
Epoch 6.14: Loss = 0.312241
Epoch 6.15: Loss = 0.382996
Epoch 6.16: Loss = 0.401306
Epoch 6.17: Loss = 0.455643
Epoch 6.18: Loss = 0.579407
Epoch 6.19: Loss = 0.376343
Epoch 6.20: Loss = 0.318008
Epoch 6.21: Loss = 0.333359
Epoch 6.22: Loss = 0.281784
Epoch 6.23: Loss = 0.309891
Epoch 6.24: Loss = 0.483978
Epoch 6.25: Loss = 0.420868
Epoch 6.26: Loss = 0.502945
Epoch 6.27: Loss = 0.462585
Epoch 6.28: Loss = 0.427567
Epoch 6.29: Loss = 0.521957
Epoch 6.30: Loss = 0.528793
Epoch 6.31: Loss = 0.337753
Epoch 6.32: Loss = 0.424789
Epoch 6.33: Loss = 0.325745
Epoch 6.34: Loss = 0.422897
Epoch 6.35: Loss = 0.398941
Epoch 6.36: Loss = 0.439789
Epoch 6.37: Loss = 0.266632
Epoch 6.38: Loss = 0.329407
Epoch 6.39: Loss = 0.331238
Epoch 6.40: Loss = 0.332413
Epoch 6.41: Loss = 0.362701
Epoch 6.42: Loss = 0.529572
Epoch 6.43: Loss = 0.319733
Epoch 6.44: Loss = 0.271408
Epoch 6.45: Loss = 0.360947
Epoch 6.46: Loss = 0.450943
Epoch 6.47: Loss = 0.33522
Epoch 6.48: Loss = 0.407242
Epoch 6.49: Loss = 0.354843
Epoch 6.50: Loss = 0.473251
Epoch 6.51: Loss = 0.324905
Epoch 6.52: Loss = 0.30954
Epoch 6.53: Loss = 0.355804
Epoch 6.54: Loss = 0.468903
Epoch 6.55: Loss = 0.377991
Epoch 6.56: Loss = 0.34491
Epoch 6.57: Loss = 0.353287
Epoch 6.58: Loss = 0.365005
Epoch 6.59: Loss = 0.434357
Epoch 6.60: Loss = 0.504639
Epoch 6.61: Loss = 0.371552
Epoch 6.62: Loss = 0.464996
Epoch 6.63: Loss = 0.545441
Epoch 6.64: Loss = 0.499985
Epoch 6.65: Loss = 0.563934
Epoch 6.66: Loss = 0.385345
Epoch 6.67: Loss = 0.397614
Epoch 6.68: Loss = 0.252228
Epoch 6.69: Loss = 0.311783
Epoch 6.70: Loss = 0.481354
Epoch 6.71: Loss = 0.340271
Epoch 6.72: Loss = 0.31337
Epoch 6.73: Loss = 0.368835
Epoch 6.74: Loss = 0.321045
Epoch 6.75: Loss = 0.594681
Epoch 6.76: Loss = 0.395142
Epoch 6.77: Loss = 0.320221
Epoch 6.78: Loss = 0.331589
Epoch 6.79: Loss = 0.42627
Epoch 6.80: Loss = 0.41629
Epoch 6.81: Loss = 0.30191
Epoch 6.82: Loss = 0.282501
Epoch 6.83: Loss = 0.448456
Epoch 6.84: Loss = 0.372375
Epoch 6.85: Loss = 0.541397
Epoch 6.86: Loss = 0.472855
Epoch 6.87: Loss = 0.280472
Epoch 6.88: Loss = 0.373138
Epoch 6.89: Loss = 0.459412
Epoch 6.90: Loss = 0.293915
Epoch 6.91: Loss = 0.478592
Epoch 6.92: Loss = 0.435883
Epoch 6.93: Loss = 0.509354
Epoch 6.94: Loss = 0.297699
Epoch 6.95: Loss = 0.383011
Epoch 6.96: Loss = 0.416443
Epoch 6.97: Loss = 0.289551
Epoch 6.98: Loss = 0.334778
Epoch 6.99: Loss = 0.409973
Epoch 6.100: Loss = 0.571274
Epoch 6.101: Loss = 0.50647
Epoch 6.102: Loss = 0.357178
Epoch 6.103: Loss = 0.356262
Epoch 6.104: Loss = 0.304184
Epoch 6.105: Loss = 0.456345
Epoch 6.106: Loss = 0.509903
Epoch 6.107: Loss = 0.293243
Epoch 6.108: Loss = 0.45369
Epoch 6.109: Loss = 0.316498
Epoch 6.110: Loss = 0.397812
Epoch 6.111: Loss = 0.291519
Epoch 6.112: Loss = 0.30687
Epoch 6.113: Loss = 0.314743
Epoch 6.114: Loss = 0.275681
Epoch 6.115: Loss = 0.273254
Epoch 6.116: Loss = 0.321625
Epoch 6.117: Loss = 0.208176
Epoch 6.118: Loss = 0.185333
Epoch 6.119: Loss = 0.236893
Epoch 6.120: Loss = 0.281326
TRAIN LOSS = 0.381042
TRAIN ACC = 89.1724 % (53506/60000)
Loss = 0.360123
Loss = 0.449081
Loss = 0.514008
Loss = 0.518112
Loss = 0.555466
Loss = 0.383759
Loss = 0.321289
Loss = 0.621719
Loss = 0.490616
Loss = 0.471664
Loss = 0.14505
Loss = 0.269287
Loss = 0.247101
Loss = 0.317139
Loss = 0.184616
Loss = 0.273102
Loss = 0.174927
Loss = 0.0514374
Loss = 0.223709
Loss = 0.500595
TEST LOSS = 0.35364
TEST ACC = 535.059 % (8977/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.357254
Epoch 7.2: Loss = 0.480072
Epoch 7.3: Loss = 0.456528
Epoch 7.4: Loss = 0.289063
Epoch 7.5: Loss = 0.275024
Epoch 7.6: Loss = 0.304153
Epoch 7.7: Loss = 0.325974
Epoch 7.8: Loss = 0.311935
Epoch 7.9: Loss = 0.328659
Epoch 7.10: Loss = 0.320221
Epoch 7.11: Loss = 0.389648
Epoch 7.12: Loss = 0.354462
Epoch 7.13: Loss = 0.300797
Epoch 7.14: Loss = 0.297073
Epoch 7.15: Loss = 0.370026
Epoch 7.16: Loss = 0.383499
Epoch 7.17: Loss = 0.435379
Epoch 7.18: Loss = 0.556671
Epoch 7.19: Loss = 0.361328
Epoch 7.20: Loss = 0.297897
Epoch 7.21: Loss = 0.340103
Epoch 7.22: Loss = 0.264496
Epoch 7.23: Loss = 0.292099
Epoch 7.24: Loss = 0.453445
Epoch 7.25: Loss = 0.421082
Epoch 7.26: Loss = 0.516785
Epoch 7.27: Loss = 0.432251
Epoch 7.28: Loss = 0.419662
Epoch 7.29: Loss = 0.508789
Epoch 7.30: Loss = 0.501404
Epoch 7.31: Loss = 0.340607
Epoch 7.32: Loss = 0.406937
Epoch 7.33: Loss = 0.310135
Epoch 7.34: Loss = 0.393143
Epoch 7.35: Loss = 0.397583
Epoch 7.36: Loss = 0.430862
Epoch 7.37: Loss = 0.267639
Epoch 7.38: Loss = 0.331406
Epoch 7.39: Loss = 0.312408
Epoch 7.40: Loss = 0.312561
Epoch 7.41: Loss = 0.339371
Epoch 7.42: Loss = 0.547226
Epoch 7.43: Loss = 0.312622
Epoch 7.44: Loss = 0.27858
Epoch 7.45: Loss = 0.364288
Epoch 7.46: Loss = 0.438293
Epoch 7.47: Loss = 0.323715
Epoch 7.48: Loss = 0.384949
Epoch 7.49: Loss = 0.337555
Epoch 7.50: Loss = 0.434433
Epoch 7.51: Loss = 0.315811
Epoch 7.52: Loss = 0.322235
Epoch 7.53: Loss = 0.320313
Epoch 7.54: Loss = 0.454453
Epoch 7.55: Loss = 0.364151
Epoch 7.56: Loss = 0.335052
Epoch 7.57: Loss = 0.348984
Epoch 7.58: Loss = 0.36998
Epoch 7.59: Loss = 0.415421
Epoch 7.60: Loss = 0.488602
Epoch 7.61: Loss = 0.365173
Epoch 7.62: Loss = 0.466354
Epoch 7.63: Loss = 0.554565
Epoch 7.64: Loss = 0.495575
Epoch 7.65: Loss = 0.565262
Epoch 7.66: Loss = 0.376114
Epoch 7.67: Loss = 0.370667
Epoch 7.68: Loss = 0.230698
Epoch 7.69: Loss = 0.304306
Epoch 7.70: Loss = 0.465973
Epoch 7.71: Loss = 0.346512
Epoch 7.72: Loss = 0.294464
Epoch 7.73: Loss = 0.35762
Epoch 7.74: Loss = 0.293884
Epoch 7.75: Loss = 0.608109
Epoch 7.76: Loss = 0.378693
Epoch 7.77: Loss = 0.320801
Epoch 7.78: Loss = 0.320724
Epoch 7.79: Loss = 0.424988
Epoch 7.80: Loss = 0.408981
Epoch 7.81: Loss = 0.302032
Epoch 7.82: Loss = 0.277512
Epoch 7.83: Loss = 0.42543
Epoch 7.84: Loss = 0.377869
Epoch 7.85: Loss = 0.542786
Epoch 7.86: Loss = 0.462006
Epoch 7.87: Loss = 0.284378
Epoch 7.88: Loss = 0.358871
Epoch 7.89: Loss = 0.444351
Epoch 7.90: Loss = 0.277985
Epoch 7.91: Loss = 0.468552
Epoch 7.92: Loss = 0.418259
Epoch 7.93: Loss = 0.50589
Epoch 7.94: Loss = 0.277878
Epoch 7.95: Loss = 0.376297
Epoch 7.96: Loss = 0.4272
Epoch 7.97: Loss = 0.291367
Epoch 7.98: Loss = 0.33046
Epoch 7.99: Loss = 0.408386
Epoch 7.100: Loss = 0.574768
Epoch 7.101: Loss = 0.517685
Epoch 7.102: Loss = 0.336136
Epoch 7.103: Loss = 0.343536
Epoch 7.104: Loss = 0.32251
Epoch 7.105: Loss = 0.458527
Epoch 7.106: Loss = 0.496002
Epoch 7.107: Loss = 0.277145
Epoch 7.108: Loss = 0.450897
Epoch 7.109: Loss = 0.310211
Epoch 7.110: Loss = 0.387177
Epoch 7.111: Loss = 0.282257
Epoch 7.112: Loss = 0.314758
Epoch 7.113: Loss = 0.314987
Epoch 7.114: Loss = 0.282455
Epoch 7.115: Loss = 0.261993
Epoch 7.116: Loss = 0.314255
Epoch 7.117: Loss = 0.210831
Epoch 7.118: Loss = 0.193924
Epoch 7.119: Loss = 0.215759
Epoch 7.120: Loss = 0.296509
TRAIN LOSS = 0.37262
TRAIN ACC = 89.5065 % (53706/60000)
Loss = 0.341034
Loss = 0.435211
Loss = 0.502411
Loss = 0.525696
Loss = 0.532135
Loss = 0.363785
Loss = 0.305298
Loss = 0.614914
Loss = 0.47995
Loss = 0.454865
Loss = 0.155746
Loss = 0.252991
Loss = 0.258896
Loss = 0.310989
Loss = 0.199722
Loss = 0.280884
Loss = 0.175888
Loss = 0.0466003
Loss = 0.208298
Loss = 0.502457
TEST LOSS = 0.347388
TEST ACC = 537.059 % (9019/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.349884
Epoch 8.2: Loss = 0.452454
Epoch 8.3: Loss = 0.465958
Epoch 8.4: Loss = 0.281418
Epoch 8.5: Loss = 0.267288
Epoch 8.6: Loss = 0.313477
Epoch 8.7: Loss = 0.322876
Epoch 8.8: Loss = 0.294815
Epoch 8.9: Loss = 0.328552
Epoch 8.10: Loss = 0.331055
Epoch 8.11: Loss = 0.383575
Epoch 8.12: Loss = 0.353989
Epoch 8.13: Loss = 0.292664
Epoch 8.14: Loss = 0.297729
Epoch 8.15: Loss = 0.361359
Epoch 8.16: Loss = 0.391052
Epoch 8.17: Loss = 0.428101
Epoch 8.18: Loss = 0.561432
Epoch 8.19: Loss = 0.365509
Epoch 8.20: Loss = 0.281433
Epoch 8.21: Loss = 0.331635
Epoch 8.22: Loss = 0.270752
Epoch 8.23: Loss = 0.291016
Epoch 8.24: Loss = 0.487915
Epoch 8.25: Loss = 0.416214
Epoch 8.26: Loss = 0.509964
Epoch 8.27: Loss = 0.434021
Epoch 8.28: Loss = 0.41217
Epoch 8.29: Loss = 0.5177
Epoch 8.30: Loss = 0.508301
Epoch 8.31: Loss = 0.328354
Epoch 8.32: Loss = 0.40889
Epoch 8.33: Loss = 0.304718
Epoch 8.34: Loss = 0.407806
Epoch 8.35: Loss = 0.386963
Epoch 8.36: Loss = 0.414032
Epoch 8.37: Loss = 0.266525
Epoch 8.38: Loss = 0.32196
Epoch 8.39: Loss = 0.302872
Epoch 8.40: Loss = 0.306076
Epoch 8.41: Loss = 0.329468
Epoch 8.42: Loss = 0.545547
Epoch 8.43: Loss = 0.317612
Epoch 8.44: Loss = 0.269638
Epoch 8.45: Loss = 0.356613
Epoch 8.46: Loss = 0.43631
Epoch 8.47: Loss = 0.320374
Epoch 8.48: Loss = 0.360672
Epoch 8.49: Loss = 0.327148
Epoch 8.50: Loss = 0.455948
Epoch 8.51: Loss = 0.307358
Epoch 8.52: Loss = 0.295944
Epoch 8.53: Loss = 0.316208
Epoch 8.54: Loss = 0.449203
Epoch 8.55: Loss = 0.363525
Epoch 8.56: Loss = 0.30603
Epoch 8.57: Loss = 0.345139
Epoch 8.58: Loss = 0.368683
Epoch 8.59: Loss = 0.430222
Epoch 8.60: Loss = 0.503967
Epoch 8.61: Loss = 0.352554
Epoch 8.62: Loss = 0.459991
Epoch 8.63: Loss = 0.547882
Epoch 8.64: Loss = 0.466919
Epoch 8.65: Loss = 0.54628
Epoch 8.66: Loss = 0.373856
Epoch 8.67: Loss = 0.346985
Epoch 8.68: Loss = 0.224838
Epoch 8.69: Loss = 0.305176
Epoch 8.70: Loss = 0.446121
Epoch 8.71: Loss = 0.345169
Epoch 8.72: Loss = 0.274094
Epoch 8.73: Loss = 0.339645
Epoch 8.74: Loss = 0.293472
Epoch 8.75: Loss = 0.639801
Epoch 8.76: Loss = 0.398529
Epoch 8.77: Loss = 0.301865
Epoch 8.78: Loss = 0.323669
Epoch 8.79: Loss = 0.40863
Epoch 8.80: Loss = 0.39386
Epoch 8.81: Loss = 0.305206
Epoch 8.82: Loss = 0.268661
Epoch 8.83: Loss = 0.435745
Epoch 8.84: Loss = 0.368713
Epoch 8.85: Loss = 0.542572
Epoch 8.86: Loss = 0.46434
Epoch 8.87: Loss = 0.267151
Epoch 8.88: Loss = 0.346298
Epoch 8.89: Loss = 0.442429
Epoch 8.90: Loss = 0.267288
Epoch 8.91: Loss = 0.467926
Epoch 8.92: Loss = 0.398224
Epoch 8.93: Loss = 0.504166
Epoch 8.94: Loss = 0.294846
Epoch 8.95: Loss = 0.364517
Epoch 8.96: Loss = 0.416779
Epoch 8.97: Loss = 0.28894
Epoch 8.98: Loss = 0.312271
Epoch 8.99: Loss = 0.39682
Epoch 8.100: Loss = 0.559341
Epoch 8.101: Loss = 0.522568
Epoch 8.102: Loss = 0.350739
Epoch 8.103: Loss = 0.342056
Epoch 8.104: Loss = 0.336624
Epoch 8.105: Loss = 0.470032
Epoch 8.106: Loss = 0.495453
Epoch 8.107: Loss = 0.273972
Epoch 8.108: Loss = 0.458908
Epoch 8.109: Loss = 0.311523
Epoch 8.110: Loss = 0.393753
Epoch 8.111: Loss = 0.273193
Epoch 8.112: Loss = 0.31694
Epoch 8.113: Loss = 0.337952
Epoch 8.114: Loss = 0.272278
Epoch 8.115: Loss = 0.263351
Epoch 8.116: Loss = 0.331131
Epoch 8.117: Loss = 0.206146
Epoch 8.118: Loss = 0.182022
Epoch 8.119: Loss = 0.217514
Epoch 8.120: Loss = 0.301102
TRAIN LOSS = 0.369308
TRAIN ACC = 89.6851 % (53813/60000)
Loss = 0.322281
Loss = 0.442932
Loss = 0.495499
Loss = 0.542542
Loss = 0.548126
Loss = 0.363693
Loss = 0.325729
Loss = 0.622864
Loss = 0.475403
Loss = 0.459641
Loss = 0.138397
Loss = 0.260712
Loss = 0.252472
Loss = 0.319244
Loss = 0.187546
Loss = 0.256836
Loss = 0.172699
Loss = 0.0550995
Loss = 0.215988
Loss = 0.498383
TEST LOSS = 0.347804
TEST ACC = 538.129 % (9036/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.335464
Epoch 9.2: Loss = 0.44487
Epoch 9.3: Loss = 0.476074
Epoch 9.4: Loss = 0.283936
Epoch 9.5: Loss = 0.273163
Epoch 9.6: Loss = 0.308975
Epoch 9.7: Loss = 0.313644
Epoch 9.8: Loss = 0.307495
Epoch 9.9: Loss = 0.335205
Epoch 9.10: Loss = 0.337372
Epoch 9.11: Loss = 0.389435
Epoch 9.12: Loss = 0.355484
Epoch 9.13: Loss = 0.280869
Epoch 9.14: Loss = 0.282425
Epoch 9.15: Loss = 0.368027
Epoch 9.16: Loss = 0.377502
Epoch 9.17: Loss = 0.431107
Epoch 9.18: Loss = 0.544357
Epoch 9.19: Loss = 0.377213
Epoch 9.20: Loss = 0.294006
Epoch 9.21: Loss = 0.339966
Epoch 9.22: Loss = 0.271729
Epoch 9.23: Loss = 0.270432
Epoch 9.24: Loss = 0.475998
Epoch 9.25: Loss = 0.393463
Epoch 9.26: Loss = 0.516418
Epoch 9.27: Loss = 0.427261
Epoch 9.28: Loss = 0.42981
Epoch 9.29: Loss = 0.526077
Epoch 9.30: Loss = 0.53154
Epoch 9.31: Loss = 0.339142
Epoch 9.32: Loss = 0.411026
Epoch 9.33: Loss = 0.315308
Epoch 9.34: Loss = 0.387695
Epoch 9.35: Loss = 0.370789
Epoch 9.36: Loss = 0.406311
Epoch 9.37: Loss = 0.280853
Epoch 9.38: Loss = 0.332245
Epoch 9.39: Loss = 0.309799
Epoch 9.40: Loss = 0.327423
Epoch 9.41: Loss = 0.315918
Epoch 9.42: Loss = 0.553818
Epoch 9.43: Loss = 0.308044
Epoch 9.44: Loss = 0.264755
Epoch 9.45: Loss = 0.37851
Epoch 9.46: Loss = 0.43927
Epoch 9.47: Loss = 0.328903
Epoch 9.48: Loss = 0.370239
Epoch 9.49: Loss = 0.354996
Epoch 9.50: Loss = 0.452408
Epoch 9.51: Loss = 0.298737
Epoch 9.52: Loss = 0.295746
Epoch 9.53: Loss = 0.336243
Epoch 9.54: Loss = 0.460922
Epoch 9.55: Loss = 0.361954
Epoch 9.56: Loss = 0.317581
Epoch 9.57: Loss = 0.352814
Epoch 9.58: Loss = 0.377151
Epoch 9.59: Loss = 0.426041
Epoch 9.60: Loss = 0.488983
Epoch 9.61: Loss = 0.349472
Epoch 9.62: Loss = 0.475098
Epoch 9.63: Loss = 0.550583
Epoch 9.64: Loss = 0.468353
Epoch 9.65: Loss = 0.551376
Epoch 9.66: Loss = 0.372589
Epoch 9.67: Loss = 0.359451
Epoch 9.68: Loss = 0.231689
Epoch 9.69: Loss = 0.308868
Epoch 9.70: Loss = 0.446243
Epoch 9.71: Loss = 0.357635
Epoch 9.72: Loss = 0.263947
Epoch 9.73: Loss = 0.352097
Epoch 9.74: Loss = 0.298569
Epoch 9.75: Loss = 0.658279
Epoch 9.76: Loss = 0.40892
Epoch 9.77: Loss = 0.297333
Epoch 9.78: Loss = 0.334534
Epoch 9.79: Loss = 0.409012
Epoch 9.80: Loss = 0.396988
Epoch 9.81: Loss = 0.310883
Epoch 9.82: Loss = 0.269012
Epoch 9.83: Loss = 0.451035
Epoch 9.84: Loss = 0.356171
Epoch 9.85: Loss = 0.533325
Epoch 9.86: Loss = 0.46994
Epoch 9.87: Loss = 0.270523
Epoch 9.88: Loss = 0.351059
Epoch 9.89: Loss = 0.447754
Epoch 9.90: Loss = 0.287003
Epoch 9.91: Loss = 0.453201
Epoch 9.92: Loss = 0.417206
Epoch 9.93: Loss = 0.543243
Epoch 9.94: Loss = 0.284973
Epoch 9.95: Loss = 0.386398
Epoch 9.96: Loss = 0.41037
Epoch 9.97: Loss = 0.288086
Epoch 9.98: Loss = 0.315094
Epoch 9.99: Loss = 0.412979
Epoch 9.100: Loss = 0.57486
Epoch 9.101: Loss = 0.533966
Epoch 9.102: Loss = 0.368683
Epoch 9.103: Loss = 0.354752
Epoch 9.104: Loss = 0.344254
Epoch 9.105: Loss = 0.480621
Epoch 9.106: Loss = 0.503036
Epoch 9.107: Loss = 0.269241
Epoch 9.108: Loss = 0.454071
Epoch 9.109: Loss = 0.310776
Epoch 9.110: Loss = 0.391541
Epoch 9.111: Loss = 0.280838
Epoch 9.112: Loss = 0.316925
Epoch 9.113: Loss = 0.343643
Epoch 9.114: Loss = 0.268463
Epoch 9.115: Loss = 0.258728
Epoch 9.116: Loss = 0.324066
Epoch 9.117: Loss = 0.216171
Epoch 9.118: Loss = 0.178452
Epoch 9.119: Loss = 0.22171
Epoch 9.120: Loss = 0.308548
TRAIN LOSS = 0.372635
TRAIN ACC = 89.7369 % (53845/60000)
Loss = 0.340225
Loss = 0.440765
Loss = 0.51004
Loss = 0.556427
Loss = 0.559418
Loss = 0.358047
Loss = 0.323761
Loss = 0.634903
Loss = 0.477676
Loss = 0.454819
Loss = 0.136612
Loss = 0.257645
Loss = 0.258362
Loss = 0.315414
Loss = 0.179428
Loss = 0.269928
Loss = 0.162552
Loss = 0.0566711
Loss = 0.20816
Loss = 0.512375
TEST LOSS = 0.350661
TEST ACC = 538.449 % (9016/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.351486
Epoch 10.2: Loss = 0.459915
Epoch 10.3: Loss = 0.513245
Epoch 10.4: Loss = 0.284683
Epoch 10.5: Loss = 0.298492
Epoch 10.6: Loss = 0.311798
Epoch 10.7: Loss = 0.315979
Epoch 10.8: Loss = 0.295822
Epoch 10.9: Loss = 0.334229
Epoch 10.10: Loss = 0.342514
Epoch 10.11: Loss = 0.39032
Epoch 10.12: Loss = 0.346664
Epoch 10.13: Loss = 0.270264
Epoch 10.14: Loss = 0.296722
Epoch 10.15: Loss = 0.368256
Epoch 10.16: Loss = 0.366547
Epoch 10.17: Loss = 0.453979
Epoch 10.18: Loss = 0.548599
Epoch 10.19: Loss = 0.393799
Epoch 10.20: Loss = 0.292557
Epoch 10.21: Loss = 0.370529
Epoch 10.22: Loss = 0.272247
Epoch 10.23: Loss = 0.266464
Epoch 10.24: Loss = 0.502502
Epoch 10.25: Loss = 0.395584
Epoch 10.26: Loss = 0.518448
Epoch 10.27: Loss = 0.447968
Epoch 10.28: Loss = 0.427628
Epoch 10.29: Loss = 0.516663
Epoch 10.30: Loss = 0.527725
Epoch 10.31: Loss = 0.340134
Epoch 10.32: Loss = 0.4263
Epoch 10.33: Loss = 0.314728
Epoch 10.34: Loss = 0.387085
Epoch 10.35: Loss = 0.395294
Epoch 10.36: Loss = 0.412109
Epoch 10.37: Loss = 0.265411
Epoch 10.38: Loss = 0.344208
Epoch 10.39: Loss = 0.317886
Epoch 10.40: Loss = 0.35379
Epoch 10.41: Loss = 0.330109
Epoch 10.42: Loss = 0.564941
Epoch 10.43: Loss = 0.329254
Epoch 10.44: Loss = 0.261765
Epoch 10.45: Loss = 0.388428
Epoch 10.46: Loss = 0.432266
Epoch 10.47: Loss = 0.35611
Epoch 10.48: Loss = 0.369888
Epoch 10.49: Loss = 0.3423
Epoch 10.50: Loss = 0.447006
Epoch 10.51: Loss = 0.283295
Epoch 10.52: Loss = 0.291397
Epoch 10.53: Loss = 0.330734
Epoch 10.54: Loss = 0.475113
Epoch 10.55: Loss = 0.342224
Epoch 10.56: Loss = 0.328934
Epoch 10.57: Loss = 0.366379
Epoch 10.58: Loss = 0.373474
Epoch 10.59: Loss = 0.415985
Epoch 10.60: Loss = 0.501434
Epoch 10.61: Loss = 0.348999
Epoch 10.62: Loss = 0.465683
Epoch 10.63: Loss = 0.565811
Epoch 10.64: Loss = 0.479492
Epoch 10.65: Loss = 0.550888
Epoch 10.66: Loss = 0.3871
Epoch 10.67: Loss = 0.354935
Epoch 10.68: Loss = 0.219727
Epoch 10.69: Loss = 0.32048
Epoch 10.70: Loss = 0.451736
Epoch 10.71: Loss = 0.3508
Epoch 10.72: Loss = 0.255539
Epoch 10.73: Loss = 0.353165
Epoch 10.74: Loss = 0.313766
Epoch 10.75: Loss = 0.651474
Epoch 10.76: Loss = 0.416824
Epoch 10.77: Loss = 0.304886
Epoch 10.78: Loss = 0.341248
Epoch 10.79: Loss = 0.411865
Epoch 10.80: Loss = 0.416962
Epoch 10.81: Loss = 0.295456
Epoch 10.82: Loss = 0.271713
Epoch 10.83: Loss = 0.461838
Epoch 10.84: Loss = 0.346268
Epoch 10.85: Loss = 0.545547
Epoch 10.86: Loss = 0.479401
Epoch 10.87: Loss = 0.288696
Epoch 10.88: Loss = 0.3694
Epoch 10.89: Loss = 0.458984
Epoch 10.90: Loss = 0.273483
Epoch 10.91: Loss = 0.484055
Epoch 10.92: Loss = 0.427353
Epoch 10.93: Loss = 0.52562
Epoch 10.94: Loss = 0.285965
Epoch 10.95: Loss = 0.380905
Epoch 10.96: Loss = 0.427856
Epoch 10.97: Loss = 0.298508
Epoch 10.98: Loss = 0.307587
Epoch 10.99: Loss = 0.39653
Epoch 10.100: Loss = 0.562851
Epoch 10.101: Loss = 0.533829
Epoch 10.102: Loss = 0.381653
Epoch 10.103: Loss = 0.337479
Epoch 10.104: Loss = 0.33168
Epoch 10.105: Loss = 0.488892
Epoch 10.106: Loss = 0.512329
Epoch 10.107: Loss = 0.265686
Epoch 10.108: Loss = 0.43927
Epoch 10.109: Loss = 0.288101
Epoch 10.110: Loss = 0.405548
Epoch 10.111: Loss = 0.289261
Epoch 10.112: Loss = 0.327454
Epoch 10.113: Loss = 0.336456
Epoch 10.114: Loss = 0.281662
Epoch 10.115: Loss = 0.260406
Epoch 10.116: Loss = 0.345352
Epoch 10.117: Loss = 0.208221
Epoch 10.118: Loss = 0.17186
Epoch 10.119: Loss = 0.232407
Epoch 10.120: Loss = 0.30098
TRAIN LOSS = 0.376312
TRAIN ACC = 89.9017 % (53943/60000)
Loss = 0.345108
Loss = 0.430145
Loss = 0.505112
Loss = 0.562531
Loss = 0.568848
Loss = 0.353531
Loss = 0.336853
Loss = 0.61969
Loss = 0.489456
Loss = 0.475983
Loss = 0.15448
Loss = 0.2491
Loss = 0.254791
Loss = 0.325195
Loss = 0.170471
Loss = 0.274033
Loss = 0.164642
Loss = 0.0558777
Loss = 0.18959
Loss = 0.509766
TEST LOSS = 0.35176
TEST ACC = 539.429 % (9048/10000)
