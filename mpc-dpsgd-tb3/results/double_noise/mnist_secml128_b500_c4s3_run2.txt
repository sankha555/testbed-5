Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.38116
Epoch 1.2: Loss = 2.34338
Epoch 1.3: Loss = 2.31203
Epoch 1.4: Loss = 2.31145
Epoch 1.5: Loss = 2.2522
Epoch 1.6: Loss = 2.22563
Epoch 1.7: Loss = 2.20128
Epoch 1.8: Loss = 2.13869
Epoch 1.9: Loss = 2.11713
Epoch 1.10: Loss = 2.0793
Epoch 1.11: Loss = 2.06828
Epoch 1.12: Loss = 2.03831
Epoch 1.13: Loss = 1.96158
Epoch 1.14: Loss = 1.99808
Epoch 1.15: Loss = 1.98854
Epoch 1.16: Loss = 1.96135
Epoch 1.17: Loss = 1.90536
Epoch 1.18: Loss = 1.88702
Epoch 1.19: Loss = 1.83595
Epoch 1.20: Loss = 1.7982
Epoch 1.21: Loss = 1.74861
Epoch 1.22: Loss = 1.7574
Epoch 1.23: Loss = 1.68195
Epoch 1.24: Loss = 1.7473
Epoch 1.25: Loss = 1.68001
Epoch 1.26: Loss = 1.67278
Epoch 1.27: Loss = 1.64813
Epoch 1.28: Loss = 1.61346
Epoch 1.29: Loss = 1.58235
Epoch 1.30: Loss = 1.61632
Epoch 1.31: Loss = 1.52019
Epoch 1.32: Loss = 1.54793
Epoch 1.33: Loss = 1.44814
Epoch 1.34: Loss = 1.46199
Epoch 1.35: Loss = 1.39322
Epoch 1.36: Loss = 1.49117
Epoch 1.37: Loss = 1.38589
Epoch 1.38: Loss = 1.28549
Epoch 1.39: Loss = 1.30013
Epoch 1.40: Loss = 1.21887
Epoch 1.41: Loss = 1.28085
Epoch 1.42: Loss = 1.26169
Epoch 1.43: Loss = 1.21208
Epoch 1.44: Loss = 1.15414
Epoch 1.45: Loss = 1.2471
Epoch 1.46: Loss = 1.16876
Epoch 1.47: Loss = 1.09421
Epoch 1.48: Loss = 1.20108
Epoch 1.49: Loss = 1.11623
Epoch 1.50: Loss = 1.1806
Epoch 1.51: Loss = 0.991196
Epoch 1.52: Loss = 1.03481
Epoch 1.53: Loss = 1.04543
Epoch 1.54: Loss = 1.06349
Epoch 1.55: Loss = 1.0638
Epoch 1.56: Loss = 0.985886
Epoch 1.57: Loss = 0.894089
Epoch 1.58: Loss = 0.979767
Epoch 1.59: Loss = 0.978821
Epoch 1.60: Loss = 1.05612
Epoch 1.61: Loss = 0.99527
Epoch 1.62: Loss = 1.03052
Epoch 1.63: Loss = 1.03661
Epoch 1.64: Loss = 0.990692
Epoch 1.65: Loss = 1.05121
Epoch 1.66: Loss = 0.911179
Epoch 1.67: Loss = 0.921021
Epoch 1.68: Loss = 0.745193
Epoch 1.69: Loss = 0.819733
Epoch 1.70: Loss = 0.913055
Epoch 1.71: Loss = 0.853958
Epoch 1.72: Loss = 0.806122
Epoch 1.73: Loss = 0.848221
Epoch 1.74: Loss = 0.731506
Epoch 1.75: Loss = 0.861542
Epoch 1.76: Loss = 0.798752
Epoch 1.77: Loss = 0.798004
Epoch 1.78: Loss = 0.75032
Epoch 1.79: Loss = 0.779358
Epoch 1.80: Loss = 0.869598
Epoch 1.81: Loss = 0.722992
Epoch 1.82: Loss = 0.703918
Epoch 1.83: Loss = 0.880859
Epoch 1.84: Loss = 0.766327
Epoch 1.85: Loss = 0.855072
Epoch 1.86: Loss = 0.765152
Epoch 1.87: Loss = 0.690643
Epoch 1.88: Loss = 0.706726
Epoch 1.89: Loss = 0.797485
Epoch 1.90: Loss = 0.677643
Epoch 1.91: Loss = 0.744705
Epoch 1.92: Loss = 0.7005
Epoch 1.93: Loss = 0.788132
Epoch 1.94: Loss = 0.583481
Epoch 1.95: Loss = 0.721283
Epoch 1.96: Loss = 0.730682
Epoch 1.97: Loss = 0.535706
Epoch 1.98: Loss = 0.659195
Epoch 1.99: Loss = 0.771576
Epoch 1.100: Loss = 0.85585
Epoch 1.101: Loss = 0.707184
Epoch 1.102: Loss = 0.655685
Epoch 1.103: Loss = 0.603363
Epoch 1.104: Loss = 0.585205
Epoch 1.105: Loss = 0.757629
Epoch 1.106: Loss = 0.687576
Epoch 1.107: Loss = 0.571136
Epoch 1.108: Loss = 0.657394
Epoch 1.109: Loss = 0.60228
Epoch 1.110: Loss = 0.637573
Epoch 1.111: Loss = 0.514938
Epoch 1.112: Loss = 0.507629
Epoch 1.113: Loss = 0.590637
Epoch 1.114: Loss = 0.544525
Epoch 1.115: Loss = 0.587265
Epoch 1.116: Loss = 0.579926
Epoch 1.117: Loss = 0.447891
Epoch 1.118: Loss = 0.419785
Epoch 1.119: Loss = 0.448715
Epoch 1.120: Loss = 0.453384
TRAIN LOSS = 1.15289
TRAIN ACC = 67.749 % (40651/60000)
Loss = 0.601852
Loss = 0.618683
Loss = 0.762741
Loss = 0.715591
Loss = 0.770981
Loss = 0.631546
Loss = 0.581573
Loss = 0.796356
Loss = 0.723938
Loss = 0.686417
Loss = 0.382904
Loss = 0.514542
Loss = 0.37471
Loss = 0.578888
Loss = 0.471588
Loss = 0.444
Loss = 0.450424
Loss = 0.224503
Loss = 0.442505
Loss = 0.699829
TEST LOSS = 0.573678
TEST ACC = 406.509 % (8264/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.572708
Epoch 2.2: Loss = 0.696838
Epoch 2.3: Loss = 0.667023
Epoch 2.4: Loss = 0.536392
Epoch 2.5: Loss = 0.537811
Epoch 2.6: Loss = 0.504105
Epoch 2.7: Loss = 0.592957
Epoch 2.8: Loss = 0.534607
Epoch 2.9: Loss = 0.540985
Epoch 2.10: Loss = 0.550049
Epoch 2.11: Loss = 0.535202
Epoch 2.12: Loss = 0.528839
Epoch 2.13: Loss = 0.479553
Epoch 2.14: Loss = 0.51683
Epoch 2.15: Loss = 0.649872
Epoch 2.16: Loss = 0.639908
Epoch 2.17: Loss = 0.615051
Epoch 2.18: Loss = 0.67981
Epoch 2.19: Loss = 0.526413
Epoch 2.20: Loss = 0.489136
Epoch 2.21: Loss = 0.485916
Epoch 2.22: Loss = 0.469406
Epoch 2.23: Loss = 0.471558
Epoch 2.24: Loss = 0.677917
Epoch 2.25: Loss = 0.552307
Epoch 2.26: Loss = 0.629318
Epoch 2.27: Loss = 0.666763
Epoch 2.28: Loss = 0.629318
Epoch 2.29: Loss = 0.690933
Epoch 2.30: Loss = 0.718674
Epoch 2.31: Loss = 0.485901
Epoch 2.32: Loss = 0.650497
Epoch 2.33: Loss = 0.510834
Epoch 2.34: Loss = 0.601089
Epoch 2.35: Loss = 0.576828
Epoch 2.36: Loss = 0.635223
Epoch 2.37: Loss = 0.493881
Epoch 2.38: Loss = 0.447235
Epoch 2.39: Loss = 0.504166
Epoch 2.40: Loss = 0.453278
Epoch 2.41: Loss = 0.562836
Epoch 2.42: Loss = 0.614502
Epoch 2.43: Loss = 0.475998
Epoch 2.44: Loss = 0.427261
Epoch 2.45: Loss = 0.545166
Epoch 2.46: Loss = 0.576874
Epoch 2.47: Loss = 0.500473
Epoch 2.48: Loss = 0.556976
Epoch 2.49: Loss = 0.542511
Epoch 2.50: Loss = 0.648117
Epoch 2.51: Loss = 0.456543
Epoch 2.52: Loss = 0.450989
Epoch 2.53: Loss = 0.48291
Epoch 2.54: Loss = 0.585312
Epoch 2.55: Loss = 0.515701
Epoch 2.56: Loss = 0.494522
Epoch 2.57: Loss = 0.456757
Epoch 2.58: Loss = 0.549408
Epoch 2.59: Loss = 0.564957
Epoch 2.60: Loss = 0.637604
Epoch 2.61: Loss = 0.614136
Epoch 2.62: Loss = 0.592499
Epoch 2.63: Loss = 0.654785
Epoch 2.64: Loss = 0.57048
Epoch 2.65: Loss = 0.719925
Epoch 2.66: Loss = 0.534225
Epoch 2.67: Loss = 0.56308
Epoch 2.68: Loss = 0.329773
Epoch 2.69: Loss = 0.438828
Epoch 2.70: Loss = 0.593109
Epoch 2.71: Loss = 0.467621
Epoch 2.72: Loss = 0.456299
Epoch 2.73: Loss = 0.483337
Epoch 2.74: Loss = 0.384018
Epoch 2.75: Loss = 0.674835
Epoch 2.76: Loss = 0.498428
Epoch 2.77: Loss = 0.448746
Epoch 2.78: Loss = 0.459854
Epoch 2.79: Loss = 0.546463
Epoch 2.80: Loss = 0.567093
Epoch 2.81: Loss = 0.451614
Epoch 2.82: Loss = 0.400574
Epoch 2.83: Loss = 0.615356
Epoch 2.84: Loss = 0.497253
Epoch 2.85: Loss = 0.626282
Epoch 2.86: Loss = 0.54715
Epoch 2.87: Loss = 0.42305
Epoch 2.88: Loss = 0.496384
Epoch 2.89: Loss = 0.581711
Epoch 2.90: Loss = 0.467712
Epoch 2.91: Loss = 0.566772
Epoch 2.92: Loss = 0.513504
Epoch 2.93: Loss = 0.625153
Epoch 2.94: Loss = 0.369431
Epoch 2.95: Loss = 0.495926
Epoch 2.96: Loss = 0.541031
Epoch 2.97: Loss = 0.381607
Epoch 2.98: Loss = 0.488632
Epoch 2.99: Loss = 0.593018
Epoch 2.100: Loss = 0.701767
Epoch 2.101: Loss = 0.583344
Epoch 2.102: Loss = 0.498062
Epoch 2.103: Loss = 0.476227
Epoch 2.104: Loss = 0.427856
Epoch 2.105: Loss = 0.617661
Epoch 2.106: Loss = 0.58194
Epoch 2.107: Loss = 0.398666
Epoch 2.108: Loss = 0.505585
Epoch 2.109: Loss = 0.430481
Epoch 2.110: Loss = 0.497009
Epoch 2.111: Loss = 0.394363
Epoch 2.112: Loss = 0.376419
Epoch 2.113: Loss = 0.451111
Epoch 2.114: Loss = 0.414932
Epoch 2.115: Loss = 0.433411
Epoch 2.116: Loss = 0.450546
Epoch 2.117: Loss = 0.317261
Epoch 2.118: Loss = 0.26236
Epoch 2.119: Loss = 0.348679
Epoch 2.120: Loss = 0.358597
TRAIN LOSS = 0.526642
TRAIN ACC = 83.9325 % (50362/60000)
Loss = 0.471954
Loss = 0.538483
Loss = 0.644852
Loss = 0.616425
Loss = 0.704956
Loss = 0.493469
Loss = 0.446442
Loss = 0.716003
Loss = 0.61438
Loss = 0.583099
Loss = 0.257584
Loss = 0.413483
Loss = 0.321747
Loss = 0.449585
Loss = 0.340271
Loss = 0.333878
Loss = 0.314163
Loss = 0.111176
Loss = 0.319855
Loss = 0.622406
TEST LOSS = 0.46571
TEST ACC = 503.619 % (8601/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.479401
Epoch 3.2: Loss = 0.575516
Epoch 3.3: Loss = 0.545776
Epoch 3.4: Loss = 0.397461
Epoch 3.5: Loss = 0.447952
Epoch 3.6: Loss = 0.412079
Epoch 3.7: Loss = 0.454178
Epoch 3.8: Loss = 0.431778
Epoch 3.9: Loss = 0.441574
Epoch 3.10: Loss = 0.448959
Epoch 3.11: Loss = 0.476929
Epoch 3.12: Loss = 0.436066
Epoch 3.13: Loss = 0.363205
Epoch 3.14: Loss = 0.391235
Epoch 3.15: Loss = 0.521774
Epoch 3.16: Loss = 0.519653
Epoch 3.17: Loss = 0.519485
Epoch 3.18: Loss = 0.647812
Epoch 3.19: Loss = 0.44371
Epoch 3.20: Loss = 0.427917
Epoch 3.21: Loss = 0.390259
Epoch 3.22: Loss = 0.36879
Epoch 3.23: Loss = 0.398148
Epoch 3.24: Loss = 0.6064
Epoch 3.25: Loss = 0.507019
Epoch 3.26: Loss = 0.585953
Epoch 3.27: Loss = 0.617996
Epoch 3.28: Loss = 0.550446
Epoch 3.29: Loss = 0.621033
Epoch 3.30: Loss = 0.635849
Epoch 3.31: Loss = 0.408844
Epoch 3.32: Loss = 0.55867
Epoch 3.33: Loss = 0.427399
Epoch 3.34: Loss = 0.499557
Epoch 3.35: Loss = 0.511353
Epoch 3.36: Loss = 0.56398
Epoch 3.37: Loss = 0.378403
Epoch 3.38: Loss = 0.409164
Epoch 3.39: Loss = 0.416702
Epoch 3.40: Loss = 0.407043
Epoch 3.41: Loss = 0.481522
Epoch 3.42: Loss = 0.592468
Epoch 3.43: Loss = 0.385315
Epoch 3.44: Loss = 0.352097
Epoch 3.45: Loss = 0.492371
Epoch 3.46: Loss = 0.514648
Epoch 3.47: Loss = 0.433975
Epoch 3.48: Loss = 0.493179
Epoch 3.49: Loss = 0.46405
Epoch 3.50: Loss = 0.57225
Epoch 3.51: Loss = 0.397003
Epoch 3.52: Loss = 0.379318
Epoch 3.53: Loss = 0.42955
Epoch 3.54: Loss = 0.547562
Epoch 3.55: Loss = 0.442581
Epoch 3.56: Loss = 0.449265
Epoch 3.57: Loss = 0.402573
Epoch 3.58: Loss = 0.505737
Epoch 3.59: Loss = 0.531754
Epoch 3.60: Loss = 0.567841
Epoch 3.61: Loss = 0.553467
Epoch 3.62: Loss = 0.554459
Epoch 3.63: Loss = 0.614151
Epoch 3.64: Loss = 0.545135
Epoch 3.65: Loss = 0.673187
Epoch 3.66: Loss = 0.44632
Epoch 3.67: Loss = 0.514603
Epoch 3.68: Loss = 0.294434
Epoch 3.69: Loss = 0.385147
Epoch 3.70: Loss = 0.55986
Epoch 3.71: Loss = 0.409332
Epoch 3.72: Loss = 0.370163
Epoch 3.73: Loss = 0.424591
Epoch 3.74: Loss = 0.367432
Epoch 3.75: Loss = 0.693878
Epoch 3.76: Loss = 0.45697
Epoch 3.77: Loss = 0.387527
Epoch 3.78: Loss = 0.434937
Epoch 3.79: Loss = 0.577805
Epoch 3.80: Loss = 0.546738
Epoch 3.81: Loss = 0.423874
Epoch 3.82: Loss = 0.347183
Epoch 3.83: Loss = 0.567352
Epoch 3.84: Loss = 0.473923
Epoch 3.85: Loss = 0.589508
Epoch 3.86: Loss = 0.524841
Epoch 3.87: Loss = 0.380508
Epoch 3.88: Loss = 0.472046
Epoch 3.89: Loss = 0.561646
Epoch 3.90: Loss = 0.405563
Epoch 3.91: Loss = 0.528244
Epoch 3.92: Loss = 0.52684
Epoch 3.93: Loss = 0.608383
Epoch 3.94: Loss = 0.342834
Epoch 3.95: Loss = 0.475174
Epoch 3.96: Loss = 0.513947
Epoch 3.97: Loss = 0.357391
Epoch 3.98: Loss = 0.468002
Epoch 3.99: Loss = 0.577896
Epoch 3.100: Loss = 0.647552
Epoch 3.101: Loss = 0.569916
Epoch 3.102: Loss = 0.44783
Epoch 3.103: Loss = 0.453979
Epoch 3.104: Loss = 0.411575
Epoch 3.105: Loss = 0.589294
Epoch 3.106: Loss = 0.576569
Epoch 3.107: Loss = 0.373367
Epoch 3.108: Loss = 0.483047
Epoch 3.109: Loss = 0.388992
Epoch 3.110: Loss = 0.478516
Epoch 3.111: Loss = 0.373215
Epoch 3.112: Loss = 0.365662
Epoch 3.113: Loss = 0.402512
Epoch 3.114: Loss = 0.372604
Epoch 3.115: Loss = 0.380325
Epoch 3.116: Loss = 0.382126
Epoch 3.117: Loss = 0.281219
Epoch 3.118: Loss = 0.198105
Epoch 3.119: Loss = 0.34108
Epoch 3.120: Loss = 0.348602
TRAIN LOSS = 0.470856
TRAIN ACC = 86.0809 % (51651/60000)
Loss = 0.422852
Loss = 0.539398
Loss = 0.626221
Loss = 0.62912
Loss = 0.683838
Loss = 0.451233
Loss = 0.414383
Loss = 0.700775
Loss = 0.633499
Loss = 0.565308
Loss = 0.24942
Loss = 0.40802
Loss = 0.344406
Loss = 0.430847
Loss = 0.28775
Loss = 0.346725
Loss = 0.262222
Loss = 0.0835571
Loss = 0.292358
Loss = 0.58876
TEST LOSS = 0.448035
TEST ACC = 516.508 % (8719/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.43399
Epoch 4.2: Loss = 0.549728
Epoch 4.3: Loss = 0.535599
Epoch 4.4: Loss = 0.363419
Epoch 4.5: Loss = 0.403122
Epoch 4.6: Loss = 0.413483
Epoch 4.7: Loss = 0.431931
Epoch 4.8: Loss = 0.421249
Epoch 4.9: Loss = 0.406845
Epoch 4.10: Loss = 0.42952
Epoch 4.11: Loss = 0.447495
Epoch 4.12: Loss = 0.414307
Epoch 4.13: Loss = 0.342209
Epoch 4.14: Loss = 0.381165
Epoch 4.15: Loss = 0.468658
Epoch 4.16: Loss = 0.519806
Epoch 4.17: Loss = 0.488297
Epoch 4.18: Loss = 0.658737
Epoch 4.19: Loss = 0.421432
Epoch 4.20: Loss = 0.413422
Epoch 4.21: Loss = 0.359467
Epoch 4.22: Loss = 0.34642
Epoch 4.23: Loss = 0.364883
Epoch 4.24: Loss = 0.575272
Epoch 4.25: Loss = 0.511169
Epoch 4.26: Loss = 0.578293
Epoch 4.27: Loss = 0.586243
Epoch 4.28: Loss = 0.558563
Epoch 4.29: Loss = 0.634476
Epoch 4.30: Loss = 0.614136
Epoch 4.31: Loss = 0.407059
Epoch 4.32: Loss = 0.506134
Epoch 4.33: Loss = 0.376556
Epoch 4.34: Loss = 0.492706
Epoch 4.35: Loss = 0.478546
Epoch 4.36: Loss = 0.551163
Epoch 4.37: Loss = 0.361313
Epoch 4.38: Loss = 0.414948
Epoch 4.39: Loss = 0.400665
Epoch 4.40: Loss = 0.408936
Epoch 4.41: Loss = 0.469482
Epoch 4.42: Loss = 0.600479
Epoch 4.43: Loss = 0.376373
Epoch 4.44: Loss = 0.339737
Epoch 4.45: Loss = 0.476105
Epoch 4.46: Loss = 0.482651
Epoch 4.47: Loss = 0.426865
Epoch 4.48: Loss = 0.499573
Epoch 4.49: Loss = 0.446075
Epoch 4.50: Loss = 0.552368
Epoch 4.51: Loss = 0.377701
Epoch 4.52: Loss = 0.360291
Epoch 4.53: Loss = 0.419678
Epoch 4.54: Loss = 0.538788
Epoch 4.55: Loss = 0.459595
Epoch 4.56: Loss = 0.445618
Epoch 4.57: Loss = 0.412415
Epoch 4.58: Loss = 0.45842
Epoch 4.59: Loss = 0.529175
Epoch 4.60: Loss = 0.547653
Epoch 4.61: Loss = 0.501755
Epoch 4.62: Loss = 0.546875
Epoch 4.63: Loss = 0.599747
Epoch 4.64: Loss = 0.538239
Epoch 4.65: Loss = 0.688324
Epoch 4.66: Loss = 0.441849
Epoch 4.67: Loss = 0.528259
Epoch 4.68: Loss = 0.264694
Epoch 4.69: Loss = 0.365723
Epoch 4.70: Loss = 0.563904
Epoch 4.71: Loss = 0.404922
Epoch 4.72: Loss = 0.335968
Epoch 4.73: Loss = 0.428284
Epoch 4.74: Loss = 0.37709
Epoch 4.75: Loss = 0.725937
Epoch 4.76: Loss = 0.46402
Epoch 4.77: Loss = 0.348465
Epoch 4.78: Loss = 0.432785
Epoch 4.79: Loss = 0.589355
Epoch 4.80: Loss = 0.473877
Epoch 4.81: Loss = 0.389587
Epoch 4.82: Loss = 0.343765
Epoch 4.83: Loss = 0.548615
Epoch 4.84: Loss = 0.45575
Epoch 4.85: Loss = 0.597305
Epoch 4.86: Loss = 0.55661
Epoch 4.87: Loss = 0.366119
Epoch 4.88: Loss = 0.463715
Epoch 4.89: Loss = 0.549622
Epoch 4.90: Loss = 0.401077
Epoch 4.91: Loss = 0.546387
Epoch 4.92: Loss = 0.526276
Epoch 4.93: Loss = 0.602005
Epoch 4.94: Loss = 0.346069
Epoch 4.95: Loss = 0.450607
Epoch 4.96: Loss = 0.52478
Epoch 4.97: Loss = 0.369019
Epoch 4.98: Loss = 0.45665
Epoch 4.99: Loss = 0.52887
Epoch 4.100: Loss = 0.655685
Epoch 4.101: Loss = 0.581177
Epoch 4.102: Loss = 0.450027
Epoch 4.103: Loss = 0.399643
Epoch 4.104: Loss = 0.384491
Epoch 4.105: Loss = 0.560364
Epoch 4.106: Loss = 0.592499
Epoch 4.107: Loss = 0.358292
Epoch 4.108: Loss = 0.492828
Epoch 4.109: Loss = 0.39064
Epoch 4.110: Loss = 0.470047
Epoch 4.111: Loss = 0.352997
Epoch 4.112: Loss = 0.358826
Epoch 4.113: Loss = 0.410614
Epoch 4.114: Loss = 0.345261
Epoch 4.115: Loss = 0.352112
Epoch 4.116: Loss = 0.35585
Epoch 4.117: Loss = 0.265869
Epoch 4.118: Loss = 0.179901
Epoch 4.119: Loss = 0.319229
Epoch 4.120: Loss = 0.342422
TRAIN LOSS = 0.457932
TRAIN ACC = 87.0132 % (52211/60000)
Loss = 0.414825
Loss = 0.540115
Loss = 0.616165
Loss = 0.664413
Loss = 0.670547
Loss = 0.445923
Loss = 0.383301
Loss = 0.713699
Loss = 0.633224
Loss = 0.538483
Loss = 0.239868
Loss = 0.409454
Loss = 0.383606
Loss = 0.41835
Loss = 0.24913
Loss = 0.341568
Loss = 0.259659
Loss = 0.0682526
Loss = 0.284698
Loss = 0.591873
TEST LOSS = 0.443358
TEST ACC = 522.108 % (8790/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.405212
Epoch 5.2: Loss = 0.533249
Epoch 5.3: Loss = 0.514297
Epoch 5.4: Loss = 0.361313
Epoch 5.5: Loss = 0.347473
Epoch 5.6: Loss = 0.387024
Epoch 5.7: Loss = 0.398804
Epoch 5.8: Loss = 0.415115
Epoch 5.9: Loss = 0.382904
Epoch 5.10: Loss = 0.421982
Epoch 5.11: Loss = 0.450699
Epoch 5.12: Loss = 0.402756
Epoch 5.13: Loss = 0.307663
Epoch 5.14: Loss = 0.394257
Epoch 5.15: Loss = 0.434891
Epoch 5.16: Loss = 0.517883
Epoch 5.17: Loss = 0.491547
Epoch 5.18: Loss = 0.694199
Epoch 5.19: Loss = 0.446335
Epoch 5.20: Loss = 0.42128
Epoch 5.21: Loss = 0.352264
Epoch 5.22: Loss = 0.333694
Epoch 5.23: Loss = 0.360107
Epoch 5.24: Loss = 0.611221
Epoch 5.25: Loss = 0.483948
Epoch 5.26: Loss = 0.62355
Epoch 5.27: Loss = 0.587814
Epoch 5.28: Loss = 0.539963
Epoch 5.29: Loss = 0.606857
Epoch 5.30: Loss = 0.642166
Epoch 5.31: Loss = 0.396454
Epoch 5.32: Loss = 0.523956
Epoch 5.33: Loss = 0.335693
Epoch 5.34: Loss = 0.481934
Epoch 5.35: Loss = 0.457779
Epoch 5.36: Loss = 0.53952
Epoch 5.37: Loss = 0.330887
Epoch 5.38: Loss = 0.394897
Epoch 5.39: Loss = 0.398346
Epoch 5.40: Loss = 0.398575
Epoch 5.41: Loss = 0.47052
Epoch 5.42: Loss = 0.610321
Epoch 5.43: Loss = 0.389496
Epoch 5.44: Loss = 0.330368
Epoch 5.45: Loss = 0.467545
Epoch 5.46: Loss = 0.457886
Epoch 5.47: Loss = 0.434723
Epoch 5.48: Loss = 0.472183
Epoch 5.49: Loss = 0.470947
Epoch 5.50: Loss = 0.57016
Epoch 5.51: Loss = 0.383438
Epoch 5.52: Loss = 0.357819
Epoch 5.53: Loss = 0.424438
Epoch 5.54: Loss = 0.540039
Epoch 5.55: Loss = 0.483704
Epoch 5.56: Loss = 0.450974
Epoch 5.57: Loss = 0.440277
Epoch 5.58: Loss = 0.466415
Epoch 5.59: Loss = 0.517532
Epoch 5.60: Loss = 0.54657
Epoch 5.61: Loss = 0.504547
Epoch 5.62: Loss = 0.517349
Epoch 5.63: Loss = 0.634521
Epoch 5.64: Loss = 0.553879
Epoch 5.65: Loss = 0.669693
Epoch 5.66: Loss = 0.490646
Epoch 5.67: Loss = 0.499451
Epoch 5.68: Loss = 0.263901
Epoch 5.69: Loss = 0.351364
Epoch 5.70: Loss = 0.546646
Epoch 5.71: Loss = 0.404541
Epoch 5.72: Loss = 0.357452
Epoch 5.73: Loss = 0.437637
Epoch 5.74: Loss = 0.375214
Epoch 5.75: Loss = 0.717773
Epoch 5.76: Loss = 0.459946
Epoch 5.77: Loss = 0.354462
Epoch 5.78: Loss = 0.430862
Epoch 5.79: Loss = 0.630234
Epoch 5.80: Loss = 0.491714
Epoch 5.81: Loss = 0.383835
Epoch 5.82: Loss = 0.35379
Epoch 5.83: Loss = 0.530548
Epoch 5.84: Loss = 0.456985
Epoch 5.85: Loss = 0.609787
Epoch 5.86: Loss = 0.613144
Epoch 5.87: Loss = 0.387787
Epoch 5.88: Loss = 0.455582
Epoch 5.89: Loss = 0.542007
Epoch 5.90: Loss = 0.43309
Epoch 5.91: Loss = 0.575607
Epoch 5.92: Loss = 0.541275
Epoch 5.93: Loss = 0.592987
Epoch 5.94: Loss = 0.369583
Epoch 5.95: Loss = 0.4599
Epoch 5.96: Loss = 0.532928
Epoch 5.97: Loss = 0.372742
Epoch 5.98: Loss = 0.43541
Epoch 5.99: Loss = 0.540054
Epoch 5.100: Loss = 0.709442
Epoch 5.101: Loss = 0.621277
Epoch 5.102: Loss = 0.414444
Epoch 5.103: Loss = 0.417389
Epoch 5.104: Loss = 0.387329
Epoch 5.105: Loss = 0.570953
Epoch 5.106: Loss = 0.596802
Epoch 5.107: Loss = 0.343796
Epoch 5.108: Loss = 0.49205
Epoch 5.109: Loss = 0.419571
Epoch 5.110: Loss = 0.494537
Epoch 5.111: Loss = 0.387909
Epoch 5.112: Loss = 0.39653
Epoch 5.113: Loss = 0.417587
Epoch 5.114: Loss = 0.338318
Epoch 5.115: Loss = 0.322327
Epoch 5.116: Loss = 0.396759
Epoch 5.117: Loss = 0.274216
Epoch 5.118: Loss = 0.197449
Epoch 5.119: Loss = 0.359055
Epoch 5.120: Loss = 0.338882
TRAIN LOSS = 0.4599
TRAIN ACC = 87.5519 % (52533/60000)
Loss = 0.429138
Loss = 0.542328
Loss = 0.629593
Loss = 0.653427
Loss = 0.669571
Loss = 0.457352
Loss = 0.36821
Loss = 0.734528
Loss = 0.644073
Loss = 0.536163
Loss = 0.225403
Loss = 0.413986
Loss = 0.355469
Loss = 0.422531
Loss = 0.260239
Loss = 0.315567
Loss = 0.234497
Loss = 0.0640259
Loss = 0.271408
Loss = 0.558441
TEST LOSS = 0.439297
TEST ACC = 525.33 % (8823/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.406845
Epoch 6.2: Loss = 0.567764
Epoch 6.3: Loss = 0.545715
Epoch 6.4: Loss = 0.367447
Epoch 6.5: Loss = 0.333466
Epoch 6.6: Loss = 0.402313
Epoch 6.7: Loss = 0.415421
Epoch 6.8: Loss = 0.408447
Epoch 6.9: Loss = 0.412308
Epoch 6.10: Loss = 0.433701
Epoch 6.11: Loss = 0.459427
Epoch 6.12: Loss = 0.405533
Epoch 6.13: Loss = 0.317291
Epoch 6.14: Loss = 0.390869
Epoch 6.15: Loss = 0.447723
Epoch 6.16: Loss = 0.508118
Epoch 6.17: Loss = 0.449982
Epoch 6.18: Loss = 0.741425
Epoch 6.19: Loss = 0.470062
Epoch 6.20: Loss = 0.414413
Epoch 6.21: Loss = 0.336685
Epoch 6.22: Loss = 0.363525
Epoch 6.23: Loss = 0.356079
Epoch 6.24: Loss = 0.59639
Epoch 6.25: Loss = 0.458435
Epoch 6.26: Loss = 0.609818
Epoch 6.27: Loss = 0.584106
Epoch 6.28: Loss = 0.52475
Epoch 6.29: Loss = 0.624329
Epoch 6.30: Loss = 0.624069
Epoch 6.31: Loss = 0.393677
Epoch 6.32: Loss = 0.487381
Epoch 6.33: Loss = 0.346283
Epoch 6.34: Loss = 0.502014
Epoch 6.35: Loss = 0.462997
Epoch 6.36: Loss = 0.555984
Epoch 6.37: Loss = 0.300049
Epoch 6.38: Loss = 0.417877
Epoch 6.39: Loss = 0.393097
Epoch 6.40: Loss = 0.420105
Epoch 6.41: Loss = 0.466293
Epoch 6.42: Loss = 0.623138
Epoch 6.43: Loss = 0.407578
Epoch 6.44: Loss = 0.336456
Epoch 6.45: Loss = 0.415329
Epoch 6.46: Loss = 0.478287
Epoch 6.47: Loss = 0.405334
Epoch 6.48: Loss = 0.500168
Epoch 6.49: Loss = 0.432144
Epoch 6.50: Loss = 0.570831
Epoch 6.51: Loss = 0.384415
Epoch 6.52: Loss = 0.380417
Epoch 6.53: Loss = 0.447144
Epoch 6.54: Loss = 0.588608
Epoch 6.55: Loss = 0.47757
Epoch 6.56: Loss = 0.451309
Epoch 6.57: Loss = 0.432144
Epoch 6.58: Loss = 0.480484
Epoch 6.59: Loss = 0.512817
Epoch 6.60: Loss = 0.535233
Epoch 6.61: Loss = 0.486694
Epoch 6.62: Loss = 0.495102
Epoch 6.63: Loss = 0.663956
Epoch 6.64: Loss = 0.599747
Epoch 6.65: Loss = 0.67601
Epoch 6.66: Loss = 0.451385
Epoch 6.67: Loss = 0.519714
Epoch 6.68: Loss = 0.297012
Epoch 6.69: Loss = 0.360138
Epoch 6.70: Loss = 0.56842
Epoch 6.71: Loss = 0.392838
Epoch 6.72: Loss = 0.390656
Epoch 6.73: Loss = 0.460281
Epoch 6.74: Loss = 0.401169
Epoch 6.75: Loss = 0.718582
Epoch 6.76: Loss = 0.452698
Epoch 6.77: Loss = 0.37001
Epoch 6.78: Loss = 0.459274
Epoch 6.79: Loss = 0.617706
Epoch 6.80: Loss = 0.495209
Epoch 6.81: Loss = 0.409424
Epoch 6.82: Loss = 0.351547
Epoch 6.83: Loss = 0.534607
Epoch 6.84: Loss = 0.457092
Epoch 6.85: Loss = 0.616684
Epoch 6.86: Loss = 0.607895
Epoch 6.87: Loss = 0.358063
Epoch 6.88: Loss = 0.461121
Epoch 6.89: Loss = 0.533722
Epoch 6.90: Loss = 0.445343
Epoch 6.91: Loss = 0.53244
Epoch 6.92: Loss = 0.588806
Epoch 6.93: Loss = 0.662537
Epoch 6.94: Loss = 0.395172
Epoch 6.95: Loss = 0.449982
Epoch 6.96: Loss = 0.578751
Epoch 6.97: Loss = 0.381897
Epoch 6.98: Loss = 0.424118
Epoch 6.99: Loss = 0.573578
Epoch 6.100: Loss = 0.745026
Epoch 6.101: Loss = 0.631119
Epoch 6.102: Loss = 0.424133
Epoch 6.103: Loss = 0.41304
Epoch 6.104: Loss = 0.39798
Epoch 6.105: Loss = 0.566605
Epoch 6.106: Loss = 0.574661
Epoch 6.107: Loss = 0.338882
Epoch 6.108: Loss = 0.512726
Epoch 6.109: Loss = 0.432999
Epoch 6.110: Loss = 0.497192
Epoch 6.111: Loss = 0.380783
Epoch 6.112: Loss = 0.41301
Epoch 6.113: Loss = 0.441696
Epoch 6.114: Loss = 0.329163
Epoch 6.115: Loss = 0.335236
Epoch 6.116: Loss = 0.399872
Epoch 6.117: Loss = 0.272308
Epoch 6.118: Loss = 0.209747
Epoch 6.119: Loss = 0.39859
Epoch 6.120: Loss = 0.381866
TRAIN LOSS = 0.465973
TRAIN ACC = 87.8021 % (52684/60000)
Loss = 0.433517
Loss = 0.565292
Loss = 0.618042
Loss = 0.691467
Loss = 0.729858
Loss = 0.473587
Loss = 0.399567
Loss = 0.739044
Loss = 0.626251
Loss = 0.543457
Loss = 0.225067
Loss = 0.392197
Loss = 0.355576
Loss = 0.428162
Loss = 0.258148
Loss = 0.329681
Loss = 0.239685
Loss = 0.0715637
Loss = 0.264664
Loss = 0.59758
TEST LOSS = 0.44912
TEST ACC = 526.839 % (8830/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.423065
Epoch 7.2: Loss = 0.536133
Epoch 7.3: Loss = 0.57225
Epoch 7.4: Loss = 0.355408
Epoch 7.5: Loss = 0.348297
Epoch 7.6: Loss = 0.407028
Epoch 7.7: Loss = 0.386169
Epoch 7.8: Loss = 0.40448
Epoch 7.9: Loss = 0.42601
Epoch 7.10: Loss = 0.436172
Epoch 7.11: Loss = 0.466324
Epoch 7.12: Loss = 0.415237
Epoch 7.13: Loss = 0.324493
Epoch 7.14: Loss = 0.413116
Epoch 7.15: Loss = 0.452957
Epoch 7.16: Loss = 0.482773
Epoch 7.17: Loss = 0.436844
Epoch 7.18: Loss = 0.734238
Epoch 7.19: Loss = 0.490295
Epoch 7.20: Loss = 0.39502
Epoch 7.21: Loss = 0.367584
Epoch 7.22: Loss = 0.365799
Epoch 7.23: Loss = 0.364746
Epoch 7.24: Loss = 0.609192
Epoch 7.25: Loss = 0.459106
Epoch 7.26: Loss = 0.634094
Epoch 7.27: Loss = 0.59201
Epoch 7.28: Loss = 0.531738
Epoch 7.29: Loss = 0.639816
Epoch 7.30: Loss = 0.728378
Epoch 7.31: Loss = 0.438278
Epoch 7.32: Loss = 0.521561
Epoch 7.33: Loss = 0.370102
Epoch 7.34: Loss = 0.519684
Epoch 7.35: Loss = 0.452789
Epoch 7.36: Loss = 0.56398
Epoch 7.37: Loss = 0.316528
Epoch 7.38: Loss = 0.428131
Epoch 7.39: Loss = 0.382843
Epoch 7.40: Loss = 0.439514
Epoch 7.41: Loss = 0.440231
Epoch 7.42: Loss = 0.706009
Epoch 7.43: Loss = 0.414383
Epoch 7.44: Loss = 0.357254
Epoch 7.45: Loss = 0.470718
Epoch 7.46: Loss = 0.505249
Epoch 7.47: Loss = 0.427246
Epoch 7.48: Loss = 0.504044
Epoch 7.49: Loss = 0.441147
Epoch 7.50: Loss = 0.608322
Epoch 7.51: Loss = 0.374573
Epoch 7.52: Loss = 0.36145
Epoch 7.53: Loss = 0.464462
Epoch 7.54: Loss = 0.597504
Epoch 7.55: Loss = 0.50618
Epoch 7.56: Loss = 0.452988
Epoch 7.57: Loss = 0.416656
Epoch 7.58: Loss = 0.518372
Epoch 7.59: Loss = 0.555756
Epoch 7.60: Loss = 0.556
Epoch 7.61: Loss = 0.475586
Epoch 7.62: Loss = 0.513824
Epoch 7.63: Loss = 0.683975
Epoch 7.64: Loss = 0.590576
Epoch 7.65: Loss = 0.716812
Epoch 7.66: Loss = 0.450211
Epoch 7.67: Loss = 0.514832
Epoch 7.68: Loss = 0.286179
Epoch 7.69: Loss = 0.362839
Epoch 7.70: Loss = 0.555786
Epoch 7.71: Loss = 0.404907
Epoch 7.72: Loss = 0.368896
Epoch 7.73: Loss = 0.467377
Epoch 7.74: Loss = 0.380386
Epoch 7.75: Loss = 0.751846
Epoch 7.76: Loss = 0.448166
Epoch 7.77: Loss = 0.354233
Epoch 7.78: Loss = 0.462021
Epoch 7.79: Loss = 0.651077
Epoch 7.80: Loss = 0.49881
Epoch 7.81: Loss = 0.424088
Epoch 7.82: Loss = 0.348145
Epoch 7.83: Loss = 0.58316
Epoch 7.84: Loss = 0.424332
Epoch 7.85: Loss = 0.656006
Epoch 7.86: Loss = 0.606476
Epoch 7.87: Loss = 0.36438
Epoch 7.88: Loss = 0.489853
Epoch 7.89: Loss = 0.524246
Epoch 7.90: Loss = 0.45816
Epoch 7.91: Loss = 0.522507
Epoch 7.92: Loss = 0.597458
Epoch 7.93: Loss = 0.657654
Epoch 7.94: Loss = 0.413589
Epoch 7.95: Loss = 0.452103
Epoch 7.96: Loss = 0.561722
Epoch 7.97: Loss = 0.361649
Epoch 7.98: Loss = 0.41864
Epoch 7.99: Loss = 0.632202
Epoch 7.100: Loss = 0.803513
Epoch 7.101: Loss = 0.647629
Epoch 7.102: Loss = 0.447739
Epoch 7.103: Loss = 0.406128
Epoch 7.104: Loss = 0.402847
Epoch 7.105: Loss = 0.618546
Epoch 7.106: Loss = 0.632736
Epoch 7.107: Loss = 0.349792
Epoch 7.108: Loss = 0.514862
Epoch 7.109: Loss = 0.412033
Epoch 7.110: Loss = 0.494644
Epoch 7.111: Loss = 0.375107
Epoch 7.112: Loss = 0.437851
Epoch 7.113: Loss = 0.423096
Epoch 7.114: Loss = 0.345215
Epoch 7.115: Loss = 0.329895
Epoch 7.116: Loss = 0.377289
Epoch 7.117: Loss = 0.302139
Epoch 7.118: Loss = 0.209503
Epoch 7.119: Loss = 0.42012
Epoch 7.120: Loss = 0.402847
TRAIN LOSS = 0.47583
TRAIN ACC = 88.0112 % (52809/60000)
Loss = 0.452469
Loss = 0.561981
Loss = 0.658829
Loss = 0.722244
Loss = 0.773209
Loss = 0.512695
Loss = 0.4207
Loss = 0.777466
Loss = 0.645477
Loss = 0.592468
Loss = 0.238831
Loss = 0.375916
Loss = 0.405243
Loss = 0.463104
Loss = 0.234039
Loss = 0.341461
Loss = 0.222351
Loss = 0.0574188
Loss = 0.288589
Loss = 0.635849
TEST LOSS = 0.469017
TEST ACC = 528.088 % (8829/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.421494
Epoch 8.2: Loss = 0.566589
Epoch 8.3: Loss = 0.594116
Epoch 8.4: Loss = 0.380096
Epoch 8.5: Loss = 0.354767
Epoch 8.6: Loss = 0.415787
Epoch 8.7: Loss = 0.394272
Epoch 8.8: Loss = 0.434464
Epoch 8.9: Loss = 0.434479
Epoch 8.10: Loss = 0.431992
Epoch 8.11: Loss = 0.491501
Epoch 8.12: Loss = 0.433411
Epoch 8.13: Loss = 0.322174
Epoch 8.14: Loss = 0.419785
Epoch 8.15: Loss = 0.474075
Epoch 8.16: Loss = 0.53035
Epoch 8.17: Loss = 0.506226
Epoch 8.18: Loss = 0.716431
Epoch 8.19: Loss = 0.496933
Epoch 8.20: Loss = 0.402435
Epoch 8.21: Loss = 0.407639
Epoch 8.22: Loss = 0.342072
Epoch 8.23: Loss = 0.381699
Epoch 8.24: Loss = 0.634888
Epoch 8.25: Loss = 0.446609
Epoch 8.26: Loss = 0.631668
Epoch 8.27: Loss = 0.652908
Epoch 8.28: Loss = 0.566757
Epoch 8.29: Loss = 0.652756
Epoch 8.30: Loss = 0.69751
Epoch 8.31: Loss = 0.457886
Epoch 8.32: Loss = 0.549957
Epoch 8.33: Loss = 0.372711
Epoch 8.34: Loss = 0.505402
Epoch 8.35: Loss = 0.447632
Epoch 8.36: Loss = 0.590759
Epoch 8.37: Loss = 0.330627
Epoch 8.38: Loss = 0.417908
Epoch 8.39: Loss = 0.389511
Epoch 8.40: Loss = 0.463776
Epoch 8.41: Loss = 0.440964
Epoch 8.42: Loss = 0.734695
Epoch 8.43: Loss = 0.412689
Epoch 8.44: Loss = 0.336655
Epoch 8.45: Loss = 0.496902
Epoch 8.46: Loss = 0.567612
Epoch 8.47: Loss = 0.437271
Epoch 8.48: Loss = 0.521988
Epoch 8.49: Loss = 0.455048
Epoch 8.50: Loss = 0.645203
Epoch 8.51: Loss = 0.38652
Epoch 8.52: Loss = 0.361252
Epoch 8.53: Loss = 0.485519
Epoch 8.54: Loss = 0.577408
Epoch 8.55: Loss = 0.505814
Epoch 8.56: Loss = 0.474319
Epoch 8.57: Loss = 0.454117
Epoch 8.58: Loss = 0.53804
Epoch 8.59: Loss = 0.557022
Epoch 8.60: Loss = 0.571884
Epoch 8.61: Loss = 0.507019
Epoch 8.62: Loss = 0.520737
Epoch 8.63: Loss = 0.705902
Epoch 8.64: Loss = 0.577255
Epoch 8.65: Loss = 0.714264
Epoch 8.66: Loss = 0.429413
Epoch 8.67: Loss = 0.54454
Epoch 8.68: Loss = 0.290634
Epoch 8.69: Loss = 0.367538
Epoch 8.70: Loss = 0.573105
Epoch 8.71: Loss = 0.381439
Epoch 8.72: Loss = 0.343918
Epoch 8.73: Loss = 0.449265
Epoch 8.74: Loss = 0.394867
Epoch 8.75: Loss = 0.789413
Epoch 8.76: Loss = 0.458771
Epoch 8.77: Loss = 0.358917
Epoch 8.78: Loss = 0.455444
Epoch 8.79: Loss = 0.682877
Epoch 8.80: Loss = 0.469208
Epoch 8.81: Loss = 0.392258
Epoch 8.82: Loss = 0.35376
Epoch 8.83: Loss = 0.589233
Epoch 8.84: Loss = 0.418961
Epoch 8.85: Loss = 0.660172
Epoch 8.86: Loss = 0.633148
Epoch 8.87: Loss = 0.373978
Epoch 8.88: Loss = 0.503326
Epoch 8.89: Loss = 0.548996
Epoch 8.90: Loss = 0.437317
Epoch 8.91: Loss = 0.569641
Epoch 8.92: Loss = 0.576004
Epoch 8.93: Loss = 0.681519
Epoch 8.94: Loss = 0.378479
Epoch 8.95: Loss = 0.487259
Epoch 8.96: Loss = 0.516541
Epoch 8.97: Loss = 0.386719
Epoch 8.98: Loss = 0.44986
Epoch 8.99: Loss = 0.621628
Epoch 8.100: Loss = 0.813751
Epoch 8.101: Loss = 0.668915
Epoch 8.102: Loss = 0.450714
Epoch 8.103: Loss = 0.420944
Epoch 8.104: Loss = 0.421249
Epoch 8.105: Loss = 0.632828
Epoch 8.106: Loss = 0.594345
Epoch 8.107: Loss = 0.391037
Epoch 8.108: Loss = 0.535248
Epoch 8.109: Loss = 0.441711
Epoch 8.110: Loss = 0.495804
Epoch 8.111: Loss = 0.377228
Epoch 8.112: Loss = 0.438187
Epoch 8.113: Loss = 0.470139
Epoch 8.114: Loss = 0.348572
Epoch 8.115: Loss = 0.310562
Epoch 8.116: Loss = 0.3862
Epoch 8.117: Loss = 0.290039
Epoch 8.118: Loss = 0.190842
Epoch 8.119: Loss = 0.418655
Epoch 8.120: Loss = 0.445221
TRAIN LOSS = 0.485489
TRAIN ACC = 88.1134 % (52871/60000)
Loss = 0.414978
Loss = 0.576538
Loss = 0.697983
Loss = 0.750229
Loss = 0.793716
Loss = 0.51947
Loss = 0.436142
Loss = 0.816422
Loss = 0.66925
Loss = 0.589432
Loss = 0.253586
Loss = 0.382507
Loss = 0.453369
Loss = 0.463577
Loss = 0.232971
Loss = 0.347061
Loss = 0.216461
Loss = 0.0561066
Loss = 0.314621
Loss = 0.675171
TEST LOSS = 0.482979
TEST ACC = 528.709 % (8857/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.437012
Epoch 9.2: Loss = 0.577499
Epoch 9.3: Loss = 0.607117
Epoch 9.4: Loss = 0.382278
Epoch 9.5: Loss = 0.387741
Epoch 9.6: Loss = 0.432175
Epoch 9.7: Loss = 0.409653
Epoch 9.8: Loss = 0.476395
Epoch 9.9: Loss = 0.424316
Epoch 9.10: Loss = 0.43811
Epoch 9.11: Loss = 0.512878
Epoch 9.12: Loss = 0.476471
Epoch 9.13: Loss = 0.320114
Epoch 9.14: Loss = 0.42659
Epoch 9.15: Loss = 0.504486
Epoch 9.16: Loss = 0.521088
Epoch 9.17: Loss = 0.529694
Epoch 9.18: Loss = 0.705963
Epoch 9.19: Loss = 0.526047
Epoch 9.20: Loss = 0.424149
Epoch 9.21: Loss = 0.441025
Epoch 9.22: Loss = 0.374847
Epoch 9.23: Loss = 0.351288
Epoch 9.24: Loss = 0.637421
Epoch 9.25: Loss = 0.481308
Epoch 9.26: Loss = 0.639679
Epoch 9.27: Loss = 0.605011
Epoch 9.28: Loss = 0.541672
Epoch 9.29: Loss = 0.698074
Epoch 9.30: Loss = 0.725189
Epoch 9.31: Loss = 0.449554
Epoch 9.32: Loss = 0.524261
Epoch 9.33: Loss = 0.394104
Epoch 9.34: Loss = 0.538025
Epoch 9.35: Loss = 0.477219
Epoch 9.36: Loss = 0.577225
Epoch 9.37: Loss = 0.352219
Epoch 9.38: Loss = 0.452499
Epoch 9.39: Loss = 0.369492
Epoch 9.40: Loss = 0.470428
Epoch 9.41: Loss = 0.461868
Epoch 9.42: Loss = 0.745392
Epoch 9.43: Loss = 0.37738
Epoch 9.44: Loss = 0.329254
Epoch 9.45: Loss = 0.534348
Epoch 9.46: Loss = 0.569733
Epoch 9.47: Loss = 0.489456
Epoch 9.48: Loss = 0.47612
Epoch 9.49: Loss = 0.448196
Epoch 9.50: Loss = 0.669861
Epoch 9.51: Loss = 0.422318
Epoch 9.52: Loss = 0.368561
Epoch 9.53: Loss = 0.48204
Epoch 9.54: Loss = 0.572754
Epoch 9.55: Loss = 0.584885
Epoch 9.56: Loss = 0.504761
Epoch 9.57: Loss = 0.468658
Epoch 9.58: Loss = 0.531967
Epoch 9.59: Loss = 0.59375
Epoch 9.60: Loss = 0.547073
Epoch 9.61: Loss = 0.503494
Epoch 9.62: Loss = 0.502869
Epoch 9.63: Loss = 0.696686
Epoch 9.64: Loss = 0.573044
Epoch 9.65: Loss = 0.733459
Epoch 9.66: Loss = 0.460983
Epoch 9.67: Loss = 0.579102
Epoch 9.68: Loss = 0.285065
Epoch 9.69: Loss = 0.383591
Epoch 9.70: Loss = 0.615372
Epoch 9.71: Loss = 0.382095
Epoch 9.72: Loss = 0.352203
Epoch 9.73: Loss = 0.458389
Epoch 9.74: Loss = 0.41951
Epoch 9.75: Loss = 0.82196
Epoch 9.76: Loss = 0.474518
Epoch 9.77: Loss = 0.372375
Epoch 9.78: Loss = 0.458298
Epoch 9.79: Loss = 0.684052
Epoch 9.80: Loss = 0.539078
Epoch 9.81: Loss = 0.410812
Epoch 9.82: Loss = 0.331253
Epoch 9.83: Loss = 0.578568
Epoch 9.84: Loss = 0.425812
Epoch 9.85: Loss = 0.722214
Epoch 9.86: Loss = 0.620651
Epoch 9.87: Loss = 0.350906
Epoch 9.88: Loss = 0.503311
Epoch 9.89: Loss = 0.573715
Epoch 9.90: Loss = 0.392166
Epoch 9.91: Loss = 0.582993
Epoch 9.92: Loss = 0.566208
Epoch 9.93: Loss = 0.69693
Epoch 9.94: Loss = 0.385971
Epoch 9.95: Loss = 0.499969
Epoch 9.96: Loss = 0.563843
Epoch 9.97: Loss = 0.386078
Epoch 9.98: Loss = 0.489456
Epoch 9.99: Loss = 0.599976
Epoch 9.100: Loss = 0.802292
Epoch 9.101: Loss = 0.657791
Epoch 9.102: Loss = 0.432953
Epoch 9.103: Loss = 0.418503
Epoch 9.104: Loss = 0.402924
Epoch 9.105: Loss = 0.629074
Epoch 9.106: Loss = 0.679001
Epoch 9.107: Loss = 0.373291
Epoch 9.108: Loss = 0.529938
Epoch 9.109: Loss = 0.470032
Epoch 9.110: Loss = 0.531525
Epoch 9.111: Loss = 0.355057
Epoch 9.112: Loss = 0.477097
Epoch 9.113: Loss = 0.492172
Epoch 9.114: Loss = 0.354782
Epoch 9.115: Loss = 0.316925
Epoch 9.116: Loss = 0.393524
Epoch 9.117: Loss = 0.276855
Epoch 9.118: Loss = 0.198318
Epoch 9.119: Loss = 0.429428
Epoch 9.120: Loss = 0.431366
TRAIN LOSS = 0.495239
TRAIN ACC = 88.002 % (52804/60000)
Loss = 0.4189
Loss = 0.573151
Loss = 0.676483
Loss = 0.754349
Loss = 0.8013
Loss = 0.542191
Loss = 0.421097
Loss = 0.811172
Loss = 0.697144
Loss = 0.602188
Loss = 0.23674
Loss = 0.405685
Loss = 0.439362
Loss = 0.453568
Loss = 0.231979
Loss = 0.396149
Loss = 0.215576
Loss = 0.0709229
Loss = 0.325058
Loss = 0.684082
TEST LOSS = 0.487855
TEST ACC = 528.04 % (8872/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.430664
Epoch 10.2: Loss = 0.569153
Epoch 10.3: Loss = 0.579224
Epoch 10.4: Loss = 0.381439
Epoch 10.5: Loss = 0.372208
Epoch 10.6: Loss = 0.419678
Epoch 10.7: Loss = 0.416245
Epoch 10.8: Loss = 0.477203
Epoch 10.9: Loss = 0.440399
Epoch 10.10: Loss = 0.429871
Epoch 10.11: Loss = 0.495377
Epoch 10.12: Loss = 0.464371
Epoch 10.13: Loss = 0.323181
Epoch 10.14: Loss = 0.440063
Epoch 10.15: Loss = 0.475403
Epoch 10.16: Loss = 0.518402
Epoch 10.17: Loss = 0.531052
Epoch 10.18: Loss = 0.780365
Epoch 10.19: Loss = 0.538422
Epoch 10.20: Loss = 0.415237
Epoch 10.21: Loss = 0.458817
Epoch 10.22: Loss = 0.382553
Epoch 10.23: Loss = 0.357635
Epoch 10.24: Loss = 0.607117
Epoch 10.25: Loss = 0.49353
Epoch 10.26: Loss = 0.67366
Epoch 10.27: Loss = 0.60437
Epoch 10.28: Loss = 0.548035
Epoch 10.29: Loss = 0.731781
Epoch 10.30: Loss = 0.710861
Epoch 10.31: Loss = 0.449768
Epoch 10.32: Loss = 0.515564
Epoch 10.33: Loss = 0.39296
Epoch 10.34: Loss = 0.578568
Epoch 10.35: Loss = 0.495605
Epoch 10.36: Loss = 0.619736
Epoch 10.37: Loss = 0.367249
Epoch 10.38: Loss = 0.426254
Epoch 10.39: Loss = 0.36972
Epoch 10.40: Loss = 0.500168
Epoch 10.41: Loss = 0.526169
Epoch 10.42: Loss = 0.750305
Epoch 10.43: Loss = 0.379684
Epoch 10.44: Loss = 0.356659
Epoch 10.45: Loss = 0.531418
Epoch 10.46: Loss = 0.5793
Epoch 10.47: Loss = 0.545532
Epoch 10.48: Loss = 0.516418
Epoch 10.49: Loss = 0.468079
Epoch 10.50: Loss = 0.708359
Epoch 10.51: Loss = 0.409958
Epoch 10.52: Loss = 0.398392
Epoch 10.53: Loss = 0.497849
Epoch 10.54: Loss = 0.633469
Epoch 10.55: Loss = 0.584274
Epoch 10.56: Loss = 0.510925
Epoch 10.57: Loss = 0.500656
Epoch 10.58: Loss = 0.544159
Epoch 10.59: Loss = 0.611969
Epoch 10.60: Loss = 0.567245
Epoch 10.61: Loss = 0.529556
Epoch 10.62: Loss = 0.533295
Epoch 10.63: Loss = 0.713257
Epoch 10.64: Loss = 0.619156
Epoch 10.65: Loss = 0.710663
Epoch 10.66: Loss = 0.475876
Epoch 10.67: Loss = 0.563843
Epoch 10.68: Loss = 0.303345
Epoch 10.69: Loss = 0.390854
Epoch 10.70: Loss = 0.600037
Epoch 10.71: Loss = 0.365799
Epoch 10.72: Loss = 0.376373
Epoch 10.73: Loss = 0.48764
Epoch 10.74: Loss = 0.431839
Epoch 10.75: Loss = 0.782028
Epoch 10.76: Loss = 0.469528
Epoch 10.77: Loss = 0.376099
Epoch 10.78: Loss = 0.473419
Epoch 10.79: Loss = 0.722305
Epoch 10.80: Loss = 0.57309
Epoch 10.81: Loss = 0.431564
Epoch 10.82: Loss = 0.347977
Epoch 10.83: Loss = 0.56842
Epoch 10.84: Loss = 0.47319
Epoch 10.85: Loss = 0.712341
Epoch 10.86: Loss = 0.648849
Epoch 10.87: Loss = 0.370071
Epoch 10.88: Loss = 0.525772
Epoch 10.89: Loss = 0.601593
Epoch 10.90: Loss = 0.447556
Epoch 10.91: Loss = 0.59201
Epoch 10.92: Loss = 0.606415
Epoch 10.93: Loss = 0.76506
Epoch 10.94: Loss = 0.366409
Epoch 10.95: Loss = 0.516129
Epoch 10.96: Loss = 0.533066
Epoch 10.97: Loss = 0.395721
Epoch 10.98: Loss = 0.509537
Epoch 10.99: Loss = 0.633209
Epoch 10.100: Loss = 0.820282
Epoch 10.101: Loss = 0.697845
Epoch 10.102: Loss = 0.496918
Epoch 10.103: Loss = 0.434586
Epoch 10.104: Loss = 0.407898
Epoch 10.105: Loss = 0.658218
Epoch 10.106: Loss = 0.732895
Epoch 10.107: Loss = 0.400436
Epoch 10.108: Loss = 0.563156
Epoch 10.109: Loss = 0.497742
Epoch 10.110: Loss = 0.556702
Epoch 10.111: Loss = 0.395233
Epoch 10.112: Loss = 0.442734
Epoch 10.113: Loss = 0.521576
Epoch 10.114: Loss = 0.367462
Epoch 10.115: Loss = 0.360413
Epoch 10.116: Loss = 0.423599
Epoch 10.117: Loss = 0.286713
Epoch 10.118: Loss = 0.199844
Epoch 10.119: Loss = 0.445877
Epoch 10.120: Loss = 0.457382
TRAIN LOSS = 0.509293
TRAIN ACC = 87.9074 % (52747/60000)
Loss = 0.469879
Loss = 0.647964
Loss = 0.71669
Loss = 0.81955
Loss = 0.884537
Loss = 0.568588
Loss = 0.414764
Loss = 0.821808
Loss = 0.757294
Loss = 0.642517
Loss = 0.252197
Loss = 0.426132
Loss = 0.447845
Loss = 0.477951
Loss = 0.250946
Loss = 0.370132
Loss = 0.229828
Loss = 0.0762177
Loss = 0.322891
Loss = 0.703979
TEST LOSS = 0.515085
TEST ACC = 527.469 % (8830/10000)
