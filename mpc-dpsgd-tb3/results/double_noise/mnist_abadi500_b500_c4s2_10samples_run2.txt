Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.33144
Epoch 1.2: Loss = 2.33539
Epoch 1.3: Loss = 2.24869
Epoch 1.4: Loss = 2.17168
Epoch 1.5: Loss = 2.12549
Epoch 1.6: Loss = 2.09706
Epoch 1.7: Loss = 2.10039
Epoch 1.8: Loss = 2.02455
Epoch 1.9: Loss = 2.00731
Epoch 1.10: Loss = 1.90756
Epoch 1.11: Loss = 1.87688
Epoch 1.12: Loss = 1.84132
Epoch 1.13: Loss = 1.78659
Epoch 1.14: Loss = 1.77751
Epoch 1.15: Loss = 1.8261
Epoch 1.16: Loss = 1.70282
Epoch 1.17: Loss = 1.65826
Epoch 1.18: Loss = 1.64409
Epoch 1.19: Loss = 1.59917
Epoch 1.20: Loss = 1.57372
Epoch 1.21: Loss = 1.46896
Epoch 1.22: Loss = 1.48907
Epoch 1.23: Loss = 1.41057
Epoch 1.24: Loss = 1.51036
Epoch 1.25: Loss = 1.44862
Epoch 1.26: Loss = 1.4648
Epoch 1.27: Loss = 1.37752
Epoch 1.28: Loss = 1.37874
Epoch 1.29: Loss = 1.37402
Epoch 1.30: Loss = 1.44762
Epoch 1.31: Loss = 1.28792
Epoch 1.32: Loss = 1.30775
Epoch 1.33: Loss = 1.24474
Epoch 1.34: Loss = 1.26419
Epoch 1.35: Loss = 1.21185
Epoch 1.36: Loss = 1.31236
Epoch 1.37: Loss = 1.1515
Epoch 1.38: Loss = 1.12053
Epoch 1.39: Loss = 1.14644
Epoch 1.40: Loss = 1.06981
Epoch 1.41: Loss = 1.11546
Epoch 1.42: Loss = 1.08246
Epoch 1.43: Loss = 1.03317
Epoch 1.44: Loss = 0.969788
Epoch 1.45: Loss = 1.10201
Epoch 1.46: Loss = 1.03603
Epoch 1.47: Loss = 0.985443
Epoch 1.48: Loss = 1.02045
Epoch 1.49: Loss = 0.973404
Epoch 1.50: Loss = 1.03619
Epoch 1.51: Loss = 0.853271
Epoch 1.52: Loss = 0.887253
Epoch 1.53: Loss = 0.925568
Epoch 1.54: Loss = 0.97905
Epoch 1.55: Loss = 0.929565
Epoch 1.56: Loss = 0.8741
Epoch 1.57: Loss = 0.805405
Epoch 1.58: Loss = 0.851685
Epoch 1.59: Loss = 0.836349
Epoch 1.60: Loss = 0.977966
Epoch 1.61: Loss = 0.910263
Epoch 1.62: Loss = 0.93631
Epoch 1.63: Loss = 0.928268
Epoch 1.64: Loss = 0.949158
Epoch 1.65: Loss = 0.939133
Epoch 1.66: Loss = 0.793823
Epoch 1.67: Loss = 0.810699
Epoch 1.68: Loss = 0.675751
Epoch 1.69: Loss = 0.772858
Epoch 1.70: Loss = 0.834183
Epoch 1.71: Loss = 0.74408
Epoch 1.72: Loss = 0.754181
Epoch 1.73: Loss = 0.786636
Epoch 1.74: Loss = 0.620392
Epoch 1.75: Loss = 0.797134
Epoch 1.76: Loss = 0.764145
Epoch 1.77: Loss = 0.727356
Epoch 1.78: Loss = 0.71701
Epoch 1.79: Loss = 0.698044
Epoch 1.80: Loss = 0.809341
Epoch 1.81: Loss = 0.699249
Epoch 1.82: Loss = 0.674011
Epoch 1.83: Loss = 0.82576
Epoch 1.84: Loss = 0.755615
Epoch 1.85: Loss = 0.805801
Epoch 1.86: Loss = 0.736725
Epoch 1.87: Loss = 0.661545
Epoch 1.88: Loss = 0.687469
Epoch 1.89: Loss = 0.753326
Epoch 1.90: Loss = 0.688522
Epoch 1.91: Loss = 0.719315
Epoch 1.92: Loss = 0.729691
Epoch 1.93: Loss = 0.764175
Epoch 1.94: Loss = 0.570862
Epoch 1.95: Loss = 0.697906
Epoch 1.96: Loss = 0.678467
Epoch 1.97: Loss = 0.517334
Epoch 1.98: Loss = 0.633301
Epoch 1.99: Loss = 0.71225
Epoch 1.100: Loss = 0.808609
Epoch 1.101: Loss = 0.731827
Epoch 1.102: Loss = 0.657318
Epoch 1.103: Loss = 0.583572
Epoch 1.104: Loss = 0.549072
Epoch 1.105: Loss = 0.675842
Epoch 1.106: Loss = 0.652481
Epoch 1.107: Loss = 0.579361
Epoch 1.108: Loss = 0.612305
Epoch 1.109: Loss = 0.59848
Epoch 1.110: Loss = 0.615585
Epoch 1.111: Loss = 0.508698
Epoch 1.112: Loss = 0.519226
Epoch 1.113: Loss = 0.589508
Epoch 1.114: Loss = 0.508087
Epoch 1.115: Loss = 0.59375
Epoch 1.116: Loss = 0.572357
Epoch 1.117: Loss = 0.485779
Epoch 1.118: Loss = 0.433487
Epoch 1.119: Loss = 0.42955
Epoch 1.120: Loss = 0.483047
TRAIN LOSS = 1.0489
TRAIN ACC = 71.0464 % (42630/60000)
Loss = 0.614471
Loss = 0.633316
Loss = 0.769592
Loss = 0.683792
Loss = 0.74614
Loss = 0.642899
Loss = 0.595657
Loss = 0.791504
Loss = 0.713684
Loss = 0.67691
Loss = 0.367157
Loss = 0.542236
Loss = 0.403809
Loss = 0.563141
Loss = 0.474304
Loss = 0.457092
Loss = 0.416885
Loss = 0.247498
Loss = 0.441727
Loss = 0.69017
TEST LOSS = 0.573599
TEST ACC = 426.299 % (8292/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.560043
Epoch 2.2: Loss = 0.695511
Epoch 2.3: Loss = 0.666382
Epoch 2.4: Loss = 0.527588
Epoch 2.5: Loss = 0.537628
Epoch 2.6: Loss = 0.526428
Epoch 2.7: Loss = 0.61203
Epoch 2.8: Loss = 0.578751
Epoch 2.9: Loss = 0.540817
Epoch 2.10: Loss = 0.531097
Epoch 2.11: Loss = 0.563522
Epoch 2.12: Loss = 0.531204
Epoch 2.13: Loss = 0.463409
Epoch 2.14: Loss = 0.520035
Epoch 2.15: Loss = 0.652679
Epoch 2.16: Loss = 0.591904
Epoch 2.17: Loss = 0.584152
Epoch 2.18: Loss = 0.652344
Epoch 2.19: Loss = 0.5513
Epoch 2.20: Loss = 0.480026
Epoch 2.21: Loss = 0.461792
Epoch 2.22: Loss = 0.463089
Epoch 2.23: Loss = 0.456955
Epoch 2.24: Loss = 0.665268
Epoch 2.25: Loss = 0.583298
Epoch 2.26: Loss = 0.668121
Epoch 2.27: Loss = 0.586655
Epoch 2.28: Loss = 0.608398
Epoch 2.29: Loss = 0.676697
Epoch 2.30: Loss = 0.771439
Epoch 2.31: Loss = 0.485947
Epoch 2.32: Loss = 0.653397
Epoch 2.33: Loss = 0.548279
Epoch 2.34: Loss = 0.568069
Epoch 2.35: Loss = 0.550293
Epoch 2.36: Loss = 0.643738
Epoch 2.37: Loss = 0.469299
Epoch 2.38: Loss = 0.476685
Epoch 2.39: Loss = 0.525742
Epoch 2.40: Loss = 0.472672
Epoch 2.41: Loss = 0.52037
Epoch 2.42: Loss = 0.592682
Epoch 2.43: Loss = 0.48584
Epoch 2.44: Loss = 0.417084
Epoch 2.45: Loss = 0.525436
Epoch 2.46: Loss = 0.588669
Epoch 2.47: Loss = 0.528229
Epoch 2.48: Loss = 0.563507
Epoch 2.49: Loss = 0.526459
Epoch 2.50: Loss = 0.60585
Epoch 2.51: Loss = 0.455688
Epoch 2.52: Loss = 0.453598
Epoch 2.53: Loss = 0.492599
Epoch 2.54: Loss = 0.588348
Epoch 2.55: Loss = 0.523514
Epoch 2.56: Loss = 0.455887
Epoch 2.57: Loss = 0.451843
Epoch 2.58: Loss = 0.510147
Epoch 2.59: Loss = 0.529114
Epoch 2.60: Loss = 0.578629
Epoch 2.61: Loss = 0.577042
Epoch 2.62: Loss = 0.57283
Epoch 2.63: Loss = 0.646408
Epoch 2.64: Loss = 0.621765
Epoch 2.65: Loss = 0.626877
Epoch 2.66: Loss = 0.500549
Epoch 2.67: Loss = 0.524765
Epoch 2.68: Loss = 0.376297
Epoch 2.69: Loss = 0.459106
Epoch 2.70: Loss = 0.571304
Epoch 2.71: Loss = 0.466537
Epoch 2.72: Loss = 0.47554
Epoch 2.73: Loss = 0.486771
Epoch 2.74: Loss = 0.372665
Epoch 2.75: Loss = 0.587677
Epoch 2.76: Loss = 0.528336
Epoch 2.77: Loss = 0.473419
Epoch 2.78: Loss = 0.502518
Epoch 2.79: Loss = 0.515823
Epoch 2.80: Loss = 0.600189
Epoch 2.81: Loss = 0.460098
Epoch 2.82: Loss = 0.427719
Epoch 2.83: Loss = 0.59198
Epoch 2.84: Loss = 0.522705
Epoch 2.85: Loss = 0.618393
Epoch 2.86: Loss = 0.524445
Epoch 2.87: Loss = 0.414444
Epoch 2.88: Loss = 0.483917
Epoch 2.89: Loss = 0.551514
Epoch 2.90: Loss = 0.440323
Epoch 2.91: Loss = 0.550034
Epoch 2.92: Loss = 0.538574
Epoch 2.93: Loss = 0.625092
Epoch 2.94: Loss = 0.406769
Epoch 2.95: Loss = 0.507172
Epoch 2.96: Loss = 0.534119
Epoch 2.97: Loss = 0.348465
Epoch 2.98: Loss = 0.46019
Epoch 2.99: Loss = 0.539688
Epoch 2.100: Loss = 0.595383
Epoch 2.101: Loss = 0.60704
Epoch 2.102: Loss = 0.48204
Epoch 2.103: Loss = 0.446396
Epoch 2.104: Loss = 0.396484
Epoch 2.105: Loss = 0.560455
Epoch 2.106: Loss = 0.557251
Epoch 2.107: Loss = 0.419098
Epoch 2.108: Loss = 0.494827
Epoch 2.109: Loss = 0.441681
Epoch 2.110: Loss = 0.49115
Epoch 2.111: Loss = 0.391769
Epoch 2.112: Loss = 0.389069
Epoch 2.113: Loss = 0.441269
Epoch 2.114: Loss = 0.392914
Epoch 2.115: Loss = 0.397705
Epoch 2.116: Loss = 0.432114
Epoch 2.117: Loss = 0.337875
Epoch 2.118: Loss = 0.273178
Epoch 2.119: Loss = 0.328842
Epoch 2.120: Loss = 0.379044
TRAIN LOSS = 0.519882
TRAIN ACC = 84.2285 % (50540/60000)
Loss = 0.450516
Loss = 0.535614
Loss = 0.637115
Loss = 0.556152
Loss = 0.602631
Loss = 0.470398
Loss = 0.436325
Loss = 0.631592
Loss = 0.589203
Loss = 0.554947
Loss = 0.232376
Loss = 0.404144
Loss = 0.316559
Loss = 0.419678
Loss = 0.289917
Loss = 0.341278
Loss = 0.309402
Loss = 0.111404
Loss = 0.315369
Loss = 0.590744
TEST LOSS = 0.439768
TEST ACC = 505.399 % (8649/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.42778
Epoch 3.2: Loss = 0.568542
Epoch 3.3: Loss = 0.533752
Epoch 3.4: Loss = 0.399429
Epoch 3.5: Loss = 0.404221
Epoch 3.6: Loss = 0.408691
Epoch 3.7: Loss = 0.442352
Epoch 3.8: Loss = 0.449997
Epoch 3.9: Loss = 0.422623
Epoch 3.10: Loss = 0.434769
Epoch 3.11: Loss = 0.502625
Epoch 3.12: Loss = 0.416016
Epoch 3.13: Loss = 0.367218
Epoch 3.14: Loss = 0.4104
Epoch 3.15: Loss = 0.508591
Epoch 3.16: Loss = 0.492706
Epoch 3.17: Loss = 0.505386
Epoch 3.18: Loss = 0.639923
Epoch 3.19: Loss = 0.44487
Epoch 3.20: Loss = 0.403229
Epoch 3.21: Loss = 0.361389
Epoch 3.22: Loss = 0.374573
Epoch 3.23: Loss = 0.350174
Epoch 3.24: Loss = 0.583969
Epoch 3.25: Loss = 0.508316
Epoch 3.26: Loss = 0.619232
Epoch 3.27: Loss = 0.511139
Epoch 3.28: Loss = 0.517883
Epoch 3.29: Loss = 0.60788
Epoch 3.30: Loss = 0.622559
Epoch 3.31: Loss = 0.425629
Epoch 3.32: Loss = 0.558823
Epoch 3.33: Loss = 0.493759
Epoch 3.34: Loss = 0.513565
Epoch 3.35: Loss = 0.459259
Epoch 3.36: Loss = 0.577896
Epoch 3.37: Loss = 0.35228
Epoch 3.38: Loss = 0.383179
Epoch 3.39: Loss = 0.451401
Epoch 3.40: Loss = 0.439056
Epoch 3.41: Loss = 0.447678
Epoch 3.42: Loss = 0.544647
Epoch 3.43: Loss = 0.420929
Epoch 3.44: Loss = 0.366455
Epoch 3.45: Loss = 0.467575
Epoch 3.46: Loss = 0.544449
Epoch 3.47: Loss = 0.457443
Epoch 3.48: Loss = 0.482544
Epoch 3.49: Loss = 0.506287
Epoch 3.50: Loss = 0.547134
Epoch 3.51: Loss = 0.400558
Epoch 3.52: Loss = 0.380661
Epoch 3.53: Loss = 0.412735
Epoch 3.54: Loss = 0.55954
Epoch 3.55: Loss = 0.491776
Epoch 3.56: Loss = 0.408249
Epoch 3.57: Loss = 0.433731
Epoch 3.58: Loss = 0.491623
Epoch 3.59: Loss = 0.491623
Epoch 3.60: Loss = 0.549683
Epoch 3.61: Loss = 0.563034
Epoch 3.62: Loss = 0.546585
Epoch 3.63: Loss = 0.629227
Epoch 3.64: Loss = 0.610031
Epoch 3.65: Loss = 0.642212
Epoch 3.66: Loss = 0.470795
Epoch 3.67: Loss = 0.48082
Epoch 3.68: Loss = 0.318817
Epoch 3.69: Loss = 0.437958
Epoch 3.70: Loss = 0.599731
Epoch 3.71: Loss = 0.432648
Epoch 3.72: Loss = 0.450806
Epoch 3.73: Loss = 0.445023
Epoch 3.74: Loss = 0.319305
Epoch 3.75: Loss = 0.603622
Epoch 3.76: Loss = 0.50383
Epoch 3.77: Loss = 0.464417
Epoch 3.78: Loss = 0.447159
Epoch 3.79: Loss = 0.502411
Epoch 3.80: Loss = 0.561035
Epoch 3.81: Loss = 0.428162
Epoch 3.82: Loss = 0.397018
Epoch 3.83: Loss = 0.597229
Epoch 3.84: Loss = 0.514297
Epoch 3.85: Loss = 0.64035
Epoch 3.86: Loss = 0.533188
Epoch 3.87: Loss = 0.354462
Epoch 3.88: Loss = 0.479614
Epoch 3.89: Loss = 0.568039
Epoch 3.90: Loss = 0.409714
Epoch 3.91: Loss = 0.509338
Epoch 3.92: Loss = 0.532654
Epoch 3.93: Loss = 0.60704
Epoch 3.94: Loss = 0.379257
Epoch 3.95: Loss = 0.494186
Epoch 3.96: Loss = 0.543976
Epoch 3.97: Loss = 0.356293
Epoch 3.98: Loss = 0.43808
Epoch 3.99: Loss = 0.547546
Epoch 3.100: Loss = 0.629349
Epoch 3.101: Loss = 0.62822
Epoch 3.102: Loss = 0.50264
Epoch 3.103: Loss = 0.430695
Epoch 3.104: Loss = 0.391922
Epoch 3.105: Loss = 0.548828
Epoch 3.106: Loss = 0.552612
Epoch 3.107: Loss = 0.416519
Epoch 3.108: Loss = 0.503983
Epoch 3.109: Loss = 0.479233
Epoch 3.110: Loss = 0.51178
Epoch 3.111: Loss = 0.358505
Epoch 3.112: Loss = 0.381882
Epoch 3.113: Loss = 0.453812
Epoch 3.114: Loss = 0.377686
Epoch 3.115: Loss = 0.399323
Epoch 3.116: Loss = 0.449738
Epoch 3.117: Loss = 0.302368
Epoch 3.118: Loss = 0.267792
Epoch 3.119: Loss = 0.355072
Epoch 3.120: Loss = 0.383011
TRAIN LOSS = 0.474884
TRAIN ACC = 85.5927 % (51358/60000)
Loss = 0.431
Loss = 0.529694
Loss = 0.618454
Loss = 0.565445
Loss = 0.652771
Loss = 0.458633
Loss = 0.439316
Loss = 0.624344
Loss = 0.585678
Loss = 0.582642
Loss = 0.253632
Loss = 0.407806
Loss = 0.335678
Loss = 0.436462
Loss = 0.277176
Loss = 0.385849
Loss = 0.28569
Loss = 0.103546
Loss = 0.298691
Loss = 0.609161
TEST LOSS = 0.444083
TEST ACC = 513.579 % (8677/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.43544
Epoch 4.2: Loss = 0.611969
Epoch 4.3: Loss = 0.556763
Epoch 4.4: Loss = 0.409241
Epoch 4.5: Loss = 0.406784
Epoch 4.6: Loss = 0.381439
Epoch 4.7: Loss = 0.44429
Epoch 4.8: Loss = 0.454086
Epoch 4.9: Loss = 0.441025
Epoch 4.10: Loss = 0.425735
Epoch 4.11: Loss = 0.467773
Epoch 4.12: Loss = 0.405869
Epoch 4.13: Loss = 0.34787
Epoch 4.14: Loss = 0.389542
Epoch 4.15: Loss = 0.499908
Epoch 4.16: Loss = 0.513107
Epoch 4.17: Loss = 0.540192
Epoch 4.18: Loss = 0.649551
Epoch 4.19: Loss = 0.462463
Epoch 4.20: Loss = 0.401611
Epoch 4.21: Loss = 0.366806
Epoch 4.22: Loss = 0.386368
Epoch 4.23: Loss = 0.38475
Epoch 4.24: Loss = 0.634323
Epoch 4.25: Loss = 0.515457
Epoch 4.26: Loss = 0.650482
Epoch 4.27: Loss = 0.541992
Epoch 4.28: Loss = 0.510345
Epoch 4.29: Loss = 0.623688
Epoch 4.30: Loss = 0.632904
Epoch 4.31: Loss = 0.40387
Epoch 4.32: Loss = 0.579269
Epoch 4.33: Loss = 0.474609
Epoch 4.34: Loss = 0.491119
Epoch 4.35: Loss = 0.464401
Epoch 4.36: Loss = 0.563675
Epoch 4.37: Loss = 0.373123
Epoch 4.38: Loss = 0.395172
Epoch 4.39: Loss = 0.478516
Epoch 4.40: Loss = 0.478134
Epoch 4.41: Loss = 0.481781
Epoch 4.42: Loss = 0.620407
Epoch 4.43: Loss = 0.424408
Epoch 4.44: Loss = 0.353622
Epoch 4.45: Loss = 0.472687
Epoch 4.46: Loss = 0.551987
Epoch 4.47: Loss = 0.488327
Epoch 4.48: Loss = 0.518066
Epoch 4.49: Loss = 0.515625
Epoch 4.50: Loss = 0.559509
Epoch 4.51: Loss = 0.413239
Epoch 4.52: Loss = 0.36496
Epoch 4.53: Loss = 0.453995
Epoch 4.54: Loss = 0.620987
Epoch 4.55: Loss = 0.537506
Epoch 4.56: Loss = 0.447739
Epoch 4.57: Loss = 0.438324
Epoch 4.58: Loss = 0.472549
Epoch 4.59: Loss = 0.555557
Epoch 4.60: Loss = 0.603394
Epoch 4.61: Loss = 0.556686
Epoch 4.62: Loss = 0.558167
Epoch 4.63: Loss = 0.623032
Epoch 4.64: Loss = 0.617355
Epoch 4.65: Loss = 0.653305
Epoch 4.66: Loss = 0.477875
Epoch 4.67: Loss = 0.459854
Epoch 4.68: Loss = 0.320419
Epoch 4.69: Loss = 0.448792
Epoch 4.70: Loss = 0.60054
Epoch 4.71: Loss = 0.42775
Epoch 4.72: Loss = 0.44635
Epoch 4.73: Loss = 0.425415
Epoch 4.74: Loss = 0.339478
Epoch 4.75: Loss = 0.627899
Epoch 4.76: Loss = 0.516174
Epoch 4.77: Loss = 0.456467
Epoch 4.78: Loss = 0.466919
Epoch 4.79: Loss = 0.511688
Epoch 4.80: Loss = 0.571121
Epoch 4.81: Loss = 0.452789
Epoch 4.82: Loss = 0.376129
Epoch 4.83: Loss = 0.627151
Epoch 4.84: Loss = 0.527557
Epoch 4.85: Loss = 0.607819
Epoch 4.86: Loss = 0.518066
Epoch 4.87: Loss = 0.367859
Epoch 4.88: Loss = 0.502548
Epoch 4.89: Loss = 0.595367
Epoch 4.90: Loss = 0.407288
Epoch 4.91: Loss = 0.533463
Epoch 4.92: Loss = 0.573471
Epoch 4.93: Loss = 0.654144
Epoch 4.94: Loss = 0.393524
Epoch 4.95: Loss = 0.498505
Epoch 4.96: Loss = 0.578964
Epoch 4.97: Loss = 0.375214
Epoch 4.98: Loss = 0.439835
Epoch 4.99: Loss = 0.573151
Epoch 4.100: Loss = 0.709625
Epoch 4.101: Loss = 0.691711
Epoch 4.102: Loss = 0.482498
Epoch 4.103: Loss = 0.430679
Epoch 4.104: Loss = 0.379089
Epoch 4.105: Loss = 0.533691
Epoch 4.106: Loss = 0.552994
Epoch 4.107: Loss = 0.384689
Epoch 4.108: Loss = 0.527283
Epoch 4.109: Loss = 0.480133
Epoch 4.110: Loss = 0.526855
Epoch 4.111: Loss = 0.358124
Epoch 4.112: Loss = 0.401093
Epoch 4.113: Loss = 0.465683
Epoch 4.114: Loss = 0.347183
Epoch 4.115: Loss = 0.391632
Epoch 4.116: Loss = 0.463562
Epoch 4.117: Loss = 0.289764
Epoch 4.118: Loss = 0.25058
Epoch 4.119: Loss = 0.348236
Epoch 4.120: Loss = 0.418167
TRAIN LOSS = 0.485245
TRAIN ACC = 85.6033 % (51364/60000)
Loss = 0.430756
Loss = 0.579453
Loss = 0.650299
Loss = 0.585373
Loss = 0.679886
Loss = 0.471817
Loss = 0.456757
Loss = 0.643082
Loss = 0.593216
Loss = 0.558136
Loss = 0.223434
Loss = 0.395874
Loss = 0.382889
Loss = 0.471848
Loss = 0.259155
Loss = 0.409302
Loss = 0.284164
Loss = 0.0917053
Loss = 0.311447
Loss = 0.624329
TEST LOSS = 0.455146
TEST ACC = 513.64 % (8709/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.417603
Epoch 5.2: Loss = 0.634644
Epoch 5.3: Loss = 0.535095
Epoch 5.4: Loss = 0.374634
Epoch 5.5: Loss = 0.429077
Epoch 5.6: Loss = 0.393356
Epoch 5.7: Loss = 0.418289
Epoch 5.8: Loss = 0.439606
Epoch 5.9: Loss = 0.410477
Epoch 5.10: Loss = 0.440277
Epoch 5.11: Loss = 0.512375
Epoch 5.12: Loss = 0.385895
Epoch 5.13: Loss = 0.362976
Epoch 5.14: Loss = 0.402176
Epoch 5.15: Loss = 0.489792
Epoch 5.16: Loss = 0.478394
Epoch 5.17: Loss = 0.50293
Epoch 5.18: Loss = 0.674988
Epoch 5.19: Loss = 0.465225
Epoch 5.20: Loss = 0.39183
Epoch 5.21: Loss = 0.383835
Epoch 5.22: Loss = 0.392288
Epoch 5.23: Loss = 0.396713
Epoch 5.24: Loss = 0.612137
Epoch 5.25: Loss = 0.484985
Epoch 5.26: Loss = 0.711777
Epoch 5.27: Loss = 0.53064
Epoch 5.28: Loss = 0.513184
Epoch 5.29: Loss = 0.609222
Epoch 5.30: Loss = 0.673218
Epoch 5.31: Loss = 0.391357
Epoch 5.32: Loss = 0.580063
Epoch 5.33: Loss = 0.457809
Epoch 5.34: Loss = 0.535004
Epoch 5.35: Loss = 0.431946
Epoch 5.36: Loss = 0.583664
Epoch 5.37: Loss = 0.372681
Epoch 5.38: Loss = 0.411392
Epoch 5.39: Loss = 0.433426
Epoch 5.40: Loss = 0.485275
Epoch 5.41: Loss = 0.439545
Epoch 5.42: Loss = 0.659943
Epoch 5.43: Loss = 0.394608
Epoch 5.44: Loss = 0.337357
Epoch 5.45: Loss = 0.425018
Epoch 5.46: Loss = 0.541687
Epoch 5.47: Loss = 0.48349
Epoch 5.48: Loss = 0.53978
Epoch 5.49: Loss = 0.502365
Epoch 5.50: Loss = 0.562241
Epoch 5.51: Loss = 0.396973
Epoch 5.52: Loss = 0.355957
Epoch 5.53: Loss = 0.471054
Epoch 5.54: Loss = 0.615631
Epoch 5.55: Loss = 0.490021
Epoch 5.56: Loss = 0.442184
Epoch 5.57: Loss = 0.439255
Epoch 5.58: Loss = 0.474472
Epoch 5.59: Loss = 0.530823
Epoch 5.60: Loss = 0.591125
Epoch 5.61: Loss = 0.578247
Epoch 5.62: Loss = 0.531708
Epoch 5.63: Loss = 0.665192
Epoch 5.64: Loss = 0.642059
Epoch 5.65: Loss = 0.61142
Epoch 5.66: Loss = 0.456558
Epoch 5.67: Loss = 0.487473
Epoch 5.68: Loss = 0.310959
Epoch 5.69: Loss = 0.469131
Epoch 5.70: Loss = 0.627777
Epoch 5.71: Loss = 0.410797
Epoch 5.72: Loss = 0.43103
Epoch 5.73: Loss = 0.508148
Epoch 5.74: Loss = 0.312927
Epoch 5.75: Loss = 0.678024
Epoch 5.76: Loss = 0.542664
Epoch 5.77: Loss = 0.506836
Epoch 5.78: Loss = 0.466125
Epoch 5.79: Loss = 0.489349
Epoch 5.80: Loss = 0.597595
Epoch 5.81: Loss = 0.442825
Epoch 5.82: Loss = 0.333145
Epoch 5.83: Loss = 0.626648
Epoch 5.84: Loss = 0.523819
Epoch 5.85: Loss = 0.623322
Epoch 5.86: Loss = 0.530426
Epoch 5.87: Loss = 0.37822
Epoch 5.88: Loss = 0.49408
Epoch 5.89: Loss = 0.582001
Epoch 5.90: Loss = 0.422836
Epoch 5.91: Loss = 0.575089
Epoch 5.92: Loss = 0.541702
Epoch 5.93: Loss = 0.637878
Epoch 5.94: Loss = 0.395157
Epoch 5.95: Loss = 0.481262
Epoch 5.96: Loss = 0.537613
Epoch 5.97: Loss = 0.385422
Epoch 5.98: Loss = 0.425385
Epoch 5.99: Loss = 0.552277
Epoch 5.100: Loss = 0.674728
Epoch 5.101: Loss = 0.672272
Epoch 5.102: Loss = 0.455917
Epoch 5.103: Loss = 0.420166
Epoch 5.104: Loss = 0.399765
Epoch 5.105: Loss = 0.517105
Epoch 5.106: Loss = 0.590317
Epoch 5.107: Loss = 0.392776
Epoch 5.108: Loss = 0.529022
Epoch 5.109: Loss = 0.456863
Epoch 5.110: Loss = 0.559967
Epoch 5.111: Loss = 0.356461
Epoch 5.112: Loss = 0.381195
Epoch 5.113: Loss = 0.448364
Epoch 5.114: Loss = 0.384171
Epoch 5.115: Loss = 0.364822
Epoch 5.116: Loss = 0.444458
Epoch 5.117: Loss = 0.274109
Epoch 5.118: Loss = 0.254837
Epoch 5.119: Loss = 0.311142
Epoch 5.120: Loss = 0.407333
TRAIN LOSS = 0.482941
TRAIN ACC = 86.2991 % (51782/60000)
Loss = 0.45195
Loss = 0.624084
Loss = 0.651062
Loss = 0.618378
Loss = 0.728104
Loss = 0.490921
Loss = 0.469223
Loss = 0.699448
Loss = 0.595047
Loss = 0.590103
Loss = 0.227448
Loss = 0.399536
Loss = 0.335815
Loss = 0.440201
Loss = 0.227844
Loss = 0.34198
Loss = 0.315506
Loss = 0.0802307
Loss = 0.267105
Loss = 0.648743
TEST LOSS = 0.460136
TEST ACC = 517.819 % (8722/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.393311
Epoch 6.2: Loss = 0.623703
Epoch 6.3: Loss = 0.527283
Epoch 6.4: Loss = 0.382599
Epoch 6.5: Loss = 0.460419
Epoch 6.6: Loss = 0.385834
Epoch 6.7: Loss = 0.411728
Epoch 6.8: Loss = 0.428741
Epoch 6.9: Loss = 0.419083
Epoch 6.10: Loss = 0.451584
Epoch 6.11: Loss = 0.485886
Epoch 6.12: Loss = 0.394333
Epoch 6.13: Loss = 0.340897
Epoch 6.14: Loss = 0.466049
Epoch 6.15: Loss = 0.491959
Epoch 6.16: Loss = 0.503601
Epoch 6.17: Loss = 0.526276
Epoch 6.18: Loss = 0.698181
Epoch 6.19: Loss = 0.51178
Epoch 6.20: Loss = 0.465317
Epoch 6.21: Loss = 0.417999
Epoch 6.22: Loss = 0.359009
Epoch 6.23: Loss = 0.40451
Epoch 6.24: Loss = 0.604691
Epoch 6.25: Loss = 0.428345
Epoch 6.26: Loss = 0.74762
Epoch 6.27: Loss = 0.532822
Epoch 6.28: Loss = 0.565842
Epoch 6.29: Loss = 0.626801
Epoch 6.30: Loss = 0.673645
Epoch 6.31: Loss = 0.395355
Epoch 6.32: Loss = 0.556458
Epoch 6.33: Loss = 0.491943
Epoch 6.34: Loss = 0.542694
Epoch 6.35: Loss = 0.459015
Epoch 6.36: Loss = 0.609177
Epoch 6.37: Loss = 0.399536
Epoch 6.38: Loss = 0.406097
Epoch 6.39: Loss = 0.450073
Epoch 6.40: Loss = 0.490616
Epoch 6.41: Loss = 0.4841
Epoch 6.42: Loss = 0.663651
Epoch 6.43: Loss = 0.447098
Epoch 6.44: Loss = 0.397858
Epoch 6.45: Loss = 0.48436
Epoch 6.46: Loss = 0.579758
Epoch 6.47: Loss = 0.526276
Epoch 6.48: Loss = 0.535873
Epoch 6.49: Loss = 0.498077
Epoch 6.50: Loss = 0.619247
Epoch 6.51: Loss = 0.405655
Epoch 6.52: Loss = 0.398819
Epoch 6.53: Loss = 0.470337
Epoch 6.54: Loss = 0.60643
Epoch 6.55: Loss = 0.453201
Epoch 6.56: Loss = 0.456284
Epoch 6.57: Loss = 0.448608
Epoch 6.58: Loss = 0.478928
Epoch 6.59: Loss = 0.54776
Epoch 6.60: Loss = 0.596207
Epoch 6.61: Loss = 0.48288
Epoch 6.62: Loss = 0.574188
Epoch 6.63: Loss = 0.668243
Epoch 6.64: Loss = 0.619476
Epoch 6.65: Loss = 0.61998
Epoch 6.66: Loss = 0.469864
Epoch 6.67: Loss = 0.457199
Epoch 6.68: Loss = 0.337921
Epoch 6.69: Loss = 0.459274
Epoch 6.70: Loss = 0.585541
Epoch 6.71: Loss = 0.40863
Epoch 6.72: Loss = 0.420242
Epoch 6.73: Loss = 0.509171
Epoch 6.74: Loss = 0.346024
Epoch 6.75: Loss = 0.687561
Epoch 6.76: Loss = 0.544449
Epoch 6.77: Loss = 0.450165
Epoch 6.78: Loss = 0.535507
Epoch 6.79: Loss = 0.576675
Epoch 6.80: Loss = 0.593918
Epoch 6.81: Loss = 0.404861
Epoch 6.82: Loss = 0.412003
Epoch 6.83: Loss = 0.620377
Epoch 6.84: Loss = 0.572586
Epoch 6.85: Loss = 0.58696
Epoch 6.86: Loss = 0.564926
Epoch 6.87: Loss = 0.381409
Epoch 6.88: Loss = 0.552521
Epoch 6.89: Loss = 0.576691
Epoch 6.90: Loss = 0.437012
Epoch 6.91: Loss = 0.601273
Epoch 6.92: Loss = 0.608643
Epoch 6.93: Loss = 0.667618
Epoch 6.94: Loss = 0.396576
Epoch 6.95: Loss = 0.504364
Epoch 6.96: Loss = 0.539795
Epoch 6.97: Loss = 0.413788
Epoch 6.98: Loss = 0.415298
Epoch 6.99: Loss = 0.621613
Epoch 6.100: Loss = 0.690079
Epoch 6.101: Loss = 0.727417
Epoch 6.102: Loss = 0.429291
Epoch 6.103: Loss = 0.480759
Epoch 6.104: Loss = 0.402115
Epoch 6.105: Loss = 0.609497
Epoch 6.106: Loss = 0.565002
Epoch 6.107: Loss = 0.40538
Epoch 6.108: Loss = 0.545273
Epoch 6.109: Loss = 0.4646
Epoch 6.110: Loss = 0.566193
Epoch 6.111: Loss = 0.388901
Epoch 6.112: Loss = 0.369476
Epoch 6.113: Loss = 0.453018
Epoch 6.114: Loss = 0.397568
Epoch 6.115: Loss = 0.387009
Epoch 6.116: Loss = 0.516525
Epoch 6.117: Loss = 0.287918
Epoch 6.118: Loss = 0.25943
Epoch 6.119: Loss = 0.377945
Epoch 6.120: Loss = 0.429337
TRAIN LOSS = 0.496735
TRAIN ACC = 86.4075 % (51846/60000)
Loss = 0.502335
Loss = 0.677322
Loss = 0.698944
Loss = 0.716553
Loss = 0.772415
Loss = 0.519562
Loss = 0.467865
Loss = 0.692123
Loss = 0.675232
Loss = 0.631821
Loss = 0.178162
Loss = 0.45993
Loss = 0.335464
Loss = 0.442062
Loss = 0.287811
Loss = 0.377106
Loss = 0.335388
Loss = 0.0604401
Loss = 0.284348
Loss = 0.708908
TEST LOSS = 0.491189
TEST ACC = 518.459 % (8680/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.452972
Epoch 7.2: Loss = 0.660416
Epoch 7.3: Loss = 0.565109
Epoch 7.4: Loss = 0.399658
Epoch 7.5: Loss = 0.445969
Epoch 7.6: Loss = 0.427765
Epoch 7.7: Loss = 0.43689
Epoch 7.8: Loss = 0.46817
Epoch 7.9: Loss = 0.415512
Epoch 7.10: Loss = 0.461884
Epoch 7.11: Loss = 0.487366
Epoch 7.12: Loss = 0.372574
Epoch 7.13: Loss = 0.337646
Epoch 7.14: Loss = 0.464142
Epoch 7.15: Loss = 0.54567
Epoch 7.16: Loss = 0.529831
Epoch 7.17: Loss = 0.551651
Epoch 7.18: Loss = 0.802231
Epoch 7.19: Loss = 0.528961
Epoch 7.20: Loss = 0.438995
Epoch 7.21: Loss = 0.423218
Epoch 7.22: Loss = 0.39444
Epoch 7.23: Loss = 0.392792
Epoch 7.24: Loss = 0.623367
Epoch 7.25: Loss = 0.507095
Epoch 7.26: Loss = 0.726196
Epoch 7.27: Loss = 0.545456
Epoch 7.28: Loss = 0.552536
Epoch 7.29: Loss = 0.654785
Epoch 7.30: Loss = 0.631561
Epoch 7.31: Loss = 0.418823
Epoch 7.32: Loss = 0.576019
Epoch 7.33: Loss = 0.550446
Epoch 7.34: Loss = 0.622986
Epoch 7.35: Loss = 0.469528
Epoch 7.36: Loss = 0.554901
Epoch 7.37: Loss = 0.372223
Epoch 7.38: Loss = 0.395645
Epoch 7.39: Loss = 0.531403
Epoch 7.40: Loss = 0.504242
Epoch 7.41: Loss = 0.500809
Epoch 7.42: Loss = 0.698593
Epoch 7.43: Loss = 0.495712
Epoch 7.44: Loss = 0.397003
Epoch 7.45: Loss = 0.530243
Epoch 7.46: Loss = 0.610535
Epoch 7.47: Loss = 0.556717
Epoch 7.48: Loss = 0.519745
Epoch 7.49: Loss = 0.520416
Epoch 7.50: Loss = 0.594635
Epoch 7.51: Loss = 0.444229
Epoch 7.52: Loss = 0.414597
Epoch 7.53: Loss = 0.527802
Epoch 7.54: Loss = 0.630966
Epoch 7.55: Loss = 0.474014
Epoch 7.56: Loss = 0.514832
Epoch 7.57: Loss = 0.517166
Epoch 7.58: Loss = 0.504105
Epoch 7.59: Loss = 0.588333
Epoch 7.60: Loss = 0.651047
Epoch 7.61: Loss = 0.497009
Epoch 7.62: Loss = 0.66716
Epoch 7.63: Loss = 0.760269
Epoch 7.64: Loss = 0.620331
Epoch 7.65: Loss = 0.665237
Epoch 7.66: Loss = 0.509323
Epoch 7.67: Loss = 0.481522
Epoch 7.68: Loss = 0.31514
Epoch 7.69: Loss = 0.461792
Epoch 7.70: Loss = 0.59819
Epoch 7.71: Loss = 0.444717
Epoch 7.72: Loss = 0.439438
Epoch 7.73: Loss = 0.56012
Epoch 7.74: Loss = 0.412567
Epoch 7.75: Loss = 0.840561
Epoch 7.76: Loss = 0.549545
Epoch 7.77: Loss = 0.495193
Epoch 7.78: Loss = 0.637665
Epoch 7.79: Loss = 0.632233
Epoch 7.80: Loss = 0.683731
Epoch 7.81: Loss = 0.402252
Epoch 7.82: Loss = 0.435883
Epoch 7.83: Loss = 0.645813
Epoch 7.84: Loss = 0.570053
Epoch 7.85: Loss = 0.599518
Epoch 7.86: Loss = 0.579269
Epoch 7.87: Loss = 0.407379
Epoch 7.88: Loss = 0.5177
Epoch 7.89: Loss = 0.585098
Epoch 7.90: Loss = 0.489868
Epoch 7.91: Loss = 0.626755
Epoch 7.92: Loss = 0.625732
Epoch 7.93: Loss = 0.735565
Epoch 7.94: Loss = 0.434555
Epoch 7.95: Loss = 0.576111
Epoch 7.96: Loss = 0.598343
Epoch 7.97: Loss = 0.424026
Epoch 7.98: Loss = 0.461807
Epoch 7.99: Loss = 0.598694
Epoch 7.100: Loss = 0.786636
Epoch 7.101: Loss = 0.797318
Epoch 7.102: Loss = 0.435684
Epoch 7.103: Loss = 0.482437
Epoch 7.104: Loss = 0.461884
Epoch 7.105: Loss = 0.676956
Epoch 7.106: Loss = 0.573013
Epoch 7.107: Loss = 0.465149
Epoch 7.108: Loss = 0.571701
Epoch 7.109: Loss = 0.46785
Epoch 7.110: Loss = 0.621445
Epoch 7.111: Loss = 0.416153
Epoch 7.112: Loss = 0.421539
Epoch 7.113: Loss = 0.489105
Epoch 7.114: Loss = 0.425858
Epoch 7.115: Loss = 0.43158
Epoch 7.116: Loss = 0.50563
Epoch 7.117: Loss = 0.381165
Epoch 7.118: Loss = 0.300156
Epoch 7.119: Loss = 0.461227
Epoch 7.120: Loss = 0.418457
TRAIN LOSS = 0.525925
TRAIN ACC = 86.2198 % (51734/60000)
Loss = 0.554428
Loss = 0.65567
Loss = 0.718903
Loss = 0.720764
Loss = 0.800613
Loss = 0.534683
Loss = 0.488922
Loss = 0.798447
Loss = 0.703674
Loss = 0.647507
Loss = 0.205719
Loss = 0.445908
Loss = 0.402878
Loss = 0.519821
Loss = 0.289642
Loss = 0.435226
Loss = 0.373505
Loss = 0.0550079
Loss = 0.294418
Loss = 0.746658
TEST LOSS = 0.51962
TEST ACC = 517.339 % (8659/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.491852
Epoch 8.2: Loss = 0.749161
Epoch 8.3: Loss = 0.642365
Epoch 8.4: Loss = 0.422745
Epoch 8.5: Loss = 0.475723
Epoch 8.6: Loss = 0.443054
Epoch 8.7: Loss = 0.445847
Epoch 8.8: Loss = 0.495026
Epoch 8.9: Loss = 0.473434
Epoch 8.10: Loss = 0.534744
Epoch 8.11: Loss = 0.533051
Epoch 8.12: Loss = 0.401398
Epoch 8.13: Loss = 0.371277
Epoch 8.14: Loss = 0.474426
Epoch 8.15: Loss = 0.559982
Epoch 8.16: Loss = 0.565369
Epoch 8.17: Loss = 0.554657
Epoch 8.18: Loss = 0.815048
Epoch 8.19: Loss = 0.535202
Epoch 8.20: Loss = 0.446976
Epoch 8.21: Loss = 0.430267
Epoch 8.22: Loss = 0.375549
Epoch 8.23: Loss = 0.415512
Epoch 8.24: Loss = 0.680695
Epoch 8.25: Loss = 0.550262
Epoch 8.26: Loss = 0.814651
Epoch 8.27: Loss = 0.558044
Epoch 8.28: Loss = 0.655319
Epoch 8.29: Loss = 0.689117
Epoch 8.30: Loss = 0.706589
Epoch 8.31: Loss = 0.435669
Epoch 8.32: Loss = 0.62204
Epoch 8.33: Loss = 0.546448
Epoch 8.34: Loss = 0.604675
Epoch 8.35: Loss = 0.486481
Epoch 8.36: Loss = 0.600952
Epoch 8.37: Loss = 0.415726
Epoch 8.38: Loss = 0.396271
Epoch 8.39: Loss = 0.531448
Epoch 8.40: Loss = 0.473282
Epoch 8.41: Loss = 0.541153
Epoch 8.42: Loss = 0.741867
Epoch 8.43: Loss = 0.525681
Epoch 8.44: Loss = 0.386032
Epoch 8.45: Loss = 0.545242
Epoch 8.46: Loss = 0.735016
Epoch 8.47: Loss = 0.636047
Epoch 8.48: Loss = 0.574936
Epoch 8.49: Loss = 0.541962
Epoch 8.50: Loss = 0.598068
Epoch 8.51: Loss = 0.472473
Epoch 8.52: Loss = 0.436935
Epoch 8.53: Loss = 0.505402
Epoch 8.54: Loss = 0.68219
Epoch 8.55: Loss = 0.579178
Epoch 8.56: Loss = 0.567139
Epoch 8.57: Loss = 0.548126
Epoch 8.58: Loss = 0.508026
Epoch 8.59: Loss = 0.59201
Epoch 8.60: Loss = 0.681564
Epoch 8.61: Loss = 0.559998
Epoch 8.62: Loss = 0.734528
Epoch 8.63: Loss = 0.798782
Epoch 8.64: Loss = 0.640945
Epoch 8.65: Loss = 0.645645
Epoch 8.66: Loss = 0.584946
Epoch 8.67: Loss = 0.549805
Epoch 8.68: Loss = 0.320251
Epoch 8.69: Loss = 0.502258
Epoch 8.70: Loss = 0.618622
Epoch 8.71: Loss = 0.494415
Epoch 8.72: Loss = 0.48761
Epoch 8.73: Loss = 0.604919
Epoch 8.74: Loss = 0.443176
Epoch 8.75: Loss = 0.818863
Epoch 8.76: Loss = 0.531265
Epoch 8.77: Loss = 0.524567
Epoch 8.78: Loss = 0.611481
Epoch 8.79: Loss = 0.605942
Epoch 8.80: Loss = 0.686218
Epoch 8.81: Loss = 0.436691
Epoch 8.82: Loss = 0.491211
Epoch 8.83: Loss = 0.610123
Epoch 8.84: Loss = 0.584213
Epoch 8.85: Loss = 0.740341
Epoch 8.86: Loss = 0.57457
Epoch 8.87: Loss = 0.440323
Epoch 8.88: Loss = 0.51265
Epoch 8.89: Loss = 0.615799
Epoch 8.90: Loss = 0.520782
Epoch 8.91: Loss = 0.699829
Epoch 8.92: Loss = 0.640411
Epoch 8.93: Loss = 0.751755
Epoch 8.94: Loss = 0.419556
Epoch 8.95: Loss = 0.523087
Epoch 8.96: Loss = 0.635849
Epoch 8.97: Loss = 0.423065
Epoch 8.98: Loss = 0.455887
Epoch 8.99: Loss = 0.545532
Epoch 8.100: Loss = 0.79747
Epoch 8.101: Loss = 0.799835
Epoch 8.102: Loss = 0.48378
Epoch 8.103: Loss = 0.487289
Epoch 8.104: Loss = 0.49147
Epoch 8.105: Loss = 0.719543
Epoch 8.106: Loss = 0.589935
Epoch 8.107: Loss = 0.50032
Epoch 8.108: Loss = 0.57811
Epoch 8.109: Loss = 0.47377
Epoch 8.110: Loss = 0.662262
Epoch 8.111: Loss = 0.414566
Epoch 8.112: Loss = 0.446243
Epoch 8.113: Loss = 0.48378
Epoch 8.114: Loss = 0.487823
Epoch 8.115: Loss = 0.429413
Epoch 8.116: Loss = 0.515747
Epoch 8.117: Loss = 0.410507
Epoch 8.118: Loss = 0.343094
Epoch 8.119: Loss = 0.413589
Epoch 8.120: Loss = 0.453354
TRAIN LOSS = 0.551575
TRAIN ACC = 86.0535 % (51634/60000)
Loss = 0.553131
Loss = 0.709564
Loss = 0.738159
Loss = 0.778595
Loss = 0.792755
Loss = 0.584869
Loss = 0.502869
Loss = 0.852356
Loss = 0.706482
Loss = 0.62851
Loss = 0.213638
Loss = 0.520767
Loss = 0.415558
Loss = 0.521713
Loss = 0.303192
Loss = 0.515808
Loss = 0.438995
Loss = 0.0617676
Loss = 0.33873
Loss = 0.811249
TEST LOSS = 0.549435
TEST ACC = 516.339 % (8644/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.537033
Epoch 9.2: Loss = 0.711655
Epoch 9.3: Loss = 0.692612
Epoch 9.4: Loss = 0.408295
Epoch 9.5: Loss = 0.524826
Epoch 9.6: Loss = 0.416306
Epoch 9.7: Loss = 0.459442
Epoch 9.8: Loss = 0.562347
Epoch 9.9: Loss = 0.495605
Epoch 9.10: Loss = 0.528625
Epoch 9.11: Loss = 0.581757
Epoch 9.12: Loss = 0.419571
Epoch 9.13: Loss = 0.359467
Epoch 9.14: Loss = 0.494736
Epoch 9.15: Loss = 0.592041
Epoch 9.16: Loss = 0.593842
Epoch 9.17: Loss = 0.581802
Epoch 9.18: Loss = 0.891556
Epoch 9.19: Loss = 0.581848
Epoch 9.20: Loss = 0.465195
Epoch 9.21: Loss = 0.504044
Epoch 9.22: Loss = 0.417435
Epoch 9.23: Loss = 0.487778
Epoch 9.24: Loss = 0.697128
Epoch 9.25: Loss = 0.591965
Epoch 9.26: Loss = 0.937531
Epoch 9.27: Loss = 0.606064
Epoch 9.28: Loss = 0.651093
Epoch 9.29: Loss = 0.754684
Epoch 9.30: Loss = 0.682755
Epoch 9.31: Loss = 0.461899
Epoch 9.32: Loss = 0.685989
Epoch 9.33: Loss = 0.541565
Epoch 9.34: Loss = 0.626434
Epoch 9.35: Loss = 0.476791
Epoch 9.36: Loss = 0.622925
Epoch 9.37: Loss = 0.447678
Epoch 9.38: Loss = 0.429916
Epoch 9.39: Loss = 0.523849
Epoch 9.40: Loss = 0.501755
Epoch 9.41: Loss = 0.571243
Epoch 9.42: Loss = 0.785187
Epoch 9.43: Loss = 0.463959
Epoch 9.44: Loss = 0.395584
Epoch 9.45: Loss = 0.543427
Epoch 9.46: Loss = 0.751602
Epoch 9.47: Loss = 0.645126
Epoch 9.48: Loss = 0.567352
Epoch 9.49: Loss = 0.608505
Epoch 9.50: Loss = 0.652237
Epoch 9.51: Loss = 0.484314
Epoch 9.52: Loss = 0.385559
Epoch 9.53: Loss = 0.506927
Epoch 9.54: Loss = 0.687103
Epoch 9.55: Loss = 0.565765
Epoch 9.56: Loss = 0.55687
Epoch 9.57: Loss = 0.580948
Epoch 9.58: Loss = 0.540146
Epoch 9.59: Loss = 0.606415
Epoch 9.60: Loss = 0.732956
Epoch 9.61: Loss = 0.52597
Epoch 9.62: Loss = 0.740448
Epoch 9.63: Loss = 0.801331
Epoch 9.64: Loss = 0.629303
Epoch 9.65: Loss = 0.682159
Epoch 9.66: Loss = 0.531555
Epoch 9.67: Loss = 0.586517
Epoch 9.68: Loss = 0.315399
Epoch 9.69: Loss = 0.536743
Epoch 9.70: Loss = 0.688461
Epoch 9.71: Loss = 0.49585
Epoch 9.72: Loss = 0.530396
Epoch 9.73: Loss = 0.587173
Epoch 9.74: Loss = 0.443375
Epoch 9.75: Loss = 0.908463
Epoch 9.76: Loss = 0.554855
Epoch 9.77: Loss = 0.481781
Epoch 9.78: Loss = 0.597519
Epoch 9.79: Loss = 0.584991
Epoch 9.80: Loss = 0.700134
Epoch 9.81: Loss = 0.416077
Epoch 9.82: Loss = 0.434952
Epoch 9.83: Loss = 0.614639
Epoch 9.84: Loss = 0.582336
Epoch 9.85: Loss = 0.721786
Epoch 9.86: Loss = 0.577286
Epoch 9.87: Loss = 0.482086
Epoch 9.88: Loss = 0.557846
Epoch 9.89: Loss = 0.594223
Epoch 9.90: Loss = 0.560654
Epoch 9.91: Loss = 0.715408
Epoch 9.92: Loss = 0.647446
Epoch 9.93: Loss = 0.82486
Epoch 9.94: Loss = 0.441132
Epoch 9.95: Loss = 0.568024
Epoch 9.96: Loss = 0.663345
Epoch 9.97: Loss = 0.45874
Epoch 9.98: Loss = 0.54776
Epoch 9.99: Loss = 0.624252
Epoch 9.100: Loss = 0.834763
Epoch 9.101: Loss = 0.833435
Epoch 9.102: Loss = 0.513031
Epoch 9.103: Loss = 0.572189
Epoch 9.104: Loss = 0.550919
Epoch 9.105: Loss = 0.812973
Epoch 9.106: Loss = 0.707245
Epoch 9.107: Loss = 0.498215
Epoch 9.108: Loss = 0.625992
Epoch 9.109: Loss = 0.507889
Epoch 9.110: Loss = 0.670975
Epoch 9.111: Loss = 0.419846
Epoch 9.112: Loss = 0.445847
Epoch 9.113: Loss = 0.505859
Epoch 9.114: Loss = 0.477478
Epoch 9.115: Loss = 0.496185
Epoch 9.116: Loss = 0.571915
Epoch 9.117: Loss = 0.423096
Epoch 9.118: Loss = 0.317551
Epoch 9.119: Loss = 0.436325
Epoch 9.120: Loss = 0.505264
TRAIN LOSS = 0.573853
TRAIN ACC = 86.1191 % (51674/60000)
Loss = 0.52002
Loss = 0.728043
Loss = 0.768799
Loss = 0.808624
Loss = 0.847687
Loss = 0.637421
Loss = 0.517044
Loss = 0.916214
Loss = 0.733963
Loss = 0.654037
Loss = 0.225174
Loss = 0.476517
Loss = 0.415283
Loss = 0.513123
Loss = 0.275513
Loss = 0.463577
Loss = 0.434479
Loss = 0.0565491
Loss = 0.327881
Loss = 0.892181
TEST LOSS = 0.560606
TEST ACC = 516.739 % (8702/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.595261
Epoch 10.2: Loss = 0.746582
Epoch 10.3: Loss = 0.686584
Epoch 10.4: Loss = 0.429138
Epoch 10.5: Loss = 0.466049
Epoch 10.6: Loss = 0.44519
Epoch 10.7: Loss = 0.469788
Epoch 10.8: Loss = 0.600037
Epoch 10.9: Loss = 0.573807
Epoch 10.10: Loss = 0.563156
Epoch 10.11: Loss = 0.598175
Epoch 10.12: Loss = 0.380844
Epoch 10.13: Loss = 0.347519
Epoch 10.14: Loss = 0.530457
Epoch 10.15: Loss = 0.605377
Epoch 10.16: Loss = 0.661011
Epoch 10.17: Loss = 0.650833
Epoch 10.18: Loss = 0.93869
Epoch 10.19: Loss = 0.637314
Epoch 10.20: Loss = 0.504227
Epoch 10.21: Loss = 0.526993
Epoch 10.22: Loss = 0.395981
Epoch 10.23: Loss = 0.464142
Epoch 10.24: Loss = 0.681839
Epoch 10.25: Loss = 0.612076
Epoch 10.26: Loss = 0.930786
Epoch 10.27: Loss = 0.621765
Epoch 10.28: Loss = 0.669312
Epoch 10.29: Loss = 0.851425
Epoch 10.30: Loss = 0.718277
Epoch 10.31: Loss = 0.457138
Epoch 10.32: Loss = 0.706711
Epoch 10.33: Loss = 0.560715
Epoch 10.34: Loss = 0.743469
Epoch 10.35: Loss = 0.562607
Epoch 10.36: Loss = 0.641388
Epoch 10.37: Loss = 0.462341
Epoch 10.38: Loss = 0.499649
Epoch 10.39: Loss = 0.575668
Epoch 10.40: Loss = 0.556808
Epoch 10.41: Loss = 0.556427
Epoch 10.42: Loss = 0.824219
Epoch 10.43: Loss = 0.545288
Epoch 10.44: Loss = 0.418335
Epoch 10.45: Loss = 0.582932
Epoch 10.46: Loss = 0.756714
Epoch 10.47: Loss = 0.703888
Epoch 10.48: Loss = 0.605621
Epoch 10.49: Loss = 0.581055
Epoch 10.50: Loss = 0.712555
Epoch 10.51: Loss = 0.539154
Epoch 10.52: Loss = 0.421249
Epoch 10.53: Loss = 0.567047
Epoch 10.54: Loss = 0.686356
Epoch 10.55: Loss = 0.58049
Epoch 10.56: Loss = 0.577667
Epoch 10.57: Loss = 0.571457
Epoch 10.58: Loss = 0.65831
Epoch 10.59: Loss = 0.59642
Epoch 10.60: Loss = 0.77121
Epoch 10.61: Loss = 0.600922
Epoch 10.62: Loss = 0.810074
Epoch 10.63: Loss = 0.790436
Epoch 10.64: Loss = 0.640854
Epoch 10.65: Loss = 0.689087
Epoch 10.66: Loss = 0.537552
Epoch 10.67: Loss = 0.613419
Epoch 10.68: Loss = 0.347916
Epoch 10.69: Loss = 0.507919
Epoch 10.70: Loss = 0.736069
Epoch 10.71: Loss = 0.551758
Epoch 10.72: Loss = 0.534317
Epoch 10.73: Loss = 0.665894
Epoch 10.74: Loss = 0.511871
Epoch 10.75: Loss = 0.963516
Epoch 10.76: Loss = 0.598236
Epoch 10.77: Loss = 0.534225
Epoch 10.78: Loss = 0.620712
Epoch 10.79: Loss = 0.626923
Epoch 10.80: Loss = 0.762604
Epoch 10.81: Loss = 0.454773
Epoch 10.82: Loss = 0.4384
Epoch 10.83: Loss = 0.713425
Epoch 10.84: Loss = 0.669647
Epoch 10.85: Loss = 0.736359
Epoch 10.86: Loss = 0.643951
Epoch 10.87: Loss = 0.511185
Epoch 10.88: Loss = 0.612885
Epoch 10.89: Loss = 0.633545
Epoch 10.90: Loss = 0.616638
Epoch 10.91: Loss = 0.756805
Epoch 10.92: Loss = 0.702408
Epoch 10.93: Loss = 0.904312
Epoch 10.94: Loss = 0.475037
Epoch 10.95: Loss = 0.588623
Epoch 10.96: Loss = 0.667526
Epoch 10.97: Loss = 0.492249
Epoch 10.98: Loss = 0.614456
Epoch 10.99: Loss = 0.633163
Epoch 10.100: Loss = 0.874939
Epoch 10.101: Loss = 0.860474
Epoch 10.102: Loss = 0.553909
Epoch 10.103: Loss = 0.541046
Epoch 10.104: Loss = 0.578918
Epoch 10.105: Loss = 0.79985
Epoch 10.106: Loss = 0.770645
Epoch 10.107: Loss = 0.486603
Epoch 10.108: Loss = 0.681519
Epoch 10.109: Loss = 0.564117
Epoch 10.110: Loss = 0.70929
Epoch 10.111: Loss = 0.435745
Epoch 10.112: Loss = 0.471298
Epoch 10.113: Loss = 0.490616
Epoch 10.114: Loss = 0.540298
Epoch 10.115: Loss = 0.525955
Epoch 10.116: Loss = 0.612518
Epoch 10.117: Loss = 0.391449
Epoch 10.118: Loss = 0.36824
Epoch 10.119: Loss = 0.491547
Epoch 10.120: Loss = 0.517349
TRAIN LOSS = 0.605865
TRAIN ACC = 86.1252 % (51678/60000)
Loss = 0.608612
Loss = 0.739716
Loss = 0.745255
Loss = 0.858047
Loss = 0.88089
Loss = 0.734161
Loss = 0.545486
Loss = 0.935898
Loss = 0.738983
Loss = 0.737762
Loss = 0.239944
Loss = 0.523376
Loss = 0.373199
Loss = 0.519394
Loss = 0.313019
Loss = 0.368576
Loss = 0.505997
Loss = 0.0601807
Loss = 0.317688
Loss = 0.852463
TEST LOSS = 0.579932
TEST ACC = 516.779 % (8699/10000)
