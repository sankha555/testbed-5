Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.35298
Epoch 1.2: Loss = 2.27214
Epoch 1.3: Loss = 2.214
Epoch 1.4: Loss = 2.13231
Epoch 1.5: Loss = 2.09993
Epoch 1.6: Loss = 2.01903
Epoch 1.7: Loss = 1.99361
Epoch 1.8: Loss = 1.96156
Epoch 1.9: Loss = 1.8974
Epoch 1.10: Loss = 1.83292
Epoch 1.11: Loss = 1.85428
Epoch 1.12: Loss = 1.78696
Epoch 1.13: Loss = 1.76015
Epoch 1.14: Loss = 1.6946
Epoch 1.15: Loss = 1.63121
Epoch 1.16: Loss = 1.62874
Epoch 1.17: Loss = 1.59102
Epoch 1.18: Loss = 1.55612
Epoch 1.19: Loss = 1.49754
Epoch 1.20: Loss = 1.50546
Epoch 1.21: Loss = 1.44791
Epoch 1.22: Loss = 1.425
Epoch 1.23: Loss = 1.37149
Epoch 1.24: Loss = 1.4399
Epoch 1.25: Loss = 1.3515
Epoch 1.26: Loss = 1.29584
Epoch 1.27: Loss = 1.26068
Epoch 1.28: Loss = 1.25499
Epoch 1.29: Loss = 1.23698
Epoch 1.30: Loss = 1.22214
Epoch 1.31: Loss = 1.23718
Epoch 1.32: Loss = 1.2166
Epoch 1.33: Loss = 1.11723
Epoch 1.34: Loss = 1.17456
Epoch 1.35: Loss = 1.22095
Epoch 1.36: Loss = 1.18349
Epoch 1.37: Loss = 1.1478
Epoch 1.38: Loss = 1.11804
Epoch 1.39: Loss = 1.08997
Epoch 1.40: Loss = 1.08942
Epoch 1.41: Loss = 1.13794
Epoch 1.42: Loss = 1.05016
Epoch 1.43: Loss = 1.04414
Epoch 1.44: Loss = 0.992111
Epoch 1.45: Loss = 1.03706
Epoch 1.46: Loss = 1.04668
Epoch 1.47: Loss = 1.01218
Epoch 1.48: Loss = 1.00542
Epoch 1.49: Loss = 1.0351
Epoch 1.50: Loss = 0.986542
Epoch 1.51: Loss = 0.941833
Epoch 1.52: Loss = 1.01607
Epoch 1.53: Loss = 1.00993
Epoch 1.54: Loss = 0.887054
Epoch 1.55: Loss = 0.970398
Epoch 1.56: Loss = 0.952972
Epoch 1.57: Loss = 0.974594
Epoch 1.58: Loss = 0.927017
Epoch 1.59: Loss = 0.947876
Epoch 1.60: Loss = 0.964615
Epoch 1.61: Loss = 0.87291
Epoch 1.62: Loss = 0.955368
Epoch 1.63: Loss = 0.819244
Epoch 1.64: Loss = 0.880814
Epoch 1.65: Loss = 0.891861
Epoch 1.66: Loss = 0.883347
Epoch 1.67: Loss = 0.821152
Epoch 1.68: Loss = 0.947357
Epoch 1.69: Loss = 0.893341
Epoch 1.70: Loss = 0.848801
Epoch 1.71: Loss = 0.806671
Epoch 1.72: Loss = 0.826523
Epoch 1.73: Loss = 0.91835
Epoch 1.74: Loss = 0.907547
Epoch 1.75: Loss = 0.821136
Epoch 1.76: Loss = 0.853928
Epoch 1.77: Loss = 0.833618
Epoch 1.78: Loss = 0.821991
Epoch 1.79: Loss = 0.778717
Epoch 1.80: Loss = 0.851944
Epoch 1.81: Loss = 0.811432
Epoch 1.82: Loss = 0.826141
Epoch 1.83: Loss = 0.869934
Epoch 1.84: Loss = 0.818024
Epoch 1.85: Loss = 0.786606
Epoch 1.86: Loss = 0.860397
Epoch 1.87: Loss = 0.851257
Epoch 1.88: Loss = 0.735367
Epoch 1.89: Loss = 0.867065
Epoch 1.90: Loss = 0.79274
Epoch 1.91: Loss = 0.875565
Epoch 1.92: Loss = 0.819839
Epoch 1.93: Loss = 0.833176
Epoch 1.94: Loss = 0.799698
Epoch 1.95: Loss = 0.839981
Epoch 1.96: Loss = 0.771912
Epoch 1.97: Loss = 0.679352
Epoch 1.98: Loss = 0.795013
Epoch 1.99: Loss = 0.79985
Epoch 1.100: Loss = 0.76265
Epoch 1.101: Loss = 0.827606
Epoch 1.102: Loss = 0.831924
Epoch 1.103: Loss = 0.808319
Epoch 1.104: Loss = 0.775513
Epoch 1.105: Loss = 0.738998
Epoch 1.106: Loss = 0.861542
Epoch 1.107: Loss = 0.776947
Epoch 1.108: Loss = 0.78479
Epoch 1.109: Loss = 0.785233
Epoch 1.110: Loss = 0.779907
Epoch 1.111: Loss = 0.740143
Epoch 1.112: Loss = 0.699936
Epoch 1.113: Loss = 0.74176
Epoch 1.114: Loss = 0.761856
Epoch 1.115: Loss = 0.757385
Epoch 1.116: Loss = 0.668137
Epoch 1.117: Loss = 0.831924
Epoch 1.118: Loss = 0.690506
Epoch 1.119: Loss = 0.735275
Epoch 1.120: Loss = 0.728958
TRAIN LOSS = 1.09068
TRAIN ACC = 64.0884 % (38455/60000)
Loss = 0.693344
Loss = 0.812057
Loss = 0.794449
Loss = 0.721161
Loss = 0.704224
Loss = 0.871658
Loss = 0.889694
Loss = 0.831436
Loss = 0.754974
Loss = 0.708496
Loss = 0.829391
Loss = 0.815445
Loss = 0.785385
Loss = 0.789261
Loss = 0.763489
Loss = 0.823471
Loss = 0.744675
Loss = 0.770676
Loss = 0.829666
Loss = 0.743027
TEST LOSS = 0.783799
TEST ACC = 384.549 % (7221/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.779327
Epoch 2.2: Loss = 0.738144
Epoch 2.3: Loss = 0.818008
Epoch 2.4: Loss = 0.684738
Epoch 2.5: Loss = 0.750153
Epoch 2.6: Loss = 0.815918
Epoch 2.7: Loss = 0.733902
Epoch 2.8: Loss = 0.790649
Epoch 2.9: Loss = 0.684082
Epoch 2.10: Loss = 0.595505
Epoch 2.11: Loss = 0.795609
Epoch 2.12: Loss = 0.751892
Epoch 2.13: Loss = 0.747635
Epoch 2.14: Loss = 0.744431
Epoch 2.15: Loss = 0.717697
Epoch 2.16: Loss = 0.783371
Epoch 2.17: Loss = 0.713165
Epoch 2.18: Loss = 0.7323
Epoch 2.19: Loss = 0.716293
Epoch 2.20: Loss = 0.806305
Epoch 2.21: Loss = 0.679901
Epoch 2.22: Loss = 0.654526
Epoch 2.23: Loss = 0.726089
Epoch 2.24: Loss = 0.831558
Epoch 2.25: Loss = 0.704102
Epoch 2.26: Loss = 0.672745
Epoch 2.27: Loss = 0.746521
Epoch 2.28: Loss = 0.720627
Epoch 2.29: Loss = 0.733429
Epoch 2.30: Loss = 0.708694
Epoch 2.31: Loss = 0.788635
Epoch 2.32: Loss = 0.731186
Epoch 2.33: Loss = 0.654526
Epoch 2.34: Loss = 0.778122
Epoch 2.35: Loss = 0.766571
Epoch 2.36: Loss = 0.766266
Epoch 2.37: Loss = 0.772964
Epoch 2.38: Loss = 0.698441
Epoch 2.39: Loss = 0.755936
Epoch 2.40: Loss = 0.717636
Epoch 2.41: Loss = 0.756973
Epoch 2.42: Loss = 0.729691
Epoch 2.43: Loss = 0.730927
Epoch 2.44: Loss = 0.639618
Epoch 2.45: Loss = 0.734299
Epoch 2.46: Loss = 0.777512
Epoch 2.47: Loss = 0.672806
Epoch 2.48: Loss = 0.676727
Epoch 2.49: Loss = 0.801758
Epoch 2.50: Loss = 0.717377
Epoch 2.51: Loss = 0.623825
Epoch 2.52: Loss = 0.759155
Epoch 2.53: Loss = 0.795975
Epoch 2.54: Loss = 0.598267
Epoch 2.55: Loss = 0.731171
Epoch 2.56: Loss = 0.723877
Epoch 2.57: Loss = 0.764755
Epoch 2.58: Loss = 0.721344
Epoch 2.59: Loss = 0.733109
Epoch 2.60: Loss = 0.733383
Epoch 2.61: Loss = 0.648773
Epoch 2.62: Loss = 0.754074
Epoch 2.63: Loss = 0.599823
Epoch 2.64: Loss = 0.639786
Epoch 2.65: Loss = 0.690842
Epoch 2.66: Loss = 0.675125
Epoch 2.67: Loss = 0.642929
Epoch 2.68: Loss = 0.77861
Epoch 2.69: Loss = 0.703644
Epoch 2.70: Loss = 0.713409
Epoch 2.71: Loss = 0.614395
Epoch 2.72: Loss = 0.679443
Epoch 2.73: Loss = 0.783112
Epoch 2.74: Loss = 0.748825
Epoch 2.75: Loss = 0.639603
Epoch 2.76: Loss = 0.675949
Epoch 2.77: Loss = 0.674683
Epoch 2.78: Loss = 0.673904
Epoch 2.79: Loss = 0.658096
Epoch 2.80: Loss = 0.684311
Epoch 2.81: Loss = 0.66127
Epoch 2.82: Loss = 0.652786
Epoch 2.83: Loss = 0.741165
Epoch 2.84: Loss = 0.660706
Epoch 2.85: Loss = 0.664688
Epoch 2.86: Loss = 0.733521
Epoch 2.87: Loss = 0.706406
Epoch 2.88: Loss = 0.612564
Epoch 2.89: Loss = 0.764969
Epoch 2.90: Loss = 0.690948
Epoch 2.91: Loss = 0.790436
Epoch 2.92: Loss = 0.700684
Epoch 2.93: Loss = 0.757477
Epoch 2.94: Loss = 0.693054
Epoch 2.95: Loss = 0.716049
Epoch 2.96: Loss = 0.660339
Epoch 2.97: Loss = 0.582977
Epoch 2.98: Loss = 0.666168
Epoch 2.99: Loss = 0.677017
Epoch 2.100: Loss = 0.670853
Epoch 2.101: Loss = 0.70845
Epoch 2.102: Loss = 0.705017
Epoch 2.103: Loss = 0.68866
Epoch 2.104: Loss = 0.653824
Epoch 2.105: Loss = 0.627914
Epoch 2.106: Loss = 0.763855
Epoch 2.107: Loss = 0.718155
Epoch 2.108: Loss = 0.714203
Epoch 2.109: Loss = 0.741852
Epoch 2.110: Loss = 0.703384
Epoch 2.111: Loss = 0.645844
Epoch 2.112: Loss = 0.620209
Epoch 2.113: Loss = 0.654617
Epoch 2.114: Loss = 0.676727
Epoch 2.115: Loss = 0.683197
Epoch 2.116: Loss = 0.602753
Epoch 2.117: Loss = 0.756577
Epoch 2.118: Loss = 0.606094
Epoch 2.119: Loss = 0.667633
Epoch 2.120: Loss = 0.643066
TRAIN LOSS = 0.708313
TRAIN ACC = 75.3967 % (45240/60000)
Loss = 0.605347
Loss = 0.759277
Loss = 0.688507
Loss = 0.627899
Loss = 0.619797
Loss = 0.820847
Loss = 0.847809
Loss = 0.784317
Loss = 0.700424
Loss = 0.623779
Loss = 0.775696
Loss = 0.770355
Loss = 0.703918
Loss = 0.698654
Loss = 0.713669
Loss = 0.766846
Loss = 0.682861
Loss = 0.708603
Loss = 0.774796
Loss = 0.690994
TEST LOSS = 0.71822
TEST ACC = 452.399 % (7563/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.705841
Epoch 3.2: Loss = 0.664078
Epoch 3.3: Loss = 0.738541
Epoch 3.4: Loss = 0.598663
Epoch 3.5: Loss = 0.662323
Epoch 3.6: Loss = 0.785385
Epoch 3.7: Loss = 0.670853
Epoch 3.8: Loss = 0.733826
Epoch 3.9: Loss = 0.595734
Epoch 3.10: Loss = 0.514542
Epoch 3.11: Loss = 0.753326
Epoch 3.12: Loss = 0.686523
Epoch 3.13: Loss = 0.716446
Epoch 3.14: Loss = 0.687363
Epoch 3.15: Loss = 0.673096
Epoch 3.16: Loss = 0.728683
Epoch 3.17: Loss = 0.626358
Epoch 3.18: Loss = 0.692154
Epoch 3.19: Loss = 0.680878
Epoch 3.20: Loss = 0.749802
Epoch 3.21: Loss = 0.59903
Epoch 3.22: Loss = 0.562195
Epoch 3.23: Loss = 0.676422
Epoch 3.24: Loss = 0.78035
Epoch 3.25: Loss = 0.621582
Epoch 3.26: Loss = 0.598007
Epoch 3.27: Loss = 0.698105
Epoch 3.28: Loss = 0.671021
Epoch 3.29: Loss = 0.685043
Epoch 3.30: Loss = 0.664871
Epoch 3.31: Loss = 0.764603
Epoch 3.32: Loss = 0.67662
Epoch 3.33: Loss = 0.595001
Epoch 3.34: Loss = 0.768845
Epoch 3.35: Loss = 0.713531
Epoch 3.36: Loss = 0.70314
Epoch 3.37: Loss = 0.723267
Epoch 3.38: Loss = 0.672333
Epoch 3.39: Loss = 0.735901
Epoch 3.40: Loss = 0.670044
Epoch 3.41: Loss = 0.711594
Epoch 3.42: Loss = 0.678268
Epoch 3.43: Loss = 0.725418
Epoch 3.44: Loss = 0.584137
Epoch 3.45: Loss = 0.694199
Epoch 3.46: Loss = 0.758224
Epoch 3.47: Loss = 0.631897
Epoch 3.48: Loss = 0.634186
Epoch 3.49: Loss = 0.779694
Epoch 3.50: Loss = 0.694107
Epoch 3.51: Loss = 0.554993
Epoch 3.52: Loss = 0.71608
Epoch 3.53: Loss = 0.766083
Epoch 3.54: Loss = 0.552979
Epoch 3.55: Loss = 0.698532
Epoch 3.56: Loss = 0.692719
Epoch 3.57: Loss = 0.738968
Epoch 3.58: Loss = 0.687363
Epoch 3.59: Loss = 0.728348
Epoch 3.60: Loss = 0.692993
Epoch 3.61: Loss = 0.62558
Epoch 3.62: Loss = 0.719009
Epoch 3.63: Loss = 0.564575
Epoch 3.64: Loss = 0.584457
Epoch 3.65: Loss = 0.657211
Epoch 3.66: Loss = 0.645721
Epoch 3.67: Loss = 0.635086
Epoch 3.68: Loss = 0.766998
Epoch 3.69: Loss = 0.677704
Epoch 3.70: Loss = 0.704163
Epoch 3.71: Loss = 0.583298
Epoch 3.72: Loss = 0.656754
Epoch 3.73: Loss = 0.768829
Epoch 3.74: Loss = 0.721161
Epoch 3.75: Loss = 0.601715
Epoch 3.76: Loss = 0.64122
Epoch 3.77: Loss = 0.654068
Epoch 3.78: Loss = 0.644592
Epoch 3.79: Loss = 0.642776
Epoch 3.80: Loss = 0.635162
Epoch 3.81: Loss = 0.623444
Epoch 3.82: Loss = 0.620865
Epoch 3.83: Loss = 0.722137
Epoch 3.84: Loss = 0.620178
Epoch 3.85: Loss = 0.628952
Epoch 3.86: Loss = 0.730957
Epoch 3.87: Loss = 0.660522
Epoch 3.88: Loss = 0.584396
Epoch 3.89: Loss = 0.759644
Epoch 3.90: Loss = 0.687988
Epoch 3.91: Loss = 0.754669
Epoch 3.92: Loss = 0.656845
Epoch 3.93: Loss = 0.731598
Epoch 3.94: Loss = 0.662338
Epoch 3.95: Loss = 0.667862
Epoch 3.96: Loss = 0.65712
Epoch 3.97: Loss = 0.553223
Epoch 3.98: Loss = 0.631973
Epoch 3.99: Loss = 0.687042
Epoch 3.100: Loss = 0.658203
Epoch 3.101: Loss = 0.698349
Epoch 3.102: Loss = 0.69487
Epoch 3.103: Loss = 0.65593
Epoch 3.104: Loss = 0.622162
Epoch 3.105: Loss = 0.625443
Epoch 3.106: Loss = 0.756363
Epoch 3.107: Loss = 0.702133
Epoch 3.108: Loss = 0.724091
Epoch 3.109: Loss = 0.759872
Epoch 3.110: Loss = 0.705124
Epoch 3.111: Loss = 0.634247
Epoch 3.112: Loss = 0.612106
Epoch 3.113: Loss = 0.620789
Epoch 3.114: Loss = 0.667419
Epoch 3.115: Loss = 0.664413
Epoch 3.116: Loss = 0.59053
Epoch 3.117: Loss = 0.735931
Epoch 3.118: Loss = 0.589508
Epoch 3.119: Loss = 0.677521
Epoch 3.120: Loss = 0.625427
TRAIN LOSS = 0.673004
TRAIN ACC = 78.006 % (46806/60000)
Loss = 0.592834
Loss = 0.748047
Loss = 0.679291
Loss = 0.601852
Loss = 0.617676
Loss = 0.827194
Loss = 0.844147
Loss = 0.784607
Loss = 0.697327
Loss = 0.621857
Loss = 0.799973
Loss = 0.794724
Loss = 0.694504
Loss = 0.689163
Loss = 0.726837
Loss = 0.763977
Loss = 0.683121
Loss = 0.7155
Loss = 0.777069
Loss = 0.684738
TEST LOSS = 0.717222
TEST ACC = 468.059 % (7714/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.692413
Epoch 4.2: Loss = 0.665009
Epoch 4.3: Loss = 0.719101
Epoch 4.4: Loss = 0.581085
Epoch 4.5: Loss = 0.653824
Epoch 4.6: Loss = 0.773727
Epoch 4.7: Loss = 0.659164
Epoch 4.8: Loss = 0.755936
Epoch 4.9: Loss = 0.549576
Epoch 4.10: Loss = 0.489197
Epoch 4.11: Loss = 0.743942
Epoch 4.12: Loss = 0.672363
Epoch 4.13: Loss = 0.731628
Epoch 4.14: Loss = 0.674744
Epoch 4.15: Loss = 0.685364
Epoch 4.16: Loss = 0.704117
Epoch 4.17: Loss = 0.591705
Epoch 4.18: Loss = 0.6754
Epoch 4.19: Loss = 0.65094
Epoch 4.20: Loss = 0.748825
Epoch 4.21: Loss = 0.562531
Epoch 4.22: Loss = 0.538223
Epoch 4.23: Loss = 0.693298
Epoch 4.24: Loss = 0.784729
Epoch 4.25: Loss = 0.591034
Epoch 4.26: Loss = 0.58847
Epoch 4.27: Loss = 0.678757
Epoch 4.28: Loss = 0.652985
Epoch 4.29: Loss = 0.662506
Epoch 4.30: Loss = 0.671005
Epoch 4.31: Loss = 0.757263
Epoch 4.32: Loss = 0.651047
Epoch 4.33: Loss = 0.576904
Epoch 4.34: Loss = 0.765121
Epoch 4.35: Loss = 0.698944
Epoch 4.36: Loss = 0.708633
Epoch 4.37: Loss = 0.712814
Epoch 4.38: Loss = 0.659882
Epoch 4.39: Loss = 0.719467
Epoch 4.40: Loss = 0.665421
Epoch 4.41: Loss = 0.718933
Epoch 4.42: Loss = 0.687927
Epoch 4.43: Loss = 0.713791
Epoch 4.44: Loss = 0.573395
Epoch 4.45: Loss = 0.677994
Epoch 4.46: Loss = 0.76741
Epoch 4.47: Loss = 0.631989
Epoch 4.48: Loss = 0.613068
Epoch 4.49: Loss = 0.765656
Epoch 4.50: Loss = 0.699005
Epoch 4.51: Loss = 0.550064
Epoch 4.52: Loss = 0.716324
Epoch 4.53: Loss = 0.762299
Epoch 4.54: Loss = 0.547012
Epoch 4.55: Loss = 0.675827
Epoch 4.56: Loss = 0.678024
Epoch 4.57: Loss = 0.748825
Epoch 4.58: Loss = 0.65921
Epoch 4.59: Loss = 0.735565
Epoch 4.60: Loss = 0.675842
Epoch 4.61: Loss = 0.614792
Epoch 4.62: Loss = 0.710449
Epoch 4.63: Loss = 0.553253
Epoch 4.64: Loss = 0.549698
Epoch 4.65: Loss = 0.668533
Epoch 4.66: Loss = 0.619843
Epoch 4.67: Loss = 0.638718
Epoch 4.68: Loss = 0.78894
Epoch 4.69: Loss = 0.657211
Epoch 4.70: Loss = 0.692093
Epoch 4.71: Loss = 0.575577
Epoch 4.72: Loss = 0.651443
Epoch 4.73: Loss = 0.779495
Epoch 4.74: Loss = 0.720657
Epoch 4.75: Loss = 0.593628
Epoch 4.76: Loss = 0.659363
Epoch 4.77: Loss = 0.652893
Epoch 4.78: Loss = 0.657288
Epoch 4.79: Loss = 0.651062
Epoch 4.80: Loss = 0.620438
Epoch 4.81: Loss = 0.619171
Epoch 4.82: Loss = 0.619171
Epoch 4.83: Loss = 0.734283
Epoch 4.84: Loss = 0.612686
Epoch 4.85: Loss = 0.633881
Epoch 4.86: Loss = 0.725922
Epoch 4.87: Loss = 0.655136
Epoch 4.88: Loss = 0.590744
Epoch 4.89: Loss = 0.763611
Epoch 4.90: Loss = 0.699951
Epoch 4.91: Loss = 0.728485
Epoch 4.92: Loss = 0.634109
Epoch 4.93: Loss = 0.739105
Epoch 4.94: Loss = 0.629486
Epoch 4.95: Loss = 0.67485
Epoch 4.96: Loss = 0.646469
Epoch 4.97: Loss = 0.543274
Epoch 4.98: Loss = 0.609818
Epoch 4.99: Loss = 0.65097
Epoch 4.100: Loss = 0.653046
Epoch 4.101: Loss = 0.682846
Epoch 4.102: Loss = 0.697952
Epoch 4.103: Loss = 0.640991
Epoch 4.104: Loss = 0.60672
Epoch 4.105: Loss = 0.6017
Epoch 4.106: Loss = 0.736603
Epoch 4.107: Loss = 0.68045
Epoch 4.108: Loss = 0.703064
Epoch 4.109: Loss = 0.747421
Epoch 4.110: Loss = 0.666397
Epoch 4.111: Loss = 0.620895
Epoch 4.112: Loss = 0.599945
Epoch 4.113: Loss = 0.608734
Epoch 4.114: Loss = 0.668167
Epoch 4.115: Loss = 0.64151
Epoch 4.116: Loss = 0.583649
Epoch 4.117: Loss = 0.725632
Epoch 4.118: Loss = 0.596481
Epoch 4.119: Loss = 0.689407
Epoch 4.120: Loss = 0.625076
TRAIN LOSS = 0.664169
TRAIN ACC = 79.2007 % (47523/60000)
Loss = 0.580246
Loss = 0.738007
Loss = 0.639267
Loss = 0.566254
Loss = 0.606522
Loss = 0.807022
Loss = 0.827087
Loss = 0.763748
Loss = 0.686935
Loss = 0.602127
Loss = 0.796921
Loss = 0.791611
Loss = 0.674728
Loss = 0.645538
Loss = 0.69838
Loss = 0.732941
Loss = 0.686676
Loss = 0.686539
Loss = 0.733414
Loss = 0.687302
TEST LOSS = 0.697563
TEST ACC = 475.229 % (7857/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.660065
Epoch 5.2: Loss = 0.655563
Epoch 5.3: Loss = 0.695938
Epoch 5.4: Loss = 0.578903
Epoch 5.5: Loss = 0.647705
Epoch 5.6: Loss = 0.75444
Epoch 5.7: Loss = 0.636536
Epoch 5.8: Loss = 0.740402
Epoch 5.9: Loss = 0.537308
Epoch 5.10: Loss = 0.468674
Epoch 5.11: Loss = 0.724869
Epoch 5.12: Loss = 0.657669
Epoch 5.13: Loss = 0.730087
Epoch 5.14: Loss = 0.646347
Epoch 5.15: Loss = 0.663239
Epoch 5.16: Loss = 0.701553
Epoch 5.17: Loss = 0.571732
Epoch 5.18: Loss = 0.673401
Epoch 5.19: Loss = 0.632675
Epoch 5.20: Loss = 0.720398
Epoch 5.21: Loss = 0.53685
Epoch 5.22: Loss = 0.502747
Epoch 5.23: Loss = 0.64679
Epoch 5.24: Loss = 0.755936
Epoch 5.25: Loss = 0.562637
Epoch 5.26: Loss = 0.572464
Epoch 5.27: Loss = 0.646957
Epoch 5.28: Loss = 0.638321
Epoch 5.29: Loss = 0.647247
Epoch 5.30: Loss = 0.645264
Epoch 5.31: Loss = 0.73555
Epoch 5.32: Loss = 0.636032
Epoch 5.33: Loss = 0.574554
Epoch 5.34: Loss = 0.737457
Epoch 5.35: Loss = 0.670166
Epoch 5.36: Loss = 0.691772
Epoch 5.37: Loss = 0.690887
Epoch 5.38: Loss = 0.656845
Epoch 5.39: Loss = 0.691772
Epoch 5.40: Loss = 0.629486
Epoch 5.41: Loss = 0.7267
Epoch 5.42: Loss = 0.656723
Epoch 5.43: Loss = 0.719131
Epoch 5.44: Loss = 0.543213
Epoch 5.45: Loss = 0.663559
Epoch 5.46: Loss = 0.764771
Epoch 5.47: Loss = 0.590256
Epoch 5.48: Loss = 0.579727
Epoch 5.49: Loss = 0.735901
Epoch 5.50: Loss = 0.702301
Epoch 5.51: Loss = 0.535492
Epoch 5.52: Loss = 0.711578
Epoch 5.53: Loss = 0.753693
Epoch 5.54: Loss = 0.534409
Epoch 5.55: Loss = 0.661758
Epoch 5.56: Loss = 0.659897
Epoch 5.57: Loss = 0.765915
Epoch 5.58: Loss = 0.635727
Epoch 5.59: Loss = 0.721298
Epoch 5.60: Loss = 0.680328
Epoch 5.61: Loss = 0.606583
Epoch 5.62: Loss = 0.703781
Epoch 5.63: Loss = 0.540253
Epoch 5.64: Loss = 0.518082
Epoch 5.65: Loss = 0.675949
Epoch 5.66: Loss = 0.589218
Epoch 5.67: Loss = 0.649017
Epoch 5.68: Loss = 0.796249
Epoch 5.69: Loss = 0.653534
Epoch 5.70: Loss = 0.689362
Epoch 5.71: Loss = 0.581253
Epoch 5.72: Loss = 0.661957
Epoch 5.73: Loss = 0.785873
Epoch 5.74: Loss = 0.715286
Epoch 5.75: Loss = 0.587585
Epoch 5.76: Loss = 0.622345
Epoch 5.77: Loss = 0.654633
Epoch 5.78: Loss = 0.654449
Epoch 5.79: Loss = 0.654755
Epoch 5.80: Loss = 0.598846
Epoch 5.81: Loss = 0.596115
Epoch 5.82: Loss = 0.610825
Epoch 5.83: Loss = 0.738876
Epoch 5.84: Loss = 0.59874
Epoch 5.85: Loss = 0.624756
Epoch 5.86: Loss = 0.72551
Epoch 5.87: Loss = 0.632736
Epoch 5.88: Loss = 0.585922
Epoch 5.89: Loss = 0.796814
Epoch 5.90: Loss = 0.719818
Epoch 5.91: Loss = 0.714218
Epoch 5.92: Loss = 0.628616
Epoch 5.93: Loss = 0.715363
Epoch 5.94: Loss = 0.642868
Epoch 5.95: Loss = 0.673157
Epoch 5.96: Loss = 0.663528
Epoch 5.97: Loss = 0.532623
Epoch 5.98: Loss = 0.629547
Epoch 5.99: Loss = 0.652557
Epoch 5.100: Loss = 0.675339
Epoch 5.101: Loss = 0.684875
Epoch 5.102: Loss = 0.698044
Epoch 5.103: Loss = 0.6492
Epoch 5.104: Loss = 0.602112
Epoch 5.105: Loss = 0.612213
Epoch 5.106: Loss = 0.763107
Epoch 5.107: Loss = 0.681488
Epoch 5.108: Loss = 0.738235
Epoch 5.109: Loss = 0.774628
Epoch 5.110: Loss = 0.670532
Epoch 5.111: Loss = 0.628616
Epoch 5.112: Loss = 0.609894
Epoch 5.113: Loss = 0.624023
Epoch 5.114: Loss = 0.695862
Epoch 5.115: Loss = 0.662888
Epoch 5.116: Loss = 0.577148
Epoch 5.117: Loss = 0.740814
Epoch 5.118: Loss = 0.598083
Epoch 5.119: Loss = 0.709183
Epoch 5.120: Loss = 0.640198
TRAIN LOSS = 0.656403
TRAIN ACC = 80.0217 % (48015/60000)
Loss = 0.597107
Loss = 0.747818
Loss = 0.64241
Loss = 0.599991
Loss = 0.632614
Loss = 0.810028
Loss = 0.843063
Loss = 0.778809
Loss = 0.70401
Loss = 0.621399
Loss = 0.83197
Loss = 0.836227
Loss = 0.705307
Loss = 0.69342
Loss = 0.727432
Loss = 0.753311
Loss = 0.71077
Loss = 0.719101
Loss = 0.753769
Loss = 0.701935
TEST LOSS = 0.720524
TEST ACC = 480.15 % (7914/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.666534
Epoch 6.2: Loss = 0.696609
Epoch 6.3: Loss = 0.725281
Epoch 6.4: Loss = 0.587234
Epoch 6.5: Loss = 0.664612
Epoch 6.6: Loss = 0.76651
Epoch 6.7: Loss = 0.645981
Epoch 6.8: Loss = 0.754669
Epoch 6.9: Loss = 0.539032
Epoch 6.10: Loss = 0.482025
Epoch 6.11: Loss = 0.733704
Epoch 6.12: Loss = 0.649292
Epoch 6.13: Loss = 0.737488
Epoch 6.14: Loss = 0.655304
Epoch 6.15: Loss = 0.683838
Epoch 6.16: Loss = 0.706375
Epoch 6.17: Loss = 0.589813
Epoch 6.18: Loss = 0.669006
Epoch 6.19: Loss = 0.62204
Epoch 6.20: Loss = 0.743134
Epoch 6.21: Loss = 0.559479
Epoch 6.22: Loss = 0.513824
Epoch 6.23: Loss = 0.639633
Epoch 6.24: Loss = 0.757751
Epoch 6.25: Loss = 0.571915
Epoch 6.26: Loss = 0.565353
Epoch 6.27: Loss = 0.653656
Epoch 6.28: Loss = 0.646729
Epoch 6.29: Loss = 0.640198
Epoch 6.30: Loss = 0.656601
Epoch 6.31: Loss = 0.761993
Epoch 6.32: Loss = 0.623672
Epoch 6.33: Loss = 0.587646
Epoch 6.34: Loss = 0.724579
Epoch 6.35: Loss = 0.663818
Epoch 6.36: Loss = 0.708832
Epoch 6.37: Loss = 0.703781
Epoch 6.38: Loss = 0.688904
Epoch 6.39: Loss = 0.6875
Epoch 6.40: Loss = 0.639023
Epoch 6.41: Loss = 0.733521
Epoch 6.42: Loss = 0.664001
Epoch 6.43: Loss = 0.694946
Epoch 6.44: Loss = 0.541046
Epoch 6.45: Loss = 0.680969
Epoch 6.46: Loss = 0.748596
Epoch 6.47: Loss = 0.593079
Epoch 6.48: Loss = 0.571838
Epoch 6.49: Loss = 0.726654
Epoch 6.50: Loss = 0.696228
Epoch 6.51: Loss = 0.524017
Epoch 6.52: Loss = 0.666122
Epoch 6.53: Loss = 0.748352
Epoch 6.54: Loss = 0.521423
Epoch 6.55: Loss = 0.661041
Epoch 6.56: Loss = 0.66304
Epoch 6.57: Loss = 0.730408
Epoch 6.58: Loss = 0.633667
Epoch 6.59: Loss = 0.734818
Epoch 6.60: Loss = 0.689682
Epoch 6.61: Loss = 0.614731
Epoch 6.62: Loss = 0.694931
Epoch 6.63: Loss = 0.529358
Epoch 6.64: Loss = 0.509674
Epoch 6.65: Loss = 0.666168
Epoch 6.66: Loss = 0.576752
Epoch 6.67: Loss = 0.634186
Epoch 6.68: Loss = 0.810867
Epoch 6.69: Loss = 0.652618
Epoch 6.70: Loss = 0.670868
Epoch 6.71: Loss = 0.568497
Epoch 6.72: Loss = 0.656281
Epoch 6.73: Loss = 0.784744
Epoch 6.74: Loss = 0.711395
Epoch 6.75: Loss = 0.585251
Epoch 6.76: Loss = 0.632401
Epoch 6.77: Loss = 0.646744
Epoch 6.78: Loss = 0.659119
Epoch 6.79: Loss = 0.646454
Epoch 6.80: Loss = 0.613327
Epoch 6.81: Loss = 0.587311
Epoch 6.82: Loss = 0.626694
Epoch 6.83: Loss = 0.742218
Epoch 6.84: Loss = 0.564148
Epoch 6.85: Loss = 0.609665
Epoch 6.86: Loss = 0.722122
Epoch 6.87: Loss = 0.627823
Epoch 6.88: Loss = 0.594467
Epoch 6.89: Loss = 0.788406
Epoch 6.90: Loss = 0.721329
Epoch 6.91: Loss = 0.707321
Epoch 6.92: Loss = 0.625641
Epoch 6.93: Loss = 0.715332
Epoch 6.94: Loss = 0.651276
Epoch 6.95: Loss = 0.678238
Epoch 6.96: Loss = 0.644562
Epoch 6.97: Loss = 0.522644
Epoch 6.98: Loss = 0.608978
Epoch 6.99: Loss = 0.634689
Epoch 6.100: Loss = 0.673035
Epoch 6.101: Loss = 0.6586
Epoch 6.102: Loss = 0.709763
Epoch 6.103: Loss = 0.638641
Epoch 6.104: Loss = 0.594116
Epoch 6.105: Loss = 0.616394
Epoch 6.106: Loss = 0.749863
Epoch 6.107: Loss = 0.68364
Epoch 6.108: Loss = 0.7742
Epoch 6.109: Loss = 0.783905
Epoch 6.110: Loss = 0.659225
Epoch 6.111: Loss = 0.61879
Epoch 6.112: Loss = 0.613129
Epoch 6.113: Loss = 0.620789
Epoch 6.114: Loss = 0.70433
Epoch 6.115: Loss = 0.646774
Epoch 6.116: Loss = 0.567902
Epoch 6.117: Loss = 0.723633
Epoch 6.118: Loss = 0.588943
Epoch 6.119: Loss = 0.723648
Epoch 6.120: Loss = 0.620758
TRAIN LOSS = 0.656204
TRAIN ACC = 80.5222 % (48316/60000)
Loss = 0.582855
Loss = 0.729065
Loss = 0.621109
Loss = 0.588547
Loss = 0.615173
Loss = 0.791214
Loss = 0.831039
Loss = 0.749756
Loss = 0.679428
Loss = 0.614517
Loss = 0.837173
Loss = 0.821106
Loss = 0.70433
Loss = 0.662827
Loss = 0.703033
Loss = 0.72934
Loss = 0.702606
Loss = 0.702499
Loss = 0.725082
Loss = 0.680832
TEST LOSS = 0.703576
TEST ACC = 483.159 % (7980/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.638367
Epoch 7.2: Loss = 0.680878
Epoch 7.3: Loss = 0.725632
Epoch 7.4: Loss = 0.593124
Epoch 7.5: Loss = 0.668747
Epoch 7.6: Loss = 0.763733
Epoch 7.7: Loss = 0.630981
Epoch 7.8: Loss = 0.728317
Epoch 7.9: Loss = 0.530243
Epoch 7.10: Loss = 0.458603
Epoch 7.11: Loss = 0.716415
Epoch 7.12: Loss = 0.631424
Epoch 7.13: Loss = 0.727432
Epoch 7.14: Loss = 0.639984
Epoch 7.15: Loss = 0.643997
Epoch 7.16: Loss = 0.692627
Epoch 7.17: Loss = 0.575745
Epoch 7.18: Loss = 0.647079
Epoch 7.19: Loss = 0.625305
Epoch 7.20: Loss = 0.750259
Epoch 7.21: Loss = 0.558929
Epoch 7.22: Loss = 0.511856
Epoch 7.23: Loss = 0.64743
Epoch 7.24: Loss = 0.773926
Epoch 7.25: Loss = 0.562653
Epoch 7.26: Loss = 0.543427
Epoch 7.27: Loss = 0.63797
Epoch 7.28: Loss = 0.63829
Epoch 7.29: Loss = 0.6521
Epoch 7.30: Loss = 0.672958
Epoch 7.31: Loss = 0.745087
Epoch 7.32: Loss = 0.631134
Epoch 7.33: Loss = 0.580688
Epoch 7.34: Loss = 0.713699
Epoch 7.35: Loss = 0.660065
Epoch 7.36: Loss = 0.700974
Epoch 7.37: Loss = 0.697327
Epoch 7.38: Loss = 0.64653
Epoch 7.39: Loss = 0.684097
Epoch 7.40: Loss = 0.633331
Epoch 7.41: Loss = 0.709732
Epoch 7.42: Loss = 0.671463
Epoch 7.43: Loss = 0.710266
Epoch 7.44: Loss = 0.533569
Epoch 7.45: Loss = 0.666061
Epoch 7.46: Loss = 0.719315
Epoch 7.47: Loss = 0.609238
Epoch 7.48: Loss = 0.556595
Epoch 7.49: Loss = 0.733963
Epoch 7.50: Loss = 0.685791
Epoch 7.51: Loss = 0.510818
Epoch 7.52: Loss = 0.662689
Epoch 7.53: Loss = 0.744614
Epoch 7.54: Loss = 0.533112
Epoch 7.55: Loss = 0.654526
Epoch 7.56: Loss = 0.662964
Epoch 7.57: Loss = 0.763458
Epoch 7.58: Loss = 0.621155
Epoch 7.59: Loss = 0.75116
Epoch 7.60: Loss = 0.700211
Epoch 7.61: Loss = 0.619934
Epoch 7.62: Loss = 0.710724
Epoch 7.63: Loss = 0.539124
Epoch 7.64: Loss = 0.511673
Epoch 7.65: Loss = 0.662476
Epoch 7.66: Loss = 0.573425
Epoch 7.67: Loss = 0.655304
Epoch 7.68: Loss = 0.821274
Epoch 7.69: Loss = 0.64296
Epoch 7.70: Loss = 0.653214
Epoch 7.71: Loss = 0.562408
Epoch 7.72: Loss = 0.661804
Epoch 7.73: Loss = 0.779556
Epoch 7.74: Loss = 0.731674
Epoch 7.75: Loss = 0.588425
Epoch 7.76: Loss = 0.615829
Epoch 7.77: Loss = 0.643494
Epoch 7.78: Loss = 0.657028
Epoch 7.79: Loss = 0.634445
Epoch 7.80: Loss = 0.600998
Epoch 7.81: Loss = 0.598083
Epoch 7.82: Loss = 0.630966
Epoch 7.83: Loss = 0.749573
Epoch 7.84: Loss = 0.563568
Epoch 7.85: Loss = 0.638718
Epoch 7.86: Loss = 0.730042
Epoch 7.87: Loss = 0.629715
Epoch 7.88: Loss = 0.60556
Epoch 7.89: Loss = 0.777573
Epoch 7.90: Loss = 0.721619
Epoch 7.91: Loss = 0.705765
Epoch 7.92: Loss = 0.628204
Epoch 7.93: Loss = 0.692352
Epoch 7.94: Loss = 0.65831
Epoch 7.95: Loss = 0.683899
Epoch 7.96: Loss = 0.646393
Epoch 7.97: Loss = 0.532349
Epoch 7.98: Loss = 0.605499
Epoch 7.99: Loss = 0.649734
Epoch 7.100: Loss = 0.669037
Epoch 7.101: Loss = 0.633087
Epoch 7.102: Loss = 0.726791
Epoch 7.103: Loss = 0.617813
Epoch 7.104: Loss = 0.588165
Epoch 7.105: Loss = 0.607681
Epoch 7.106: Loss = 0.766724
Epoch 7.107: Loss = 0.669983
Epoch 7.108: Loss = 0.767258
Epoch 7.109: Loss = 0.782639
Epoch 7.110: Loss = 0.667999
Epoch 7.111: Loss = 0.619186
Epoch 7.112: Loss = 0.599625
Epoch 7.113: Loss = 0.632324
Epoch 7.114: Loss = 0.707367
Epoch 7.115: Loss = 0.654053
Epoch 7.116: Loss = 0.566116
Epoch 7.117: Loss = 0.730118
Epoch 7.118: Loss = 0.596619
Epoch 7.119: Loss = 0.723679
Epoch 7.120: Loss = 0.634003
TRAIN LOSS = 0.653641
TRAIN ACC = 80.9906 % (48597/60000)
Loss = 0.585754
Loss = 0.721512
Loss = 0.625702
Loss = 0.606644
Loss = 0.620789
Loss = 0.788681
Loss = 0.816711
Loss = 0.740494
Loss = 0.677155
Loss = 0.634918
Loss = 0.848129
Loss = 0.839462
Loss = 0.715988
Loss = 0.656311
Loss = 0.724792
Loss = 0.728073
Loss = 0.725098
Loss = 0.714691
Loss = 0.737778
Loss = 0.676407
TEST LOSS = 0.709254
TEST ACC = 485.97 % (8027/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.643036
Epoch 8.2: Loss = 0.698608
Epoch 8.3: Loss = 0.719528
Epoch 8.4: Loss = 0.5896
Epoch 8.5: Loss = 0.684219
Epoch 8.6: Loss = 0.762695
Epoch 8.7: Loss = 0.625366
Epoch 8.8: Loss = 0.719009
Epoch 8.9: Loss = 0.540298
Epoch 8.10: Loss = 0.462387
Epoch 8.11: Loss = 0.719681
Epoch 8.12: Loss = 0.61824
Epoch 8.13: Loss = 0.761444
Epoch 8.14: Loss = 0.647461
Epoch 8.15: Loss = 0.681885
Epoch 8.16: Loss = 0.70813
Epoch 8.17: Loss = 0.578354
Epoch 8.18: Loss = 0.673981
Epoch 8.19: Loss = 0.62291
Epoch 8.20: Loss = 0.751617
Epoch 8.21: Loss = 0.570297
Epoch 8.22: Loss = 0.510986
Epoch 8.23: Loss = 0.630295
Epoch 8.24: Loss = 0.772934
Epoch 8.25: Loss = 0.565521
Epoch 8.26: Loss = 0.549728
Epoch 8.27: Loss = 0.629776
Epoch 8.28: Loss = 0.658569
Epoch 8.29: Loss = 0.628952
Epoch 8.30: Loss = 0.674927
Epoch 8.31: Loss = 0.725998
Epoch 8.32: Loss = 0.630539
Epoch 8.33: Loss = 0.610809
Epoch 8.34: Loss = 0.746033
Epoch 8.35: Loss = 0.67572
Epoch 8.36: Loss = 0.733627
Epoch 8.37: Loss = 0.71785
Epoch 8.38: Loss = 0.668991
Epoch 8.39: Loss = 0.696793
Epoch 8.40: Loss = 0.637695
Epoch 8.41: Loss = 0.720276
Epoch 8.42: Loss = 0.696823
Epoch 8.43: Loss = 0.71788
Epoch 8.44: Loss = 0.527527
Epoch 8.45: Loss = 0.667801
Epoch 8.46: Loss = 0.745834
Epoch 8.47: Loss = 0.634201
Epoch 8.48: Loss = 0.587936
Epoch 8.49: Loss = 0.751083
Epoch 8.50: Loss = 0.700531
Epoch 8.51: Loss = 0.52037
Epoch 8.52: Loss = 0.667419
Epoch 8.53: Loss = 0.733719
Epoch 8.54: Loss = 0.532257
Epoch 8.55: Loss = 0.659653
Epoch 8.56: Loss = 0.671066
Epoch 8.57: Loss = 0.755066
Epoch 8.58: Loss = 0.626617
Epoch 8.59: Loss = 0.775635
Epoch 8.60: Loss = 0.726791
Epoch 8.61: Loss = 0.646454
Epoch 8.62: Loss = 0.72908
Epoch 8.63: Loss = 0.542038
Epoch 8.64: Loss = 0.50621
Epoch 8.65: Loss = 0.67424
Epoch 8.66: Loss = 0.572449
Epoch 8.67: Loss = 0.647934
Epoch 8.68: Loss = 0.847214
Epoch 8.69: Loss = 0.644547
Epoch 8.70: Loss = 0.663803
Epoch 8.71: Loss = 0.577591
Epoch 8.72: Loss = 0.67572
Epoch 8.73: Loss = 0.816238
Epoch 8.74: Loss = 0.757492
Epoch 8.75: Loss = 0.592529
Epoch 8.76: Loss = 0.627228
Epoch 8.77: Loss = 0.6745
Epoch 8.78: Loss = 0.66539
Epoch 8.79: Loss = 0.662415
Epoch 8.80: Loss = 0.616608
Epoch 8.81: Loss = 0.61438
Epoch 8.82: Loss = 0.651474
Epoch 8.83: Loss = 0.752625
Epoch 8.84: Loss = 0.564041
Epoch 8.85: Loss = 0.650269
Epoch 8.86: Loss = 0.724426
Epoch 8.87: Loss = 0.609146
Epoch 8.88: Loss = 0.615341
Epoch 8.89: Loss = 0.824265
Epoch 8.90: Loss = 0.753967
Epoch 8.91: Loss = 0.704956
Epoch 8.92: Loss = 0.635162
Epoch 8.93: Loss = 0.711197
Epoch 8.94: Loss = 0.688538
Epoch 8.95: Loss = 0.708908
Epoch 8.96: Loss = 0.646942
Epoch 8.97: Loss = 0.517654
Epoch 8.98: Loss = 0.625977
Epoch 8.99: Loss = 0.662003
Epoch 8.100: Loss = 0.702011
Epoch 8.101: Loss = 0.653641
Epoch 8.102: Loss = 0.736694
Epoch 8.103: Loss = 0.641312
Epoch 8.104: Loss = 0.59613
Epoch 8.105: Loss = 0.648178
Epoch 8.106: Loss = 0.806046
Epoch 8.107: Loss = 0.680374
Epoch 8.108: Loss = 0.796661
Epoch 8.109: Loss = 0.818756
Epoch 8.110: Loss = 0.700638
Epoch 8.111: Loss = 0.647858
Epoch 8.112: Loss = 0.632477
Epoch 8.113: Loss = 0.666565
Epoch 8.114: Loss = 0.753525
Epoch 8.115: Loss = 0.657623
Epoch 8.116: Loss = 0.593536
Epoch 8.117: Loss = 0.721237
Epoch 8.118: Loss = 0.618179
Epoch 8.119: Loss = 0.733032
Epoch 8.120: Loss = 0.667419
TRAIN LOSS = 0.666153
TRAIN ACC = 81.34 % (48806/60000)
Loss = 0.588913
Loss = 0.747192
Loss = 0.644333
Loss = 0.619385
Loss = 0.646912
Loss = 0.816101
Loss = 0.851013
Loss = 0.767685
Loss = 0.70134
Loss = 0.650909
Loss = 0.877792
Loss = 0.876328
Loss = 0.733841
Loss = 0.69606
Loss = 0.74646
Loss = 0.724091
Loss = 0.725571
Loss = 0.749283
Loss = 0.758606
Loss = 0.692062
TEST LOSS = 0.730694
TEST ACC = 488.058 % (8038/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.658615
Epoch 9.2: Loss = 0.725586
Epoch 9.3: Loss = 0.748077
Epoch 9.4: Loss = 0.584045
Epoch 9.5: Loss = 0.702286
Epoch 9.6: Loss = 0.766006
Epoch 9.7: Loss = 0.640579
Epoch 9.8: Loss = 0.769958
Epoch 9.9: Loss = 0.558487
Epoch 9.10: Loss = 0.468918
Epoch 9.11: Loss = 0.744141
Epoch 9.12: Loss = 0.654526
Epoch 9.13: Loss = 0.74118
Epoch 9.14: Loss = 0.660049
Epoch 9.15: Loss = 0.705582
Epoch 9.16: Loss = 0.723511
Epoch 9.17: Loss = 0.608414
Epoch 9.18: Loss = 0.682739
Epoch 9.19: Loss = 0.62056
Epoch 9.20: Loss = 0.795898
Epoch 9.21: Loss = 0.564026
Epoch 9.22: Loss = 0.526047
Epoch 9.23: Loss = 0.662323
Epoch 9.24: Loss = 0.81012
Epoch 9.25: Loss = 0.588531
Epoch 9.26: Loss = 0.549789
Epoch 9.27: Loss = 0.634018
Epoch 9.28: Loss = 0.676483
Epoch 9.29: Loss = 0.664093
Epoch 9.30: Loss = 0.711212
Epoch 9.31: Loss = 0.741089
Epoch 9.32: Loss = 0.651047
Epoch 9.33: Loss = 0.608719
Epoch 9.34: Loss = 0.761017
Epoch 9.35: Loss = 0.696945
Epoch 9.36: Loss = 0.737106
Epoch 9.37: Loss = 0.72702
Epoch 9.38: Loss = 0.671387
Epoch 9.39: Loss = 0.710922
Epoch 9.40: Loss = 0.672882
Epoch 9.41: Loss = 0.748962
Epoch 9.42: Loss = 0.725769
Epoch 9.43: Loss = 0.737961
Epoch 9.44: Loss = 0.532394
Epoch 9.45: Loss = 0.680344
Epoch 9.46: Loss = 0.794174
Epoch 9.47: Loss = 0.650909
Epoch 9.48: Loss = 0.599091
Epoch 9.49: Loss = 0.791916
Epoch 9.50: Loss = 0.700775
Epoch 9.51: Loss = 0.528763
Epoch 9.52: Loss = 0.692993
Epoch 9.53: Loss = 0.757629
Epoch 9.54: Loss = 0.555328
Epoch 9.55: Loss = 0.677612
Epoch 9.56: Loss = 0.679962
Epoch 9.57: Loss = 0.783722
Epoch 9.58: Loss = 0.648361
Epoch 9.59: Loss = 0.767792
Epoch 9.60: Loss = 0.744125
Epoch 9.61: Loss = 0.648605
Epoch 9.62: Loss = 0.736633
Epoch 9.63: Loss = 0.548157
Epoch 9.64: Loss = 0.506256
Epoch 9.65: Loss = 0.686508
Epoch 9.66: Loss = 0.566696
Epoch 9.67: Loss = 0.640472
Epoch 9.68: Loss = 0.842957
Epoch 9.69: Loss = 0.636871
Epoch 9.70: Loss = 0.659515
Epoch 9.71: Loss = 0.57222
Epoch 9.72: Loss = 0.693497
Epoch 9.73: Loss = 0.846191
Epoch 9.74: Loss = 0.765228
Epoch 9.75: Loss = 0.602356
Epoch 9.76: Loss = 0.647385
Epoch 9.77: Loss = 0.68013
Epoch 9.78: Loss = 0.679245
Epoch 9.79: Loss = 0.668777
Epoch 9.80: Loss = 0.649353
Epoch 9.81: Loss = 0.629761
Epoch 9.82: Loss = 0.646133
Epoch 9.83: Loss = 0.768631
Epoch 9.84: Loss = 0.566101
Epoch 9.85: Loss = 0.637405
Epoch 9.86: Loss = 0.715347
Epoch 9.87: Loss = 0.603745
Epoch 9.88: Loss = 0.627533
Epoch 9.89: Loss = 0.823944
Epoch 9.90: Loss = 0.741608
Epoch 9.91: Loss = 0.706116
Epoch 9.92: Loss = 0.633072
Epoch 9.93: Loss = 0.698074
Epoch 9.94: Loss = 0.672684
Epoch 9.95: Loss = 0.712143
Epoch 9.96: Loss = 0.663422
Epoch 9.97: Loss = 0.532608
Epoch 9.98: Loss = 0.633881
Epoch 9.99: Loss = 0.668182
Epoch 9.100: Loss = 0.692505
Epoch 9.101: Loss = 0.646713
Epoch 9.102: Loss = 0.741241
Epoch 9.103: Loss = 0.632721
Epoch 9.104: Loss = 0.603745
Epoch 9.105: Loss = 0.623154
Epoch 9.106: Loss = 0.795975
Epoch 9.107: Loss = 0.679413
Epoch 9.108: Loss = 0.810211
Epoch 9.109: Loss = 0.81691
Epoch 9.110: Loss = 0.716385
Epoch 9.111: Loss = 0.628235
Epoch 9.112: Loss = 0.62999
Epoch 9.113: Loss = 0.651382
Epoch 9.114: Loss = 0.72908
Epoch 9.115: Loss = 0.663391
Epoch 9.116: Loss = 0.591461
Epoch 9.117: Loss = 0.708328
Epoch 9.118: Loss = 0.628067
Epoch 9.119: Loss = 0.728241
Epoch 9.120: Loss = 0.663132
TRAIN LOSS = 0.675522
TRAIN ACC = 81.4545 % (48874/60000)
Loss = 0.598633
Loss = 0.743713
Loss = 0.646805
Loss = 0.615723
Loss = 0.673645
Loss = 0.81427
Loss = 0.841141
Loss = 0.744278
Loss = 0.701797
Loss = 0.641663
Loss = 0.875763
Loss = 0.852036
Loss = 0.73999
Loss = 0.691895
Loss = 0.716751
Loss = 0.691071
Loss = 0.711365
Loss = 0.751953
Loss = 0.732117
Loss = 0.672531
TEST LOSS = 0.722857
TEST ACC = 488.739 % (8056/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.649124
Epoch 10.2: Loss = 0.720978
Epoch 10.3: Loss = 0.773209
Epoch 10.4: Loss = 0.572311
Epoch 10.5: Loss = 0.720825
Epoch 10.6: Loss = 0.780197
Epoch 10.7: Loss = 0.651291
Epoch 10.8: Loss = 0.75531
Epoch 10.9: Loss = 0.558029
Epoch 10.10: Loss = 0.487686
Epoch 10.11: Loss = 0.758987
Epoch 10.12: Loss = 0.651184
Epoch 10.13: Loss = 0.755264
Epoch 10.14: Loss = 0.650742
Epoch 10.15: Loss = 0.692719
Epoch 10.16: Loss = 0.723907
Epoch 10.17: Loss = 0.606216
Epoch 10.18: Loss = 0.678436
Epoch 10.19: Loss = 0.643585
Epoch 10.20: Loss = 0.773193
Epoch 10.21: Loss = 0.544159
Epoch 10.22: Loss = 0.527679
Epoch 10.23: Loss = 0.636612
Epoch 10.24: Loss = 0.809265
Epoch 10.25: Loss = 0.585236
Epoch 10.26: Loss = 0.543396
Epoch 10.27: Loss = 0.630798
Epoch 10.28: Loss = 0.673126
Epoch 10.29: Loss = 0.672653
Epoch 10.30: Loss = 0.695114
Epoch 10.31: Loss = 0.749802
Epoch 10.32: Loss = 0.631912
Epoch 10.33: Loss = 0.606262
Epoch 10.34: Loss = 0.7565
Epoch 10.35: Loss = 0.69165
Epoch 10.36: Loss = 0.714233
Epoch 10.37: Loss = 0.732971
Epoch 10.38: Loss = 0.661102
Epoch 10.39: Loss = 0.72583
Epoch 10.40: Loss = 0.637573
Epoch 10.41: Loss = 0.725113
Epoch 10.42: Loss = 0.708328
Epoch 10.43: Loss = 0.745178
Epoch 10.44: Loss = 0.522827
Epoch 10.45: Loss = 0.682785
Epoch 10.46: Loss = 0.795151
Epoch 10.47: Loss = 0.633591
Epoch 10.48: Loss = 0.589386
Epoch 10.49: Loss = 0.81131
Epoch 10.50: Loss = 0.715927
Epoch 10.51: Loss = 0.534149
Epoch 10.52: Loss = 0.688843
Epoch 10.53: Loss = 0.751846
Epoch 10.54: Loss = 0.578857
Epoch 10.55: Loss = 0.684143
Epoch 10.56: Loss = 0.690002
Epoch 10.57: Loss = 0.811722
Epoch 10.58: Loss = 0.643417
Epoch 10.59: Loss = 0.76976
Epoch 10.60: Loss = 0.728516
Epoch 10.61: Loss = 0.661972
Epoch 10.62: Loss = 0.739761
Epoch 10.63: Loss = 0.5569
Epoch 10.64: Loss = 0.513931
Epoch 10.65: Loss = 0.661987
Epoch 10.66: Loss = 0.577225
Epoch 10.67: Loss = 0.6474
Epoch 10.68: Loss = 0.869858
Epoch 10.69: Loss = 0.648026
Epoch 10.70: Loss = 0.673996
Epoch 10.71: Loss = 0.585495
Epoch 10.72: Loss = 0.70108
Epoch 10.73: Loss = 0.828995
Epoch 10.74: Loss = 0.752792
Epoch 10.75: Loss = 0.61615
Epoch 10.76: Loss = 0.646042
Epoch 10.77: Loss = 0.690247
Epoch 10.78: Loss = 0.68042
Epoch 10.79: Loss = 0.683823
Epoch 10.80: Loss = 0.631943
Epoch 10.81: Loss = 0.626404
Epoch 10.82: Loss = 0.645218
Epoch 10.83: Loss = 0.772949
Epoch 10.84: Loss = 0.569824
Epoch 10.85: Loss = 0.664185
Epoch 10.86: Loss = 0.70842
Epoch 10.87: Loss = 0.608765
Epoch 10.88: Loss = 0.641052
Epoch 10.89: Loss = 0.842209
Epoch 10.90: Loss = 0.753235
Epoch 10.91: Loss = 0.721878
Epoch 10.92: Loss = 0.627289
Epoch 10.93: Loss = 0.714859
Epoch 10.94: Loss = 0.686279
Epoch 10.95: Loss = 0.714783
Epoch 10.96: Loss = 0.679581
Epoch 10.97: Loss = 0.537628
Epoch 10.98: Loss = 0.631622
Epoch 10.99: Loss = 0.680618
Epoch 10.100: Loss = 0.720367
Epoch 10.101: Loss = 0.656937
Epoch 10.102: Loss = 0.777695
Epoch 10.103: Loss = 0.639847
Epoch 10.104: Loss = 0.638214
Epoch 10.105: Loss = 0.630127
Epoch 10.106: Loss = 0.796112
Epoch 10.107: Loss = 0.692856
Epoch 10.108: Loss = 0.838577
Epoch 10.109: Loss = 0.840607
Epoch 10.110: Loss = 0.728928
Epoch 10.111: Loss = 0.649811
Epoch 10.112: Loss = 0.6371
Epoch 10.113: Loss = 0.660461
Epoch 10.114: Loss = 0.762115
Epoch 10.115: Loss = 0.673431
Epoch 10.116: Loss = 0.594604
Epoch 10.117: Loss = 0.726715
Epoch 10.118: Loss = 0.627533
Epoch 10.119: Loss = 0.723816
Epoch 10.120: Loss = 0.68335
TRAIN LOSS = 0.679474
TRAIN ACC = 81.5872 % (48955/60000)
Loss = 0.606674
Loss = 0.761536
Loss = 0.634186
Loss = 0.649292
Loss = 0.677353
Loss = 0.826462
Loss = 0.853149
Loss = 0.756958
Loss = 0.700729
Loss = 0.63826
Loss = 0.868744
Loss = 0.867065
Loss = 0.739761
Loss = 0.716171
Loss = 0.738083
Loss = 0.723831
Loss = 0.724609
Loss = 0.753006
Loss = 0.747803
Loss = 0.69458
TEST LOSS = 0.733912
TEST ACC = 489.549 % (8111/10000)
