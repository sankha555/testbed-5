Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 600]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 10
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.30466
Epoch 1.2: Loss = 2.2587
Epoch 1.3: Loss = 2.20288
Epoch 1.4: Loss = 2.18567
Epoch 1.5: Loss = 2.10754
Epoch 1.6: Loss = 2.06326
Epoch 1.7: Loss = 2.01491
Epoch 1.8: Loss = 1.97804
Epoch 1.9: Loss = 1.93645
Epoch 1.10: Loss = 1.91818
Epoch 1.11: Loss = 1.86913
Epoch 1.12: Loss = 1.81467
Epoch 1.13: Loss = 1.77788
Epoch 1.14: Loss = 1.77473
Epoch 1.15: Loss = 1.69191
Epoch 1.16: Loss = 1.69714
Epoch 1.17: Loss = 1.63818
Epoch 1.18: Loss = 1.59497
Epoch 1.19: Loss = 1.53284
Epoch 1.20: Loss = 1.55594
Epoch 1.21: Loss = 1.5063
Epoch 1.22: Loss = 1.46364
Epoch 1.23: Loss = 1.50296
Epoch 1.24: Loss = 1.474
Epoch 1.25: Loss = 1.383
Epoch 1.26: Loss = 1.37497
Epoch 1.27: Loss = 1.41933
Epoch 1.28: Loss = 1.35785
Epoch 1.29: Loss = 1.27873
Epoch 1.30: Loss = 1.28305
Epoch 1.31: Loss = 1.25745
Epoch 1.32: Loss = 1.27554
Epoch 1.33: Loss = 1.24989
Epoch 1.34: Loss = 1.22545
Epoch 1.35: Loss = 1.16928
Epoch 1.36: Loss = 1.13535
Epoch 1.37: Loss = 1.17749
Epoch 1.38: Loss = 1.12518
Epoch 1.39: Loss = 1.12538
Epoch 1.40: Loss = 1.08875
Epoch 1.41: Loss = 1.08267
Epoch 1.42: Loss = 1.08014
Epoch 1.43: Loss = 1.09709
Epoch 1.44: Loss = 1.10721
Epoch 1.45: Loss = 1.03322
Epoch 1.46: Loss = 1.06114
Epoch 1.47: Loss = 1.00362
Epoch 1.48: Loss = 0.9711
Epoch 1.49: Loss = 0.966156
Epoch 1.50: Loss = 0.959091
Epoch 1.51: Loss = 0.92923
Epoch 1.52: Loss = 0.958298
Epoch 1.53: Loss = 0.901474
Epoch 1.54: Loss = 0.994019
Epoch 1.55: Loss = 0.947586
Epoch 1.56: Loss = 0.940704
Epoch 1.57: Loss = 0.894104
Epoch 1.58: Loss = 0.829605
Epoch 1.59: Loss = 0.911102
Epoch 1.60: Loss = 0.857407
Epoch 1.61: Loss = 0.836533
Epoch 1.62: Loss = 0.818146
Epoch 1.63: Loss = 0.836029
Epoch 1.64: Loss = 0.843613
Epoch 1.65: Loss = 0.806198
Epoch 1.66: Loss = 0.815689
Epoch 1.67: Loss = 0.82077
Epoch 1.68: Loss = 0.835373
Epoch 1.69: Loss = 0.803375
Epoch 1.70: Loss = 0.757034
Epoch 1.71: Loss = 0.67099
Epoch 1.72: Loss = 0.746994
Epoch 1.73: Loss = 0.744736
Epoch 1.74: Loss = 0.730438
Epoch 1.75: Loss = 0.774124
Epoch 1.76: Loss = 0.729706
Epoch 1.77: Loss = 0.710388
Epoch 1.78: Loss = 0.752899
Epoch 1.79: Loss = 0.697281
Epoch 1.80: Loss = 0.712463
Epoch 1.81: Loss = 0.647034
Epoch 1.82: Loss = 0.775665
Epoch 1.83: Loss = 0.754013
Epoch 1.84: Loss = 0.681381
Epoch 1.85: Loss = 0.732986
Epoch 1.86: Loss = 0.702744
Epoch 1.87: Loss = 0.680527
Epoch 1.88: Loss = 0.695465
Epoch 1.89: Loss = 0.786072
Epoch 1.90: Loss = 0.624405
Epoch 1.91: Loss = 0.678574
Epoch 1.92: Loss = 0.714081
Epoch 1.93: Loss = 0.664581
Epoch 1.94: Loss = 0.634079
Epoch 1.95: Loss = 0.65361
Epoch 1.96: Loss = 0.721741
Epoch 1.97: Loss = 0.644989
Epoch 1.98: Loss = 0.622528
Epoch 1.99: Loss = 0.627899
Epoch 1.100: Loss = 0.652466
TRAIN LOSS = 1.12456
TRAIN ACC = 70.2728 % (42166/60000)
Loss = 0.675171
Loss = 0.676193
Loss = 0.812653
Loss = 0.743347
Loss = 0.652481
Loss = 0.682449
Loss = 0.760727
Loss = 0.737656
Loss = 0.536865
Loss = 0.508575
Loss = 0.436218
Loss = 0.529785
Loss = 0.501389
Loss = 0.489517
Loss = 0.308289
Loss = 0.474396
Loss = 0.748062
TEST LOSS = 0.601465
TEST ACC = 421.66 % (8303/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.615448
Epoch 2.2: Loss = 0.641907
Epoch 2.3: Loss = 0.620407
Epoch 2.4: Loss = 0.592896
Epoch 2.5: Loss = 0.620636
Epoch 2.6: Loss = 0.602234
Epoch 2.7: Loss = 0.604889
Epoch 2.8: Loss = 0.634186
Epoch 2.9: Loss = 0.584427
Epoch 2.10: Loss = 0.616333
Epoch 2.11: Loss = 0.562897
Epoch 2.12: Loss = 0.546738
Epoch 2.13: Loss = 0.582977
Epoch 2.14: Loss = 0.600266
Epoch 2.15: Loss = 0.617935
Epoch 2.16: Loss = 0.556564
Epoch 2.17: Loss = 0.61351
Epoch 2.18: Loss = 0.579742
Epoch 2.19: Loss = 0.585495
Epoch 2.20: Loss = 0.598877
Epoch 2.21: Loss = 0.612106
Epoch 2.22: Loss = 0.608109
Epoch 2.23: Loss = 0.59288
Epoch 2.24: Loss = 0.549942
Epoch 2.25: Loss = 0.539139
Epoch 2.26: Loss = 0.607712
Epoch 2.27: Loss = 0.544632
Epoch 2.28: Loss = 0.582687
Epoch 2.29: Loss = 0.55423
Epoch 2.30: Loss = 0.493576
Epoch 2.31: Loss = 0.574188
Epoch 2.32: Loss = 0.608734
Epoch 2.33: Loss = 0.592422
Epoch 2.34: Loss = 0.547272
Epoch 2.35: Loss = 0.611832
Epoch 2.36: Loss = 0.569824
Epoch 2.37: Loss = 0.525192
Epoch 2.38: Loss = 0.540283
Epoch 2.39: Loss = 0.604248
Epoch 2.40: Loss = 0.543655
Epoch 2.41: Loss = 0.526581
Epoch 2.42: Loss = 0.579926
Epoch 2.43: Loss = 0.556976
Epoch 2.44: Loss = 0.473892
Epoch 2.45: Loss = 0.572723
Epoch 2.46: Loss = 0.52597
Epoch 2.47: Loss = 0.509842
Epoch 2.48: Loss = 0.539566
Epoch 2.49: Loss = 0.537796
Epoch 2.50: Loss = 0.553574
Epoch 2.51: Loss = 0.524628
Epoch 2.52: Loss = 0.499496
Epoch 2.53: Loss = 0.46933
Epoch 2.54: Loss = 0.599869
Epoch 2.55: Loss = 0.514023
Epoch 2.56: Loss = 0.56221
Epoch 2.57: Loss = 0.544693
Epoch 2.58: Loss = 0.476685
Epoch 2.59: Loss = 0.553802
Epoch 2.60: Loss = 0.523575
Epoch 2.61: Loss = 0.526474
Epoch 2.62: Loss = 0.466064
Epoch 2.63: Loss = 0.526733
Epoch 2.64: Loss = 0.412231
Epoch 2.65: Loss = 0.519409
Epoch 2.66: Loss = 0.520615
Epoch 2.67: Loss = 0.532883
Epoch 2.68: Loss = 0.505112
Epoch 2.69: Loss = 0.523697
Epoch 2.70: Loss = 0.561615
Epoch 2.71: Loss = 0.5103
Epoch 2.72: Loss = 0.521591
Epoch 2.73: Loss = 0.483871
Epoch 2.74: Loss = 0.500854
Epoch 2.75: Loss = 0.530487
Epoch 2.76: Loss = 0.487061
Epoch 2.77: Loss = 0.529007
Epoch 2.78: Loss = 0.469589
Epoch 2.79: Loss = 0.530685
Epoch 2.80: Loss = 0.485962
Epoch 2.81: Loss = 0.509033
Epoch 2.82: Loss = 0.490433
Epoch 2.83: Loss = 0.526779
Epoch 2.84: Loss = 0.529282
Epoch 2.85: Loss = 0.47258
Epoch 2.86: Loss = 0.496231
Epoch 2.87: Loss = 0.564804
Epoch 2.88: Loss = 0.455521
Epoch 2.89: Loss = 0.433517
Epoch 2.90: Loss = 0.518066
Epoch 2.91: Loss = 0.508972
Epoch 2.92: Loss = 0.487793
Epoch 2.93: Loss = 0.458023
Epoch 2.94: Loss = 0.493134
Epoch 2.95: Loss = 0.494156
Epoch 2.96: Loss = 0.503708
Epoch 2.97: Loss = 0.483032
Epoch 2.98: Loss = 0.454636
Epoch 2.99: Loss = 0.491348
Epoch 2.100: Loss = 0.476059
TRAIN LOSS = 0.540207
TRAIN ACC = 83.9569 % (50377/60000)
Loss = 0.496262
Loss = 0.532471
Loss = 0.652222
Loss = 0.596603
Loss = 0.485916
Loss = 0.51181
Loss = 0.623535
Loss = 0.593323
Loss = 0.390015
Loss = 0.360443
Loss = 0.355865
Loss = 0.360184
Loss = 0.324127
Loss = 0.398224
Loss = 0.158752
Loss = 0.316772
Loss = 0.6064
TEST LOSS = 0.453647
TEST ACC = 503.769 % (8641/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.497879
Epoch 3.2: Loss = 0.47229
Epoch 3.3: Loss = 0.507553
Epoch 3.4: Loss = 0.491501
Epoch 3.5: Loss = 0.474823
Epoch 3.6: Loss = 0.501175
Epoch 3.7: Loss = 0.484161
Epoch 3.8: Loss = 0.462448
Epoch 3.9: Loss = 0.471207
Epoch 3.10: Loss = 0.50148
Epoch 3.11: Loss = 0.482864
Epoch 3.12: Loss = 0.428467
Epoch 3.13: Loss = 0.424255
Epoch 3.14: Loss = 0.514908
Epoch 3.15: Loss = 0.410248
Epoch 3.16: Loss = 0.50708
Epoch 3.17: Loss = 0.47316
Epoch 3.18: Loss = 0.419571
Epoch 3.19: Loss = 0.421997
Epoch 3.20: Loss = 0.4599
Epoch 3.21: Loss = 0.535675
Epoch 3.22: Loss = 0.502304
Epoch 3.23: Loss = 0.479065
Epoch 3.24: Loss = 0.474487
Epoch 3.25: Loss = 0.431778
Epoch 3.26: Loss = 0.487808
Epoch 3.27: Loss = 0.407013
Epoch 3.28: Loss = 0.528748
Epoch 3.29: Loss = 0.472473
Epoch 3.30: Loss = 0.416641
Epoch 3.31: Loss = 0.445343
Epoch 3.32: Loss = 0.442841
Epoch 3.33: Loss = 0.499298
Epoch 3.34: Loss = 0.454117
Epoch 3.35: Loss = 0.465363
Epoch 3.36: Loss = 0.434509
Epoch 3.37: Loss = 0.453659
Epoch 3.38: Loss = 0.3918
Epoch 3.39: Loss = 0.438263
Epoch 3.40: Loss = 0.428864
Epoch 3.41: Loss = 0.504211
Epoch 3.42: Loss = 0.425781
Epoch 3.43: Loss = 0.468414
Epoch 3.44: Loss = 0.49881
Epoch 3.45: Loss = 0.418945
Epoch 3.46: Loss = 0.451782
Epoch 3.47: Loss = 0.392197
Epoch 3.48: Loss = 0.507584
Epoch 3.49: Loss = 0.427597
Epoch 3.50: Loss = 0.3862
Epoch 3.51: Loss = 0.445831
Epoch 3.52: Loss = 0.446487
Epoch 3.53: Loss = 0.462524
Epoch 3.54: Loss = 0.516815
Epoch 3.55: Loss = 0.497131
Epoch 3.56: Loss = 0.529709
Epoch 3.57: Loss = 0.490479
Epoch 3.58: Loss = 0.465988
Epoch 3.59: Loss = 0.506409
Epoch 3.60: Loss = 0.53685
Epoch 3.61: Loss = 0.375916
Epoch 3.62: Loss = 0.493027
Epoch 3.63: Loss = 0.454071
Epoch 3.64: Loss = 0.482162
Epoch 3.65: Loss = 0.449554
Epoch 3.66: Loss = 0.40773
Epoch 3.67: Loss = 0.415115
Epoch 3.68: Loss = 0.352097
Epoch 3.69: Loss = 0.460388
Epoch 3.70: Loss = 0.49234
Epoch 3.71: Loss = 0.435669
Epoch 3.72: Loss = 0.401642
Epoch 3.73: Loss = 0.456329
Epoch 3.74: Loss = 0.458893
Epoch 3.75: Loss = 0.416458
Epoch 3.76: Loss = 0.428055
Epoch 3.77: Loss = 0.44577
Epoch 3.78: Loss = 0.459885
Epoch 3.79: Loss = 0.393265
Epoch 3.80: Loss = 0.471313
Epoch 3.81: Loss = 0.442169
Epoch 3.82: Loss = 0.45079
Epoch 3.83: Loss = 0.399124
Epoch 3.84: Loss = 0.480881
Epoch 3.85: Loss = 0.34874
Epoch 3.86: Loss = 0.461609
Epoch 3.87: Loss = 0.433289
Epoch 3.88: Loss = 0.475357
Epoch 3.89: Loss = 0.439789
Epoch 3.90: Loss = 0.40535
Epoch 3.91: Loss = 0.424408
Epoch 3.92: Loss = 0.487015
Epoch 3.93: Loss = 0.494278
Epoch 3.94: Loss = 0.459732
Epoch 3.95: Loss = 0.444458
Epoch 3.96: Loss = 0.459763
Epoch 3.97: Loss = 0.484879
Epoch 3.98: Loss = 0.481598
Epoch 3.99: Loss = 0.406281
Epoch 3.100: Loss = 0.432663
TRAIN LOSS = 0.456406
TRAIN ACC = 86.1328 % (51682/60000)
Loss = 0.45076
Loss = 0.493988
Loss = 0.614258
Loss = 0.562912
Loss = 0.427658
Loss = 0.461487
Loss = 0.582062
Loss = 0.538788
Loss = 0.353867
Loss = 0.318863
Loss = 0.307022
Loss = 0.309418
Loss = 0.261612
Loss = 0.359543
Loss = 0.119415
Loss = 0.26355
Loss = 0.601349
TEST LOSS = 0.409566
TEST ACC = 516.82 % (8764/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.413345
Epoch 4.2: Loss = 0.473083
Epoch 4.3: Loss = 0.405396
Epoch 4.4: Loss = 0.442749
Epoch 4.5: Loss = 0.387131
Epoch 4.6: Loss = 0.407135
Epoch 4.7: Loss = 0.415436
Epoch 4.8: Loss = 0.4151
Epoch 4.9: Loss = 0.364075
Epoch 4.10: Loss = 0.439484
Epoch 4.11: Loss = 0.426224
Epoch 4.12: Loss = 0.404037
Epoch 4.13: Loss = 0.391968
Epoch 4.14: Loss = 0.408203
Epoch 4.15: Loss = 0.421265
Epoch 4.16: Loss = 0.456894
Epoch 4.17: Loss = 0.466904
Epoch 4.18: Loss = 0.430115
Epoch 4.19: Loss = 0.325775
Epoch 4.20: Loss = 0.391464
Epoch 4.21: Loss = 0.453323
Epoch 4.22: Loss = 0.438156
Epoch 4.23: Loss = 0.483215
Epoch 4.24: Loss = 0.46521
Epoch 4.25: Loss = 0.390762
Epoch 4.26: Loss = 0.46051
Epoch 4.27: Loss = 0.432831
Epoch 4.28: Loss = 0.428879
Epoch 4.29: Loss = 0.416718
Epoch 4.30: Loss = 0.374268
Epoch 4.31: Loss = 0.418427
Epoch 4.32: Loss = 0.431992
Epoch 4.33: Loss = 0.446854
Epoch 4.34: Loss = 0.376663
Epoch 4.35: Loss = 0.44371
Epoch 4.36: Loss = 0.474106
Epoch 4.37: Loss = 0.414398
Epoch 4.38: Loss = 0.435028
Epoch 4.39: Loss = 0.378326
Epoch 4.40: Loss = 0.407043
Epoch 4.41: Loss = 0.40863
Epoch 4.42: Loss = 0.41806
Epoch 4.43: Loss = 0.377777
Epoch 4.44: Loss = 0.467117
Epoch 4.45: Loss = 0.417053
Epoch 4.46: Loss = 0.337051
Epoch 4.47: Loss = 0.531372
Epoch 4.48: Loss = 0.411972
Epoch 4.49: Loss = 0.435349
Epoch 4.50: Loss = 0.468018
Epoch 4.51: Loss = 0.437943
Epoch 4.52: Loss = 0.459335
Epoch 4.53: Loss = 0.462814
Epoch 4.54: Loss = 0.444885
Epoch 4.55: Loss = 0.525604
Epoch 4.56: Loss = 0.411652
Epoch 4.57: Loss = 0.444183
Epoch 4.58: Loss = 0.429642
Epoch 4.59: Loss = 0.486603
Epoch 4.60: Loss = 0.432648
Epoch 4.61: Loss = 0.427017
Epoch 4.62: Loss = 0.501236
Epoch 4.63: Loss = 0.401779
Epoch 4.64: Loss = 0.410782
Epoch 4.65: Loss = 0.483917
Epoch 4.66: Loss = 0.440765
Epoch 4.67: Loss = 0.407532
Epoch 4.68: Loss = 0.439423
Epoch 4.69: Loss = 0.393875
Epoch 4.70: Loss = 0.506149
Epoch 4.71: Loss = 0.472443
Epoch 4.72: Loss = 0.511093
Epoch 4.73: Loss = 0.392456
Epoch 4.74: Loss = 0.441772
Epoch 4.75: Loss = 0.387726
Epoch 4.76: Loss = 0.37645
Epoch 4.77: Loss = 0.418777
Epoch 4.78: Loss = 0.451172
Epoch 4.79: Loss = 0.417725
Epoch 4.80: Loss = 0.422348
Epoch 4.81: Loss = 0.412827
Epoch 4.82: Loss = 0.539856
Epoch 4.83: Loss = 0.45134
Epoch 4.84: Loss = 0.437027
Epoch 4.85: Loss = 0.430695
Epoch 4.86: Loss = 0.365143
Epoch 4.87: Loss = 0.388062
Epoch 4.88: Loss = 0.452377
Epoch 4.89: Loss = 0.459717
Epoch 4.90: Loss = 0.391678
Epoch 4.91: Loss = 0.423004
Epoch 4.92: Loss = 0.412308
Epoch 4.93: Loss = 0.395493
Epoch 4.94: Loss = 0.478806
Epoch 4.95: Loss = 0.414551
Epoch 4.96: Loss = 0.432419
Epoch 4.97: Loss = 0.387207
Epoch 4.98: Loss = 0.395691
Epoch 4.99: Loss = 0.432312
Epoch 4.100: Loss = 0.359161
TRAIN LOSS = 0.428268
TRAIN ACC = 87.1185 % (52274/60000)
Loss = 0.432693
Loss = 0.483124
Loss = 0.599258
Loss = 0.546692
Loss = 0.396347
Loss = 0.415298
Loss = 0.558685
Loss = 0.490738
Loss = 0.328629
Loss = 0.306061
Loss = 0.304932
Loss = 0.280121
Loss = 0.223389
Loss = 0.347534
Loss = 0.106873
Loss = 0.244003
Loss = 0.62674
TEST LOSS = 0.388932
TEST ACC = 522.739 % (8859/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.495361
Epoch 5.2: Loss = 0.358139
Epoch 5.3: Loss = 0.425598
Epoch 5.4: Loss = 0.431992
Epoch 5.5: Loss = 0.428146
Epoch 5.6: Loss = 0.364441
Epoch 5.7: Loss = 0.4673
Epoch 5.8: Loss = 0.395218
Epoch 5.9: Loss = 0.403824
Epoch 5.10: Loss = 0.364502
Epoch 5.11: Loss = 0.453629
Epoch 5.12: Loss = 0.349808
Epoch 5.13: Loss = 0.519241
Epoch 5.14: Loss = 0.421661
Epoch 5.15: Loss = 0.470367
Epoch 5.16: Loss = 0.381119
Epoch 5.17: Loss = 0.381607
Epoch 5.18: Loss = 0.386215
Epoch 5.19: Loss = 0.533463
Epoch 5.20: Loss = 0.352448
Epoch 5.21: Loss = 0.396301
Epoch 5.22: Loss = 0.422012
Epoch 5.23: Loss = 0.520203
Epoch 5.24: Loss = 0.405167
Epoch 5.25: Loss = 0.425461
Epoch 5.26: Loss = 0.431732
Epoch 5.27: Loss = 0.406998
Epoch 5.28: Loss = 0.377151
Epoch 5.29: Loss = 0.365158
Epoch 5.30: Loss = 0.42659
Epoch 5.31: Loss = 0.430069
Epoch 5.32: Loss = 0.338028
Epoch 5.33: Loss = 0.389877
Epoch 5.34: Loss = 0.409897
Epoch 5.35: Loss = 0.495056
Epoch 5.36: Loss = 0.362228
Epoch 5.37: Loss = 0.385376
Epoch 5.38: Loss = 0.526733
Epoch 5.39: Loss = 0.371857
Epoch 5.40: Loss = 0.380066
Epoch 5.41: Loss = 0.456985
Epoch 5.42: Loss = 0.447449
Epoch 5.43: Loss = 0.398758
Epoch 5.44: Loss = 0.407104
Epoch 5.45: Loss = 0.454544
Epoch 5.46: Loss = 0.3535
Epoch 5.47: Loss = 0.412918
Epoch 5.48: Loss = 0.397339
Epoch 5.49: Loss = 0.380219
Epoch 5.50: Loss = 0.361496
Epoch 5.51: Loss = 0.485382
Epoch 5.52: Loss = 0.426407
Epoch 5.53: Loss = 0.436508
Epoch 5.54: Loss = 0.364594
Epoch 5.55: Loss = 0.38353
Epoch 5.56: Loss = 0.491165
Epoch 5.57: Loss = 0.428726
Epoch 5.58: Loss = 0.393616
Epoch 5.59: Loss = 0.4384
Epoch 5.60: Loss = 0.400177
Epoch 5.61: Loss = 0.465714
Epoch 5.62: Loss = 0.426208
Epoch 5.63: Loss = 0.379028
Epoch 5.64: Loss = 0.368027
Epoch 5.65: Loss = 0.395157
Epoch 5.66: Loss = 0.422577
Epoch 5.67: Loss = 0.385162
Epoch 5.68: Loss = 0.430023
Epoch 5.69: Loss = 0.442429
Epoch 5.70: Loss = 0.370758
Epoch 5.71: Loss = 0.391205
Epoch 5.72: Loss = 0.411224
Epoch 5.73: Loss = 0.385239
Epoch 5.74: Loss = 0.360291
Epoch 5.75: Loss = 0.340988
Epoch 5.76: Loss = 0.425247
Epoch 5.77: Loss = 0.37204
Epoch 5.78: Loss = 0.460861
Epoch 5.79: Loss = 0.430984
Epoch 5.80: Loss = 0.434296
Epoch 5.81: Loss = 0.4543
Epoch 5.82: Loss = 0.388245
Epoch 5.83: Loss = 0.406143
Epoch 5.84: Loss = 0.503769
Epoch 5.85: Loss = 0.409515
Epoch 5.86: Loss = 0.492142
Epoch 5.87: Loss = 0.455582
Epoch 5.88: Loss = 0.479218
Epoch 5.89: Loss = 0.384109
Epoch 5.90: Loss = 0.423874
Epoch 5.91: Loss = 0.369644
Epoch 5.92: Loss = 0.356308
Epoch 5.93: Loss = 0.440414
Epoch 5.94: Loss = 0.360916
Epoch 5.95: Loss = 0.381607
Epoch 5.96: Loss = 0.390564
Epoch 5.97: Loss = 0.399155
Epoch 5.98: Loss = 0.399078
Epoch 5.99: Loss = 0.401398
Epoch 5.100: Loss = 0.392303
TRAIN LOSS = 0.413559
TRAIN ACC = 87.7579 % (52657/60000)
Loss = 0.422318
Loss = 0.461426
Loss = 0.568604
Loss = 0.53772
Loss = 0.387085
Loss = 0.399414
Loss = 0.564438
Loss = 0.480011
Loss = 0.320587
Loss = 0.288879
Loss = 0.323456
Loss = 0.264221
Loss = 0.191818
Loss = 0.321121
Loss = 0.0919189
Loss = 0.235184
Loss = 0.621643
TEST LOSS = 0.376358
TEST ACC = 526.569 % (8911/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.437119
Epoch 6.2: Loss = 0.404327
Epoch 6.3: Loss = 0.421661
Epoch 6.4: Loss = 0.489059
Epoch 6.5: Loss = 0.361008
Epoch 6.6: Loss = 0.420197
Epoch 6.7: Loss = 0.365768
Epoch 6.8: Loss = 0.364517
Epoch 6.9: Loss = 0.421844
Epoch 6.10: Loss = 0.39537
Epoch 6.11: Loss = 0.455032
Epoch 6.12: Loss = 0.32515
Epoch 6.13: Loss = 0.392975
Epoch 6.14: Loss = 0.375061
Epoch 6.15: Loss = 0.418106
Epoch 6.16: Loss = 0.460373
Epoch 6.17: Loss = 0.422531
Epoch 6.18: Loss = 0.429306
Epoch 6.19: Loss = 0.447617
Epoch 6.20: Loss = 0.397644
Epoch 6.21: Loss = 0.441406
Epoch 6.22: Loss = 0.375854
Epoch 6.23: Loss = 0.393921
Epoch 6.24: Loss = 0.377548
Epoch 6.25: Loss = 0.362457
Epoch 6.26: Loss = 0.438812
Epoch 6.27: Loss = 0.366638
Epoch 6.28: Loss = 0.374359
Epoch 6.29: Loss = 0.453476
Epoch 6.30: Loss = 0.397842
Epoch 6.31: Loss = 0.459915
Epoch 6.32: Loss = 0.403519
Epoch 6.33: Loss = 0.387314
Epoch 6.34: Loss = 0.369888
Epoch 6.35: Loss = 0.482834
Epoch 6.36: Loss = 0.356049
Epoch 6.37: Loss = 0.342575
Epoch 6.38: Loss = 0.379547
Epoch 6.39: Loss = 0.355865
Epoch 6.40: Loss = 0.392395
Epoch 6.41: Loss = 0.339417
Epoch 6.42: Loss = 0.450775
Epoch 6.43: Loss = 0.431274
Epoch 6.44: Loss = 0.460632
Epoch 6.45: Loss = 0.419327
Epoch 6.46: Loss = 0.468506
Epoch 6.47: Loss = 0.50766
Epoch 6.48: Loss = 0.383911
Epoch 6.49: Loss = 0.466797
Epoch 6.50: Loss = 0.376007
Epoch 6.51: Loss = 0.429962
Epoch 6.52: Loss = 0.343872
Epoch 6.53: Loss = 0.365021
Epoch 6.54: Loss = 0.479858
Epoch 6.55: Loss = 0.374924
Epoch 6.56: Loss = 0.426422
Epoch 6.57: Loss = 0.404312
Epoch 6.58: Loss = 0.452393
Epoch 6.59: Loss = 0.407944
Epoch 6.60: Loss = 0.462997
Epoch 6.61: Loss = 0.428558
Epoch 6.62: Loss = 0.449753
Epoch 6.63: Loss = 0.398453
Epoch 6.64: Loss = 0.39476
Epoch 6.65: Loss = 0.343185
Epoch 6.66: Loss = 0.472763
Epoch 6.67: Loss = 0.369141
Epoch 6.68: Loss = 0.402954
Epoch 6.69: Loss = 0.432724
Epoch 6.70: Loss = 0.467072
Epoch 6.71: Loss = 0.428406
Epoch 6.72: Loss = 0.473434
Epoch 6.73: Loss = 0.537048
Epoch 6.74: Loss = 0.45784
Epoch 6.75: Loss = 0.385162
Epoch 6.76: Loss = 0.37735
Epoch 6.77: Loss = 0.449615
Epoch 6.78: Loss = 0.347992
Epoch 6.79: Loss = 0.439728
Epoch 6.80: Loss = 0.331757
Epoch 6.81: Loss = 0.359589
Epoch 6.82: Loss = 0.353653
Epoch 6.83: Loss = 0.341629
Epoch 6.84: Loss = 0.445541
Epoch 6.85: Loss = 0.391098
Epoch 6.86: Loss = 0.418137
Epoch 6.87: Loss = 0.442505
Epoch 6.88: Loss = 0.401428
Epoch 6.89: Loss = 0.400192
Epoch 6.90: Loss = 0.393478
Epoch 6.91: Loss = 0.387299
Epoch 6.92: Loss = 0.344406
Epoch 6.93: Loss = 0.379715
Epoch 6.94: Loss = 0.393112
Epoch 6.95: Loss = 0.349854
Epoch 6.96: Loss = 0.42691
Epoch 6.97: Loss = 0.423294
Epoch 6.98: Loss = 0.357147
Epoch 6.99: Loss = 0.45784
Epoch 6.100: Loss = 0.363739
TRAIN LOSS = 0.407883
TRAIN ACC = 88.1454 % (52890/60000)
Loss = 0.440781
Loss = 0.467239
Loss = 0.587463
Loss = 0.544159
Loss = 0.395554
Loss = 0.405228
Loss = 0.553589
Loss = 0.493271
Loss = 0.304779
Loss = 0.270279
Loss = 0.323959
Loss = 0.244263
Loss = 0.214111
Loss = 0.312561
Loss = 0.0784607
Loss = 0.225998
Loss = 0.624176
TEST LOSS = 0.376669
TEST ACC = 528.899 % (8910/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.461899
Epoch 7.2: Loss = 0.41777
Epoch 7.3: Loss = 0.454697
Epoch 7.4: Loss = 0.439377
Epoch 7.5: Loss = 0.402237
Epoch 7.6: Loss = 0.448471
Epoch 7.7: Loss = 0.3367
Epoch 7.8: Loss = 0.362579
Epoch 7.9: Loss = 0.415695
Epoch 7.10: Loss = 0.420685
Epoch 7.11: Loss = 0.395294
Epoch 7.12: Loss = 0.41507
Epoch 7.13: Loss = 0.392609
Epoch 7.14: Loss = 0.403275
Epoch 7.15: Loss = 0.448959
Epoch 7.16: Loss = 0.451202
Epoch 7.17: Loss = 0.375992
Epoch 7.18: Loss = 0.42775
Epoch 7.19: Loss = 0.514038
Epoch 7.20: Loss = 0.362671
Epoch 7.21: Loss = 0.430679
Epoch 7.22: Loss = 0.384705
Epoch 7.23: Loss = 0.353058
Epoch 7.24: Loss = 0.380417
Epoch 7.25: Loss = 0.440857
Epoch 7.26: Loss = 0.397156
Epoch 7.27: Loss = 0.429321
Epoch 7.28: Loss = 0.444244
Epoch 7.29: Loss = 0.40535
Epoch 7.30: Loss = 0.434021
Epoch 7.31: Loss = 0.485443
Epoch 7.32: Loss = 0.404053
Epoch 7.33: Loss = 0.440811
Epoch 7.34: Loss = 0.372421
Epoch 7.35: Loss = 0.388092
Epoch 7.36: Loss = 0.459503
Epoch 7.37: Loss = 0.26442
Epoch 7.38: Loss = 0.330734
Epoch 7.39: Loss = 0.523041
Epoch 7.40: Loss = 0.482941
Epoch 7.41: Loss = 0.385925
Epoch 7.42: Loss = 0.389282
Epoch 7.43: Loss = 0.318344
Epoch 7.44: Loss = 0.448318
Epoch 7.45: Loss = 0.375107
Epoch 7.46: Loss = 0.347839
Epoch 7.47: Loss = 0.355728
Epoch 7.48: Loss = 0.425888
Epoch 7.49: Loss = 0.402588
Epoch 7.50: Loss = 0.49826
Epoch 7.51: Loss = 0.438599
Epoch 7.52: Loss = 0.344711
Epoch 7.53: Loss = 0.436203
Epoch 7.54: Loss = 0.369781
Epoch 7.55: Loss = 0.381012
Epoch 7.56: Loss = 0.384308
Epoch 7.57: Loss = 0.44841
Epoch 7.58: Loss = 0.376266
Epoch 7.59: Loss = 0.427719
Epoch 7.60: Loss = 0.409454
Epoch 7.61: Loss = 0.441147
Epoch 7.62: Loss = 0.452164
Epoch 7.63: Loss = 0.386826
Epoch 7.64: Loss = 0.342545
Epoch 7.65: Loss = 0.396484
Epoch 7.66: Loss = 0.38504
Epoch 7.67: Loss = 0.375977
Epoch 7.68: Loss = 0.417847
Epoch 7.69: Loss = 0.471893
Epoch 7.70: Loss = 0.472733
Epoch 7.71: Loss = 0.419037
Epoch 7.72: Loss = 0.357559
Epoch 7.73: Loss = 0.332077
Epoch 7.74: Loss = 0.369156
Epoch 7.75: Loss = 0.394913
Epoch 7.76: Loss = 0.395477
Epoch 7.77: Loss = 0.366043
Epoch 7.78: Loss = 0.390121
Epoch 7.79: Loss = 0.451538
Epoch 7.80: Loss = 0.311417
Epoch 7.81: Loss = 0.419586
Epoch 7.82: Loss = 0.383728
Epoch 7.83: Loss = 0.409805
Epoch 7.84: Loss = 0.349243
Epoch 7.85: Loss = 0.453857
Epoch 7.86: Loss = 0.423004
Epoch 7.87: Loss = 0.266052
Epoch 7.88: Loss = 0.388733
Epoch 7.89: Loss = 0.371063
Epoch 7.90: Loss = 0.368378
Epoch 7.91: Loss = 0.376205
Epoch 7.92: Loss = 0.446457
Epoch 7.93: Loss = 0.431854
Epoch 7.94: Loss = 0.288666
Epoch 7.95: Loss = 0.372589
Epoch 7.96: Loss = 0.384949
Epoch 7.97: Loss = 0.503708
Epoch 7.98: Loss = 0.446701
Epoch 7.99: Loss = 0.492966
Epoch 7.100: Loss = 0.405991
TRAIN LOSS = 0.40477
TRAIN ACC = 88.3301 % (53001/60000)
Loss = 0.414734
Loss = 0.444244
Loss = 0.596222
Loss = 0.540894
Loss = 0.389191
Loss = 0.388657
Loss = 0.548859
Loss = 0.476212
Loss = 0.31041
Loss = 0.264069
Loss = 0.314972
Loss = 0.24469
Loss = 0.203308
Loss = 0.310989
Loss = 0.0794678
Loss = 0.223862
Loss = 0.609375
TEST LOSS = 0.369422
TEST ACC = 530.009 % (8962/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.36232
Epoch 8.2: Loss = 0.45549
Epoch 8.3: Loss = 0.449295
Epoch 8.4: Loss = 0.412979
Epoch 8.5: Loss = 0.401749
Epoch 8.6: Loss = 0.324707
Epoch 8.7: Loss = 0.482864
Epoch 8.8: Loss = 0.469086
Epoch 8.9: Loss = 0.303986
Epoch 8.10: Loss = 0.412415
Epoch 8.11: Loss = 0.35321
Epoch 8.12: Loss = 0.387985
Epoch 8.13: Loss = 0.348068
Epoch 8.14: Loss = 0.410782
Epoch 8.15: Loss = 0.412308
Epoch 8.16: Loss = 0.485168
Epoch 8.17: Loss = 0.445358
Epoch 8.18: Loss = 0.445877
Epoch 8.19: Loss = 0.270111
Epoch 8.20: Loss = 0.432724
Epoch 8.21: Loss = 0.46373
Epoch 8.22: Loss = 0.37999
Epoch 8.23: Loss = 0.377365
Epoch 8.24: Loss = 0.422928
Epoch 8.25: Loss = 0.354431
Epoch 8.26: Loss = 0.365677
Epoch 8.27: Loss = 0.476013
Epoch 8.28: Loss = 0.396744
Epoch 8.29: Loss = 0.419189
Epoch 8.30: Loss = 0.37207
Epoch 8.31: Loss = 0.409164
Epoch 8.32: Loss = 0.407166
Epoch 8.33: Loss = 0.376923
Epoch 8.34: Loss = 0.466522
Epoch 8.35: Loss = 0.444748
Epoch 8.36: Loss = 0.406067
Epoch 8.37: Loss = 0.478561
Epoch 8.38: Loss = 0.415039
Epoch 8.39: Loss = 0.362122
Epoch 8.40: Loss = 0.486862
Epoch 8.41: Loss = 0.368576
Epoch 8.42: Loss = 0.312332
Epoch 8.43: Loss = 0.375061
Epoch 8.44: Loss = 0.341507
Epoch 8.45: Loss = 0.347763
Epoch 8.46: Loss = 0.427948
Epoch 8.47: Loss = 0.269745
Epoch 8.48: Loss = 0.452652
Epoch 8.49: Loss = 0.387894
Epoch 8.50: Loss = 0.367661
Epoch 8.51: Loss = 0.390091
Epoch 8.52: Loss = 0.353622
Epoch 8.53: Loss = 0.348724
Epoch 8.54: Loss = 0.315689
Epoch 8.55: Loss = 0.502472
Epoch 8.56: Loss = 0.410248
Epoch 8.57: Loss = 0.326385
Epoch 8.58: Loss = 0.407196
Epoch 8.59: Loss = 0.327957
Epoch 8.60: Loss = 0.394684
Epoch 8.61: Loss = 0.392868
Epoch 8.62: Loss = 0.463394
Epoch 8.63: Loss = 0.448746
Epoch 8.64: Loss = 0.326797
Epoch 8.65: Loss = 0.444
Epoch 8.66: Loss = 0.367325
Epoch 8.67: Loss = 0.417511
Epoch 8.68: Loss = 0.27861
Epoch 8.69: Loss = 0.436188
Epoch 8.70: Loss = 0.435562
Epoch 8.71: Loss = 0.410446
Epoch 8.72: Loss = 0.354263
Epoch 8.73: Loss = 0.38269
Epoch 8.74: Loss = 0.45137
Epoch 8.75: Loss = 0.344467
Epoch 8.76: Loss = 0.393341
Epoch 8.77: Loss = 0.484726
Epoch 8.78: Loss = 0.368835
Epoch 8.79: Loss = 0.421967
Epoch 8.80: Loss = 0.427124
Epoch 8.81: Loss = 0.437195
Epoch 8.82: Loss = 0.394745
Epoch 8.83: Loss = 0.389664
Epoch 8.84: Loss = 0.352539
Epoch 8.85: Loss = 0.433655
Epoch 8.86: Loss = 0.336853
Epoch 8.87: Loss = 0.39679
Epoch 8.88: Loss = 0.306656
Epoch 8.89: Loss = 0.4897
Epoch 8.90: Loss = 0.377655
Epoch 8.91: Loss = 0.372498
Epoch 8.92: Loss = 0.371811
Epoch 8.93: Loss = 0.428055
Epoch 8.94: Loss = 0.405869
Epoch 8.95: Loss = 0.355804
Epoch 8.96: Loss = 0.414764
Epoch 8.97: Loss = 0.336792
Epoch 8.98: Loss = 0.440094
Epoch 8.99: Loss = 0.36499
Epoch 8.100: Loss = 0.399857
TRAIN LOSS = 0.396317
TRAIN ACC = 88.7741 % (53267/60000)
Loss = 0.403137
Loss = 0.457047
Loss = 0.6008
Loss = 0.5327
Loss = 0.382675
Loss = 0.388901
Loss = 0.550858
Loss = 0.478409
Loss = 0.309937
Loss = 0.24382
Loss = 0.310181
Loss = 0.222275
Loss = 0.193283
Loss = 0.295288
Loss = 0.0738525
Loss = 0.216705
Loss = 0.624451
TEST LOSS = 0.36457
TEST ACC = 532.669 % (8971/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.454468
Epoch 9.2: Loss = 0.371643
Epoch 9.3: Loss = 0.361908
Epoch 9.4: Loss = 0.366669
Epoch 9.5: Loss = 0.286316
Epoch 9.6: Loss = 0.376312
Epoch 9.7: Loss = 0.401901
Epoch 9.8: Loss = 0.40683
Epoch 9.9: Loss = 0.331024
Epoch 9.10: Loss = 0.438248
Epoch 9.11: Loss = 0.395844
Epoch 9.12: Loss = 0.427841
Epoch 9.13: Loss = 0.326416
Epoch 9.14: Loss = 0.347397
Epoch 9.15: Loss = 0.33902
Epoch 9.16: Loss = 0.367783
Epoch 9.17: Loss = 0.300598
Epoch 9.18: Loss = 0.406204
Epoch 9.19: Loss = 0.364624
Epoch 9.20: Loss = 0.415985
Epoch 9.21: Loss = 0.359024
Epoch 9.22: Loss = 0.384399
Epoch 9.23: Loss = 0.388962
Epoch 9.24: Loss = 0.370483
Epoch 9.25: Loss = 0.442642
Epoch 9.26: Loss = 0.34494
Epoch 9.27: Loss = 0.427979
Epoch 9.28: Loss = 0.445541
Epoch 9.29: Loss = 0.37851
Epoch 9.30: Loss = 0.396408
Epoch 9.31: Loss = 0.421066
Epoch 9.32: Loss = 0.412689
Epoch 9.33: Loss = 0.362579
Epoch 9.34: Loss = 0.387299
Epoch 9.35: Loss = 0.32579
Epoch 9.36: Loss = 0.465179
Epoch 9.37: Loss = 0.439407
Epoch 9.38: Loss = 0.38501
Epoch 9.39: Loss = 0.349594
Epoch 9.40: Loss = 0.380859
Epoch 9.41: Loss = 0.291397
Epoch 9.42: Loss = 0.29689
Epoch 9.43: Loss = 0.396378
Epoch 9.44: Loss = 0.321259
Epoch 9.45: Loss = 0.380402
Epoch 9.46: Loss = 0.316666
Epoch 9.47: Loss = 0.487671
Epoch 9.48: Loss = 0.372589
Epoch 9.49: Loss = 0.432571
Epoch 9.50: Loss = 0.508087
Epoch 9.51: Loss = 0.359329
Epoch 9.52: Loss = 0.384598
Epoch 9.53: Loss = 0.489975
Epoch 9.54: Loss = 0.378021
Epoch 9.55: Loss = 0.374329
Epoch 9.56: Loss = 0.431534
Epoch 9.57: Loss = 0.367599
Epoch 9.58: Loss = 0.330963
Epoch 9.59: Loss = 0.355164
Epoch 9.60: Loss = 0.433029
Epoch 9.61: Loss = 0.378403
Epoch 9.62: Loss = 0.445206
Epoch 9.63: Loss = 0.438797
Epoch 9.64: Loss = 0.450668
Epoch 9.65: Loss = 0.438934
Epoch 9.66: Loss = 0.407394
Epoch 9.67: Loss = 0.364044
Epoch 9.68: Loss = 0.29155
Epoch 9.69: Loss = 0.443161
Epoch 9.70: Loss = 0.39537
Epoch 9.71: Loss = 0.48291
Epoch 9.72: Loss = 0.501785
Epoch 9.73: Loss = 0.40419
Epoch 9.74: Loss = 0.449738
Epoch 9.75: Loss = 0.375977
Epoch 9.76: Loss = 0.35704
Epoch 9.77: Loss = 0.412964
Epoch 9.78: Loss = 0.396103
Epoch 9.79: Loss = 0.488632
Epoch 9.80: Loss = 0.368881
Epoch 9.81: Loss = 0.347412
Epoch 9.82: Loss = 0.461319
Epoch 9.83: Loss = 0.435242
Epoch 9.84: Loss = 0.359833
Epoch 9.85: Loss = 0.339523
Epoch 9.86: Loss = 0.347198
Epoch 9.87: Loss = 0.424225
Epoch 9.88: Loss = 0.328766
Epoch 9.89: Loss = 0.498306
Epoch 9.90: Loss = 0.41925
Epoch 9.91: Loss = 0.364471
Epoch 9.92: Loss = 0.395248
Epoch 9.93: Loss = 0.464157
Epoch 9.94: Loss = 0.558792
Epoch 9.95: Loss = 0.399689
Epoch 9.96: Loss = 0.371552
Epoch 9.97: Loss = 0.369705
Epoch 9.98: Loss = 0.368729
Epoch 9.99: Loss = 0.305618
Epoch 9.100: Loss = 0.425781
TRAIN LOSS = 0.393417
TRAIN ACC = 88.9221 % (53356/60000)
Loss = 0.40155
Loss = 0.460373
Loss = 0.603653
Loss = 0.554977
Loss = 0.39032
Loss = 0.397461
Loss = 0.570755
Loss = 0.497375
Loss = 0.316208
Loss = 0.239227
Loss = 0.293869
Loss = 0.222809
Loss = 0.190018
Loss = 0.275024
Loss = 0.0676422
Loss = 0.214172
Loss = 0.626007
TEST LOSS = 0.366766
TEST ACC = 533.559 % (8975/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.421997
Epoch 10.2: Loss = 0.360825
Epoch 10.3: Loss = 0.361298
Epoch 10.4: Loss = 0.343018
Epoch 10.5: Loss = 0.393188
Epoch 10.6: Loss = 0.377274
Epoch 10.7: Loss = 0.379959
Epoch 10.8: Loss = 0.365768
Epoch 10.9: Loss = 0.398254
Epoch 10.10: Loss = 0.385269
Epoch 10.11: Loss = 0.372421
Epoch 10.12: Loss = 0.397614
Epoch 10.13: Loss = 0.389328
Epoch 10.14: Loss = 0.459076
Epoch 10.15: Loss = 0.341354
Epoch 10.16: Loss = 0.383789
Epoch 10.17: Loss = 0.408691
Epoch 10.18: Loss = 0.416885
Epoch 10.19: Loss = 0.372116
Epoch 10.20: Loss = 0.467957
Epoch 10.21: Loss = 0.344986
Epoch 10.22: Loss = 0.430771
Epoch 10.23: Loss = 0.470184
Epoch 10.24: Loss = 0.414551
Epoch 10.25: Loss = 0.380936
Epoch 10.26: Loss = 0.400681
Epoch 10.27: Loss = 0.39621
Epoch 10.28: Loss = 0.397339
Epoch 10.29: Loss = 0.488953
Epoch 10.30: Loss = 0.368851
Epoch 10.31: Loss = 0.431976
Epoch 10.32: Loss = 0.378983
Epoch 10.33: Loss = 0.333817
Epoch 10.34: Loss = 0.343613
Epoch 10.35: Loss = 0.36879
Epoch 10.36: Loss = 0.391052
Epoch 10.37: Loss = 0.359482
Epoch 10.38: Loss = 0.39743
Epoch 10.39: Loss = 0.432205
Epoch 10.40: Loss = 0.371857
Epoch 10.41: Loss = 0.436996
Epoch 10.42: Loss = 0.330582
Epoch 10.43: Loss = 0.33519
Epoch 10.44: Loss = 0.408691
Epoch 10.45: Loss = 0.319687
Epoch 10.46: Loss = 0.422638
Epoch 10.47: Loss = 0.348419
Epoch 10.48: Loss = 0.41655
Epoch 10.49: Loss = 0.396942
Epoch 10.50: Loss = 0.353363
Epoch 10.51: Loss = 0.360687
Epoch 10.52: Loss = 0.405319
Epoch 10.53: Loss = 0.465561
Epoch 10.54: Loss = 0.390366
Epoch 10.55: Loss = 0.456024
Epoch 10.56: Loss = 0.325455
Epoch 10.57: Loss = 0.415588
Epoch 10.58: Loss = 0.325195
Epoch 10.59: Loss = 0.418365
Epoch 10.60: Loss = 0.40773
Epoch 10.61: Loss = 0.334976
Epoch 10.62: Loss = 0.37561
Epoch 10.63: Loss = 0.444107
Epoch 10.64: Loss = 0.413864
Epoch 10.65: Loss = 0.456512
Epoch 10.66: Loss = 0.402649
Epoch 10.67: Loss = 0.447372
Epoch 10.68: Loss = 0.332153
Epoch 10.69: Loss = 0.350266
Epoch 10.70: Loss = 0.438248
Epoch 10.71: Loss = 0.38501
Epoch 10.72: Loss = 0.372406
Epoch 10.73: Loss = 0.358795
Epoch 10.74: Loss = 0.454971
Epoch 10.75: Loss = 0.426132
Epoch 10.76: Loss = 0.392624
Epoch 10.77: Loss = 0.356552
Epoch 10.78: Loss = 0.368332
Epoch 10.79: Loss = 0.367752
Epoch 10.80: Loss = 0.435822
Epoch 10.81: Loss = 0.334381
Epoch 10.82: Loss = 0.404572
Epoch 10.83: Loss = 0.365036
Epoch 10.84: Loss = 0.398682
Epoch 10.85: Loss = 0.408325
Epoch 10.86: Loss = 0.410965
Epoch 10.87: Loss = 0.366959
Epoch 10.88: Loss = 0.458908
Epoch 10.89: Loss = 0.450836
Epoch 10.90: Loss = 0.387299
Epoch 10.91: Loss = 0.527466
Epoch 10.92: Loss = 0.416306
Epoch 10.93: Loss = 0.369125
Epoch 10.94: Loss = 0.343521
Epoch 10.95: Loss = 0.451965
Epoch 10.96: Loss = 0.357635
Epoch 10.97: Loss = 0.434525
Epoch 10.98: Loss = 0.406281
Epoch 10.99: Loss = 0.334625
Epoch 10.100: Loss = 0.419815
TRAIN LOSS = 0.393997
TRAIN ACC = 88.9221 % (53356/60000)
Loss = 0.405212
Loss = 0.455673
Loss = 0.585114
Loss = 0.557419
Loss = 0.391556
Loss = 0.38768
Loss = 0.579849
Loss = 0.501801
Loss = 0.299988
Loss = 0.241623
Loss = 0.305237
Loss = 0.224091
Loss = 0.186615
Loss = 0.285797
Loss = 0.0556335
Loss = 0.22673
Loss = 0.619781
TEST LOSS = 0.366192
TEST ACC = 533.559 % (8971/10000)
The following benchmarks are including preprocessing (offline phase).
Time = 25953.3 seconds 
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
