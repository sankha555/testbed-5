Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.3438
Epoch 1.2: Loss = 2.29416
Epoch 1.3: Loss = 2.2618
Epoch 1.4: Loss = 2.26257
Epoch 1.5: Loss = 2.17223
Epoch 1.6: Loss = 2.1644
Epoch 1.7: Loss = 2.13832
Epoch 1.8: Loss = 2.12292
Epoch 1.9: Loss = 2.08292
Epoch 1.10: Loss = 2.04152
Epoch 1.11: Loss = 2.00075
Epoch 1.12: Loss = 2.00035
Epoch 1.13: Loss = 1.95297
Epoch 1.14: Loss = 1.93297
Epoch 1.15: Loss = 1.97256
Epoch 1.16: Loss = 1.89217
Epoch 1.17: Loss = 1.8586
Epoch 1.18: Loss = 1.81239
Epoch 1.19: Loss = 1.78349
Epoch 1.20: Loss = 1.74181
Epoch 1.21: Loss = 1.71013
Epoch 1.22: Loss = 1.67046
Epoch 1.23: Loss = 1.61746
Epoch 1.24: Loss = 1.7058
Epoch 1.25: Loss = 1.61794
Epoch 1.26: Loss = 1.6199
Epoch 1.27: Loss = 1.57336
Epoch 1.28: Loss = 1.56084
Epoch 1.29: Loss = 1.53445
Epoch 1.30: Loss = 1.62221
Epoch 1.31: Loss = 1.47467
Epoch 1.32: Loss = 1.47408
Epoch 1.33: Loss = 1.40851
Epoch 1.34: Loss = 1.45741
Epoch 1.35: Loss = 1.36624
Epoch 1.36: Loss = 1.46008
Epoch 1.37: Loss = 1.32455
Epoch 1.38: Loss = 1.27374
Epoch 1.39: Loss = 1.27504
Epoch 1.40: Loss = 1.21692
Epoch 1.41: Loss = 1.24196
Epoch 1.42: Loss = 1.20729
Epoch 1.43: Loss = 1.15807
Epoch 1.44: Loss = 1.08351
Epoch 1.45: Loss = 1.21936
Epoch 1.46: Loss = 1.14468
Epoch 1.47: Loss = 1.07039
Epoch 1.48: Loss = 1.15419
Epoch 1.49: Loss = 1.08679
Epoch 1.50: Loss = 1.13762
Epoch 1.51: Loss = 0.969391
Epoch 1.52: Loss = 1.00128
Epoch 1.53: Loss = 1.04007
Epoch 1.54: Loss = 1.05612
Epoch 1.55: Loss = 1.05775
Epoch 1.56: Loss = 0.962006
Epoch 1.57: Loss = 0.870712
Epoch 1.58: Loss = 0.928955
Epoch 1.59: Loss = 0.968155
Epoch 1.60: Loss = 1.06172
Epoch 1.61: Loss = 0.978989
Epoch 1.62: Loss = 1.00952
Epoch 1.63: Loss = 0.992081
Epoch 1.64: Loss = 0.982162
Epoch 1.65: Loss = 1.01291
Epoch 1.66: Loss = 0.895645
Epoch 1.67: Loss = 0.860443
Epoch 1.68: Loss = 0.731171
Epoch 1.69: Loss = 0.808456
Epoch 1.70: Loss = 0.884583
Epoch 1.71: Loss = 0.800537
Epoch 1.72: Loss = 0.785492
Epoch 1.73: Loss = 0.807037
Epoch 1.74: Loss = 0.680573
Epoch 1.75: Loss = 0.831177
Epoch 1.76: Loss = 0.798431
Epoch 1.77: Loss = 0.755264
Epoch 1.78: Loss = 0.730865
Epoch 1.79: Loss = 0.732224
Epoch 1.80: Loss = 0.853088
Epoch 1.81: Loss = 0.707443
Epoch 1.82: Loss = 0.674301
Epoch 1.83: Loss = 0.827011
Epoch 1.84: Loss = 0.74707
Epoch 1.85: Loss = 0.80661
Epoch 1.86: Loss = 0.761765
Epoch 1.87: Loss = 0.64151
Epoch 1.88: Loss = 0.713074
Epoch 1.89: Loss = 0.756317
Epoch 1.90: Loss = 0.657059
Epoch 1.91: Loss = 0.743881
Epoch 1.92: Loss = 0.701691
Epoch 1.93: Loss = 0.733109
Epoch 1.94: Loss = 0.591644
Epoch 1.95: Loss = 0.713242
Epoch 1.96: Loss = 0.68837
Epoch 1.97: Loss = 0.54863
Epoch 1.98: Loss = 0.635117
Epoch 1.99: Loss = 0.705536
Epoch 1.100: Loss = 0.814178
Epoch 1.101: Loss = 0.72113
Epoch 1.102: Loss = 0.6586
Epoch 1.103: Loss = 0.62355
Epoch 1.104: Loss = 0.57135
Epoch 1.105: Loss = 0.686569
Epoch 1.106: Loss = 0.671356
Epoch 1.107: Loss = 0.579422
Epoch 1.108: Loss = 0.626801
Epoch 1.109: Loss = 0.594299
Epoch 1.110: Loss = 0.623016
Epoch 1.111: Loss = 0.523575
Epoch 1.112: Loss = 0.511581
Epoch 1.113: Loss = 0.587067
Epoch 1.114: Loss = 0.491486
Epoch 1.115: Loss = 0.570679
Epoch 1.116: Loss = 0.569565
Epoch 1.117: Loss = 0.472565
Epoch 1.118: Loss = 0.426666
Epoch 1.119: Loss = 0.444153
Epoch 1.120: Loss = 0.440262
TRAIN LOSS = 1.12259
TRAIN ACC = 68.7622 % (41259/60000)
Loss = 0.607773
Loss = 0.649902
Loss = 0.737595
Loss = 0.704102
Loss = 0.722916
Loss = 0.609955
Loss = 0.592667
Loss = 0.769424
Loss = 0.701279
Loss = 0.659454
Loss = 0.333191
Loss = 0.478806
Loss = 0.391266
Loss = 0.536789
Loss = 0.414917
Loss = 0.439224
Loss = 0.394897
Loss = 0.238922
Loss = 0.405792
Loss = 0.658997
TEST LOSS = 0.552393
TEST ACC = 412.59 % (8403/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.535736
Epoch 2.2: Loss = 0.654175
Epoch 2.3: Loss = 0.649948
Epoch 2.4: Loss = 0.509583
Epoch 2.5: Loss = 0.513718
Epoch 2.6: Loss = 0.52948
Epoch 2.7: Loss = 0.553177
Epoch 2.8: Loss = 0.537292
Epoch 2.9: Loss = 0.524719
Epoch 2.10: Loss = 0.540558
Epoch 2.11: Loss = 0.520615
Epoch 2.12: Loss = 0.513336
Epoch 2.13: Loss = 0.471008
Epoch 2.14: Loss = 0.509796
Epoch 2.15: Loss = 0.628799
Epoch 2.16: Loss = 0.595856
Epoch 2.17: Loss = 0.601074
Epoch 2.18: Loss = 0.653946
Epoch 2.19: Loss = 0.536133
Epoch 2.20: Loss = 0.517929
Epoch 2.21: Loss = 0.485367
Epoch 2.22: Loss = 0.470917
Epoch 2.23: Loss = 0.460205
Epoch 2.24: Loss = 0.666367
Epoch 2.25: Loss = 0.546371
Epoch 2.26: Loss = 0.609848
Epoch 2.27: Loss = 0.563614
Epoch 2.28: Loss = 0.586914
Epoch 2.29: Loss = 0.653519
Epoch 2.30: Loss = 0.749359
Epoch 2.31: Loss = 0.483002
Epoch 2.32: Loss = 0.612457
Epoch 2.33: Loss = 0.525894
Epoch 2.34: Loss = 0.608963
Epoch 2.35: Loss = 0.530289
Epoch 2.36: Loss = 0.630371
Epoch 2.37: Loss = 0.46756
Epoch 2.38: Loss = 0.459869
Epoch 2.39: Loss = 0.516403
Epoch 2.40: Loss = 0.483612
Epoch 2.41: Loss = 0.560944
Epoch 2.42: Loss = 0.586395
Epoch 2.43: Loss = 0.474487
Epoch 2.44: Loss = 0.411896
Epoch 2.45: Loss = 0.565109
Epoch 2.46: Loss = 0.557907
Epoch 2.47: Loss = 0.46138
Epoch 2.48: Loss = 0.570663
Epoch 2.49: Loss = 0.524475
Epoch 2.50: Loss = 0.615875
Epoch 2.51: Loss = 0.466751
Epoch 2.52: Loss = 0.454956
Epoch 2.53: Loss = 0.51767
Epoch 2.54: Loss = 0.582916
Epoch 2.55: Loss = 0.556778
Epoch 2.56: Loss = 0.47876
Epoch 2.57: Loss = 0.441879
Epoch 2.58: Loss = 0.508011
Epoch 2.59: Loss = 0.573959
Epoch 2.60: Loss = 0.626892
Epoch 2.61: Loss = 0.569534
Epoch 2.62: Loss = 0.597855
Epoch 2.63: Loss = 0.629684
Epoch 2.64: Loss = 0.59256
Epoch 2.65: Loss = 0.703003
Epoch 2.66: Loss = 0.508301
Epoch 2.67: Loss = 0.52037
Epoch 2.68: Loss = 0.346115
Epoch 2.69: Loss = 0.441833
Epoch 2.70: Loss = 0.597153
Epoch 2.71: Loss = 0.427841
Epoch 2.72: Loss = 0.456299
Epoch 2.73: Loss = 0.499191
Epoch 2.74: Loss = 0.381027
Epoch 2.75: Loss = 0.633423
Epoch 2.76: Loss = 0.537506
Epoch 2.77: Loss = 0.464066
Epoch 2.78: Loss = 0.470078
Epoch 2.79: Loss = 0.546127
Epoch 2.80: Loss = 0.560867
Epoch 2.81: Loss = 0.452988
Epoch 2.82: Loss = 0.404648
Epoch 2.83: Loss = 0.582672
Epoch 2.84: Loss = 0.507278
Epoch 2.85: Loss = 0.656479
Epoch 2.86: Loss = 0.563248
Epoch 2.87: Loss = 0.389069
Epoch 2.88: Loss = 0.483383
Epoch 2.89: Loss = 0.546555
Epoch 2.90: Loss = 0.438934
Epoch 2.91: Loss = 0.553757
Epoch 2.92: Loss = 0.526184
Epoch 2.93: Loss = 0.578033
Epoch 2.94: Loss = 0.394897
Epoch 2.95: Loss = 0.483368
Epoch 2.96: Loss = 0.530975
Epoch 2.97: Loss = 0.416092
Epoch 2.98: Loss = 0.462631
Epoch 2.99: Loss = 0.563004
Epoch 2.100: Loss = 0.626022
Epoch 2.101: Loss = 0.606003
Epoch 2.102: Loss = 0.540405
Epoch 2.103: Loss = 0.507065
Epoch 2.104: Loss = 0.425354
Epoch 2.105: Loss = 0.557465
Epoch 2.106: Loss = 0.54892
Epoch 2.107: Loss = 0.445557
Epoch 2.108: Loss = 0.500626
Epoch 2.109: Loss = 0.442596
Epoch 2.110: Loss = 0.481369
Epoch 2.111: Loss = 0.379456
Epoch 2.112: Loss = 0.361557
Epoch 2.113: Loss = 0.451111
Epoch 2.114: Loss = 0.364288
Epoch 2.115: Loss = 0.429489
Epoch 2.116: Loss = 0.443939
Epoch 2.117: Loss = 0.320816
Epoch 2.118: Loss = 0.264099
Epoch 2.119: Loss = 0.323593
Epoch 2.120: Loss = 0.344421
TRAIN LOSS = 0.517731
TRAIN ACC = 84.2834 % (50572/60000)
Loss = 0.477646
Loss = 0.574997
Loss = 0.639023
Loss = 0.631104
Loss = 0.657867
Loss = 0.505203
Loss = 0.491653
Loss = 0.691254
Loss = 0.597015
Loss = 0.585541
Loss = 0.222153
Loss = 0.352371
Loss = 0.323624
Loss = 0.404968
Loss = 0.252518
Loss = 0.323486
Loss = 0.267197
Loss = 0.130432
Loss = 0.278
Loss = 0.600952
TEST LOSS = 0.45035
TEST ACC = 505.719 % (8646/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.428314
Epoch 3.2: Loss = 0.571884
Epoch 3.3: Loss = 0.548721
Epoch 3.4: Loss = 0.406586
Epoch 3.5: Loss = 0.433197
Epoch 3.6: Loss = 0.408508
Epoch 3.7: Loss = 0.430817
Epoch 3.8: Loss = 0.460724
Epoch 3.9: Loss = 0.42337
Epoch 3.10: Loss = 0.43988
Epoch 3.11: Loss = 0.471069
Epoch 3.12: Loss = 0.417526
Epoch 3.13: Loss = 0.380005
Epoch 3.14: Loss = 0.417892
Epoch 3.15: Loss = 0.499908
Epoch 3.16: Loss = 0.513763
Epoch 3.17: Loss = 0.50325
Epoch 3.18: Loss = 0.617859
Epoch 3.19: Loss = 0.474243
Epoch 3.20: Loss = 0.464706
Epoch 3.21: Loss = 0.374969
Epoch 3.22: Loss = 0.366241
Epoch 3.23: Loss = 0.398239
Epoch 3.24: Loss = 0.611847
Epoch 3.25: Loss = 0.492355
Epoch 3.26: Loss = 0.564606
Epoch 3.27: Loss = 0.50647
Epoch 3.28: Loss = 0.502335
Epoch 3.29: Loss = 0.609833
Epoch 3.30: Loss = 0.662888
Epoch 3.31: Loss = 0.408997
Epoch 3.32: Loss = 0.508957
Epoch 3.33: Loss = 0.452042
Epoch 3.34: Loss = 0.538605
Epoch 3.35: Loss = 0.431625
Epoch 3.36: Loss = 0.548401
Epoch 3.37: Loss = 0.398941
Epoch 3.38: Loss = 0.401184
Epoch 3.39: Loss = 0.445114
Epoch 3.40: Loss = 0.432358
Epoch 3.41: Loss = 0.44278
Epoch 3.42: Loss = 0.577682
Epoch 3.43: Loss = 0.388306
Epoch 3.44: Loss = 0.334686
Epoch 3.45: Loss = 0.463058
Epoch 3.46: Loss = 0.488861
Epoch 3.47: Loss = 0.404343
Epoch 3.48: Loss = 0.505569
Epoch 3.49: Loss = 0.469025
Epoch 3.50: Loss = 0.544907
Epoch 3.51: Loss = 0.384064
Epoch 3.52: Loss = 0.391113
Epoch 3.53: Loss = 0.470596
Epoch 3.54: Loss = 0.543198
Epoch 3.55: Loss = 0.469376
Epoch 3.56: Loss = 0.424316
Epoch 3.57: Loss = 0.393753
Epoch 3.58: Loss = 0.453629
Epoch 3.59: Loss = 0.521652
Epoch 3.60: Loss = 0.569077
Epoch 3.61: Loss = 0.497589
Epoch 3.62: Loss = 0.517105
Epoch 3.63: Loss = 0.603241
Epoch 3.64: Loss = 0.553741
Epoch 3.65: Loss = 0.634689
Epoch 3.66: Loss = 0.452469
Epoch 3.67: Loss = 0.4552
Epoch 3.68: Loss = 0.291428
Epoch 3.69: Loss = 0.379898
Epoch 3.70: Loss = 0.555206
Epoch 3.71: Loss = 0.360855
Epoch 3.72: Loss = 0.371048
Epoch 3.73: Loss = 0.438492
Epoch 3.74: Loss = 0.322937
Epoch 3.75: Loss = 0.651062
Epoch 3.76: Loss = 0.476105
Epoch 3.77: Loss = 0.37442
Epoch 3.78: Loss = 0.426392
Epoch 3.79: Loss = 0.495132
Epoch 3.80: Loss = 0.478119
Epoch 3.81: Loss = 0.392014
Epoch 3.82: Loss = 0.348495
Epoch 3.83: Loss = 0.528107
Epoch 3.84: Loss = 0.459717
Epoch 3.85: Loss = 0.639648
Epoch 3.86: Loss = 0.513733
Epoch 3.87: Loss = 0.333206
Epoch 3.88: Loss = 0.443634
Epoch 3.89: Loss = 0.4785
Epoch 3.90: Loss = 0.413681
Epoch 3.91: Loss = 0.517563
Epoch 3.92: Loss = 0.527161
Epoch 3.93: Loss = 0.535477
Epoch 3.94: Loss = 0.346649
Epoch 3.95: Loss = 0.438705
Epoch 3.96: Loss = 0.507736
Epoch 3.97: Loss = 0.397385
Epoch 3.98: Loss = 0.408127
Epoch 3.99: Loss = 0.51535
Epoch 3.100: Loss = 0.559433
Epoch 3.101: Loss = 0.586823
Epoch 3.102: Loss = 0.487854
Epoch 3.103: Loss = 0.451675
Epoch 3.104: Loss = 0.389877
Epoch 3.105: Loss = 0.535202
Epoch 3.106: Loss = 0.554855
Epoch 3.107: Loss = 0.385513
Epoch 3.108: Loss = 0.455276
Epoch 3.109: Loss = 0.390198
Epoch 3.110: Loss = 0.45607
Epoch 3.111: Loss = 0.375
Epoch 3.112: Loss = 0.3517
Epoch 3.113: Loss = 0.416412
Epoch 3.114: Loss = 0.346313
Epoch 3.115: Loss = 0.361343
Epoch 3.116: Loss = 0.424393
Epoch 3.117: Loss = 0.260712
Epoch 3.118: Loss = 0.24884
Epoch 3.119: Loss = 0.305023
Epoch 3.120: Loss = 0.331833
TRAIN LOSS = 0.458023
TRAIN ACC = 86.3937 % (51839/60000)
Loss = 0.433655
Loss = 0.527039
Loss = 0.581879
Loss = 0.607468
Loss = 0.615356
Loss = 0.422302
Loss = 0.450668
Loss = 0.671371
Loss = 0.564514
Loss = 0.548157
Loss = 0.203339
Loss = 0.327072
Loss = 0.341278
Loss = 0.39093
Loss = 0.226501
Loss = 0.300034
Loss = 0.230087
Loss = 0.0921021
Loss = 0.242111
Loss = 0.600739
TEST LOSS = 0.41883
TEST ACC = 518.388 % (8771/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.415329
Epoch 4.2: Loss = 0.523727
Epoch 4.3: Loss = 0.553467
Epoch 4.4: Loss = 0.365372
Epoch 4.5: Loss = 0.403152
Epoch 4.6: Loss = 0.37886
Epoch 4.7: Loss = 0.38472
Epoch 4.8: Loss = 0.418701
Epoch 4.9: Loss = 0.394272
Epoch 4.10: Loss = 0.436386
Epoch 4.11: Loss = 0.459656
Epoch 4.12: Loss = 0.402985
Epoch 4.13: Loss = 0.373474
Epoch 4.14: Loss = 0.402908
Epoch 4.15: Loss = 0.484009
Epoch 4.16: Loss = 0.480621
Epoch 4.17: Loss = 0.523254
Epoch 4.18: Loss = 0.635742
Epoch 4.19: Loss = 0.458023
Epoch 4.20: Loss = 0.440857
Epoch 4.21: Loss = 0.354736
Epoch 4.22: Loss = 0.335709
Epoch 4.23: Loss = 0.369125
Epoch 4.24: Loss = 0.625458
Epoch 4.25: Loss = 0.467636
Epoch 4.26: Loss = 0.540939
Epoch 4.27: Loss = 0.481125
Epoch 4.28: Loss = 0.482574
Epoch 4.29: Loss = 0.564011
Epoch 4.30: Loss = 0.618774
Epoch 4.31: Loss = 0.366058
Epoch 4.32: Loss = 0.497299
Epoch 4.33: Loss = 0.430984
Epoch 4.34: Loss = 0.524673
Epoch 4.35: Loss = 0.427917
Epoch 4.36: Loss = 0.557236
Epoch 4.37: Loss = 0.361298
Epoch 4.38: Loss = 0.375183
Epoch 4.39: Loss = 0.429367
Epoch 4.40: Loss = 0.415787
Epoch 4.41: Loss = 0.401688
Epoch 4.42: Loss = 0.565506
Epoch 4.43: Loss = 0.359146
Epoch 4.44: Loss = 0.31955
Epoch 4.45: Loss = 0.450241
Epoch 4.46: Loss = 0.449844
Epoch 4.47: Loss = 0.383652
Epoch 4.48: Loss = 0.491562
Epoch 4.49: Loss = 0.46373
Epoch 4.50: Loss = 0.519302
Epoch 4.51: Loss = 0.354279
Epoch 4.52: Loss = 0.359482
Epoch 4.53: Loss = 0.434097
Epoch 4.54: Loss = 0.533493
Epoch 4.55: Loss = 0.453125
Epoch 4.56: Loss = 0.401779
Epoch 4.57: Loss = 0.373367
Epoch 4.58: Loss = 0.440018
Epoch 4.59: Loss = 0.536911
Epoch 4.60: Loss = 0.549866
Epoch 4.61: Loss = 0.445511
Epoch 4.62: Loss = 0.538452
Epoch 4.63: Loss = 0.588959
Epoch 4.64: Loss = 0.538055
Epoch 4.65: Loss = 0.631699
Epoch 4.66: Loss = 0.438766
Epoch 4.67: Loss = 0.43573
Epoch 4.68: Loss = 0.27565
Epoch 4.69: Loss = 0.351288
Epoch 4.70: Loss = 0.538147
Epoch 4.71: Loss = 0.374435
Epoch 4.72: Loss = 0.347061
Epoch 4.73: Loss = 0.427292
Epoch 4.74: Loss = 0.336594
Epoch 4.75: Loss = 0.644669
Epoch 4.76: Loss = 0.469513
Epoch 4.77: Loss = 0.357849
Epoch 4.78: Loss = 0.43013
Epoch 4.79: Loss = 0.532135
Epoch 4.80: Loss = 0.477493
Epoch 4.81: Loss = 0.382675
Epoch 4.82: Loss = 0.33252
Epoch 4.83: Loss = 0.529861
Epoch 4.84: Loss = 0.438889
Epoch 4.85: Loss = 0.607422
Epoch 4.86: Loss = 0.479248
Epoch 4.87: Loss = 0.308319
Epoch 4.88: Loss = 0.428223
Epoch 4.89: Loss = 0.465683
Epoch 4.90: Loss = 0.385117
Epoch 4.91: Loss = 0.50621
Epoch 4.92: Loss = 0.508499
Epoch 4.93: Loss = 0.564224
Epoch 4.94: Loss = 0.352478
Epoch 4.95: Loss = 0.440979
Epoch 4.96: Loss = 0.551086
Epoch 4.97: Loss = 0.395386
Epoch 4.98: Loss = 0.44458
Epoch 4.99: Loss = 0.538239
Epoch 4.100: Loss = 0.552017
Epoch 4.101: Loss = 0.599884
Epoch 4.102: Loss = 0.485947
Epoch 4.103: Loss = 0.432205
Epoch 4.104: Loss = 0.392441
Epoch 4.105: Loss = 0.563171
Epoch 4.106: Loss = 0.576553
Epoch 4.107: Loss = 0.365402
Epoch 4.108: Loss = 0.451797
Epoch 4.109: Loss = 0.414719
Epoch 4.110: Loss = 0.471817
Epoch 4.111: Loss = 0.386871
Epoch 4.112: Loss = 0.32962
Epoch 4.113: Loss = 0.413864
Epoch 4.114: Loss = 0.334793
Epoch 4.115: Loss = 0.342926
Epoch 4.116: Loss = 0.406448
Epoch 4.117: Loss = 0.268921
Epoch 4.118: Loss = 0.234894
Epoch 4.119: Loss = 0.326294
Epoch 4.120: Loss = 0.331192
TRAIN LOSS = 0.446014
TRAIN ACC = 87.3856 % (52434/60000)
Loss = 0.426437
Loss = 0.536224
Loss = 0.568924
Loss = 0.606537
Loss = 0.606125
Loss = 0.442444
Loss = 0.427872
Loss = 0.671112
Loss = 0.58403
Loss = 0.549225
Loss = 0.20369
Loss = 0.322021
Loss = 0.347748
Loss = 0.389862
Loss = 0.215302
Loss = 0.360733
Loss = 0.237701
Loss = 0.0705261
Loss = 0.248428
Loss = 0.572983
TEST LOSS = 0.419396
TEST ACC = 524.339 % (8831/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.39447
Epoch 5.2: Loss = 0.506287
Epoch 5.3: Loss = 0.595398
Epoch 5.4: Loss = 0.372223
Epoch 5.5: Loss = 0.381088
Epoch 5.6: Loss = 0.361877
Epoch 5.7: Loss = 0.376251
Epoch 5.8: Loss = 0.414734
Epoch 5.9: Loss = 0.359146
Epoch 5.10: Loss = 0.394104
Epoch 5.11: Loss = 0.441605
Epoch 5.12: Loss = 0.412216
Epoch 5.13: Loss = 0.359406
Epoch 5.14: Loss = 0.374557
Epoch 5.15: Loss = 0.473755
Epoch 5.16: Loss = 0.477478
Epoch 5.17: Loss = 0.540604
Epoch 5.18: Loss = 0.653687
Epoch 5.19: Loss = 0.453186
Epoch 5.20: Loss = 0.426285
Epoch 5.21: Loss = 0.330475
Epoch 5.22: Loss = 0.351898
Epoch 5.23: Loss = 0.368271
Epoch 5.24: Loss = 0.616287
Epoch 5.25: Loss = 0.481491
Epoch 5.26: Loss = 0.537643
Epoch 5.27: Loss = 0.487228
Epoch 5.28: Loss = 0.487137
Epoch 5.29: Loss = 0.561249
Epoch 5.30: Loss = 0.617172
Epoch 5.31: Loss = 0.369232
Epoch 5.32: Loss = 0.491409
Epoch 5.33: Loss = 0.41568
Epoch 5.34: Loss = 0.495117
Epoch 5.35: Loss = 0.412811
Epoch 5.36: Loss = 0.517563
Epoch 5.37: Loss = 0.363892
Epoch 5.38: Loss = 0.381348
Epoch 5.39: Loss = 0.435654
Epoch 5.40: Loss = 0.408585
Epoch 5.41: Loss = 0.393555
Epoch 5.42: Loss = 0.575485
Epoch 5.43: Loss = 0.369614
Epoch 5.44: Loss = 0.29509
Epoch 5.45: Loss = 0.437241
Epoch 5.46: Loss = 0.460754
Epoch 5.47: Loss = 0.37262
Epoch 5.48: Loss = 0.489792
Epoch 5.49: Loss = 0.455276
Epoch 5.50: Loss = 0.513245
Epoch 5.51: Loss = 0.359848
Epoch 5.52: Loss = 0.358536
Epoch 5.53: Loss = 0.431076
Epoch 5.54: Loss = 0.543091
Epoch 5.55: Loss = 0.449982
Epoch 5.56: Loss = 0.413376
Epoch 5.57: Loss = 0.402054
Epoch 5.58: Loss = 0.455505
Epoch 5.59: Loss = 0.521927
Epoch 5.60: Loss = 0.540955
Epoch 5.61: Loss = 0.469772
Epoch 5.62: Loss = 0.530121
Epoch 5.63: Loss = 0.606689
Epoch 5.64: Loss = 0.546066
Epoch 5.65: Loss = 0.613449
Epoch 5.66: Loss = 0.428024
Epoch 5.67: Loss = 0.431778
Epoch 5.68: Loss = 0.281158
Epoch 5.69: Loss = 0.363022
Epoch 5.70: Loss = 0.515213
Epoch 5.71: Loss = 0.380325
Epoch 5.72: Loss = 0.334824
Epoch 5.73: Loss = 0.439911
Epoch 5.74: Loss = 0.345047
Epoch 5.75: Loss = 0.742355
Epoch 5.76: Loss = 0.461533
Epoch 5.77: Loss = 0.369461
Epoch 5.78: Loss = 0.427124
Epoch 5.79: Loss = 0.575531
Epoch 5.80: Loss = 0.494629
Epoch 5.81: Loss = 0.36261
Epoch 5.82: Loss = 0.325531
Epoch 5.83: Loss = 0.527481
Epoch 5.84: Loss = 0.439987
Epoch 5.85: Loss = 0.638214
Epoch 5.86: Loss = 0.483292
Epoch 5.87: Loss = 0.317688
Epoch 5.88: Loss = 0.419861
Epoch 5.89: Loss = 0.495239
Epoch 5.90: Loss = 0.356903
Epoch 5.91: Loss = 0.521805
Epoch 5.92: Loss = 0.514084
Epoch 5.93: Loss = 0.578339
Epoch 5.94: Loss = 0.319763
Epoch 5.95: Loss = 0.399017
Epoch 5.96: Loss = 0.517563
Epoch 5.97: Loss = 0.400879
Epoch 5.98: Loss = 0.434204
Epoch 5.99: Loss = 0.524231
Epoch 5.100: Loss = 0.561661
Epoch 5.101: Loss = 0.607986
Epoch 5.102: Loss = 0.507614
Epoch 5.103: Loss = 0.450882
Epoch 5.104: Loss = 0.39769
Epoch 5.105: Loss = 0.588791
Epoch 5.106: Loss = 0.595795
Epoch 5.107: Loss = 0.365402
Epoch 5.108: Loss = 0.458755
Epoch 5.109: Loss = 0.403717
Epoch 5.110: Loss = 0.457581
Epoch 5.111: Loss = 0.376831
Epoch 5.112: Loss = 0.359268
Epoch 5.113: Loss = 0.408768
Epoch 5.114: Loss = 0.338242
Epoch 5.115: Loss = 0.34317
Epoch 5.116: Loss = 0.436722
Epoch 5.117: Loss = 0.232224
Epoch 5.118: Loss = 0.214401
Epoch 5.119: Loss = 0.294693
Epoch 5.120: Loss = 0.346268
TRAIN LOSS = 0.445145
TRAIN ACC = 87.8235 % (52697/60000)
Loss = 0.429199
Loss = 0.539856
Loss = 0.571136
Loss = 0.607941
Loss = 0.637238
Loss = 0.411545
Loss = 0.437363
Loss = 0.679382
Loss = 0.60498
Loss = 0.553665
Loss = 0.208099
Loss = 0.300659
Loss = 0.327682
Loss = 0.403503
Loss = 0.214233
Loss = 0.342697
Loss = 0.233276
Loss = 0.0765533
Loss = 0.223236
Loss = 0.61879
TEST LOSS = 0.421052
TEST ACC = 526.968 % (8859/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.394485
Epoch 6.2: Loss = 0.510712
Epoch 6.3: Loss = 0.554535
Epoch 6.4: Loss = 0.385681
Epoch 6.5: Loss = 0.369354
Epoch 6.6: Loss = 0.352402
Epoch 6.7: Loss = 0.360565
Epoch 6.8: Loss = 0.400848
Epoch 6.9: Loss = 0.380615
Epoch 6.10: Loss = 0.410538
Epoch 6.11: Loss = 0.435699
Epoch 6.12: Loss = 0.442657
Epoch 6.13: Loss = 0.356201
Epoch 6.14: Loss = 0.385513
Epoch 6.15: Loss = 0.473587
Epoch 6.16: Loss = 0.463135
Epoch 6.17: Loss = 0.521011
Epoch 6.18: Loss = 0.681519
Epoch 6.19: Loss = 0.444046
Epoch 6.20: Loss = 0.415695
Epoch 6.21: Loss = 0.340561
Epoch 6.22: Loss = 0.365631
Epoch 6.23: Loss = 0.361252
Epoch 6.24: Loss = 0.643936
Epoch 6.25: Loss = 0.502777
Epoch 6.26: Loss = 0.533188
Epoch 6.27: Loss = 0.537384
Epoch 6.28: Loss = 0.477341
Epoch 6.29: Loss = 0.602875
Epoch 6.30: Loss = 0.624634
Epoch 6.31: Loss = 0.401596
Epoch 6.32: Loss = 0.475525
Epoch 6.33: Loss = 0.448013
Epoch 6.34: Loss = 0.519531
Epoch 6.35: Loss = 0.427979
Epoch 6.36: Loss = 0.503052
Epoch 6.37: Loss = 0.383636
Epoch 6.38: Loss = 0.406693
Epoch 6.39: Loss = 0.424942
Epoch 6.40: Loss = 0.450287
Epoch 6.41: Loss = 0.393188
Epoch 6.42: Loss = 0.595093
Epoch 6.43: Loss = 0.361526
Epoch 6.44: Loss = 0.321625
Epoch 6.45: Loss = 0.431549
Epoch 6.46: Loss = 0.465088
Epoch 6.47: Loss = 0.379791
Epoch 6.48: Loss = 0.479996
Epoch 6.49: Loss = 0.439148
Epoch 6.50: Loss = 0.51268
Epoch 6.51: Loss = 0.353821
Epoch 6.52: Loss = 0.342804
Epoch 6.53: Loss = 0.433914
Epoch 6.54: Loss = 0.547989
Epoch 6.55: Loss = 0.432816
Epoch 6.56: Loss = 0.417267
Epoch 6.57: Loss = 0.419235
Epoch 6.58: Loss = 0.425598
Epoch 6.59: Loss = 0.530746
Epoch 6.60: Loss = 0.563553
Epoch 6.61: Loss = 0.466904
Epoch 6.62: Loss = 0.544022
Epoch 6.63: Loss = 0.605103
Epoch 6.64: Loss = 0.570984
Epoch 6.65: Loss = 0.60556
Epoch 6.66: Loss = 0.390091
Epoch 6.67: Loss = 0.447815
Epoch 6.68: Loss = 0.284653
Epoch 6.69: Loss = 0.349182
Epoch 6.70: Loss = 0.527481
Epoch 6.71: Loss = 0.380951
Epoch 6.72: Loss = 0.30603
Epoch 6.73: Loss = 0.427536
Epoch 6.74: Loss = 0.355942
Epoch 6.75: Loss = 0.704834
Epoch 6.76: Loss = 0.462219
Epoch 6.77: Loss = 0.362381
Epoch 6.78: Loss = 0.461761
Epoch 6.79: Loss = 0.576355
Epoch 6.80: Loss = 0.494507
Epoch 6.81: Loss = 0.359283
Epoch 6.82: Loss = 0.318085
Epoch 6.83: Loss = 0.535568
Epoch 6.84: Loss = 0.4375
Epoch 6.85: Loss = 0.655533
Epoch 6.86: Loss = 0.497437
Epoch 6.87: Loss = 0.334198
Epoch 6.88: Loss = 0.438324
Epoch 6.89: Loss = 0.471771
Epoch 6.90: Loss = 0.392227
Epoch 6.91: Loss = 0.513367
Epoch 6.92: Loss = 0.530304
Epoch 6.93: Loss = 0.603989
Epoch 6.94: Loss = 0.302261
Epoch 6.95: Loss = 0.423828
Epoch 6.96: Loss = 0.504837
Epoch 6.97: Loss = 0.389252
Epoch 6.98: Loss = 0.443817
Epoch 6.99: Loss = 0.517136
Epoch 6.100: Loss = 0.539017
Epoch 6.101: Loss = 0.626404
Epoch 6.102: Loss = 0.481567
Epoch 6.103: Loss = 0.41362
Epoch 6.104: Loss = 0.396759
Epoch 6.105: Loss = 0.580719
Epoch 6.106: Loss = 0.58371
Epoch 6.107: Loss = 0.360535
Epoch 6.108: Loss = 0.449539
Epoch 6.109: Loss = 0.402832
Epoch 6.110: Loss = 0.465454
Epoch 6.111: Loss = 0.367752
Epoch 6.112: Loss = 0.327133
Epoch 6.113: Loss = 0.390396
Epoch 6.114: Loss = 0.323242
Epoch 6.115: Loss = 0.311707
Epoch 6.116: Loss = 0.423187
Epoch 6.117: Loss = 0.222794
Epoch 6.118: Loss = 0.203049
Epoch 6.119: Loss = 0.343109
Epoch 6.120: Loss = 0.35231
TRAIN LOSS = 0.446457
TRAIN ACC = 88.0203 % (52815/60000)
Loss = 0.427292
Loss = 0.538589
Loss = 0.594574
Loss = 0.588028
Loss = 0.642609
Loss = 0.410233
Loss = 0.432098
Loss = 0.694153
Loss = 0.564148
Loss = 0.516479
Loss = 0.17215
Loss = 0.319702
Loss = 0.35376
Loss = 0.378357
Loss = 0.214462
Loss = 0.31456
Loss = 0.193054
Loss = 0.0635529
Loss = 0.2276
Loss = 0.603195
TEST LOSS = 0.41243
TEST ACC = 528.149 % (8913/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.398178
Epoch 7.2: Loss = 0.473572
Epoch 7.3: Loss = 0.54715
Epoch 7.4: Loss = 0.353577
Epoch 7.5: Loss = 0.380646
Epoch 7.6: Loss = 0.355484
Epoch 7.7: Loss = 0.368256
Epoch 7.8: Loss = 0.429733
Epoch 7.9: Loss = 0.368454
Epoch 7.10: Loss = 0.391327
Epoch 7.11: Loss = 0.431839
Epoch 7.12: Loss = 0.437988
Epoch 7.13: Loss = 0.35585
Epoch 7.14: Loss = 0.392441
Epoch 7.15: Loss = 0.479538
Epoch 7.16: Loss = 0.475464
Epoch 7.17: Loss = 0.513504
Epoch 7.18: Loss = 0.702225
Epoch 7.19: Loss = 0.476929
Epoch 7.20: Loss = 0.440186
Epoch 7.21: Loss = 0.334091
Epoch 7.22: Loss = 0.34874
Epoch 7.23: Loss = 0.35051
Epoch 7.24: Loss = 0.590759
Epoch 7.25: Loss = 0.440033
Epoch 7.26: Loss = 0.555145
Epoch 7.27: Loss = 0.519318
Epoch 7.28: Loss = 0.494644
Epoch 7.29: Loss = 0.569473
Epoch 7.30: Loss = 0.586716
Epoch 7.31: Loss = 0.396454
Epoch 7.32: Loss = 0.472412
Epoch 7.33: Loss = 0.40155
Epoch 7.34: Loss = 0.518036
Epoch 7.35: Loss = 0.445541
Epoch 7.36: Loss = 0.524734
Epoch 7.37: Loss = 0.385468
Epoch 7.38: Loss = 0.415695
Epoch 7.39: Loss = 0.429626
Epoch 7.40: Loss = 0.424454
Epoch 7.41: Loss = 0.408463
Epoch 7.42: Loss = 0.613358
Epoch 7.43: Loss = 0.395828
Epoch 7.44: Loss = 0.316742
Epoch 7.45: Loss = 0.453873
Epoch 7.46: Loss = 0.455353
Epoch 7.47: Loss = 0.39592
Epoch 7.48: Loss = 0.471771
Epoch 7.49: Loss = 0.441483
Epoch 7.50: Loss = 0.541534
Epoch 7.51: Loss = 0.364716
Epoch 7.52: Loss = 0.357544
Epoch 7.53: Loss = 0.447144
Epoch 7.54: Loss = 0.57132
Epoch 7.55: Loss = 0.448257
Epoch 7.56: Loss = 0.442215
Epoch 7.57: Loss = 0.411575
Epoch 7.58: Loss = 0.402573
Epoch 7.59: Loss = 0.530167
Epoch 7.60: Loss = 0.570084
Epoch 7.61: Loss = 0.471466
Epoch 7.62: Loss = 0.518402
Epoch 7.63: Loss = 0.610687
Epoch 7.64: Loss = 0.562317
Epoch 7.65: Loss = 0.648621
Epoch 7.66: Loss = 0.399841
Epoch 7.67: Loss = 0.435089
Epoch 7.68: Loss = 0.2939
Epoch 7.69: Loss = 0.338852
Epoch 7.70: Loss = 0.531525
Epoch 7.71: Loss = 0.386292
Epoch 7.72: Loss = 0.330872
Epoch 7.73: Loss = 0.449295
Epoch 7.74: Loss = 0.335159
Epoch 7.75: Loss = 0.762238
Epoch 7.76: Loss = 0.475113
Epoch 7.77: Loss = 0.365433
Epoch 7.78: Loss = 0.482788
Epoch 7.79: Loss = 0.607895
Epoch 7.80: Loss = 0.506805
Epoch 7.81: Loss = 0.345367
Epoch 7.82: Loss = 0.31662
Epoch 7.83: Loss = 0.542313
Epoch 7.84: Loss = 0.450348
Epoch 7.85: Loss = 0.69281
Epoch 7.86: Loss = 0.525391
Epoch 7.87: Loss = 0.333054
Epoch 7.88: Loss = 0.443283
Epoch 7.89: Loss = 0.498978
Epoch 7.90: Loss = 0.368042
Epoch 7.91: Loss = 0.516907
Epoch 7.92: Loss = 0.510651
Epoch 7.93: Loss = 0.616211
Epoch 7.94: Loss = 0.335022
Epoch 7.95: Loss = 0.445801
Epoch 7.96: Loss = 0.494461
Epoch 7.97: Loss = 0.395081
Epoch 7.98: Loss = 0.440613
Epoch 7.99: Loss = 0.541809
Epoch 7.100: Loss = 0.578201
Epoch 7.101: Loss = 0.643555
Epoch 7.102: Loss = 0.48642
Epoch 7.103: Loss = 0.438934
Epoch 7.104: Loss = 0.411224
Epoch 7.105: Loss = 0.599075
Epoch 7.106: Loss = 0.614868
Epoch 7.107: Loss = 0.34111
Epoch 7.108: Loss = 0.487427
Epoch 7.109: Loss = 0.413315
Epoch 7.110: Loss = 0.503006
Epoch 7.111: Loss = 0.385605
Epoch 7.112: Loss = 0.34761
Epoch 7.113: Loss = 0.393906
Epoch 7.114: Loss = 0.339706
Epoch 7.115: Loss = 0.330383
Epoch 7.116: Loss = 0.432877
Epoch 7.117: Loss = 0.22377
Epoch 7.118: Loss = 0.226517
Epoch 7.119: Loss = 0.308304
Epoch 7.120: Loss = 0.33989
TRAIN LOSS = 0.451553
TRAIN ACC = 88.2477 % (52951/60000)
Loss = 0.448654
Loss = 0.582703
Loss = 0.656097
Loss = 0.644989
Loss = 0.664825
Loss = 0.42421
Loss = 0.466614
Loss = 0.711304
Loss = 0.617691
Loss = 0.537918
Loss = 0.177765
Loss = 0.317093
Loss = 0.38942
Loss = 0.418076
Loss = 0.223389
Loss = 0.35054
Loss = 0.212112
Loss = 0.0680695
Loss = 0.236053
Loss = 0.627289
TEST LOSS = 0.43874
TEST ACC = 529.509 % (8915/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.420471
Epoch 8.2: Loss = 0.527664
Epoch 8.3: Loss = 0.569122
Epoch 8.4: Loss = 0.360245
Epoch 8.5: Loss = 0.383255
Epoch 8.6: Loss = 0.379868
Epoch 8.7: Loss = 0.368561
Epoch 8.8: Loss = 0.441269
Epoch 8.9: Loss = 0.38855
Epoch 8.10: Loss = 0.422775
Epoch 8.11: Loss = 0.445404
Epoch 8.12: Loss = 0.453339
Epoch 8.13: Loss = 0.364517
Epoch 8.14: Loss = 0.385498
Epoch 8.15: Loss = 0.457748
Epoch 8.16: Loss = 0.479782
Epoch 8.17: Loss = 0.525284
Epoch 8.18: Loss = 0.728424
Epoch 8.19: Loss = 0.500763
Epoch 8.20: Loss = 0.420319
Epoch 8.21: Loss = 0.371399
Epoch 8.22: Loss = 0.340256
Epoch 8.23: Loss = 0.354553
Epoch 8.24: Loss = 0.566391
Epoch 8.25: Loss = 0.441101
Epoch 8.26: Loss = 0.58548
Epoch 8.27: Loss = 0.55423
Epoch 8.28: Loss = 0.516663
Epoch 8.29: Loss = 0.596527
Epoch 8.30: Loss = 0.597198
Epoch 8.31: Loss = 0.417557
Epoch 8.32: Loss = 0.520325
Epoch 8.33: Loss = 0.427963
Epoch 8.34: Loss = 0.543365
Epoch 8.35: Loss = 0.460236
Epoch 8.36: Loss = 0.522919
Epoch 8.37: Loss = 0.383041
Epoch 8.38: Loss = 0.432449
Epoch 8.39: Loss = 0.434418
Epoch 8.40: Loss = 0.419495
Epoch 8.41: Loss = 0.416199
Epoch 8.42: Loss = 0.610916
Epoch 8.43: Loss = 0.389862
Epoch 8.44: Loss = 0.323502
Epoch 8.45: Loss = 0.474319
Epoch 8.46: Loss = 0.501038
Epoch 8.47: Loss = 0.385132
Epoch 8.48: Loss = 0.485458
Epoch 8.49: Loss = 0.425827
Epoch 8.50: Loss = 0.581619
Epoch 8.51: Loss = 0.346756
Epoch 8.52: Loss = 0.360779
Epoch 8.53: Loss = 0.431503
Epoch 8.54: Loss = 0.589615
Epoch 8.55: Loss = 0.45372
Epoch 8.56: Loss = 0.445557
Epoch 8.57: Loss = 0.455292
Epoch 8.58: Loss = 0.404587
Epoch 8.59: Loss = 0.525085
Epoch 8.60: Loss = 0.545837
Epoch 8.61: Loss = 0.496765
Epoch 8.62: Loss = 0.556656
Epoch 8.63: Loss = 0.636826
Epoch 8.64: Loss = 0.582626
Epoch 8.65: Loss = 0.647903
Epoch 8.66: Loss = 0.412415
Epoch 8.67: Loss = 0.443008
Epoch 8.68: Loss = 0.323105
Epoch 8.69: Loss = 0.364563
Epoch 8.70: Loss = 0.55954
Epoch 8.71: Loss = 0.440186
Epoch 8.72: Loss = 0.362015
Epoch 8.73: Loss = 0.497253
Epoch 8.74: Loss = 0.349365
Epoch 8.75: Loss = 0.708801
Epoch 8.76: Loss = 0.498489
Epoch 8.77: Loss = 0.373199
Epoch 8.78: Loss = 0.455505
Epoch 8.79: Loss = 0.63298
Epoch 8.80: Loss = 0.507965
Epoch 8.81: Loss = 0.304413
Epoch 8.82: Loss = 0.318985
Epoch 8.83: Loss = 0.575439
Epoch 8.84: Loss = 0.474731
Epoch 8.85: Loss = 0.695343
Epoch 8.86: Loss = 0.592102
Epoch 8.87: Loss = 0.370575
Epoch 8.88: Loss = 0.440948
Epoch 8.89: Loss = 0.476685
Epoch 8.90: Loss = 0.401093
Epoch 8.91: Loss = 0.532852
Epoch 8.92: Loss = 0.555786
Epoch 8.93: Loss = 0.671936
Epoch 8.94: Loss = 0.318665
Epoch 8.95: Loss = 0.475327
Epoch 8.96: Loss = 0.534454
Epoch 8.97: Loss = 0.416855
Epoch 8.98: Loss = 0.44751
Epoch 8.99: Loss = 0.548401
Epoch 8.100: Loss = 0.593079
Epoch 8.101: Loss = 0.682144
Epoch 8.102: Loss = 0.473724
Epoch 8.103: Loss = 0.408386
Epoch 8.104: Loss = 0.431686
Epoch 8.105: Loss = 0.582062
Epoch 8.106: Loss = 0.591492
Epoch 8.107: Loss = 0.334747
Epoch 8.108: Loss = 0.4953
Epoch 8.109: Loss = 0.442627
Epoch 8.110: Loss = 0.520279
Epoch 8.111: Loss = 0.405197
Epoch 8.112: Loss = 0.350662
Epoch 8.113: Loss = 0.406906
Epoch 8.114: Loss = 0.353561
Epoch 8.115: Loss = 0.339554
Epoch 8.116: Loss = 0.417542
Epoch 8.117: Loss = 0.226013
Epoch 8.118: Loss = 0.197174
Epoch 8.119: Loss = 0.288361
Epoch 8.120: Loss = 0.338135
TRAIN LOSS = 0.46283
TRAIN ACC = 88.4079 % (53047/60000)
Loss = 0.48114
Loss = 0.558517
Loss = 0.677963
Loss = 0.647369
Loss = 0.720688
Loss = 0.444489
Loss = 0.447021
Loss = 0.697983
Loss = 0.622116
Loss = 0.548065
Loss = 0.161575
Loss = 0.317627
Loss = 0.409592
Loss = 0.422974
Loss = 0.226669
Loss = 0.342667
Loss = 0.233887
Loss = 0.0766754
Loss = 0.272751
Loss = 0.699112
TEST LOSS = 0.450444
TEST ACC = 530.469 % (8927/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.450165
Epoch 9.2: Loss = 0.535202
Epoch 9.3: Loss = 0.571365
Epoch 9.4: Loss = 0.358078
Epoch 9.5: Loss = 0.400803
Epoch 9.6: Loss = 0.353012
Epoch 9.7: Loss = 0.362717
Epoch 9.8: Loss = 0.473221
Epoch 9.9: Loss = 0.400986
Epoch 9.10: Loss = 0.39032
Epoch 9.11: Loss = 0.465912
Epoch 9.12: Loss = 0.453232
Epoch 9.13: Loss = 0.374786
Epoch 9.14: Loss = 0.414185
Epoch 9.15: Loss = 0.4496
Epoch 9.16: Loss = 0.47113
Epoch 9.17: Loss = 0.503601
Epoch 9.18: Loss = 0.768234
Epoch 9.19: Loss = 0.50235
Epoch 9.20: Loss = 0.430344
Epoch 9.21: Loss = 0.356094
Epoch 9.22: Loss = 0.322128
Epoch 9.23: Loss = 0.356705
Epoch 9.24: Loss = 0.605515
Epoch 9.25: Loss = 0.455887
Epoch 9.26: Loss = 0.604218
Epoch 9.27: Loss = 0.542465
Epoch 9.28: Loss = 0.520905
Epoch 9.29: Loss = 0.5923
Epoch 9.30: Loss = 0.597351
Epoch 9.31: Loss = 0.410126
Epoch 9.32: Loss = 0.513916
Epoch 9.33: Loss = 0.426254
Epoch 9.34: Loss = 0.543747
Epoch 9.35: Loss = 0.473602
Epoch 9.36: Loss = 0.516495
Epoch 9.37: Loss = 0.359589
Epoch 9.38: Loss = 0.454056
Epoch 9.39: Loss = 0.416748
Epoch 9.40: Loss = 0.428421
Epoch 9.41: Loss = 0.418915
Epoch 9.42: Loss = 0.640549
Epoch 9.43: Loss = 0.424469
Epoch 9.44: Loss = 0.36702
Epoch 9.45: Loss = 0.492569
Epoch 9.46: Loss = 0.488998
Epoch 9.47: Loss = 0.393326
Epoch 9.48: Loss = 0.482285
Epoch 9.49: Loss = 0.430603
Epoch 9.50: Loss = 0.602325
Epoch 9.51: Loss = 0.374557
Epoch 9.52: Loss = 0.362427
Epoch 9.53: Loss = 0.443237
Epoch 9.54: Loss = 0.625275
Epoch 9.55: Loss = 0.504303
Epoch 9.56: Loss = 0.468292
Epoch 9.57: Loss = 0.451675
Epoch 9.58: Loss = 0.402802
Epoch 9.59: Loss = 0.570023
Epoch 9.60: Loss = 0.562134
Epoch 9.61: Loss = 0.500687
Epoch 9.62: Loss = 0.574173
Epoch 9.63: Loss = 0.6763
Epoch 9.64: Loss = 0.615433
Epoch 9.65: Loss = 0.732132
Epoch 9.66: Loss = 0.421432
Epoch 9.67: Loss = 0.430847
Epoch 9.68: Loss = 0.342834
Epoch 9.69: Loss = 0.356415
Epoch 9.70: Loss = 0.55957
Epoch 9.71: Loss = 0.420502
Epoch 9.72: Loss = 0.333389
Epoch 9.73: Loss = 0.506821
Epoch 9.74: Loss = 0.355225
Epoch 9.75: Loss = 0.809021
Epoch 9.76: Loss = 0.498383
Epoch 9.77: Loss = 0.363922
Epoch 9.78: Loss = 0.48349
Epoch 9.79: Loss = 0.672043
Epoch 9.80: Loss = 0.513123
Epoch 9.81: Loss = 0.336609
Epoch 9.82: Loss = 0.340164
Epoch 9.83: Loss = 0.610397
Epoch 9.84: Loss = 0.474808
Epoch 9.85: Loss = 0.646118
Epoch 9.86: Loss = 0.576721
Epoch 9.87: Loss = 0.37236
Epoch 9.88: Loss = 0.433334
Epoch 9.89: Loss = 0.506973
Epoch 9.90: Loss = 0.42244
Epoch 9.91: Loss = 0.585464
Epoch 9.92: Loss = 0.560669
Epoch 9.93: Loss = 0.713257
Epoch 9.94: Loss = 0.342163
Epoch 9.95: Loss = 0.489059
Epoch 9.96: Loss = 0.54599
Epoch 9.97: Loss = 0.436752
Epoch 9.98: Loss = 0.457977
Epoch 9.99: Loss = 0.573166
Epoch 9.100: Loss = 0.626587
Epoch 9.101: Loss = 0.674072
Epoch 9.102: Loss = 0.454773
Epoch 9.103: Loss = 0.474106
Epoch 9.104: Loss = 0.473572
Epoch 9.105: Loss = 0.633636
Epoch 9.106: Loss = 0.626205
Epoch 9.107: Loss = 0.336487
Epoch 9.108: Loss = 0.523407
Epoch 9.109: Loss = 0.46431
Epoch 9.110: Loss = 0.499268
Epoch 9.111: Loss = 0.402222
Epoch 9.112: Loss = 0.383224
Epoch 9.113: Loss = 0.402084
Epoch 9.114: Loss = 0.372345
Epoch 9.115: Loss = 0.341827
Epoch 9.116: Loss = 0.419479
Epoch 9.117: Loss = 0.235764
Epoch 9.118: Loss = 0.232651
Epoch 9.119: Loss = 0.350708
Epoch 9.120: Loss = 0.34494
TRAIN LOSS = 0.474945
TRAIN ACC = 88.4827 % (53092/60000)
Loss = 0.485535
Loss = 0.579407
Loss = 0.707977
Loss = 0.656982
Loss = 0.711441
Loss = 0.460312
Loss = 0.451813
Loss = 0.762695
Loss = 0.638901
Loss = 0.526459
Loss = 0.190567
Loss = 0.327408
Loss = 0.424454
Loss = 0.43718
Loss = 0.216873
Loss = 0.355408
Loss = 0.221954
Loss = 0.0776062
Loss = 0.29213
Loss = 0.679901
TEST LOSS = 0.46025
TEST ACC = 530.919 % (8948/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.433868
Epoch 10.2: Loss = 0.565125
Epoch 10.3: Loss = 0.598206
Epoch 10.4: Loss = 0.370529
Epoch 10.5: Loss = 0.407578
Epoch 10.6: Loss = 0.370804
Epoch 10.7: Loss = 0.372269
Epoch 10.8: Loss = 0.455734
Epoch 10.9: Loss = 0.384354
Epoch 10.10: Loss = 0.381256
Epoch 10.11: Loss = 0.428284
Epoch 10.12: Loss = 0.468628
Epoch 10.13: Loss = 0.380035
Epoch 10.14: Loss = 0.412109
Epoch 10.15: Loss = 0.48381
Epoch 10.16: Loss = 0.488831
Epoch 10.17: Loss = 0.555725
Epoch 10.18: Loss = 0.776627
Epoch 10.19: Loss = 0.532181
Epoch 10.20: Loss = 0.410843
Epoch 10.21: Loss = 0.372223
Epoch 10.22: Loss = 0.36644
Epoch 10.23: Loss = 0.378754
Epoch 10.24: Loss = 0.61348
Epoch 10.25: Loss = 0.503387
Epoch 10.26: Loss = 0.609436
Epoch 10.27: Loss = 0.547913
Epoch 10.28: Loss = 0.534454
Epoch 10.29: Loss = 0.61499
Epoch 10.30: Loss = 0.597382
Epoch 10.31: Loss = 0.423813
Epoch 10.32: Loss = 0.524902
Epoch 10.33: Loss = 0.432999
Epoch 10.34: Loss = 0.572372
Epoch 10.35: Loss = 0.467178
Epoch 10.36: Loss = 0.527939
Epoch 10.37: Loss = 0.375809
Epoch 10.38: Loss = 0.436569
Epoch 10.39: Loss = 0.458786
Epoch 10.40: Loss = 0.483917
Epoch 10.41: Loss = 0.433395
Epoch 10.42: Loss = 0.692596
Epoch 10.43: Loss = 0.423538
Epoch 10.44: Loss = 0.364807
Epoch 10.45: Loss = 0.486725
Epoch 10.46: Loss = 0.512756
Epoch 10.47: Loss = 0.389725
Epoch 10.48: Loss = 0.471741
Epoch 10.49: Loss = 0.416214
Epoch 10.50: Loss = 0.637177
Epoch 10.51: Loss = 0.394684
Epoch 10.52: Loss = 0.405167
Epoch 10.53: Loss = 0.425919
Epoch 10.54: Loss = 0.670776
Epoch 10.55: Loss = 0.561386
Epoch 10.56: Loss = 0.449692
Epoch 10.57: Loss = 0.434479
Epoch 10.58: Loss = 0.4245
Epoch 10.59: Loss = 0.588242
Epoch 10.60: Loss = 0.577606
Epoch 10.61: Loss = 0.521072
Epoch 10.62: Loss = 0.559128
Epoch 10.63: Loss = 0.656616
Epoch 10.64: Loss = 0.600876
Epoch 10.65: Loss = 0.751511
Epoch 10.66: Loss = 0.438538
Epoch 10.67: Loss = 0.467224
Epoch 10.68: Loss = 0.348099
Epoch 10.69: Loss = 0.355103
Epoch 10.70: Loss = 0.549744
Epoch 10.71: Loss = 0.402939
Epoch 10.72: Loss = 0.333527
Epoch 10.73: Loss = 0.526093
Epoch 10.74: Loss = 0.367371
Epoch 10.75: Loss = 0.819122
Epoch 10.76: Loss = 0.509827
Epoch 10.77: Loss = 0.338211
Epoch 10.78: Loss = 0.51178
Epoch 10.79: Loss = 0.62114
Epoch 10.80: Loss = 0.493607
Epoch 10.81: Loss = 0.365265
Epoch 10.82: Loss = 0.346695
Epoch 10.83: Loss = 0.584122
Epoch 10.84: Loss = 0.46524
Epoch 10.85: Loss = 0.690186
Epoch 10.86: Loss = 0.583969
Epoch 10.87: Loss = 0.352386
Epoch 10.88: Loss = 0.435013
Epoch 10.89: Loss = 0.488617
Epoch 10.90: Loss = 0.434235
Epoch 10.91: Loss = 0.5923
Epoch 10.92: Loss = 0.554474
Epoch 10.93: Loss = 0.685455
Epoch 10.94: Loss = 0.309967
Epoch 10.95: Loss = 0.493256
Epoch 10.96: Loss = 0.527573
Epoch 10.97: Loss = 0.417557
Epoch 10.98: Loss = 0.438416
Epoch 10.99: Loss = 0.581558
Epoch 10.100: Loss = 0.639099
Epoch 10.101: Loss = 0.690552
Epoch 10.102: Loss = 0.440338
Epoch 10.103: Loss = 0.432129
Epoch 10.104: Loss = 0.482651
Epoch 10.105: Loss = 0.647461
Epoch 10.106: Loss = 0.60054
Epoch 10.107: Loss = 0.386292
Epoch 10.108: Loss = 0.520645
Epoch 10.109: Loss = 0.420578
Epoch 10.110: Loss = 0.483078
Epoch 10.111: Loss = 0.414078
Epoch 10.112: Loss = 0.402359
Epoch 10.113: Loss = 0.416519
Epoch 10.114: Loss = 0.373093
Epoch 10.115: Loss = 0.328445
Epoch 10.116: Loss = 0.443985
Epoch 10.117: Loss = 0.247742
Epoch 10.118: Loss = 0.223831
Epoch 10.119: Loss = 0.327927
Epoch 10.120: Loss = 0.356552
TRAIN LOSS = 0.480637
TRAIN ACC = 88.559 % (53137/60000)
Loss = 0.453918
Loss = 0.549103
Loss = 0.706009
Loss = 0.644928
Loss = 0.746323
Loss = 0.483719
Loss = 0.408203
Loss = 0.76059
Loss = 0.658249
Loss = 0.552017
Loss = 0.20639
Loss = 0.325333
Loss = 0.446518
Loss = 0.416901
Loss = 0.208542
Loss = 0.312927
Loss = 0.200333
Loss = 0.0662384
Loss = 0.299164
Loss = 0.680496
TEST LOSS = 0.456295
TEST ACC = 531.369 % (8950/10000)
