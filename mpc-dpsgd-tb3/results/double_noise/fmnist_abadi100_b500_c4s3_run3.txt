Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.48567
Epoch 1.2: Loss = 2.38324
Epoch 1.3: Loss = 2.28592
Epoch 1.4: Loss = 2.21841
Epoch 1.5: Loss = 2.17342
Epoch 1.6: Loss = 2.10518
Epoch 1.7: Loss = 2.03421
Epoch 1.8: Loss = 1.95845
Epoch 1.9: Loss = 1.91116
Epoch 1.10: Loss = 1.83267
Epoch 1.11: Loss = 1.85548
Epoch 1.12: Loss = 1.80522
Epoch 1.13: Loss = 1.75568
Epoch 1.14: Loss = 1.69421
Epoch 1.15: Loss = 1.67235
Epoch 1.16: Loss = 1.64517
Epoch 1.17: Loss = 1.61195
Epoch 1.18: Loss = 1.57042
Epoch 1.19: Loss = 1.52364
Epoch 1.20: Loss = 1.5258
Epoch 1.21: Loss = 1.46811
Epoch 1.22: Loss = 1.42996
Epoch 1.23: Loss = 1.41455
Epoch 1.24: Loss = 1.46327
Epoch 1.25: Loss = 1.41052
Epoch 1.26: Loss = 1.31444
Epoch 1.27: Loss = 1.30795
Epoch 1.28: Loss = 1.28728
Epoch 1.29: Loss = 1.29457
Epoch 1.30: Loss = 1.24103
Epoch 1.31: Loss = 1.27605
Epoch 1.32: Loss = 1.23958
Epoch 1.33: Loss = 1.15974
Epoch 1.34: Loss = 1.23979
Epoch 1.35: Loss = 1.24887
Epoch 1.36: Loss = 1.21442
Epoch 1.37: Loss = 1.20605
Epoch 1.38: Loss = 1.14119
Epoch 1.39: Loss = 1.13847
Epoch 1.40: Loss = 1.10451
Epoch 1.41: Loss = 1.15845
Epoch 1.42: Loss = 1.08209
Epoch 1.43: Loss = 1.06023
Epoch 1.44: Loss = 1.03867
Epoch 1.45: Loss = 1.08987
Epoch 1.46: Loss = 1.05684
Epoch 1.47: Loss = 1.04178
Epoch 1.48: Loss = 0.995987
Epoch 1.49: Loss = 1.05312
Epoch 1.50: Loss = 1.00139
Epoch 1.51: Loss = 0.953293
Epoch 1.52: Loss = 1.02838
Epoch 1.53: Loss = 1.00302
Epoch 1.54: Loss = 0.900284
Epoch 1.55: Loss = 0.979904
Epoch 1.56: Loss = 0.981186
Epoch 1.57: Loss = 0.991806
Epoch 1.58: Loss = 0.959412
Epoch 1.59: Loss = 0.952148
Epoch 1.60: Loss = 0.994278
Epoch 1.61: Loss = 0.899353
Epoch 1.62: Loss = 0.975464
Epoch 1.63: Loss = 0.848404
Epoch 1.64: Loss = 0.881973
Epoch 1.65: Loss = 0.903137
Epoch 1.66: Loss = 0.924957
Epoch 1.67: Loss = 0.853088
Epoch 1.68: Loss = 0.946991
Epoch 1.69: Loss = 0.911728
Epoch 1.70: Loss = 0.889938
Epoch 1.71: Loss = 0.833572
Epoch 1.72: Loss = 0.851685
Epoch 1.73: Loss = 0.914398
Epoch 1.74: Loss = 0.916763
Epoch 1.75: Loss = 0.867554
Epoch 1.76: Loss = 0.849884
Epoch 1.77: Loss = 0.834351
Epoch 1.78: Loss = 0.831528
Epoch 1.79: Loss = 0.787613
Epoch 1.80: Loss = 0.846573
Epoch 1.81: Loss = 0.812836
Epoch 1.82: Loss = 0.826416
Epoch 1.83: Loss = 0.863358
Epoch 1.84: Loss = 0.839813
Epoch 1.85: Loss = 0.798828
Epoch 1.86: Loss = 0.870255
Epoch 1.87: Loss = 0.879974
Epoch 1.88: Loss = 0.741302
Epoch 1.89: Loss = 0.870087
Epoch 1.90: Loss = 0.806793
Epoch 1.91: Loss = 0.88208
Epoch 1.92: Loss = 0.80925
Epoch 1.93: Loss = 0.830231
Epoch 1.94: Loss = 0.817917
Epoch 1.95: Loss = 0.846634
Epoch 1.96: Loss = 0.770538
Epoch 1.97: Loss = 0.702454
Epoch 1.98: Loss = 0.811172
Epoch 1.99: Loss = 0.780273
Epoch 1.100: Loss = 0.760391
Epoch 1.101: Loss = 0.82196
Epoch 1.102: Loss = 0.814163
Epoch 1.103: Loss = 0.816635
Epoch 1.104: Loss = 0.76033
Epoch 1.105: Loss = 0.73233
Epoch 1.106: Loss = 0.865326
Epoch 1.107: Loss = 0.787964
Epoch 1.108: Loss = 0.791626
Epoch 1.109: Loss = 0.774979
Epoch 1.110: Loss = 0.787201
Epoch 1.111: Loss = 0.737106
Epoch 1.112: Loss = 0.704758
Epoch 1.113: Loss = 0.764679
Epoch 1.114: Loss = 0.762939
Epoch 1.115: Loss = 0.750244
Epoch 1.116: Loss = 0.688065
Epoch 1.117: Loss = 0.829224
Epoch 1.118: Loss = 0.692764
Epoch 1.119: Loss = 0.73938
Epoch 1.120: Loss = 0.69722
TRAIN LOSS = 1.10985
TRAIN ACC = 63.7909 % (38277/60000)
Loss = 0.69223
Loss = 0.802917
Loss = 0.787872
Loss = 0.70871
Loss = 0.708221
Loss = 0.833969
Loss = 0.863297
Loss = 0.826797
Loss = 0.739822
Loss = 0.713379
Loss = 0.832047
Loss = 0.766235
Loss = 0.774246
Loss = 0.788284
Loss = 0.733292
Loss = 0.806442
Loss = 0.7202
Loss = 0.76857
Loss = 0.808548
Loss = 0.765701
TEST LOSS = 0.772039
TEST ACC = 382.77 % (7288/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.755371
Epoch 2.2: Loss = 0.722336
Epoch 2.3: Loss = 0.801834
Epoch 2.4: Loss = 0.667267
Epoch 2.5: Loss = 0.748749
Epoch 2.6: Loss = 0.811508
Epoch 2.7: Loss = 0.759018
Epoch 2.8: Loss = 0.786011
Epoch 2.9: Loss = 0.66713
Epoch 2.10: Loss = 0.588379
Epoch 2.11: Loss = 0.789703
Epoch 2.12: Loss = 0.742722
Epoch 2.13: Loss = 0.73674
Epoch 2.14: Loss = 0.729965
Epoch 2.15: Loss = 0.728394
Epoch 2.16: Loss = 0.789566
Epoch 2.17: Loss = 0.713272
Epoch 2.18: Loss = 0.731827
Epoch 2.19: Loss = 0.703781
Epoch 2.20: Loss = 0.78833
Epoch 2.21: Loss = 0.683441
Epoch 2.22: Loss = 0.647095
Epoch 2.23: Loss = 0.747696
Epoch 2.24: Loss = 0.803635
Epoch 2.25: Loss = 0.721069
Epoch 2.26: Loss = 0.655594
Epoch 2.27: Loss = 0.698135
Epoch 2.28: Loss = 0.709702
Epoch 2.29: Loss = 0.740433
Epoch 2.30: Loss = 0.687988
Epoch 2.31: Loss = 0.758469
Epoch 2.32: Loss = 0.71608
Epoch 2.33: Loss = 0.637466
Epoch 2.34: Loss = 0.764069
Epoch 2.35: Loss = 0.768692
Epoch 2.36: Loss = 0.766235
Epoch 2.37: Loss = 0.745834
Epoch 2.38: Loss = 0.690186
Epoch 2.39: Loss = 0.780136
Epoch 2.40: Loss = 0.716812
Epoch 2.41: Loss = 0.749298
Epoch 2.42: Loss = 0.739304
Epoch 2.43: Loss = 0.710251
Epoch 2.44: Loss = 0.65094
Epoch 2.45: Loss = 0.737701
Epoch 2.46: Loss = 0.772781
Epoch 2.47: Loss = 0.681183
Epoch 2.48: Loss = 0.645782
Epoch 2.49: Loss = 0.743317
Epoch 2.50: Loss = 0.705765
Epoch 2.51: Loss = 0.59996
Epoch 2.52: Loss = 0.736786
Epoch 2.53: Loss = 0.76004
Epoch 2.54: Loss = 0.595749
Epoch 2.55: Loss = 0.721375
Epoch 2.56: Loss = 0.702408
Epoch 2.57: Loss = 0.731735
Epoch 2.58: Loss = 0.714417
Epoch 2.59: Loss = 0.710526
Epoch 2.60: Loss = 0.723175
Epoch 2.61: Loss = 0.663406
Epoch 2.62: Loss = 0.760651
Epoch 2.63: Loss = 0.594757
Epoch 2.64: Loss = 0.620987
Epoch 2.65: Loss = 0.669846
Epoch 2.66: Loss = 0.682312
Epoch 2.67: Loss = 0.63382
Epoch 2.68: Loss = 0.763351
Epoch 2.69: Loss = 0.69278
Epoch 2.70: Loss = 0.714066
Epoch 2.71: Loss = 0.610626
Epoch 2.72: Loss = 0.665024
Epoch 2.73: Loss = 0.759705
Epoch 2.74: Loss = 0.728729
Epoch 2.75: Loss = 0.640854
Epoch 2.76: Loss = 0.6586
Epoch 2.77: Loss = 0.655106
Epoch 2.78: Loss = 0.672104
Epoch 2.79: Loss = 0.642319
Epoch 2.80: Loss = 0.670258
Epoch 2.81: Loss = 0.638351
Epoch 2.82: Loss = 0.62764
Epoch 2.83: Loss = 0.713623
Epoch 2.84: Loss = 0.658905
Epoch 2.85: Loss = 0.66246
Epoch 2.86: Loss = 0.712311
Epoch 2.87: Loss = 0.718399
Epoch 2.88: Loss = 0.603027
Epoch 2.89: Loss = 0.776886
Epoch 2.90: Loss = 0.67421
Epoch 2.91: Loss = 0.779694
Epoch 2.92: Loss = 0.672424
Epoch 2.93: Loss = 0.707047
Epoch 2.94: Loss = 0.657822
Epoch 2.95: Loss = 0.684097
Epoch 2.96: Loss = 0.627655
Epoch 2.97: Loss = 0.569397
Epoch 2.98: Loss = 0.670456
Epoch 2.99: Loss = 0.685623
Epoch 2.100: Loss = 0.66745
Epoch 2.101: Loss = 0.722198
Epoch 2.102: Loss = 0.716171
Epoch 2.103: Loss = 0.680649
Epoch 2.104: Loss = 0.625259
Epoch 2.105: Loss = 0.617706
Epoch 2.106: Loss = 0.752838
Epoch 2.107: Loss = 0.710922
Epoch 2.108: Loss = 0.733032
Epoch 2.109: Loss = 0.720642
Epoch 2.110: Loss = 0.694321
Epoch 2.111: Loss = 0.636322
Epoch 2.112: Loss = 0.623291
Epoch 2.113: Loss = 0.657394
Epoch 2.114: Loss = 0.672943
Epoch 2.115: Loss = 0.661392
Epoch 2.116: Loss = 0.605362
Epoch 2.117: Loss = 0.745728
Epoch 2.118: Loss = 0.606293
Epoch 2.119: Loss = 0.642654
Epoch 2.120: Loss = 0.601868
TRAIN LOSS = 0.698074
TRAIN ACC = 76.1475 % (45690/60000)
Loss = 0.611786
Loss = 0.740555
Loss = 0.684601
Loss = 0.612366
Loss = 0.627762
Loss = 0.780945
Loss = 0.793671
Loss = 0.775436
Loss = 0.677109
Loss = 0.60585
Loss = 0.770248
Loss = 0.73056
Loss = 0.709213
Loss = 0.722076
Loss = 0.663895
Loss = 0.735992
Loss = 0.632828
Loss = 0.718765
Loss = 0.743912
Loss = 0.689651
TEST LOSS = 0.701361
TEST ACC = 456.898 % (7597/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.677856
Epoch 3.2: Loss = 0.655853
Epoch 3.3: Loss = 0.736145
Epoch 3.4: Loss = 0.603088
Epoch 3.5: Loss = 0.689209
Epoch 3.6: Loss = 0.755264
Epoch 3.7: Loss = 0.682007
Epoch 3.8: Loss = 0.749481
Epoch 3.9: Loss = 0.571198
Epoch 3.10: Loss = 0.520889
Epoch 3.11: Loss = 0.745514
Epoch 3.12: Loss = 0.663864
Epoch 3.13: Loss = 0.692139
Epoch 3.14: Loss = 0.658966
Epoch 3.15: Loss = 0.674835
Epoch 3.16: Loss = 0.744614
Epoch 3.17: Loss = 0.651764
Epoch 3.18: Loss = 0.671524
Epoch 3.19: Loss = 0.650253
Epoch 3.20: Loss = 0.726791
Epoch 3.21: Loss = 0.591553
Epoch 3.22: Loss = 0.550262
Epoch 3.23: Loss = 0.688309
Epoch 3.24: Loss = 0.761734
Epoch 3.25: Loss = 0.631851
Epoch 3.26: Loss = 0.581741
Epoch 3.27: Loss = 0.646255
Epoch 3.28: Loss = 0.633423
Epoch 3.29: Loss = 0.689713
Epoch 3.30: Loss = 0.640625
Epoch 3.31: Loss = 0.713333
Epoch 3.32: Loss = 0.648178
Epoch 3.33: Loss = 0.560791
Epoch 3.34: Loss = 0.711853
Epoch 3.35: Loss = 0.710373
Epoch 3.36: Loss = 0.711945
Epoch 3.37: Loss = 0.680008
Epoch 3.38: Loss = 0.631607
Epoch 3.39: Loss = 0.724503
Epoch 3.40: Loss = 0.662903
Epoch 3.41: Loss = 0.71283
Epoch 3.42: Loss = 0.669861
Epoch 3.43: Loss = 0.645889
Epoch 3.44: Loss = 0.579086
Epoch 3.45: Loss = 0.677368
Epoch 3.46: Loss = 0.742447
Epoch 3.47: Loss = 0.623322
Epoch 3.48: Loss = 0.5737
Epoch 3.49: Loss = 0.690216
Epoch 3.50: Loss = 0.674118
Epoch 3.51: Loss = 0.538605
Epoch 3.52: Loss = 0.707367
Epoch 3.53: Loss = 0.709427
Epoch 3.54: Loss = 0.538925
Epoch 3.55: Loss = 0.66806
Epoch 3.56: Loss = 0.660782
Epoch 3.57: Loss = 0.699219
Epoch 3.58: Loss = 0.658829
Epoch 3.59: Loss = 0.701004
Epoch 3.60: Loss = 0.657516
Epoch 3.61: Loss = 0.609161
Epoch 3.62: Loss = 0.694748
Epoch 3.63: Loss = 0.548538
Epoch 3.64: Loss = 0.564362
Epoch 3.65: Loss = 0.652374
Epoch 3.66: Loss = 0.630966
Epoch 3.67: Loss = 0.616089
Epoch 3.68: Loss = 0.756683
Epoch 3.69: Loss = 0.661194
Epoch 3.70: Loss = 0.683868
Epoch 3.71: Loss = 0.573212
Epoch 3.72: Loss = 0.648285
Epoch 3.73: Loss = 0.754944
Epoch 3.74: Loss = 0.690811
Epoch 3.75: Loss = 0.612
Epoch 3.76: Loss = 0.63031
Epoch 3.77: Loss = 0.6418
Epoch 3.78: Loss = 0.663162
Epoch 3.79: Loss = 0.613739
Epoch 3.80: Loss = 0.614044
Epoch 3.81: Loss = 0.615448
Epoch 3.82: Loss = 0.590424
Epoch 3.83: Loss = 0.697357
Epoch 3.84: Loss = 0.630173
Epoch 3.85: Loss = 0.625854
Epoch 3.86: Loss = 0.678421
Epoch 3.87: Loss = 0.673645
Epoch 3.88: Loss = 0.569565
Epoch 3.89: Loss = 0.739838
Epoch 3.90: Loss = 0.657349
Epoch 3.91: Loss = 0.75502
Epoch 3.92: Loss = 0.658142
Epoch 3.93: Loss = 0.698975
Epoch 3.94: Loss = 0.652237
Epoch 3.95: Loss = 0.672348
Epoch 3.96: Loss = 0.603271
Epoch 3.97: Loss = 0.560516
Epoch 3.98: Loss = 0.630432
Epoch 3.99: Loss = 0.654099
Epoch 3.100: Loss = 0.6418
Epoch 3.101: Loss = 0.690582
Epoch 3.102: Loss = 0.691071
Epoch 3.103: Loss = 0.621262
Epoch 3.104: Loss = 0.576797
Epoch 3.105: Loss = 0.589005
Epoch 3.106: Loss = 0.726807
Epoch 3.107: Loss = 0.684326
Epoch 3.108: Loss = 0.753128
Epoch 3.109: Loss = 0.702347
Epoch 3.110: Loss = 0.675369
Epoch 3.111: Loss = 0.590378
Epoch 3.112: Loss = 0.610535
Epoch 3.113: Loss = 0.615433
Epoch 3.114: Loss = 0.648682
Epoch 3.115: Loss = 0.637711
Epoch 3.116: Loss = 0.59877
Epoch 3.117: Loss = 0.727997
Epoch 3.118: Loss = 0.558121
Epoch 3.119: Loss = 0.620956
Epoch 3.120: Loss = 0.592819
TRAIN LOSS = 0.654999
TRAIN ACC = 78.3829 % (47032/60000)
Loss = 0.59581
Loss = 0.733704
Loss = 0.664093
Loss = 0.575882
Loss = 0.613831
Loss = 0.752686
Loss = 0.777359
Loss = 0.749207
Loss = 0.672394
Loss = 0.588379
Loss = 0.766281
Loss = 0.737427
Loss = 0.68988
Loss = 0.717468
Loss = 0.653992
Loss = 0.732605
Loss = 0.614059
Loss = 0.709183
Loss = 0.739838
Loss = 0.685654
TEST LOSS = 0.688486
TEST ACC = 470.319 % (7748/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.656754
Epoch 4.2: Loss = 0.643143
Epoch 4.3: Loss = 0.703873
Epoch 4.4: Loss = 0.575989
Epoch 4.5: Loss = 0.666336
Epoch 4.6: Loss = 0.746994
Epoch 4.7: Loss = 0.660248
Epoch 4.8: Loss = 0.754105
Epoch 4.9: Loss = 0.537247
Epoch 4.10: Loss = 0.498611
Epoch 4.11: Loss = 0.730606
Epoch 4.12: Loss = 0.652252
Epoch 4.13: Loss = 0.661621
Epoch 4.14: Loss = 0.641983
Epoch 4.15: Loss = 0.660873
Epoch 4.16: Loss = 0.709518
Epoch 4.17: Loss = 0.616653
Epoch 4.18: Loss = 0.641129
Epoch 4.19: Loss = 0.606583
Epoch 4.20: Loss = 0.6996
Epoch 4.21: Loss = 0.55751
Epoch 4.22: Loss = 0.495987
Epoch 4.23: Loss = 0.66011
Epoch 4.24: Loss = 0.751694
Epoch 4.25: Loss = 0.607681
Epoch 4.26: Loss = 0.549866
Epoch 4.27: Loss = 0.615662
Epoch 4.28: Loss = 0.617325
Epoch 4.29: Loss = 0.669937
Epoch 4.30: Loss = 0.612701
Epoch 4.31: Loss = 0.71492
Epoch 4.32: Loss = 0.617035
Epoch 4.33: Loss = 0.533722
Epoch 4.34: Loss = 0.702499
Epoch 4.35: Loss = 0.682053
Epoch 4.36: Loss = 0.689346
Epoch 4.37: Loss = 0.670166
Epoch 4.38: Loss = 0.598907
Epoch 4.39: Loss = 0.715118
Epoch 4.40: Loss = 0.637497
Epoch 4.41: Loss = 0.679199
Epoch 4.42: Loss = 0.644043
Epoch 4.43: Loss = 0.641159
Epoch 4.44: Loss = 0.545944
Epoch 4.45: Loss = 0.667267
Epoch 4.46: Loss = 0.744751
Epoch 4.47: Loss = 0.603699
Epoch 4.48: Loss = 0.541214
Epoch 4.49: Loss = 0.673141
Epoch 4.50: Loss = 0.678986
Epoch 4.51: Loss = 0.523148
Epoch 4.52: Loss = 0.703629
Epoch 4.53: Loss = 0.685913
Epoch 4.54: Loss = 0.51001
Epoch 4.55: Loss = 0.643021
Epoch 4.56: Loss = 0.65213
Epoch 4.57: Loss = 0.691864
Epoch 4.58: Loss = 0.633545
Epoch 4.59: Loss = 0.681107
Epoch 4.60: Loss = 0.660965
Epoch 4.61: Loss = 0.585571
Epoch 4.62: Loss = 0.691162
Epoch 4.63: Loss = 0.526825
Epoch 4.64: Loss = 0.539063
Epoch 4.65: Loss = 0.65036
Epoch 4.66: Loss = 0.587021
Epoch 4.67: Loss = 0.601273
Epoch 4.68: Loss = 0.746567
Epoch 4.69: Loss = 0.626816
Epoch 4.70: Loss = 0.665756
Epoch 4.71: Loss = 0.553268
Epoch 4.72: Loss = 0.647766
Epoch 4.73: Loss = 0.75563
Epoch 4.74: Loss = 0.675995
Epoch 4.75: Loss = 0.591293
Epoch 4.76: Loss = 0.608185
Epoch 4.77: Loss = 0.618515
Epoch 4.78: Loss = 0.650848
Epoch 4.79: Loss = 0.600159
Epoch 4.80: Loss = 0.577469
Epoch 4.81: Loss = 0.605865
Epoch 4.82: Loss = 0.574097
Epoch 4.83: Loss = 0.688614
Epoch 4.84: Loss = 0.592316
Epoch 4.85: Loss = 0.619308
Epoch 4.86: Loss = 0.668076
Epoch 4.87: Loss = 0.674286
Epoch 4.88: Loss = 0.566986
Epoch 4.89: Loss = 0.757675
Epoch 4.90: Loss = 0.652695
Epoch 4.91: Loss = 0.717972
Epoch 4.92: Loss = 0.635376
Epoch 4.93: Loss = 0.673386
Epoch 4.94: Loss = 0.652069
Epoch 4.95: Loss = 0.644562
Epoch 4.96: Loss = 0.602661
Epoch 4.97: Loss = 0.556427
Epoch 4.98: Loss = 0.632294
Epoch 4.99: Loss = 0.636292
Epoch 4.100: Loss = 0.633072
Epoch 4.101: Loss = 0.691147
Epoch 4.102: Loss = 0.676102
Epoch 4.103: Loss = 0.615417
Epoch 4.104: Loss = 0.569733
Epoch 4.105: Loss = 0.577438
Epoch 4.106: Loss = 0.683685
Epoch 4.107: Loss = 0.64563
Epoch 4.108: Loss = 0.755051
Epoch 4.109: Loss = 0.721237
Epoch 4.110: Loss = 0.654037
Epoch 4.111: Loss = 0.589966
Epoch 4.112: Loss = 0.609772
Epoch 4.113: Loss = 0.618866
Epoch 4.114: Loss = 0.660583
Epoch 4.115: Loss = 0.623474
Epoch 4.116: Loss = 0.58609
Epoch 4.117: Loss = 0.712479
Epoch 4.118: Loss = 0.559448
Epoch 4.119: Loss = 0.615097
Epoch 4.120: Loss = 0.589401
TRAIN LOSS = 0.638397
TRAIN ACC = 79.4434 % (47668/60000)
Loss = 0.581863
Loss = 0.701065
Loss = 0.636536
Loss = 0.567459
Loss = 0.624191
Loss = 0.735214
Loss = 0.755615
Loss = 0.734283
Loss = 0.663483
Loss = 0.604095
Loss = 0.775604
Loss = 0.733551
Loss = 0.680756
Loss = 0.71608
Loss = 0.659576
Loss = 0.718781
Loss = 0.605652
Loss = 0.694214
Loss = 0.743408
Loss = 0.663605
TEST LOSS = 0.679751
TEST ACC = 476.678 % (7858/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.639206
Epoch 5.2: Loss = 0.634369
Epoch 5.3: Loss = 0.6772
Epoch 5.4: Loss = 0.56871
Epoch 5.5: Loss = 0.659943
Epoch 5.6: Loss = 0.757294
Epoch 5.7: Loss = 0.681168
Epoch 5.8: Loss = 0.740067
Epoch 5.9: Loss = 0.506439
Epoch 5.10: Loss = 0.47496
Epoch 5.11: Loss = 0.736923
Epoch 5.12: Loss = 0.644714
Epoch 5.13: Loss = 0.671158
Epoch 5.14: Loss = 0.622589
Epoch 5.15: Loss = 0.663147
Epoch 5.16: Loss = 0.710983
Epoch 5.17: Loss = 0.624863
Epoch 5.18: Loss = 0.668823
Epoch 5.19: Loss = 0.612015
Epoch 5.20: Loss = 0.716736
Epoch 5.21: Loss = 0.55838
Epoch 5.22: Loss = 0.487274
Epoch 5.23: Loss = 0.654068
Epoch 5.24: Loss = 0.749664
Epoch 5.25: Loss = 0.604248
Epoch 5.26: Loss = 0.53038
Epoch 5.27: Loss = 0.595886
Epoch 5.28: Loss = 0.602402
Epoch 5.29: Loss = 0.668167
Epoch 5.30: Loss = 0.606537
Epoch 5.31: Loss = 0.717636
Epoch 5.32: Loss = 0.615906
Epoch 5.33: Loss = 0.541656
Epoch 5.34: Loss = 0.708054
Epoch 5.35: Loss = 0.676926
Epoch 5.36: Loss = 0.678696
Epoch 5.37: Loss = 0.662186
Epoch 5.38: Loss = 0.593719
Epoch 5.39: Loss = 0.712433
Epoch 5.40: Loss = 0.643936
Epoch 5.41: Loss = 0.693512
Epoch 5.42: Loss = 0.647202
Epoch 5.43: Loss = 0.642609
Epoch 5.44: Loss = 0.551041
Epoch 5.45: Loss = 0.669571
Epoch 5.46: Loss = 0.731125
Epoch 5.47: Loss = 0.618332
Epoch 5.48: Loss = 0.54213
Epoch 5.49: Loss = 0.679077
Epoch 5.50: Loss = 0.692596
Epoch 5.51: Loss = 0.522751
Epoch 5.52: Loss = 0.712509
Epoch 5.53: Loss = 0.692657
Epoch 5.54: Loss = 0.498306
Epoch 5.55: Loss = 0.655411
Epoch 5.56: Loss = 0.665329
Epoch 5.57: Loss = 0.68988
Epoch 5.58: Loss = 0.62532
Epoch 5.59: Loss = 0.716141
Epoch 5.60: Loss = 0.683945
Epoch 5.61: Loss = 0.602921
Epoch 5.62: Loss = 0.689194
Epoch 5.63: Loss = 0.552856
Epoch 5.64: Loss = 0.537949
Epoch 5.65: Loss = 0.673431
Epoch 5.66: Loss = 0.600906
Epoch 5.67: Loss = 0.626038
Epoch 5.68: Loss = 0.781738
Epoch 5.69: Loss = 0.634888
Epoch 5.70: Loss = 0.667465
Epoch 5.71: Loss = 0.541168
Epoch 5.72: Loss = 0.678543
Epoch 5.73: Loss = 0.767151
Epoch 5.74: Loss = 0.684921
Epoch 5.75: Loss = 0.605133
Epoch 5.76: Loss = 0.614471
Epoch 5.77: Loss = 0.643326
Epoch 5.78: Loss = 0.637802
Epoch 5.79: Loss = 0.614319
Epoch 5.80: Loss = 0.599182
Epoch 5.81: Loss = 0.620132
Epoch 5.82: Loss = 0.584427
Epoch 5.83: Loss = 0.707382
Epoch 5.84: Loss = 0.59285
Epoch 5.85: Loss = 0.618362
Epoch 5.86: Loss = 0.655029
Epoch 5.87: Loss = 0.668121
Epoch 5.88: Loss = 0.569641
Epoch 5.89: Loss = 0.769363
Epoch 5.90: Loss = 0.661453
Epoch 5.91: Loss = 0.746338
Epoch 5.92: Loss = 0.644226
Epoch 5.93: Loss = 0.693558
Epoch 5.94: Loss = 0.652786
Epoch 5.95: Loss = 0.675507
Epoch 5.96: Loss = 0.608719
Epoch 5.97: Loss = 0.570267
Epoch 5.98: Loss = 0.628357
Epoch 5.99: Loss = 0.652496
Epoch 5.100: Loss = 0.638519
Epoch 5.101: Loss = 0.68512
Epoch 5.102: Loss = 0.693008
Epoch 5.103: Loss = 0.612061
Epoch 5.104: Loss = 0.568832
Epoch 5.105: Loss = 0.597626
Epoch 5.106: Loss = 0.739243
Epoch 5.107: Loss = 0.651108
Epoch 5.108: Loss = 0.800323
Epoch 5.109: Loss = 0.723709
Epoch 5.110: Loss = 0.640305
Epoch 5.111: Loss = 0.611237
Epoch 5.112: Loss = 0.642197
Epoch 5.113: Loss = 0.629623
Epoch 5.114: Loss = 0.7005
Epoch 5.115: Loss = 0.629013
Epoch 5.116: Loss = 0.610214
Epoch 5.117: Loss = 0.727676
Epoch 5.118: Loss = 0.550552
Epoch 5.119: Loss = 0.617661
Epoch 5.120: Loss = 0.599808
TRAIN LOSS = 0.644089
TRAIN ACC = 79.8782 % (47929/60000)
Loss = 0.575241
Loss = 0.676392
Loss = 0.632339
Loss = 0.55629
Loss = 0.611771
Loss = 0.739853
Loss = 0.785156
Loss = 0.729248
Loss = 0.67421
Loss = 0.594513
Loss = 0.776398
Loss = 0.735947
Loss = 0.683945
Loss = 0.732239
Loss = 0.64061
Loss = 0.710739
Loss = 0.617859
Loss = 0.685135
Loss = 0.733383
Loss = 0.662231
TEST LOSS = 0.677675
TEST ACC = 479.289 % (7928/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.6194
Epoch 6.2: Loss = 0.672028
Epoch 6.3: Loss = 0.671448
Epoch 6.4: Loss = 0.56868
Epoch 6.5: Loss = 0.669205
Epoch 6.6: Loss = 0.765594
Epoch 6.7: Loss = 0.659348
Epoch 6.8: Loss = 0.726944
Epoch 6.9: Loss = 0.508881
Epoch 6.10: Loss = 0.480591
Epoch 6.11: Loss = 0.751007
Epoch 6.12: Loss = 0.652222
Epoch 6.13: Loss = 0.685303
Epoch 6.14: Loss = 0.632111
Epoch 6.15: Loss = 0.659042
Epoch 6.16: Loss = 0.744156
Epoch 6.17: Loss = 0.622559
Epoch 6.18: Loss = 0.657944
Epoch 6.19: Loss = 0.622925
Epoch 6.20: Loss = 0.738663
Epoch 6.21: Loss = 0.549057
Epoch 6.22: Loss = 0.475708
Epoch 6.23: Loss = 0.656647
Epoch 6.24: Loss = 0.752197
Epoch 6.25: Loss = 0.61261
Epoch 6.26: Loss = 0.514496
Epoch 6.27: Loss = 0.585831
Epoch 6.28: Loss = 0.621155
Epoch 6.29: Loss = 0.654465
Epoch 6.30: Loss = 0.615967
Epoch 6.31: Loss = 0.719116
Epoch 6.32: Loss = 0.601532
Epoch 6.33: Loss = 0.530945
Epoch 6.34: Loss = 0.694717
Epoch 6.35: Loss = 0.684982
Epoch 6.36: Loss = 0.678467
Epoch 6.37: Loss = 0.672882
Epoch 6.38: Loss = 0.585983
Epoch 6.39: Loss = 0.706497
Epoch 6.40: Loss = 0.648239
Epoch 6.41: Loss = 0.672516
Epoch 6.42: Loss = 0.630951
Epoch 6.43: Loss = 0.657547
Epoch 6.44: Loss = 0.576187
Epoch 6.45: Loss = 0.694016
Epoch 6.46: Loss = 0.757065
Epoch 6.47: Loss = 0.615112
Epoch 6.48: Loss = 0.542328
Epoch 6.49: Loss = 0.660873
Epoch 6.50: Loss = 0.693054
Epoch 6.51: Loss = 0.514771
Epoch 6.52: Loss = 0.719513
Epoch 6.53: Loss = 0.692429
Epoch 6.54: Loss = 0.507309
Epoch 6.55: Loss = 0.673111
Epoch 6.56: Loss = 0.640732
Epoch 6.57: Loss = 0.700851
Epoch 6.58: Loss = 0.608719
Epoch 6.59: Loss = 0.720352
Epoch 6.60: Loss = 0.672348
Epoch 6.61: Loss = 0.612854
Epoch 6.62: Loss = 0.665863
Epoch 6.63: Loss = 0.541672
Epoch 6.64: Loss = 0.524063
Epoch 6.65: Loss = 0.646881
Epoch 6.66: Loss = 0.596573
Epoch 6.67: Loss = 0.613754
Epoch 6.68: Loss = 0.780716
Epoch 6.69: Loss = 0.614944
Epoch 6.70: Loss = 0.653259
Epoch 6.71: Loss = 0.532379
Epoch 6.72: Loss = 0.687851
Epoch 6.73: Loss = 0.74617
Epoch 6.74: Loss = 0.659683
Epoch 6.75: Loss = 0.592453
Epoch 6.76: Loss = 0.614655
Epoch 6.77: Loss = 0.65274
Epoch 6.78: Loss = 0.640091
Epoch 6.79: Loss = 0.6138
Epoch 6.80: Loss = 0.588058
Epoch 6.81: Loss = 0.626205
Epoch 6.82: Loss = 0.592041
Epoch 6.83: Loss = 0.691818
Epoch 6.84: Loss = 0.579071
Epoch 6.85: Loss = 0.630539
Epoch 6.86: Loss = 0.658325
Epoch 6.87: Loss = 0.666763
Epoch 6.88: Loss = 0.581009
Epoch 6.89: Loss = 0.780716
Epoch 6.90: Loss = 0.6492
Epoch 6.91: Loss = 0.721786
Epoch 6.92: Loss = 0.630798
Epoch 6.93: Loss = 0.662643
Epoch 6.94: Loss = 0.640518
Epoch 6.95: Loss = 0.680649
Epoch 6.96: Loss = 0.607056
Epoch 6.97: Loss = 0.564133
Epoch 6.98: Loss = 0.635254
Epoch 6.99: Loss = 0.636337
Epoch 6.100: Loss = 0.642395
Epoch 6.101: Loss = 0.683395
Epoch 6.102: Loss = 0.704575
Epoch 6.103: Loss = 0.62352
Epoch 6.104: Loss = 0.564438
Epoch 6.105: Loss = 0.586151
Epoch 6.106: Loss = 0.744568
Epoch 6.107: Loss = 0.662659
Epoch 6.108: Loss = 0.805511
Epoch 6.109: Loss = 0.722672
Epoch 6.110: Loss = 0.628967
Epoch 6.111: Loss = 0.628845
Epoch 6.112: Loss = 0.623657
Epoch 6.113: Loss = 0.619415
Epoch 6.114: Loss = 0.694504
Epoch 6.115: Loss = 0.627594
Epoch 6.116: Loss = 0.59108
Epoch 6.117: Loss = 0.719131
Epoch 6.118: Loss = 0.536255
Epoch 6.119: Loss = 0.620331
Epoch 6.120: Loss = 0.578964
TRAIN LOSS = 0.642029
TRAIN ACC = 80.304 % (48185/60000)
Loss = 0.567505
Loss = 0.667068
Loss = 0.614426
Loss = 0.545013
Loss = 0.60022
Loss = 0.715057
Loss = 0.776321
Loss = 0.727356
Loss = 0.667145
Loss = 0.592499
Loss = 0.788971
Loss = 0.737213
Loss = 0.669693
Loss = 0.720139
Loss = 0.626251
Loss = 0.698654
Loss = 0.613983
Loss = 0.689301
Loss = 0.706146
Loss = 0.651505
TEST LOSS = 0.668723
TEST ACC = 481.85 % (7946/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.612656
Epoch 7.2: Loss = 0.660568
Epoch 7.3: Loss = 0.650146
Epoch 7.4: Loss = 0.554718
Epoch 7.5: Loss = 0.67804
Epoch 7.6: Loss = 0.737595
Epoch 7.7: Loss = 0.632782
Epoch 7.8: Loss = 0.724854
Epoch 7.9: Loss = 0.494293
Epoch 7.10: Loss = 0.455994
Epoch 7.11: Loss = 0.745743
Epoch 7.12: Loss = 0.64827
Epoch 7.13: Loss = 0.681351
Epoch 7.14: Loss = 0.611343
Epoch 7.15: Loss = 0.634384
Epoch 7.16: Loss = 0.717819
Epoch 7.17: Loss = 0.609787
Epoch 7.18: Loss = 0.656006
Epoch 7.19: Loss = 0.596085
Epoch 7.20: Loss = 0.715088
Epoch 7.21: Loss = 0.544632
Epoch 7.22: Loss = 0.474991
Epoch 7.23: Loss = 0.654419
Epoch 7.24: Loss = 0.757431
Epoch 7.25: Loss = 0.636963
Epoch 7.26: Loss = 0.518173
Epoch 7.27: Loss = 0.587067
Epoch 7.28: Loss = 0.615555
Epoch 7.29: Loss = 0.65065
Epoch 7.30: Loss = 0.625641
Epoch 7.31: Loss = 0.688538
Epoch 7.32: Loss = 0.618378
Epoch 7.33: Loss = 0.542542
Epoch 7.34: Loss = 0.689026
Epoch 7.35: Loss = 0.706528
Epoch 7.36: Loss = 0.665878
Epoch 7.37: Loss = 0.66217
Epoch 7.38: Loss = 0.603714
Epoch 7.39: Loss = 0.690643
Epoch 7.40: Loss = 0.656509
Epoch 7.41: Loss = 0.666473
Epoch 7.42: Loss = 0.632462
Epoch 7.43: Loss = 0.640366
Epoch 7.44: Loss = 0.564178
Epoch 7.45: Loss = 0.690689
Epoch 7.46: Loss = 0.74263
Epoch 7.47: Loss = 0.629486
Epoch 7.48: Loss = 0.555161
Epoch 7.49: Loss = 0.638138
Epoch 7.50: Loss = 0.706665
Epoch 7.51: Loss = 0.523575
Epoch 7.52: Loss = 0.69809
Epoch 7.53: Loss = 0.689911
Epoch 7.54: Loss = 0.507187
Epoch 7.55: Loss = 0.655106
Epoch 7.56: Loss = 0.634552
Epoch 7.57: Loss = 0.683777
Epoch 7.58: Loss = 0.606842
Epoch 7.59: Loss = 0.739365
Epoch 7.60: Loss = 0.657928
Epoch 7.61: Loss = 0.620575
Epoch 7.62: Loss = 0.702484
Epoch 7.63: Loss = 0.53775
Epoch 7.64: Loss = 0.531296
Epoch 7.65: Loss = 0.655396
Epoch 7.66: Loss = 0.578064
Epoch 7.67: Loss = 0.608414
Epoch 7.68: Loss = 0.807465
Epoch 7.69: Loss = 0.6091
Epoch 7.70: Loss = 0.654816
Epoch 7.71: Loss = 0.537277
Epoch 7.72: Loss = 0.69252
Epoch 7.73: Loss = 0.740921
Epoch 7.74: Loss = 0.666901
Epoch 7.75: Loss = 0.591827
Epoch 7.76: Loss = 0.616913
Epoch 7.77: Loss = 0.644897
Epoch 7.78: Loss = 0.64386
Epoch 7.79: Loss = 0.605194
Epoch 7.80: Loss = 0.572525
Epoch 7.81: Loss = 0.57872
Epoch 7.82: Loss = 0.609879
Epoch 7.83: Loss = 0.68576
Epoch 7.84: Loss = 0.563583
Epoch 7.85: Loss = 0.591141
Epoch 7.86: Loss = 0.64537
Epoch 7.87: Loss = 0.64772
Epoch 7.88: Loss = 0.576401
Epoch 7.89: Loss = 0.768616
Epoch 7.90: Loss = 0.670273
Epoch 7.91: Loss = 0.704117
Epoch 7.92: Loss = 0.640976
Epoch 7.93: Loss = 0.639175
Epoch 7.94: Loss = 0.637833
Epoch 7.95: Loss = 0.672256
Epoch 7.96: Loss = 0.587723
Epoch 7.97: Loss = 0.535141
Epoch 7.98: Loss = 0.638947
Epoch 7.99: Loss = 0.624207
Epoch 7.100: Loss = 0.670151
Epoch 7.101: Loss = 0.666016
Epoch 7.102: Loss = 0.697037
Epoch 7.103: Loss = 0.623505
Epoch 7.104: Loss = 0.575974
Epoch 7.105: Loss = 0.59433
Epoch 7.106: Loss = 0.716385
Epoch 7.107: Loss = 0.654373
Epoch 7.108: Loss = 0.795807
Epoch 7.109: Loss = 0.711655
Epoch 7.110: Loss = 0.630905
Epoch 7.111: Loss = 0.621948
Epoch 7.112: Loss = 0.62944
Epoch 7.113: Loss = 0.593887
Epoch 7.114: Loss = 0.699402
Epoch 7.115: Loss = 0.608643
Epoch 7.116: Loss = 0.592056
Epoch 7.117: Loss = 0.71553
Epoch 7.118: Loss = 0.543823
Epoch 7.119: Loss = 0.603561
Epoch 7.120: Loss = 0.592911
TRAIN LOSS = 0.637039
TRAIN ACC = 80.6686 % (48403/60000)
Loss = 0.55809
Loss = 0.677765
Loss = 0.62146
Loss = 0.564117
Loss = 0.621124
Loss = 0.727158
Loss = 0.795303
Loss = 0.732193
Loss = 0.685394
Loss = 0.586212
Loss = 0.812546
Loss = 0.764938
Loss = 0.7034
Loss = 0.72551
Loss = 0.64888
Loss = 0.70929
Loss = 0.641281
Loss = 0.708679
Loss = 0.720703
Loss = 0.668991
TEST LOSS = 0.683652
TEST ACC = 484.029 % (7976/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.606308
Epoch 8.2: Loss = 0.665771
Epoch 8.3: Loss = 0.653534
Epoch 8.4: Loss = 0.550079
Epoch 8.5: Loss = 0.667343
Epoch 8.6: Loss = 0.732117
Epoch 8.7: Loss = 0.619736
Epoch 8.8: Loss = 0.721603
Epoch 8.9: Loss = 0.488434
Epoch 8.10: Loss = 0.476089
Epoch 8.11: Loss = 0.747284
Epoch 8.12: Loss = 0.642075
Epoch 8.13: Loss = 0.678345
Epoch 8.14: Loss = 0.617798
Epoch 8.15: Loss = 0.652954
Epoch 8.16: Loss = 0.722351
Epoch 8.17: Loss = 0.627243
Epoch 8.18: Loss = 0.663696
Epoch 8.19: Loss = 0.605118
Epoch 8.20: Loss = 0.725739
Epoch 8.21: Loss = 0.516586
Epoch 8.22: Loss = 0.480423
Epoch 8.23: Loss = 0.647171
Epoch 8.24: Loss = 0.752289
Epoch 8.25: Loss = 0.62854
Epoch 8.26: Loss = 0.520355
Epoch 8.27: Loss = 0.590454
Epoch 8.28: Loss = 0.605423
Epoch 8.29: Loss = 0.651047
Epoch 8.30: Loss = 0.634735
Epoch 8.31: Loss = 0.697525
Epoch 8.32: Loss = 0.595245
Epoch 8.33: Loss = 0.523697
Epoch 8.34: Loss = 0.685654
Epoch 8.35: Loss = 0.694962
Epoch 8.36: Loss = 0.675766
Epoch 8.37: Loss = 0.665863
Epoch 8.38: Loss = 0.588058
Epoch 8.39: Loss = 0.7108
Epoch 8.40: Loss = 0.631622
Epoch 8.41: Loss = 0.669739
Epoch 8.42: Loss = 0.630768
Epoch 8.43: Loss = 0.62912
Epoch 8.44: Loss = 0.569992
Epoch 8.45: Loss = 0.671188
Epoch 8.46: Loss = 0.73645
Epoch 8.47: Loss = 0.628769
Epoch 8.48: Loss = 0.539795
Epoch 8.49: Loss = 0.642853
Epoch 8.50: Loss = 0.699249
Epoch 8.51: Loss = 0.509567
Epoch 8.52: Loss = 0.703674
Epoch 8.53: Loss = 0.676788
Epoch 8.54: Loss = 0.530823
Epoch 8.55: Loss = 0.63533
Epoch 8.56: Loss = 0.632813
Epoch 8.57: Loss = 0.697479
Epoch 8.58: Loss = 0.620651
Epoch 8.59: Loss = 0.727158
Epoch 8.60: Loss = 0.679947
Epoch 8.61: Loss = 0.626373
Epoch 8.62: Loss = 0.687363
Epoch 8.63: Loss = 0.523895
Epoch 8.64: Loss = 0.543442
Epoch 8.65: Loss = 0.616943
Epoch 8.66: Loss = 0.574829
Epoch 8.67: Loss = 0.619202
Epoch 8.68: Loss = 0.820557
Epoch 8.69: Loss = 0.605011
Epoch 8.70: Loss = 0.659302
Epoch 8.71: Loss = 0.549011
Epoch 8.72: Loss = 0.692413
Epoch 8.73: Loss = 0.764359
Epoch 8.74: Loss = 0.658188
Epoch 8.75: Loss = 0.606369
Epoch 8.76: Loss = 0.602539
Epoch 8.77: Loss = 0.637894
Epoch 8.78: Loss = 0.622147
Epoch 8.79: Loss = 0.623917
Epoch 8.80: Loss = 0.584366
Epoch 8.81: Loss = 0.57457
Epoch 8.82: Loss = 0.598145
Epoch 8.83: Loss = 0.70369
Epoch 8.84: Loss = 0.575592
Epoch 8.85: Loss = 0.594894
Epoch 8.86: Loss = 0.656219
Epoch 8.87: Loss = 0.670151
Epoch 8.88: Loss = 0.580673
Epoch 8.89: Loss = 0.770035
Epoch 8.90: Loss = 0.674713
Epoch 8.91: Loss = 0.707855
Epoch 8.92: Loss = 0.622147
Epoch 8.93: Loss = 0.647202
Epoch 8.94: Loss = 0.631393
Epoch 8.95: Loss = 0.693497
Epoch 8.96: Loss = 0.590332
Epoch 8.97: Loss = 0.541641
Epoch 8.98: Loss = 0.64624
Epoch 8.99: Loss = 0.622513
Epoch 8.100: Loss = 0.675735
Epoch 8.101: Loss = 0.655045
Epoch 8.102: Loss = 0.707458
Epoch 8.103: Loss = 0.634659
Epoch 8.104: Loss = 0.572449
Epoch 8.105: Loss = 0.595901
Epoch 8.106: Loss = 0.725494
Epoch 8.107: Loss = 0.661758
Epoch 8.108: Loss = 0.797119
Epoch 8.109: Loss = 0.702087
Epoch 8.110: Loss = 0.615082
Epoch 8.111: Loss = 0.633347
Epoch 8.112: Loss = 0.622269
Epoch 8.113: Loss = 0.608368
Epoch 8.114: Loss = 0.699005
Epoch 8.115: Loss = 0.617111
Epoch 8.116: Loss = 0.591141
Epoch 8.117: Loss = 0.699371
Epoch 8.118: Loss = 0.534195
Epoch 8.119: Loss = 0.603409
Epoch 8.120: Loss = 0.591736
TRAIN LOSS = 0.637131
TRAIN ACC = 81.0059 % (48606/60000)
Loss = 0.550476
Loss = 0.659546
Loss = 0.607727
Loss = 0.556137
Loss = 0.599487
Loss = 0.722855
Loss = 0.797562
Loss = 0.698578
Loss = 0.680756
Loss = 0.58374
Loss = 0.796265
Loss = 0.767212
Loss = 0.677277
Loss = 0.715637
Loss = 0.63913
Loss = 0.690964
Loss = 0.637726
Loss = 0.688736
Loss = 0.712204
Loss = 0.669662
TEST LOSS = 0.672584
TEST ACC = 486.06 % (7974/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.573669
Epoch 9.2: Loss = 0.641098
Epoch 9.3: Loss = 0.639236
Epoch 9.4: Loss = 0.542419
Epoch 9.5: Loss = 0.678085
Epoch 9.6: Loss = 0.74559
Epoch 9.7: Loss = 0.599182
Epoch 9.8: Loss = 0.727646
Epoch 9.9: Loss = 0.485947
Epoch 9.10: Loss = 0.476227
Epoch 9.11: Loss = 0.741867
Epoch 9.12: Loss = 0.646271
Epoch 9.13: Loss = 0.646881
Epoch 9.14: Loss = 0.613235
Epoch 9.15: Loss = 0.65918
Epoch 9.16: Loss = 0.711334
Epoch 9.17: Loss = 0.611664
Epoch 9.18: Loss = 0.645737
Epoch 9.19: Loss = 0.587463
Epoch 9.20: Loss = 0.697617
Epoch 9.21: Loss = 0.529602
Epoch 9.22: Loss = 0.464127
Epoch 9.23: Loss = 0.65509
Epoch 9.24: Loss = 0.746918
Epoch 9.25: Loss = 0.622406
Epoch 9.26: Loss = 0.50061
Epoch 9.27: Loss = 0.583771
Epoch 9.28: Loss = 0.598557
Epoch 9.29: Loss = 0.632324
Epoch 9.30: Loss = 0.656158
Epoch 9.31: Loss = 0.708908
Epoch 9.32: Loss = 0.581131
Epoch 9.33: Loss = 0.527145
Epoch 9.34: Loss = 0.679733
Epoch 9.35: Loss = 0.717377
Epoch 9.36: Loss = 0.675415
Epoch 9.37: Loss = 0.678055
Epoch 9.38: Loss = 0.59845
Epoch 9.39: Loss = 0.706131
Epoch 9.40: Loss = 0.639175
Epoch 9.41: Loss = 0.67984
Epoch 9.42: Loss = 0.636246
Epoch 9.43: Loss = 0.630783
Epoch 9.44: Loss = 0.560822
Epoch 9.45: Loss = 0.700287
Epoch 9.46: Loss = 0.71965
Epoch 9.47: Loss = 0.625015
Epoch 9.48: Loss = 0.532852
Epoch 9.49: Loss = 0.643112
Epoch 9.50: Loss = 0.717102
Epoch 9.51: Loss = 0.504227
Epoch 9.52: Loss = 0.712616
Epoch 9.53: Loss = 0.669571
Epoch 9.54: Loss = 0.547729
Epoch 9.55: Loss = 0.650497
Epoch 9.56: Loss = 0.620453
Epoch 9.57: Loss = 0.689224
Epoch 9.58: Loss = 0.619019
Epoch 9.59: Loss = 0.738403
Epoch 9.60: Loss = 0.68277
Epoch 9.61: Loss = 0.627518
Epoch 9.62: Loss = 0.709106
Epoch 9.63: Loss = 0.527054
Epoch 9.64: Loss = 0.53598
Epoch 9.65: Loss = 0.623978
Epoch 9.66: Loss = 0.572845
Epoch 9.67: Loss = 0.624969
Epoch 9.68: Loss = 0.837173
Epoch 9.69: Loss = 0.590729
Epoch 9.70: Loss = 0.648285
Epoch 9.71: Loss = 0.529816
Epoch 9.72: Loss = 0.697144
Epoch 9.73: Loss = 0.769104
Epoch 9.74: Loss = 0.674072
Epoch 9.75: Loss = 0.594727
Epoch 9.76: Loss = 0.607315
Epoch 9.77: Loss = 0.651581
Epoch 9.78: Loss = 0.62175
Epoch 9.79: Loss = 0.620041
Epoch 9.80: Loss = 0.612137
Epoch 9.81: Loss = 0.587112
Epoch 9.82: Loss = 0.610703
Epoch 9.83: Loss = 0.673279
Epoch 9.84: Loss = 0.569824
Epoch 9.85: Loss = 0.585022
Epoch 9.86: Loss = 0.672668
Epoch 9.87: Loss = 0.709091
Epoch 9.88: Loss = 0.570343
Epoch 9.89: Loss = 0.764435
Epoch 9.90: Loss = 0.652222
Epoch 9.91: Loss = 0.723068
Epoch 9.92: Loss = 0.634201
Epoch 9.93: Loss = 0.647125
Epoch 9.94: Loss = 0.647827
Epoch 9.95: Loss = 0.6875
Epoch 9.96: Loss = 0.617386
Epoch 9.97: Loss = 0.544876
Epoch 9.98: Loss = 0.613052
Epoch 9.99: Loss = 0.635452
Epoch 9.100: Loss = 0.685699
Epoch 9.101: Loss = 0.689087
Epoch 9.102: Loss = 0.737961
Epoch 9.103: Loss = 0.65184
Epoch 9.104: Loss = 0.602966
Epoch 9.105: Loss = 0.607056
Epoch 9.106: Loss = 0.750305
Epoch 9.107: Loss = 0.689697
Epoch 9.108: Loss = 0.858734
Epoch 9.109: Loss = 0.757034
Epoch 9.110: Loss = 0.652191
Epoch 9.111: Loss = 0.636902
Epoch 9.112: Loss = 0.629929
Epoch 9.113: Loss = 0.624023
Epoch 9.114: Loss = 0.691269
Epoch 9.115: Loss = 0.658096
Epoch 9.116: Loss = 0.623825
Epoch 9.117: Loss = 0.687714
Epoch 9.118: Loss = 0.553864
Epoch 9.119: Loss = 0.628433
Epoch 9.120: Loss = 0.622589
TRAIN LOSS = 0.640945
TRAIN ACC = 81.0196 % (48614/60000)
Loss = 0.570358
Loss = 0.686172
Loss = 0.628143
Loss = 0.58725
Loss = 0.631821
Loss = 0.737366
Loss = 0.801865
Loss = 0.707916
Loss = 0.713776
Loss = 0.621552
Loss = 0.803238
Loss = 0.79805
Loss = 0.693619
Loss = 0.762436
Loss = 0.64827
Loss = 0.7164
Loss = 0.649963
Loss = 0.704498
Loss = 0.733505
Loss = 0.682098
TEST LOSS = 0.693915
TEST ACC = 486.139 % (7979/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.582703
Epoch 10.2: Loss = 0.669846
Epoch 10.3: Loss = 0.661423
Epoch 10.4: Loss = 0.561737
Epoch 10.5: Loss = 0.691956
Epoch 10.6: Loss = 0.775116
Epoch 10.7: Loss = 0.631363
Epoch 10.8: Loss = 0.752625
Epoch 10.9: Loss = 0.511658
Epoch 10.10: Loss = 0.493088
Epoch 10.11: Loss = 0.765488
Epoch 10.12: Loss = 0.650787
Epoch 10.13: Loss = 0.653717
Epoch 10.14: Loss = 0.629715
Epoch 10.15: Loss = 0.652771
Epoch 10.16: Loss = 0.737167
Epoch 10.17: Loss = 0.622055
Epoch 10.18: Loss = 0.658691
Epoch 10.19: Loss = 0.624283
Epoch 10.20: Loss = 0.729218
Epoch 10.21: Loss = 0.545944
Epoch 10.22: Loss = 0.474106
Epoch 10.23: Loss = 0.662491
Epoch 10.24: Loss = 0.785645
Epoch 10.25: Loss = 0.635773
Epoch 10.26: Loss = 0.513962
Epoch 10.27: Loss = 0.595856
Epoch 10.28: Loss = 0.613647
Epoch 10.29: Loss = 0.669174
Epoch 10.30: Loss = 0.664871
Epoch 10.31: Loss = 0.737015
Epoch 10.32: Loss = 0.5681
Epoch 10.33: Loss = 0.53038
Epoch 10.34: Loss = 0.710251
Epoch 10.35: Loss = 0.743195
Epoch 10.36: Loss = 0.711624
Epoch 10.37: Loss = 0.669846
Epoch 10.38: Loss = 0.623611
Epoch 10.39: Loss = 0.732788
Epoch 10.40: Loss = 0.651917
Epoch 10.41: Loss = 0.694626
Epoch 10.42: Loss = 0.643356
Epoch 10.43: Loss = 0.665482
Epoch 10.44: Loss = 0.562149
Epoch 10.45: Loss = 0.715118
Epoch 10.46: Loss = 0.7509
Epoch 10.47: Loss = 0.638992
Epoch 10.48: Loss = 0.553162
Epoch 10.49: Loss = 0.655426
Epoch 10.50: Loss = 0.754105
Epoch 10.51: Loss = 0.516434
Epoch 10.52: Loss = 0.7099
Epoch 10.53: Loss = 0.676529
Epoch 10.54: Loss = 0.540924
Epoch 10.55: Loss = 0.660934
Epoch 10.56: Loss = 0.666214
Epoch 10.57: Loss = 0.702347
Epoch 10.58: Loss = 0.648285
Epoch 10.59: Loss = 0.774628
Epoch 10.60: Loss = 0.692703
Epoch 10.61: Loss = 0.633087
Epoch 10.62: Loss = 0.6707
Epoch 10.63: Loss = 0.541534
Epoch 10.64: Loss = 0.539078
Epoch 10.65: Loss = 0.640793
Epoch 10.66: Loss = 0.582886
Epoch 10.67: Loss = 0.640198
Epoch 10.68: Loss = 0.862686
Epoch 10.69: Loss = 0.615463
Epoch 10.70: Loss = 0.65715
Epoch 10.71: Loss = 0.52951
Epoch 10.72: Loss = 0.721329
Epoch 10.73: Loss = 0.800552
Epoch 10.74: Loss = 0.688782
Epoch 10.75: Loss = 0.622208
Epoch 10.76: Loss = 0.624649
Epoch 10.77: Loss = 0.6595
Epoch 10.78: Loss = 0.61499
Epoch 10.79: Loss = 0.614273
Epoch 10.80: Loss = 0.632034
Epoch 10.81: Loss = 0.612717
Epoch 10.82: Loss = 0.611954
Epoch 10.83: Loss = 0.703598
Epoch 10.84: Loss = 0.572723
Epoch 10.85: Loss = 0.611557
Epoch 10.86: Loss = 0.701401
Epoch 10.87: Loss = 0.715988
Epoch 10.88: Loss = 0.584564
Epoch 10.89: Loss = 0.769836
Epoch 10.90: Loss = 0.690552
Epoch 10.91: Loss = 0.72496
Epoch 10.92: Loss = 0.645035
Epoch 10.93: Loss = 0.661087
Epoch 10.94: Loss = 0.669662
Epoch 10.95: Loss = 0.677383
Epoch 10.96: Loss = 0.624725
Epoch 10.97: Loss = 0.560196
Epoch 10.98: Loss = 0.626953
Epoch 10.99: Loss = 0.623489
Epoch 10.100: Loss = 0.703857
Epoch 10.101: Loss = 0.693176
Epoch 10.102: Loss = 0.72757
Epoch 10.103: Loss = 0.649445
Epoch 10.104: Loss = 0.59494
Epoch 10.105: Loss = 0.602737
Epoch 10.106: Loss = 0.763321
Epoch 10.107: Loss = 0.693726
Epoch 10.108: Loss = 0.861511
Epoch 10.109: Loss = 0.764481
Epoch 10.110: Loss = 0.644577
Epoch 10.111: Loss = 0.6633
Epoch 10.112: Loss = 0.64534
Epoch 10.113: Loss = 0.629364
Epoch 10.114: Loss = 0.69664
Epoch 10.115: Loss = 0.648407
Epoch 10.116: Loss = 0.621399
Epoch 10.117: Loss = 0.705292
Epoch 10.118: Loss = 0.554214
Epoch 10.119: Loss = 0.650864
Epoch 10.120: Loss = 0.624893
TRAIN LOSS = 0.65448
TRAIN ACC = 81.1707 % (48705/60000)
Loss = 0.575394
Loss = 0.689362
Loss = 0.654144
Loss = 0.593597
Loss = 0.634964
Loss = 0.754913
Loss = 0.824158
Loss = 0.748795
Loss = 0.735718
Loss = 0.627808
Loss = 0.839661
Loss = 0.820114
Loss = 0.698364
Loss = 0.764557
Loss = 0.666748
Loss = 0.729233
Loss = 0.657486
Loss = 0.731613
Loss = 0.754303
Loss = 0.70047
TEST LOSS = 0.71007
TEST ACC = 487.05 % (7990/10000)
