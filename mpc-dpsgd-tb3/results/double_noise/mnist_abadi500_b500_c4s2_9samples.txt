Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.40251
Epoch 1.2: Loss = 2.33928
Epoch 1.3: Loss = 2.29791
Epoch 1.4: Loss = 2.22914
Epoch 1.5: Loss = 2.18323
Epoch 1.6: Loss = 2.13777
Epoch 1.7: Loss = 2.09492
Epoch 1.8: Loss = 2.04089
Epoch 1.9: Loss = 1.98949
Epoch 1.10: Loss = 1.96252
Epoch 1.11: Loss = 1.86591
Epoch 1.12: Loss = 1.86809
Epoch 1.13: Loss = 1.80795
Epoch 1.14: Loss = 1.82721
Epoch 1.15: Loss = 1.85403
Epoch 1.16: Loss = 1.75691
Epoch 1.17: Loss = 1.69531
Epoch 1.18: Loss = 1.67987
Epoch 1.19: Loss = 1.65164
Epoch 1.20: Loss = 1.60689
Epoch 1.21: Loss = 1.55287
Epoch 1.22: Loss = 1.53534
Epoch 1.23: Loss = 1.46114
Epoch 1.24: Loss = 1.5652
Epoch 1.25: Loss = 1.45163
Epoch 1.26: Loss = 1.50644
Epoch 1.27: Loss = 1.44315
Epoch 1.28: Loss = 1.41066
Epoch 1.29: Loss = 1.4328
Epoch 1.30: Loss = 1.49965
Epoch 1.31: Loss = 1.34267
Epoch 1.32: Loss = 1.33322
Epoch 1.33: Loss = 1.31084
Epoch 1.34: Loss = 1.30522
Epoch 1.35: Loss = 1.22444
Epoch 1.36: Loss = 1.35136
Epoch 1.37: Loss = 1.2198
Epoch 1.38: Loss = 1.15627
Epoch 1.39: Loss = 1.13734
Epoch 1.40: Loss = 1.07169
Epoch 1.41: Loss = 1.10133
Epoch 1.42: Loss = 1.1044
Epoch 1.43: Loss = 1.1077
Epoch 1.44: Loss = 0.976608
Epoch 1.45: Loss = 1.11963
Epoch 1.46: Loss = 1.03128
Epoch 1.47: Loss = 0.988739
Epoch 1.48: Loss = 1.06252
Epoch 1.49: Loss = 0.983948
Epoch 1.50: Loss = 1.06087
Epoch 1.51: Loss = 0.929459
Epoch 1.52: Loss = 0.926636
Epoch 1.53: Loss = 1.00262
Epoch 1.54: Loss = 0.989365
Epoch 1.55: Loss = 0.989517
Epoch 1.56: Loss = 0.897781
Epoch 1.57: Loss = 0.807877
Epoch 1.58: Loss = 0.856796
Epoch 1.59: Loss = 0.899063
Epoch 1.60: Loss = 1.00929
Epoch 1.61: Loss = 0.910263
Epoch 1.62: Loss = 0.952118
Epoch 1.63: Loss = 0.941833
Epoch 1.64: Loss = 0.908249
Epoch 1.65: Loss = 0.958618
Epoch 1.66: Loss = 0.826233
Epoch 1.67: Loss = 0.847733
Epoch 1.68: Loss = 0.701294
Epoch 1.69: Loss = 0.756393
Epoch 1.70: Loss = 0.854797
Epoch 1.71: Loss = 0.775681
Epoch 1.72: Loss = 0.764343
Epoch 1.73: Loss = 0.789749
Epoch 1.74: Loss = 0.65802
Epoch 1.75: Loss = 0.795624
Epoch 1.76: Loss = 0.757523
Epoch 1.77: Loss = 0.728424
Epoch 1.78: Loss = 0.690125
Epoch 1.79: Loss = 0.701752
Epoch 1.80: Loss = 0.796722
Epoch 1.81: Loss = 0.671127
Epoch 1.82: Loss = 0.635529
Epoch 1.83: Loss = 0.801025
Epoch 1.84: Loss = 0.731888
Epoch 1.85: Loss = 0.793549
Epoch 1.86: Loss = 0.713242
Epoch 1.87: Loss = 0.638687
Epoch 1.88: Loss = 0.66568
Epoch 1.89: Loss = 0.752945
Epoch 1.90: Loss = 0.619095
Epoch 1.91: Loss = 0.697647
Epoch 1.92: Loss = 0.694733
Epoch 1.93: Loss = 0.726364
Epoch 1.94: Loss = 0.577057
Epoch 1.95: Loss = 0.705444
Epoch 1.96: Loss = 0.676575
Epoch 1.97: Loss = 0.508789
Epoch 1.98: Loss = 0.615234
Epoch 1.99: Loss = 0.717056
Epoch 1.100: Loss = 0.822372
Epoch 1.101: Loss = 0.696106
Epoch 1.102: Loss = 0.646194
Epoch 1.103: Loss = 0.584824
Epoch 1.104: Loss = 0.550262
Epoch 1.105: Loss = 0.655167
Epoch 1.106: Loss = 0.644028
Epoch 1.107: Loss = 0.545639
Epoch 1.108: Loss = 0.626251
Epoch 1.109: Loss = 0.602371
Epoch 1.110: Loss = 0.614944
Epoch 1.111: Loss = 0.511826
Epoch 1.112: Loss = 0.502686
Epoch 1.113: Loss = 0.585556
Epoch 1.114: Loss = 0.518997
Epoch 1.115: Loss = 0.566681
Epoch 1.116: Loss = 0.595032
Epoch 1.117: Loss = 0.45108
Epoch 1.118: Loss = 0.404526
Epoch 1.119: Loss = 0.436279
Epoch 1.120: Loss = 0.428558
TRAIN LOSS = 1.06247
TRAIN ACC = 70.2194 % (42134/60000)
Loss = 0.644714
Loss = 0.639465
Loss = 0.744904
Loss = 0.708237
Loss = 0.75293
Loss = 0.621628
Loss = 0.606354
Loss = 0.754211
Loss = 0.717712
Loss = 0.672607
Loss = 0.306473
Loss = 0.493027
Loss = 0.346832
Loss = 0.504517
Loss = 0.443634
Loss = 0.40509
Loss = 0.387604
Loss = 0.232895
Loss = 0.401108
Loss = 0.669983
TEST LOSS = 0.552696
TEST ACC = 421.339 % (8317/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.525452
Epoch 2.2: Loss = 0.651627
Epoch 2.3: Loss = 0.640579
Epoch 2.4: Loss = 0.498856
Epoch 2.5: Loss = 0.522308
Epoch 2.6: Loss = 0.51355
Epoch 2.7: Loss = 0.581558
Epoch 2.8: Loss = 0.524353
Epoch 2.9: Loss = 0.534576
Epoch 2.10: Loss = 0.600571
Epoch 2.11: Loss = 0.529236
Epoch 2.12: Loss = 0.522202
Epoch 2.13: Loss = 0.456955
Epoch 2.14: Loss = 0.510468
Epoch 2.15: Loss = 0.655762
Epoch 2.16: Loss = 0.610947
Epoch 2.17: Loss = 0.592041
Epoch 2.18: Loss = 0.702942
Epoch 2.19: Loss = 0.528458
Epoch 2.20: Loss = 0.511566
Epoch 2.21: Loss = 0.478821
Epoch 2.22: Loss = 0.46637
Epoch 2.23: Loss = 0.475693
Epoch 2.24: Loss = 0.677261
Epoch 2.25: Loss = 0.573608
Epoch 2.26: Loss = 0.605576
Epoch 2.27: Loss = 0.61618
Epoch 2.28: Loss = 0.599365
Epoch 2.29: Loss = 0.655045
Epoch 2.30: Loss = 0.778214
Epoch 2.31: Loss = 0.480927
Epoch 2.32: Loss = 0.624268
Epoch 2.33: Loss = 0.545441
Epoch 2.34: Loss = 0.574921
Epoch 2.35: Loss = 0.549591
Epoch 2.36: Loss = 0.632904
Epoch 2.37: Loss = 0.480621
Epoch 2.38: Loss = 0.457199
Epoch 2.39: Loss = 0.51297
Epoch 2.40: Loss = 0.47937
Epoch 2.41: Loss = 0.508148
Epoch 2.42: Loss = 0.622025
Epoch 2.43: Loss = 0.478363
Epoch 2.44: Loss = 0.434097
Epoch 2.45: Loss = 0.528168
Epoch 2.46: Loss = 0.574463
Epoch 2.47: Loss = 0.498428
Epoch 2.48: Loss = 0.579178
Epoch 2.49: Loss = 0.524277
Epoch 2.50: Loss = 0.604156
Epoch 2.51: Loss = 0.479736
Epoch 2.52: Loss = 0.453171
Epoch 2.53: Loss = 0.510696
Epoch 2.54: Loss = 0.618698
Epoch 2.55: Loss = 0.560287
Epoch 2.56: Loss = 0.497696
Epoch 2.57: Loss = 0.456177
Epoch 2.58: Loss = 0.518219
Epoch 2.59: Loss = 0.566406
Epoch 2.60: Loss = 0.622543
Epoch 2.61: Loss = 0.575333
Epoch 2.62: Loss = 0.615677
Epoch 2.63: Loss = 0.661255
Epoch 2.64: Loss = 0.599213
Epoch 2.65: Loss = 0.724213
Epoch 2.66: Loss = 0.517426
Epoch 2.67: Loss = 0.579086
Epoch 2.68: Loss = 0.39064
Epoch 2.69: Loss = 0.4673
Epoch 2.70: Loss = 0.620163
Epoch 2.71: Loss = 0.444885
Epoch 2.72: Loss = 0.465927
Epoch 2.73: Loss = 0.503006
Epoch 2.74: Loss = 0.416138
Epoch 2.75: Loss = 0.622543
Epoch 2.76: Loss = 0.534363
Epoch 2.77: Loss = 0.440033
Epoch 2.78: Loss = 0.463333
Epoch 2.79: Loss = 0.549225
Epoch 2.80: Loss = 0.617493
Epoch 2.81: Loss = 0.470627
Epoch 2.82: Loss = 0.407288
Epoch 2.83: Loss = 0.62262
Epoch 2.84: Loss = 0.481293
Epoch 2.85: Loss = 0.65239
Epoch 2.86: Loss = 0.537689
Epoch 2.87: Loss = 0.428009
Epoch 2.88: Loss = 0.508987
Epoch 2.89: Loss = 0.56694
Epoch 2.90: Loss = 0.428192
Epoch 2.91: Loss = 0.544495
Epoch 2.92: Loss = 0.527084
Epoch 2.93: Loss = 0.613113
Epoch 2.94: Loss = 0.396179
Epoch 2.95: Loss = 0.534363
Epoch 2.96: Loss = 0.584427
Epoch 2.97: Loss = 0.364273
Epoch 2.98: Loss = 0.438675
Epoch 2.99: Loss = 0.583984
Epoch 2.100: Loss = 0.628754
Epoch 2.101: Loss = 0.623459
Epoch 2.102: Loss = 0.50914
Epoch 2.103: Loss = 0.447693
Epoch 2.104: Loss = 0.401993
Epoch 2.105: Loss = 0.546921
Epoch 2.106: Loss = 0.548782
Epoch 2.107: Loss = 0.414017
Epoch 2.108: Loss = 0.499496
Epoch 2.109: Loss = 0.435959
Epoch 2.110: Loss = 0.502289
Epoch 2.111: Loss = 0.403824
Epoch 2.112: Loss = 0.39299
Epoch 2.113: Loss = 0.459152
Epoch 2.114: Loss = 0.38855
Epoch 2.115: Loss = 0.426926
Epoch 2.116: Loss = 0.451477
Epoch 2.117: Loss = 0.323624
Epoch 2.118: Loss = 0.234543
Epoch 2.119: Loss = 0.303909
Epoch 2.120: Loss = 0.349243
TRAIN LOSS = 0.524216
TRAIN ACC = 84.1476 % (50491/60000)
Loss = 0.490021
Loss = 0.577911
Loss = 0.640915
Loss = 0.618637
Loss = 0.661301
Loss = 0.488678
Loss = 0.472336
Loss = 0.725952
Loss = 0.612808
Loss = 0.615265
Loss = 0.249985
Loss = 0.371124
Loss = 0.309799
Loss = 0.409729
Loss = 0.320053
Loss = 0.31369
Loss = 0.316803
Loss = 0.135117
Loss = 0.325241
Loss = 0.601074
TEST LOSS = 0.462822
TEST ACC = 504.909 % (8599/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.426361
Epoch 3.2: Loss = 0.563995
Epoch 3.3: Loss = 0.552765
Epoch 3.4: Loss = 0.360794
Epoch 3.5: Loss = 0.410049
Epoch 3.6: Loss = 0.426178
Epoch 3.7: Loss = 0.459137
Epoch 3.8: Loss = 0.418137
Epoch 3.9: Loss = 0.420197
Epoch 3.10: Loss = 0.460938
Epoch 3.11: Loss = 0.447357
Epoch 3.12: Loss = 0.4048
Epoch 3.13: Loss = 0.365982
Epoch 3.14: Loss = 0.404114
Epoch 3.15: Loss = 0.541992
Epoch 3.16: Loss = 0.494705
Epoch 3.17: Loss = 0.498337
Epoch 3.18: Loss = 0.634842
Epoch 3.19: Loss = 0.454483
Epoch 3.20: Loss = 0.423599
Epoch 3.21: Loss = 0.406479
Epoch 3.22: Loss = 0.37529
Epoch 3.23: Loss = 0.421204
Epoch 3.24: Loss = 0.598572
Epoch 3.25: Loss = 0.500198
Epoch 3.26: Loss = 0.630783
Epoch 3.27: Loss = 0.552094
Epoch 3.28: Loss = 0.524719
Epoch 3.29: Loss = 0.581635
Epoch 3.30: Loss = 0.650955
Epoch 3.31: Loss = 0.40741
Epoch 3.32: Loss = 0.577103
Epoch 3.33: Loss = 0.465469
Epoch 3.34: Loss = 0.518967
Epoch 3.35: Loss = 0.479645
Epoch 3.36: Loss = 0.515823
Epoch 3.37: Loss = 0.40918
Epoch 3.38: Loss = 0.363831
Epoch 3.39: Loss = 0.445892
Epoch 3.40: Loss = 0.427216
Epoch 3.41: Loss = 0.439468
Epoch 3.42: Loss = 0.582382
Epoch 3.43: Loss = 0.407211
Epoch 3.44: Loss = 0.376175
Epoch 3.45: Loss = 0.493927
Epoch 3.46: Loss = 0.525864
Epoch 3.47: Loss = 0.441483
Epoch 3.48: Loss = 0.496078
Epoch 3.49: Loss = 0.46524
Epoch 3.50: Loss = 0.585236
Epoch 3.51: Loss = 0.413315
Epoch 3.52: Loss = 0.416565
Epoch 3.53: Loss = 0.419601
Epoch 3.54: Loss = 0.542694
Epoch 3.55: Loss = 0.522949
Epoch 3.56: Loss = 0.466721
Epoch 3.57: Loss = 0.425034
Epoch 3.58: Loss = 0.498215
Epoch 3.59: Loss = 0.56459
Epoch 3.60: Loss = 0.559387
Epoch 3.61: Loss = 0.513611
Epoch 3.62: Loss = 0.552399
Epoch 3.63: Loss = 0.6306
Epoch 3.64: Loss = 0.543182
Epoch 3.65: Loss = 0.707657
Epoch 3.66: Loss = 0.498947
Epoch 3.67: Loss = 0.500687
Epoch 3.68: Loss = 0.3461
Epoch 3.69: Loss = 0.39444
Epoch 3.70: Loss = 0.594223
Epoch 3.71: Loss = 0.419678
Epoch 3.72: Loss = 0.409454
Epoch 3.73: Loss = 0.472946
Epoch 3.74: Loss = 0.352661
Epoch 3.75: Loss = 0.666611
Epoch 3.76: Loss = 0.493317
Epoch 3.77: Loss = 0.409439
Epoch 3.78: Loss = 0.457748
Epoch 3.79: Loss = 0.513504
Epoch 3.80: Loss = 0.540436
Epoch 3.81: Loss = 0.435562
Epoch 3.82: Loss = 0.383682
Epoch 3.83: Loss = 0.554092
Epoch 3.84: Loss = 0.424683
Epoch 3.85: Loss = 0.615829
Epoch 3.86: Loss = 0.527588
Epoch 3.87: Loss = 0.380981
Epoch 3.88: Loss = 0.51091
Epoch 3.89: Loss = 0.569336
Epoch 3.90: Loss = 0.41713
Epoch 3.91: Loss = 0.524994
Epoch 3.92: Loss = 0.522919
Epoch 3.93: Loss = 0.599976
Epoch 3.94: Loss = 0.400787
Epoch 3.95: Loss = 0.492264
Epoch 3.96: Loss = 0.57077
Epoch 3.97: Loss = 0.36116
Epoch 3.98: Loss = 0.422455
Epoch 3.99: Loss = 0.589783
Epoch 3.100: Loss = 0.697586
Epoch 3.101: Loss = 0.621948
Epoch 3.102: Loss = 0.443771
Epoch 3.103: Loss = 0.459656
Epoch 3.104: Loss = 0.386765
Epoch 3.105: Loss = 0.550827
Epoch 3.106: Loss = 0.56868
Epoch 3.107: Loss = 0.386658
Epoch 3.108: Loss = 0.490982
Epoch 3.109: Loss = 0.405167
Epoch 3.110: Loss = 0.50238
Epoch 3.111: Loss = 0.382217
Epoch 3.112: Loss = 0.384583
Epoch 3.113: Loss = 0.412598
Epoch 3.114: Loss = 0.389389
Epoch 3.115: Loss = 0.39798
Epoch 3.116: Loss = 0.431885
Epoch 3.117: Loss = 0.258575
Epoch 3.118: Loss = 0.220596
Epoch 3.119: Loss = 0.321625
Epoch 3.120: Loss = 0.348129
TRAIN LOSS = 0.475403
TRAIN ACC = 85.6308 % (51381/60000)
Loss = 0.441986
Loss = 0.563065
Loss = 0.645264
Loss = 0.604019
Loss = 0.666946
Loss = 0.496246
Loss = 0.480179
Loss = 0.718872
Loss = 0.601944
Loss = 0.587738
Loss = 0.211761
Loss = 0.407883
Loss = 0.316925
Loss = 0.393036
Loss = 0.297028
Loss = 0.333282
Loss = 0.289169
Loss = 0.117722
Loss = 0.300369
Loss = 0.605148
TEST LOSS = 0.453929
TEST ACC = 513.809 % (8633/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.404922
Epoch 4.2: Loss = 0.573914
Epoch 4.3: Loss = 0.559494
Epoch 4.4: Loss = 0.341583
Epoch 4.5: Loss = 0.407318
Epoch 4.6: Loss = 0.422287
Epoch 4.7: Loss = 0.43483
Epoch 4.8: Loss = 0.432312
Epoch 4.9: Loss = 0.429779
Epoch 4.10: Loss = 0.481506
Epoch 4.11: Loss = 0.429321
Epoch 4.12: Loss = 0.395981
Epoch 4.13: Loss = 0.411118
Epoch 4.14: Loss = 0.405365
Epoch 4.15: Loss = 0.55629
Epoch 4.16: Loss = 0.538254
Epoch 4.17: Loss = 0.519012
Epoch 4.18: Loss = 0.673859
Epoch 4.19: Loss = 0.494125
Epoch 4.20: Loss = 0.398849
Epoch 4.21: Loss = 0.408493
Epoch 4.22: Loss = 0.357544
Epoch 4.23: Loss = 0.428757
Epoch 4.24: Loss = 0.617218
Epoch 4.25: Loss = 0.485138
Epoch 4.26: Loss = 0.595459
Epoch 4.27: Loss = 0.568787
Epoch 4.28: Loss = 0.516296
Epoch 4.29: Loss = 0.621384
Epoch 4.30: Loss = 0.696335
Epoch 4.31: Loss = 0.402802
Epoch 4.32: Loss = 0.544052
Epoch 4.33: Loss = 0.463318
Epoch 4.34: Loss = 0.557938
Epoch 4.35: Loss = 0.434204
Epoch 4.36: Loss = 0.501312
Epoch 4.37: Loss = 0.428833
Epoch 4.38: Loss = 0.395172
Epoch 4.39: Loss = 0.472519
Epoch 4.40: Loss = 0.440445
Epoch 4.41: Loss = 0.443542
Epoch 4.42: Loss = 0.574768
Epoch 4.43: Loss = 0.414841
Epoch 4.44: Loss = 0.368103
Epoch 4.45: Loss = 0.49472
Epoch 4.46: Loss = 0.505096
Epoch 4.47: Loss = 0.490005
Epoch 4.48: Loss = 0.544617
Epoch 4.49: Loss = 0.476334
Epoch 4.50: Loss = 0.613541
Epoch 4.51: Loss = 0.424652
Epoch 4.52: Loss = 0.428696
Epoch 4.53: Loss = 0.449814
Epoch 4.54: Loss = 0.554642
Epoch 4.55: Loss = 0.475006
Epoch 4.56: Loss = 0.478546
Epoch 4.57: Loss = 0.449295
Epoch 4.58: Loss = 0.48761
Epoch 4.59: Loss = 0.551941
Epoch 4.60: Loss = 0.577774
Epoch 4.61: Loss = 0.531158
Epoch 4.62: Loss = 0.564972
Epoch 4.63: Loss = 0.649124
Epoch 4.64: Loss = 0.553207
Epoch 4.65: Loss = 0.732758
Epoch 4.66: Loss = 0.471497
Epoch 4.67: Loss = 0.548599
Epoch 4.68: Loss = 0.345184
Epoch 4.69: Loss = 0.384201
Epoch 4.70: Loss = 0.59375
Epoch 4.71: Loss = 0.438477
Epoch 4.72: Loss = 0.443741
Epoch 4.73: Loss = 0.483704
Epoch 4.74: Loss = 0.367325
Epoch 4.75: Loss = 0.747269
Epoch 4.76: Loss = 0.54155
Epoch 4.77: Loss = 0.466354
Epoch 4.78: Loss = 0.496216
Epoch 4.79: Loss = 0.582672
Epoch 4.80: Loss = 0.553574
Epoch 4.81: Loss = 0.427948
Epoch 4.82: Loss = 0.402771
Epoch 4.83: Loss = 0.586472
Epoch 4.84: Loss = 0.446777
Epoch 4.85: Loss = 0.673874
Epoch 4.86: Loss = 0.620636
Epoch 4.87: Loss = 0.365723
Epoch 4.88: Loss = 0.514053
Epoch 4.89: Loss = 0.626175
Epoch 4.90: Loss = 0.427734
Epoch 4.91: Loss = 0.566818
Epoch 4.92: Loss = 0.553253
Epoch 4.93: Loss = 0.634186
Epoch 4.94: Loss = 0.412552
Epoch 4.95: Loss = 0.550919
Epoch 4.96: Loss = 0.579712
Epoch 4.97: Loss = 0.399872
Epoch 4.98: Loss = 0.462738
Epoch 4.99: Loss = 0.665878
Epoch 4.100: Loss = 0.711044
Epoch 4.101: Loss = 0.661331
Epoch 4.102: Loss = 0.501129
Epoch 4.103: Loss = 0.483322
Epoch 4.104: Loss = 0.408493
Epoch 4.105: Loss = 0.573761
Epoch 4.106: Loss = 0.638153
Epoch 4.107: Loss = 0.407257
Epoch 4.108: Loss = 0.505814
Epoch 4.109: Loss = 0.40509
Epoch 4.110: Loss = 0.529572
Epoch 4.111: Loss = 0.397293
Epoch 4.112: Loss = 0.412766
Epoch 4.113: Loss = 0.440292
Epoch 4.114: Loss = 0.348236
Epoch 4.115: Loss = 0.39325
Epoch 4.116: Loss = 0.464752
Epoch 4.117: Loss = 0.257431
Epoch 4.118: Loss = 0.220688
Epoch 4.119: Loss = 0.319168
Epoch 4.120: Loss = 0.357193
TRAIN LOSS = 0.490829
TRAIN ACC = 85.6735 % (51407/60000)
Loss = 0.451126
Loss = 0.523407
Loss = 0.609802
Loss = 0.605911
Loss = 0.70137
Loss = 0.496323
Loss = 0.489838
Loss = 0.719711
Loss = 0.564148
Loss = 0.596024
Loss = 0.217224
Loss = 0.425232
Loss = 0.291245
Loss = 0.431091
Loss = 0.297455
Loss = 0.388916
Loss = 0.303253
Loss = 0.103836
Loss = 0.293121
Loss = 0.624039
TEST LOSS = 0.456653
TEST ACC = 514.069 % (8664/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.421463
Epoch 5.2: Loss = 0.611557
Epoch 5.3: Loss = 0.588608
Epoch 5.4: Loss = 0.337921
Epoch 5.5: Loss = 0.442642
Epoch 5.6: Loss = 0.462952
Epoch 5.7: Loss = 0.442169
Epoch 5.8: Loss = 0.444611
Epoch 5.9: Loss = 0.440842
Epoch 5.10: Loss = 0.465637
Epoch 5.11: Loss = 0.482559
Epoch 5.12: Loss = 0.428833
Epoch 5.13: Loss = 0.402649
Epoch 5.14: Loss = 0.430817
Epoch 5.15: Loss = 0.584259
Epoch 5.16: Loss = 0.537354
Epoch 5.17: Loss = 0.581848
Epoch 5.18: Loss = 0.691635
Epoch 5.19: Loss = 0.486771
Epoch 5.20: Loss = 0.41655
Epoch 5.21: Loss = 0.409393
Epoch 5.22: Loss = 0.344772
Epoch 5.23: Loss = 0.423264
Epoch 5.24: Loss = 0.650299
Epoch 5.25: Loss = 0.518646
Epoch 5.26: Loss = 0.617508
Epoch 5.27: Loss = 0.594299
Epoch 5.28: Loss = 0.545975
Epoch 5.29: Loss = 0.605667
Epoch 5.30: Loss = 0.653595
Epoch 5.31: Loss = 0.403091
Epoch 5.32: Loss = 0.560516
Epoch 5.33: Loss = 0.441025
Epoch 5.34: Loss = 0.603409
Epoch 5.35: Loss = 0.477936
Epoch 5.36: Loss = 0.537552
Epoch 5.37: Loss = 0.397324
Epoch 5.38: Loss = 0.407791
Epoch 5.39: Loss = 0.469604
Epoch 5.40: Loss = 0.438889
Epoch 5.41: Loss = 0.422058
Epoch 5.42: Loss = 0.616516
Epoch 5.43: Loss = 0.42012
Epoch 5.44: Loss = 0.38063
Epoch 5.45: Loss = 0.469559
Epoch 5.46: Loss = 0.513474
Epoch 5.47: Loss = 0.499863
Epoch 5.48: Loss = 0.569916
Epoch 5.49: Loss = 0.46788
Epoch 5.50: Loss = 0.558304
Epoch 5.51: Loss = 0.412598
Epoch 5.52: Loss = 0.428406
Epoch 5.53: Loss = 0.450592
Epoch 5.54: Loss = 0.59021
Epoch 5.55: Loss = 0.445419
Epoch 5.56: Loss = 0.459793
Epoch 5.57: Loss = 0.404602
Epoch 5.58: Loss = 0.50032
Epoch 5.59: Loss = 0.561295
Epoch 5.60: Loss = 0.557999
Epoch 5.61: Loss = 0.540894
Epoch 5.62: Loss = 0.590714
Epoch 5.63: Loss = 0.68486
Epoch 5.64: Loss = 0.647476
Epoch 5.65: Loss = 0.796127
Epoch 5.66: Loss = 0.471634
Epoch 5.67: Loss = 0.564362
Epoch 5.68: Loss = 0.304672
Epoch 5.69: Loss = 0.409836
Epoch 5.70: Loss = 0.58577
Epoch 5.71: Loss = 0.499985
Epoch 5.72: Loss = 0.415756
Epoch 5.73: Loss = 0.471848
Epoch 5.74: Loss = 0.351974
Epoch 5.75: Loss = 0.790573
Epoch 5.76: Loss = 0.554962
Epoch 5.77: Loss = 0.475937
Epoch 5.78: Loss = 0.521729
Epoch 5.79: Loss = 0.540131
Epoch 5.80: Loss = 0.559219
Epoch 5.81: Loss = 0.471649
Epoch 5.82: Loss = 0.432785
Epoch 5.83: Loss = 0.631241
Epoch 5.84: Loss = 0.459488
Epoch 5.85: Loss = 0.695404
Epoch 5.86: Loss = 0.63681
Epoch 5.87: Loss = 0.345062
Epoch 5.88: Loss = 0.570221
Epoch 5.89: Loss = 0.599411
Epoch 5.90: Loss = 0.383392
Epoch 5.91: Loss = 0.51329
Epoch 5.92: Loss = 0.544983
Epoch 5.93: Loss = 0.65448
Epoch 5.94: Loss = 0.366364
Epoch 5.95: Loss = 0.567123
Epoch 5.96: Loss = 0.564957
Epoch 5.97: Loss = 0.374481
Epoch 5.98: Loss = 0.437454
Epoch 5.99: Loss = 0.660568
Epoch 5.100: Loss = 0.672699
Epoch 5.101: Loss = 0.659348
Epoch 5.102: Loss = 0.493103
Epoch 5.103: Loss = 0.479462
Epoch 5.104: Loss = 0.401642
Epoch 5.105: Loss = 0.589828
Epoch 5.106: Loss = 0.642227
Epoch 5.107: Loss = 0.402435
Epoch 5.108: Loss = 0.559906
Epoch 5.109: Loss = 0.443375
Epoch 5.110: Loss = 0.538513
Epoch 5.111: Loss = 0.387329
Epoch 5.112: Loss = 0.409393
Epoch 5.113: Loss = 0.439011
Epoch 5.114: Loss = 0.321548
Epoch 5.115: Loss = 0.441803
Epoch 5.116: Loss = 0.455444
Epoch 5.117: Loss = 0.300446
Epoch 5.118: Loss = 0.202454
Epoch 5.119: Loss = 0.335251
Epoch 5.120: Loss = 0.377594
TRAIN LOSS = 0.498108
TRAIN ACC = 85.9985 % (51601/60000)
Loss = 0.452835
Loss = 0.552933
Loss = 0.670563
Loss = 0.603867
Loss = 0.716202
Loss = 0.465134
Loss = 0.489563
Loss = 0.772949
Loss = 0.649078
Loss = 0.619278
Loss = 0.209686
Loss = 0.411636
Loss = 0.310074
Loss = 0.424271
Loss = 0.265335
Loss = 0.34552
Loss = 0.30632
Loss = 0.0970917
Loss = 0.289154
Loss = 0.723007
TEST LOSS = 0.468725
TEST ACC = 516.01 % (8710/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.410919
Epoch 6.2: Loss = 0.66217
Epoch 6.3: Loss = 0.565582
Epoch 6.4: Loss = 0.342148
Epoch 6.5: Loss = 0.443802
Epoch 6.6: Loss = 0.435455
Epoch 6.7: Loss = 0.465851
Epoch 6.8: Loss = 0.433975
Epoch 6.9: Loss = 0.41922
Epoch 6.10: Loss = 0.497818
Epoch 6.11: Loss = 0.459335
Epoch 6.12: Loss = 0.432449
Epoch 6.13: Loss = 0.395874
Epoch 6.14: Loss = 0.427277
Epoch 6.15: Loss = 0.582352
Epoch 6.16: Loss = 0.496017
Epoch 6.17: Loss = 0.573044
Epoch 6.18: Loss = 0.693802
Epoch 6.19: Loss = 0.542572
Epoch 6.20: Loss = 0.395386
Epoch 6.21: Loss = 0.422195
Epoch 6.22: Loss = 0.382782
Epoch 6.23: Loss = 0.405228
Epoch 6.24: Loss = 0.646576
Epoch 6.25: Loss = 0.53511
Epoch 6.26: Loss = 0.637085
Epoch 6.27: Loss = 0.562485
Epoch 6.28: Loss = 0.559158
Epoch 6.29: Loss = 0.650955
Epoch 6.30: Loss = 0.692398
Epoch 6.31: Loss = 0.408997
Epoch 6.32: Loss = 0.546127
Epoch 6.33: Loss = 0.439438
Epoch 6.34: Loss = 0.578766
Epoch 6.35: Loss = 0.503693
Epoch 6.36: Loss = 0.527634
Epoch 6.37: Loss = 0.391556
Epoch 6.38: Loss = 0.400787
Epoch 6.39: Loss = 0.492325
Epoch 6.40: Loss = 0.442505
Epoch 6.41: Loss = 0.464996
Epoch 6.42: Loss = 0.686707
Epoch 6.43: Loss = 0.426239
Epoch 6.44: Loss = 0.38829
Epoch 6.45: Loss = 0.507019
Epoch 6.46: Loss = 0.561554
Epoch 6.47: Loss = 0.552002
Epoch 6.48: Loss = 0.585922
Epoch 6.49: Loss = 0.465973
Epoch 6.50: Loss = 0.60437
Epoch 6.51: Loss = 0.461823
Epoch 6.52: Loss = 0.411011
Epoch 6.53: Loss = 0.440994
Epoch 6.54: Loss = 0.619263
Epoch 6.55: Loss = 0.467163
Epoch 6.56: Loss = 0.477631
Epoch 6.57: Loss = 0.44751
Epoch 6.58: Loss = 0.468628
Epoch 6.59: Loss = 0.573181
Epoch 6.60: Loss = 0.558868
Epoch 6.61: Loss = 0.464874
Epoch 6.62: Loss = 0.600128
Epoch 6.63: Loss = 0.663712
Epoch 6.64: Loss = 0.694199
Epoch 6.65: Loss = 0.859406
Epoch 6.66: Loss = 0.474854
Epoch 6.67: Loss = 0.513931
Epoch 6.68: Loss = 0.312164
Epoch 6.69: Loss = 0.429489
Epoch 6.70: Loss = 0.617401
Epoch 6.71: Loss = 0.518829
Epoch 6.72: Loss = 0.370209
Epoch 6.73: Loss = 0.485977
Epoch 6.74: Loss = 0.348419
Epoch 6.75: Loss = 0.836594
Epoch 6.76: Loss = 0.53627
Epoch 6.77: Loss = 0.456085
Epoch 6.78: Loss = 0.525375
Epoch 6.79: Loss = 0.5867
Epoch 6.80: Loss = 0.595459
Epoch 6.81: Loss = 0.510254
Epoch 6.82: Loss = 0.432816
Epoch 6.83: Loss = 0.626511
Epoch 6.84: Loss = 0.45578
Epoch 6.85: Loss = 0.634827
Epoch 6.86: Loss = 0.671097
Epoch 6.87: Loss = 0.375946
Epoch 6.88: Loss = 0.596558
Epoch 6.89: Loss = 0.662766
Epoch 6.90: Loss = 0.408493
Epoch 6.91: Loss = 0.583755
Epoch 6.92: Loss = 0.589691
Epoch 6.93: Loss = 0.69696
Epoch 6.94: Loss = 0.387238
Epoch 6.95: Loss = 0.600769
Epoch 6.96: Loss = 0.58255
Epoch 6.97: Loss = 0.387009
Epoch 6.98: Loss = 0.459869
Epoch 6.99: Loss = 0.668671
Epoch 6.100: Loss = 0.718338
Epoch 6.101: Loss = 0.690384
Epoch 6.102: Loss = 0.546127
Epoch 6.103: Loss = 0.500778
Epoch 6.104: Loss = 0.46553
Epoch 6.105: Loss = 0.638107
Epoch 6.106: Loss = 0.644745
Epoch 6.107: Loss = 0.385117
Epoch 6.108: Loss = 0.589386
Epoch 6.109: Loss = 0.433823
Epoch 6.110: Loss = 0.560028
Epoch 6.111: Loss = 0.404587
Epoch 6.112: Loss = 0.387344
Epoch 6.113: Loss = 0.407104
Epoch 6.114: Loss = 0.353943
Epoch 6.115: Loss = 0.461182
Epoch 6.116: Loss = 0.494354
Epoch 6.117: Loss = 0.257782
Epoch 6.118: Loss = 0.205551
Epoch 6.119: Loss = 0.359772
Epoch 6.120: Loss = 0.395096
TRAIN LOSS = 0.509933
TRAIN ACC = 86.3632 % (51821/60000)
Loss = 0.49408
Loss = 0.570129
Loss = 0.66156
Loss = 0.630203
Loss = 0.753845
Loss = 0.488312
Loss = 0.49704
Loss = 0.743561
Loss = 0.67128
Loss = 0.631348
Loss = 0.252335
Loss = 0.468719
Loss = 0.280853
Loss = 0.44931
Loss = 0.299957
Loss = 0.373367
Loss = 0.335022
Loss = 0.134521
Loss = 0.28743
Loss = 0.681824
TEST LOSS = 0.485235
TEST ACC = 518.21 % (8695/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.474686
Epoch 7.2: Loss = 0.673096
Epoch 7.3: Loss = 0.557236
Epoch 7.4: Loss = 0.328018
Epoch 7.5: Loss = 0.40773
Epoch 7.6: Loss = 0.454575
Epoch 7.7: Loss = 0.463181
Epoch 7.8: Loss = 0.464462
Epoch 7.9: Loss = 0.464127
Epoch 7.10: Loss = 0.545395
Epoch 7.11: Loss = 0.500336
Epoch 7.12: Loss = 0.443878
Epoch 7.13: Loss = 0.41188
Epoch 7.14: Loss = 0.394562
Epoch 7.15: Loss = 0.560455
Epoch 7.16: Loss = 0.529739
Epoch 7.17: Loss = 0.579681
Epoch 7.18: Loss = 0.656448
Epoch 7.19: Loss = 0.556427
Epoch 7.20: Loss = 0.436996
Epoch 7.21: Loss = 0.443634
Epoch 7.22: Loss = 0.409576
Epoch 7.23: Loss = 0.39595
Epoch 7.24: Loss = 0.748917
Epoch 7.25: Loss = 0.564026
Epoch 7.26: Loss = 0.683945
Epoch 7.27: Loss = 0.624985
Epoch 7.28: Loss = 0.552765
Epoch 7.29: Loss = 0.659653
Epoch 7.30: Loss = 0.670074
Epoch 7.31: Loss = 0.384125
Epoch 7.32: Loss = 0.545868
Epoch 7.33: Loss = 0.439957
Epoch 7.34: Loss = 0.561264
Epoch 7.35: Loss = 0.515549
Epoch 7.36: Loss = 0.525726
Epoch 7.37: Loss = 0.383148
Epoch 7.38: Loss = 0.425156
Epoch 7.39: Loss = 0.458542
Epoch 7.40: Loss = 0.466248
Epoch 7.41: Loss = 0.482895
Epoch 7.42: Loss = 0.703552
Epoch 7.43: Loss = 0.39238
Epoch 7.44: Loss = 0.395584
Epoch 7.45: Loss = 0.587997
Epoch 7.46: Loss = 0.581894
Epoch 7.47: Loss = 0.551712
Epoch 7.48: Loss = 0.544571
Epoch 7.49: Loss = 0.513931
Epoch 7.50: Loss = 0.626831
Epoch 7.51: Loss = 0.451584
Epoch 7.52: Loss = 0.423141
Epoch 7.53: Loss = 0.461166
Epoch 7.54: Loss = 0.66243
Epoch 7.55: Loss = 0.414474
Epoch 7.56: Loss = 0.449295
Epoch 7.57: Loss = 0.476883
Epoch 7.58: Loss = 0.506973
Epoch 7.59: Loss = 0.537399
Epoch 7.60: Loss = 0.573196
Epoch 7.61: Loss = 0.573242
Epoch 7.62: Loss = 0.571045
Epoch 7.63: Loss = 0.701004
Epoch 7.64: Loss = 0.677017
Epoch 7.65: Loss = 0.899094
Epoch 7.66: Loss = 0.541168
Epoch 7.67: Loss = 0.579041
Epoch 7.68: Loss = 0.305313
Epoch 7.69: Loss = 0.430603
Epoch 7.70: Loss = 0.586319
Epoch 7.71: Loss = 0.507355
Epoch 7.72: Loss = 0.441391
Epoch 7.73: Loss = 0.567398
Epoch 7.74: Loss = 0.385025
Epoch 7.75: Loss = 0.886917
Epoch 7.76: Loss = 0.600784
Epoch 7.77: Loss = 0.447708
Epoch 7.78: Loss = 0.530212
Epoch 7.79: Loss = 0.639465
Epoch 7.80: Loss = 0.540909
Epoch 7.81: Loss = 0.476593
Epoch 7.82: Loss = 0.474091
Epoch 7.83: Loss = 0.755035
Epoch 7.84: Loss = 0.470352
Epoch 7.85: Loss = 0.728378
Epoch 7.86: Loss = 0.675125
Epoch 7.87: Loss = 0.408081
Epoch 7.88: Loss = 0.575699
Epoch 7.89: Loss = 0.747665
Epoch 7.90: Loss = 0.377258
Epoch 7.91: Loss = 0.581924
Epoch 7.92: Loss = 0.599808
Epoch 7.93: Loss = 0.788086
Epoch 7.94: Loss = 0.444107
Epoch 7.95: Loss = 0.597275
Epoch 7.96: Loss = 0.623917
Epoch 7.97: Loss = 0.439804
Epoch 7.98: Loss = 0.534042
Epoch 7.99: Loss = 0.678543
Epoch 7.100: Loss = 0.76384
Epoch 7.101: Loss = 0.727036
Epoch 7.102: Loss = 0.573242
Epoch 7.103: Loss = 0.488312
Epoch 7.104: Loss = 0.475586
Epoch 7.105: Loss = 0.729614
Epoch 7.106: Loss = 0.68634
Epoch 7.107: Loss = 0.42775
Epoch 7.108: Loss = 0.619873
Epoch 7.109: Loss = 0.47287
Epoch 7.110: Loss = 0.583282
Epoch 7.111: Loss = 0.481613
Epoch 7.112: Loss = 0.402481
Epoch 7.113: Loss = 0.479019
Epoch 7.114: Loss = 0.407974
Epoch 7.115: Loss = 0.458511
Epoch 7.116: Loss = 0.552643
Epoch 7.117: Loss = 0.317413
Epoch 7.118: Loss = 0.242294
Epoch 7.119: Loss = 0.381226
Epoch 7.120: Loss = 0.507507
TRAIN LOSS = 0.532227
TRAIN ACC = 86.145 % (51690/60000)
Loss = 0.557068
Loss = 0.63858
Loss = 0.77478
Loss = 0.694641
Loss = 0.802094
Loss = 0.480316
Loss = 0.572556
Loss = 0.821838
Loss = 0.730392
Loss = 0.65683
Loss = 0.286606
Loss = 0.477829
Loss = 0.372986
Loss = 0.524261
Loss = 0.321793
Loss = 0.418579
Loss = 0.414688
Loss = 0.105072
Loss = 0.355316
Loss = 0.789627
TEST LOSS = 0.539792
TEST ACC = 516.899 % (8624/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.506149
Epoch 8.2: Loss = 0.663696
Epoch 8.3: Loss = 0.598755
Epoch 8.4: Loss = 0.354553
Epoch 8.5: Loss = 0.43277
Epoch 8.6: Loss = 0.445511
Epoch 8.7: Loss = 0.468781
Epoch 8.8: Loss = 0.50264
Epoch 8.9: Loss = 0.497482
Epoch 8.10: Loss = 0.594574
Epoch 8.11: Loss = 0.51741
Epoch 8.12: Loss = 0.511719
Epoch 8.13: Loss = 0.408768
Epoch 8.14: Loss = 0.442825
Epoch 8.15: Loss = 0.571701
Epoch 8.16: Loss = 0.559174
Epoch 8.17: Loss = 0.634506
Epoch 8.18: Loss = 0.792023
Epoch 8.19: Loss = 0.557312
Epoch 8.20: Loss = 0.432205
Epoch 8.21: Loss = 0.495529
Epoch 8.22: Loss = 0.404938
Epoch 8.23: Loss = 0.411209
Epoch 8.24: Loss = 0.715088
Epoch 8.25: Loss = 0.641586
Epoch 8.26: Loss = 0.659973
Epoch 8.27: Loss = 0.606033
Epoch 8.28: Loss = 0.591309
Epoch 8.29: Loss = 0.740036
Epoch 8.30: Loss = 0.746399
Epoch 8.31: Loss = 0.456406
Epoch 8.32: Loss = 0.652802
Epoch 8.33: Loss = 0.509384
Epoch 8.34: Loss = 0.62001
Epoch 8.35: Loss = 0.534393
Epoch 8.36: Loss = 0.655716
Epoch 8.37: Loss = 0.51149
Epoch 8.38: Loss = 0.493134
Epoch 8.39: Loss = 0.463989
Epoch 8.40: Loss = 0.524353
Epoch 8.41: Loss = 0.556503
Epoch 8.42: Loss = 0.790009
Epoch 8.43: Loss = 0.474792
Epoch 8.44: Loss = 0.43782
Epoch 8.45: Loss = 0.609116
Epoch 8.46: Loss = 0.667389
Epoch 8.47: Loss = 0.569153
Epoch 8.48: Loss = 0.55098
Epoch 8.49: Loss = 0.548782
Epoch 8.50: Loss = 0.638046
Epoch 8.51: Loss = 0.454224
Epoch 8.52: Loss = 0.455948
Epoch 8.53: Loss = 0.513794
Epoch 8.54: Loss = 0.65918
Epoch 8.55: Loss = 0.450592
Epoch 8.56: Loss = 0.506958
Epoch 8.57: Loss = 0.519196
Epoch 8.58: Loss = 0.567169
Epoch 8.59: Loss = 0.571198
Epoch 8.60: Loss = 0.567001
Epoch 8.61: Loss = 0.647354
Epoch 8.62: Loss = 0.653183
Epoch 8.63: Loss = 0.6866
Epoch 8.64: Loss = 0.731644
Epoch 8.65: Loss = 0.859955
Epoch 8.66: Loss = 0.503998
Epoch 8.67: Loss = 0.567001
