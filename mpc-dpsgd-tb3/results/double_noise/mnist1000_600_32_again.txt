Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 1000]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 10
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 8
***********************************************************
Epoch 1.1: Loss = 2.32236
Epoch 1.2: Loss = 2.28909
Epoch 1.3: Loss = 2.23468
Epoch 1.4: Loss = 2.1812
Epoch 1.5: Loss = 2.12442
Epoch 1.6: Loss = 2.11655
Epoch 1.7: Loss = 2.06696
Epoch 1.8: Loss = 2.01247
Epoch 1.9: Loss = 1.99013
Epoch 1.10: Loss = 1.93324
Epoch 1.11: Loss = 1.88229
Epoch 1.12: Loss = 1.84088
Epoch 1.13: Loss = 1.81433
Epoch 1.14: Loss = 1.78159
Epoch 1.15: Loss = 1.72598
Epoch 1.16: Loss = 1.68974
Epoch 1.17: Loss = 1.69351
Epoch 1.18: Loss = 1.63171
Epoch 1.19: Loss = 1.58406
Epoch 1.20: Loss = 1.5511
Epoch 1.21: Loss = 1.54758
Epoch 1.22: Loss = 1.50034
Epoch 1.23: Loss = 1.44725
Epoch 1.24: Loss = 1.45317
Epoch 1.25: Loss = 1.43027
Epoch 1.26: Loss = 1.32924
Epoch 1.27: Loss = 1.35863
Epoch 1.28: Loss = 1.37151
Epoch 1.29: Loss = 1.34178
Epoch 1.30: Loss = 1.30734
Epoch 1.31: Loss = 1.25706
Epoch 1.32: Loss = 1.21178
Epoch 1.33: Loss = 1.16995
Epoch 1.34: Loss = 1.21028
Epoch 1.35: Loss = 1.16725
Epoch 1.36: Loss = 1.18938
Epoch 1.37: Loss = 1.12737
Epoch 1.38: Loss = 1.12413
Epoch 1.39: Loss = 1.14085
Epoch 1.40: Loss = 1.07378
Epoch 1.41: Loss = 1.1106
Epoch 1.42: Loss = 1.03349
Epoch 1.43: Loss = 1.0519
Epoch 1.44: Loss = 0.995163
Epoch 1.45: Loss = 0.995941
Epoch 1.46: Loss = 1.00011
Epoch 1.47: Loss = 0.979568
Epoch 1.48: Loss = 0.945724
Epoch 1.49: Loss = 0.92041
Epoch 1.50: Loss = 0.907425
Epoch 1.51: Loss = 0.88559
Epoch 1.52: Loss = 0.901718
Epoch 1.53: Loss = 0.904236
Epoch 1.54: Loss = 0.91452
Epoch 1.55: Loss = 0.82132
Epoch 1.56: Loss = 0.885742
Epoch 1.57: Loss = 0.925858
Epoch 1.58: Loss = 0.859573
Epoch 1.59: Loss = 0.853409
Epoch 1.60: Loss = 0.892212
Epoch 1.61: Loss = 0.79895
Epoch 1.62: Loss = 0.813522
Epoch 1.63: Loss = 0.819107
Epoch 1.64: Loss = 0.749573
Epoch 1.65: Loss = 0.763931
Epoch 1.66: Loss = 0.829117
Epoch 1.67: Loss = 0.7742
Epoch 1.68: Loss = 0.811356
Epoch 1.69: Loss = 0.774597
Epoch 1.70: Loss = 0.733032
Epoch 1.71: Loss = 0.753586
Epoch 1.72: Loss = 0.75679
Epoch 1.73: Loss = 0.715317
Epoch 1.74: Loss = 0.732468
Epoch 1.75: Loss = 0.762253
Epoch 1.76: Loss = 0.741867
Epoch 1.77: Loss = 0.749664
Epoch 1.78: Loss = 0.774551
Epoch 1.79: Loss = 0.755249
Epoch 1.80: Loss = 0.705276
Epoch 1.81: Loss = 0.725784
Epoch 1.82: Loss = 0.735901
Epoch 1.83: Loss = 0.712753
Epoch 1.84: Loss = 0.672668
Epoch 1.85: Loss = 0.72168
Epoch 1.86: Loss = 0.721756
Epoch 1.87: Loss = 0.743088
Epoch 1.88: Loss = 0.7034
Epoch 1.89: Loss = 0.663635
Epoch 1.90: Loss = 0.711761
Epoch 1.91: Loss = 0.630386
Epoch 1.92: Loss = 0.66153
Epoch 1.93: Loss = 0.650436
Epoch 1.94: Loss = 0.591431
Epoch 1.95: Loss = 0.667953
Epoch 1.96: Loss = 0.654907
Epoch 1.97: Loss = 0.64975
Epoch 1.98: Loss = 0.601822
Epoch 1.99: Loss = 0.693146
Epoch 1.100: Loss = 0.636017
TRAIN LOSS = 1.11876
TRAIN ACC = 68.8217 % (41295/60000)
Loss = 0.716492
Loss = 0.721298
Loss = 0.832108
Loss = 0.816055
Loss = 0.732132
Loss = 0.716888
Loss = 0.793549
Loss = 0.780716
Loss = 0.570572
Loss = 0.543686
Loss = 0.424713
Loss = 0.541016
Loss = 0.507965
Loss = 0.467773
Loss = 0.296967
Loss = 0.457993
Loss = 0.774231
TEST LOSS = 0.626164
TEST ACC = 412.949 % (8083/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.687607
Epoch 2.2: Loss = 0.555389
Epoch 2.3: Loss = 0.66478
Epoch 2.4: Loss = 0.59166
Epoch 2.5: Loss = 0.581421
Epoch 2.6: Loss = 0.671036
Epoch 2.7: Loss = 0.620972
Epoch 2.8: Loss = 0.614227
Epoch 2.9: Loss = 0.702057
Epoch 2.10: Loss = 0.645294
Epoch 2.11: Loss = 0.682312
Epoch 2.12: Loss = 0.598038
Epoch 2.13: Loss = 0.648148
Epoch 2.14: Loss = 0.598816
Epoch 2.15: Loss = 0.5811
Epoch 2.16: Loss = 0.573135
Epoch 2.17: Loss = 0.63562
Epoch 2.18: Loss = 0.613556
Epoch 2.19: Loss = 0.610733
Epoch 2.20: Loss = 0.563019
Epoch 2.21: Loss = 0.651627
Epoch 2.22: Loss = 0.622009
Epoch 2.23: Loss = 0.639481
Epoch 2.24: Loss = 0.563843
Epoch 2.25: Loss = 0.544662
Epoch 2.26: Loss = 0.532715
Epoch 2.27: Loss = 0.628433
Epoch 2.28: Loss = 0.59137
Epoch 2.29: Loss = 0.558151
Epoch 2.30: Loss = 0.585846
Epoch 2.31: Loss = 0.58223
Epoch 2.32: Loss = 0.542114
Epoch 2.33: Loss = 0.525787
Epoch 2.34: Loss = 0.590027
Epoch 2.35: Loss = 0.580124
Epoch 2.36: Loss = 0.587662
Epoch 2.37: Loss = 0.554596
Epoch 2.38: Loss = 0.541321
Epoch 2.39: Loss = 0.646225
Epoch 2.40: Loss = 0.488632
Epoch 2.41: Loss = 0.533188
Epoch 2.42: Loss = 0.597916
Epoch 2.43: Loss = 0.494415
Epoch 2.44: Loss = 0.655563
Epoch 2.45: Loss = 0.523712
Epoch 2.46: Loss = 0.548889
Epoch 2.47: Loss = 0.600967
Epoch 2.48: Loss = 0.560333
Epoch 2.49: Loss = 0.599594
Epoch 2.50: Loss = 0.528976
Epoch 2.51: Loss = 0.600159
Epoch 2.52: Loss = 0.61055
Epoch 2.53: Loss = 0.554932
Epoch 2.54: Loss = 0.599533
Epoch 2.55: Loss = 0.636597
Epoch 2.56: Loss = 0.531631
Epoch 2.57: Loss = 0.539902
Epoch 2.58: Loss = 0.586853
Epoch 2.59: Loss = 0.535477
Epoch 2.60: Loss = 0.573837
Epoch 2.61: Loss = 0.497498
Epoch 2.62: Loss = 0.466766
Epoch 2.63: Loss = 0.51123
Epoch 2.64: Loss = 0.527618
Epoch 2.65: Loss = 0.594391
Epoch 2.66: Loss = 0.606781
Epoch 2.67: Loss = 0.497131
Epoch 2.68: Loss = 0.500519
Epoch 2.69: Loss = 0.522278
Epoch 2.70: Loss = 0.481033
Epoch 2.71: Loss = 0.495148
Epoch 2.72: Loss = 0.584702
Epoch 2.73: Loss = 0.549927
Epoch 2.74: Loss = 0.582199
Epoch 2.75: Loss = 0.550766
Epoch 2.76: Loss = 0.514221
Epoch 2.77: Loss = 0.572327
Epoch 2.78: Loss = 0.565918
Epoch 2.79: Loss = 0.549545
Epoch 2.80: Loss = 0.473541
Epoch 2.81: Loss = 0.510559
Epoch 2.82: Loss = 0.543686
Epoch 2.83: Loss = 0.498093
Epoch 2.84: Loss = 0.513992
Epoch 2.85: Loss = 0.558029
Epoch 2.86: Loss = 0.48822
Epoch 2.87: Loss = 0.542587
Epoch 2.88: Loss = 0.628006
Epoch 2.89: Loss = 0.482864
Epoch 2.90: Loss = 0.535355
Epoch 2.91: Loss = 0.466949
Epoch 2.92: Loss = 0.497467
Epoch 2.93: Loss = 0.509567
Epoch 2.94: Loss = 0.555603
Epoch 2.95: Loss = 0.504913
Epoch 2.96: Loss = 0.577408
Epoch 2.97: Loss = 0.54277
Epoch 2.98: Loss = 0.549194
Epoch 2.99: Loss = 0.566727
Epoch 2.100: Loss = 0.469131
TRAIN LOSS = 0.565918
TRAIN ACC = 82.4341 % (49463/60000)
Loss = 0.541779
Loss = 0.629501
Loss = 0.72789
Loss = 0.698578
Loss = 0.568192
Loss = 0.566544
Loss = 0.647354
Loss = 0.631851
Loss = 0.430939
Loss = 0.452835
Loss = 0.317963
Loss = 0.395218
Loss = 0.342789
Loss = 0.363815
Loss = 0.175522
Loss = 0.310791
Loss = 0.668579
TEST LOSS = 0.494837
TEST ACC = 494.629 % (8468/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.563522
Epoch 3.2: Loss = 0.459442
Epoch 3.3: Loss = 0.527985
Epoch 3.4: Loss = 0.591232
Epoch 3.5: Loss = 0.509918
Epoch 3.6: Loss = 0.545456
Epoch 3.7: Loss = 0.495132
Epoch 3.8: Loss = 0.502335
Epoch 3.9: Loss = 0.507751
Epoch 3.10: Loss = 0.565826
Epoch 3.11: Loss = 0.526428
Epoch 3.12: Loss = 0.537216
Epoch 3.13: Loss = 0.537399
Epoch 3.14: Loss = 0.65889
Epoch 3.15: Loss = 0.530334
Epoch 3.16: Loss = 0.662994
Epoch 3.17: Loss = 0.500946
Epoch 3.18: Loss = 0.549576
Epoch 3.19: Loss = 0.50882
Epoch 3.20: Loss = 0.54425
Epoch 3.21: Loss = 0.536179
Epoch 3.22: Loss = 0.53511
Epoch 3.23: Loss = 0.518036
Epoch 3.24: Loss = 0.523666
Epoch 3.25: Loss = 0.483261
Epoch 3.26: Loss = 0.542313
Epoch 3.27: Loss = 0.546677
Epoch 3.28: Loss = 0.485672
Epoch 3.29: Loss = 0.465927
Epoch 3.30: Loss = 0.481766
Epoch 3.31: Loss = 0.54837
Epoch 3.32: Loss = 0.480072
Epoch 3.33: Loss = 0.557404
Epoch 3.34: Loss = 0.620102
Epoch 3.35: Loss = 0.529251
Epoch 3.36: Loss = 0.447739
Epoch 3.37: Loss = 0.554764
Epoch 3.38: Loss = 0.549484
Epoch 3.39: Loss = 0.491013
Epoch 3.40: Loss = 0.520676
Epoch 3.41: Loss = 0.440323
Epoch 3.42: Loss = 0.434265
Epoch 3.43: Loss = 0.443497
Epoch 3.44: Loss = 0.45575
Epoch 3.45: Loss = 0.535431
Epoch 3.46: Loss = 0.478333
Epoch 3.47: Loss = 0.506912
Epoch 3.48: Loss = 0.474106
Epoch 3.49: Loss = 0.518051
Epoch 3.50: Loss = 0.49765
Epoch 3.51: Loss = 0.491364
Epoch 3.52: Loss = 0.465927
Epoch 3.53: Loss = 0.506485
Epoch 3.54: Loss = 0.421997
Epoch 3.55: Loss = 0.471878
Epoch 3.56: Loss = 0.572891
Epoch 3.57: Loss = 0.474899
Epoch 3.58: Loss = 0.501068
Epoch 3.59: Loss = 0.398972
Epoch 3.60: Loss = 0.480301
Epoch 3.61: Loss = 0.556732
Epoch 3.62: Loss = 0.539352
Epoch 3.63: Loss = 0.514114
Epoch 3.64: Loss = 0.536606
Epoch 3.65: Loss = 0.486465
Epoch 3.66: Loss = 0.466309
Epoch 3.67: Loss = 0.535614
Epoch 3.68: Loss = 0.539322
Epoch 3.69: Loss = 0.458572
Epoch 3.70: Loss = 0.455841
Epoch 3.71: Loss = 0.538925
Epoch 3.72: Loss = 0.546112
Epoch 3.73: Loss = 0.435715
Epoch 3.74: Loss = 0.480484
Epoch 3.75: Loss = 0.526291
Epoch 3.76: Loss = 0.576584
Epoch 3.77: Loss = 0.496674
Epoch 3.78: Loss = 0.513916
Epoch 3.79: Loss = 0.440384
Epoch 3.80: Loss = 0.506256
Epoch 3.81: Loss = 0.500916
Epoch 3.82: Loss = 0.4711
Epoch 3.83: Loss = 0.487549
Epoch 3.84: Loss = 0.529938
Epoch 3.85: Loss = 0.547897
Epoch 3.86: Loss = 0.500702
Epoch 3.87: Loss = 0.585709
Epoch 3.88: Loss = 0.55603
Epoch 3.89: Loss = 0.492279
Epoch 3.90: Loss = 0.460846
Epoch 3.91: Loss = 0.415176
Epoch 3.92: Loss = 0.576691
Epoch 3.93: Loss = 0.522522
Epoch 3.94: Loss = 0.532135
Epoch 3.95: Loss = 0.534302
Epoch 3.96: Loss = 0.489441
Epoch 3.97: Loss = 0.51123
Epoch 3.98: Loss = 0.562347
Epoch 3.99: Loss = 0.490723
Epoch 3.100: Loss = 0.481934
TRAIN LOSS = 0.512161
TRAIN ACC = 84.1934 % (50519/60000)
Loss = 0.516174
Loss = 0.634033
Loss = 0.728287
Loss = 0.710968
Loss = 0.500351
Loss = 0.532608
Loss = 0.642761
Loss = 0.624588
Loss = 0.436768
Loss = 0.376892
Loss = 0.294769
Loss = 0.384064
Loss = 0.32547
Loss = 0.373001
Loss = 0.147186
Loss = 0.328705
Loss = 0.767105
TEST LOSS = 0.484082
TEST ACC = 505.19 % (8519/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.439178
Epoch 4.2: Loss = 0.537308
Epoch 4.3: Loss = 0.435394
Epoch 4.4: Loss = 0.530258
Epoch 4.5: Loss = 0.46344
Epoch 4.6: Loss = 0.448761
Epoch 4.7: Loss = 0.513824
Epoch 4.8: Loss = 0.436432
Epoch 4.9: Loss = 0.366653
Epoch 4.10: Loss = 0.470917
Epoch 4.11: Loss = 0.638657
Epoch 4.12: Loss = 0.500946
Epoch 4.13: Loss = 0.490295
Epoch 4.14: Loss = 0.563004
Epoch 4.15: Loss = 0.478546
Epoch 4.16: Loss = 0.487183
Epoch 4.17: Loss = 0.539368
Epoch 4.18: Loss = 0.542709
Epoch 4.19: Loss = 0.565369
Epoch 4.20: Loss = 0.437424
Epoch 4.21: Loss = 0.523697
Epoch 4.22: Loss = 0.487564
Epoch 4.23: Loss = 0.582764
Epoch 4.24: Loss = 0.451874
Epoch 4.25: Loss = 0.534119
Epoch 4.26: Loss = 0.450012
Epoch 4.27: Loss = 0.458496
Epoch 4.28: Loss = 0.527832
Epoch 4.29: Loss = 0.451111
Epoch 4.30: Loss = 0.453201
Epoch 4.31: Loss = 0.512634
Epoch 4.32: Loss = 0.492569
Epoch 4.33: Loss = 0.481354
Epoch 4.34: Loss = 0.472778
Epoch 4.35: Loss = 0.46048
Epoch 4.36: Loss = 0.496002
Epoch 4.37: Loss = 0.44574
Epoch 4.38: Loss = 0.450439
Epoch 4.39: Loss = 0.516037
Epoch 4.40: Loss = 0.382477
Epoch 4.41: Loss = 0.493393
Epoch 4.42: Loss = 0.472931
Epoch 4.43: Loss = 0.462631
Epoch 4.44: Loss = 0.504028
Epoch 4.45: Loss = 0.526566
Epoch 4.46: Loss = 0.581787
Epoch 4.47: Loss = 0.490128
Epoch 4.48: Loss = 0.574677
Epoch 4.49: Loss = 0.549271
Epoch 4.50: Loss = 0.502243
Epoch 4.51: Loss = 0.468994
Epoch 4.52: Loss = 0.543365
Epoch 4.53: Loss = 0.557022
Epoch 4.54: Loss = 0.426468
Epoch 4.55: Loss = 0.487076
Epoch 4.56: Loss = 0.569305
Epoch 4.57: Loss = 0.545074
Epoch 4.58: Loss = 0.573441
Epoch 4.59: Loss = 0.491699
Epoch 4.60: Loss = 0.488373
Epoch 4.61: Loss = 0.497726
Epoch 4.62: Loss = 0.548431
Epoch 4.63: Loss = 0.545822
Epoch 4.64: Loss = 0.526535
Epoch 4.65: Loss = 0.520676
Epoch 4.66: Loss = 0.481186
Epoch 4.67: Loss = 0.390488
Epoch 4.68: Loss = 0.546768
Epoch 4.69: Loss = 0.485718
Epoch 4.70: Loss = 0.494324
Epoch 4.71: Loss = 0.485443
Epoch 4.72: Loss = 0.459259
Epoch 4.73: Loss = 0.517044
Epoch 4.74: Loss = 0.490646
Epoch 4.75: Loss = 0.445938
Epoch 4.76: Loss = 0.47319
Epoch 4.77: Loss = 0.494034
Epoch 4.78: Loss = 0.460022
Epoch 4.79: Loss = 0.600449
Epoch 4.80: Loss = 0.556396
Epoch 4.81: Loss = 0.490738
Epoch 4.82: Loss = 0.539429
Epoch 4.83: Loss = 0.492325
Epoch 4.84: Loss = 0.513306
Epoch 4.85: Loss = 0.467911
Epoch 4.86: Loss = 0.544281
Epoch 4.87: Loss = 0.432602
Epoch 4.88: Loss = 0.477951
Epoch 4.89: Loss = 0.474533
Epoch 4.90: Loss = 0.453278
Epoch 4.91: Loss = 0.418549
Epoch 4.92: Loss = 0.462555
Epoch 4.93: Loss = 0.465469
Epoch 4.94: Loss = 0.422989
Epoch 4.95: Loss = 0.512863
Epoch 4.96: Loss = 0.420334
Epoch 4.97: Loss = 0.577026
Epoch 4.98: Loss = 0.477753
Epoch 4.99: Loss = 0.530991
Epoch 4.100: Loss = 0.584702
TRAIN LOSS = 0.496063
TRAIN ACC = 85.3104 % (51189/60000)
Loss = 0.547119
Loss = 0.624664
Loss = 0.729813
Loss = 0.725449
Loss = 0.461395
Loss = 0.502579
Loss = 0.618591
Loss = 0.578857
Loss = 0.429245
Loss = 0.406937
Loss = 0.311981
Loss = 0.31073
Loss = 0.306656
Loss = 0.358093
Loss = 0.125626
Loss = 0.287292
Loss = 0.75177
TEST LOSS = 0.469572
TEST ACC = 511.89 % (8626/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.425247
Epoch 5.2: Loss = 0.45105
Epoch 5.3: Loss = 0.389038
Epoch 5.4: Loss = 0.528244
Epoch 5.5: Loss = 0.452881
Epoch 5.6: Loss = 0.490631
Epoch 5.7: Loss = 0.448456
Epoch 5.8: Loss = 0.560455
Epoch 5.9: Loss = 0.481339
Epoch 5.10: Loss = 0.524994
Epoch 5.11: Loss = 0.532547
Epoch 5.12: Loss = 0.500504
Epoch 5.13: Loss = 0.585724
Epoch 5.14: Loss = 0.463211
Epoch 5.15: Loss = 0.517487
Epoch 5.16: Loss = 0.558197
Epoch 5.17: Loss = 0.502777
Epoch 5.18: Loss = 0.454758
Epoch 5.19: Loss = 0.498795
Epoch 5.20: Loss = 0.459457
Epoch 5.21: Loss = 0.476501
Epoch 5.22: Loss = 0.489517
Epoch 5.23: Loss = 0.563034
Epoch 5.24: Loss = 0.493454
Epoch 5.25: Loss = 0.447647
Epoch 5.26: Loss = 0.479019
Epoch 5.27: Loss = 0.487091
Epoch 5.28: Loss = 0.533966
Epoch 5.29: Loss = 0.557816
Epoch 5.30: Loss = 0.537399
Epoch 5.31: Loss = 0.525925
Epoch 5.32: Loss = 0.448975
Epoch 5.33: Loss = 0.535797
Epoch 5.34: Loss = 0.458435
Epoch 5.35: Loss = 0.494736
Epoch 5.36: Loss = 0.542236
Epoch 5.37: Loss = 0.500946
Epoch 5.38: Loss = 0.520355
Epoch 5.39: Loss = 0.615662
Epoch 5.40: Loss = 0.493011
Epoch 5.41: Loss = 0.581863
Epoch 5.42: Loss = 0.560638
Epoch 5.43: Loss = 0.435501
Epoch 5.44: Loss = 0.482056
Epoch 5.45: Loss = 0.539825
Epoch 5.46: Loss = 0.446243
Epoch 5.47: Loss = 0.511948
Epoch 5.48: Loss = 0.481598
Epoch 5.49: Loss = 0.513107
Epoch 5.50: Loss = 0.543716
Epoch 5.51: Loss = 0.393356
Epoch 5.52: Loss = 0.492355
Epoch 5.53: Loss = 0.54567
Epoch 5.54: Loss = 0.559814
Epoch 5.55: Loss = 0.416611
Epoch 5.56: Loss = 0.482529
Epoch 5.57: Loss = 0.531067
Epoch 5.58: Loss = 0.503845
Epoch 5.59: Loss = 0.376083
Epoch 5.60: Loss = 0.458237
Epoch 5.61: Loss = 0.462891
Epoch 5.62: Loss = 0.48761
Epoch 5.63: Loss = 0.503433
Epoch 5.64: Loss = 0.531448
Epoch 5.65: Loss = 0.492523
Epoch 5.66: Loss = 0.470627
Epoch 5.67: Loss = 0.518555
Epoch 5.68: Loss = 0.463303
Epoch 5.69: Loss = 0.43364
Epoch 5.70: Loss = 0.494171
Epoch 5.71: Loss = 0.50528
Epoch 5.72: Loss = 0.443253
Epoch 5.73: Loss = 0.560181
Epoch 5.74: Loss = 0.497192
Epoch 5.75: Loss = 0.522949
Epoch 5.76: Loss = 0.455215
Epoch 5.77: Loss = 0.430588
Epoch 5.78: Loss = 0.418472
Epoch 5.79: Loss = 0.568588
Epoch 5.80: Loss = 0.504333
Epoch 5.81: Loss = 0.528046
Epoch 5.82: Loss = 0.510254
Epoch 5.83: Loss = 0.480194
Epoch 5.84: Loss = 0.51825
Epoch 5.85: Loss = 0.452286
Epoch 5.86: Loss = 0.496704
Epoch 5.87: Loss = 0.515228
Epoch 5.88: Loss = 0.450867
Epoch 5.89: Loss = 0.507126
Epoch 5.90: Loss = 0.541031
Epoch 5.91: Loss = 0.529083
Epoch 5.92: Loss = 0.554077
Epoch 5.93: Loss = 0.566132
Epoch 5.94: Loss = 0.543213
Epoch 5.95: Loss = 0.438644
Epoch 5.96: Loss = 0.494598
Epoch 5.97: Loss = 0.459274
Epoch 5.98: Loss = 0.519104
Epoch 5.99: Loss = 0.461731
Epoch 5.100: Loss = 0.485443
TRAIN LOSS = 0.497009
TRAIN ACC = 85.6323 % (51382/60000)
Loss = 0.513138
Loss = 0.582703
Loss = 0.672409
Loss = 0.732895
Loss = 0.432434
Loss = 0.502365
Loss = 0.639206
Loss = 0.577301
Loss = 0.404221
Loss = 0.375824
Loss = 0.323166
Loss = 0.302505
Loss = 0.293365
Loss = 0.367233
Loss = 0.120422
Loss = 0.290115
Loss = 0.808914
TEST LOSS = 0.460115
TEST ACC = 513.82 % (8637/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.489273
Epoch 6.2: Loss = 0.513306
Epoch 6.3: Loss = 0.45575
Epoch 6.4: Loss = 0.554733
Epoch 6.5: Loss = 0.480759
Epoch 6.6: Loss = 0.589218
Epoch 6.7: Loss = 0.436493
Epoch 6.8: Loss = 0.441162
Epoch 6.9: Loss = 0.483871
Epoch 6.10: Loss = 0.458603
Epoch 6.11: Loss = 0.529327
Epoch 6.12: Loss = 0.576706
Epoch 6.13: Loss = 0.52034
Epoch 6.14: Loss = 0.450409
Epoch 6.15: Loss = 0.540192
Epoch 6.16: Loss = 0.45488
Epoch 6.17: Loss = 0.439499
Epoch 6.18: Loss = 0.495911
Epoch 6.19: Loss = 0.603592
Epoch 6.20: Loss = 0.398254
Epoch 6.21: Loss = 0.471313
Epoch 6.22: Loss = 0.515503
Epoch 6.23: Loss = 0.550247
Epoch 6.24: Loss = 0.427475
Epoch 6.25: Loss = 0.503738
Epoch 6.26: Loss = 0.474472
Epoch 6.27: Loss = 0.490356
Epoch 6.28: Loss = 0.570068
Epoch 6.29: Loss = 0.432739
Epoch 6.30: Loss = 0.43692
Epoch 6.31: Loss = 0.48848
Epoch 6.32: Loss = 0.442886
Epoch 6.33: Loss = 0.433182
Epoch 6.34: Loss = 0.602539
Epoch 6.35: Loss = 0.507065
Epoch 6.36: Loss = 0.492676
Epoch 6.37: Loss = 0.440674
Epoch 6.38: Loss = 0.626587
Epoch 6.39: Loss = 0.521118
Epoch 6.40: Loss = 0.482895
Epoch 6.41: Loss = 0.509521
Epoch 6.42: Loss = 0.40892
Epoch 6.43: Loss = 0.499786
Epoch 6.44: Loss = 0.480896
Epoch 6.45: Loss = 0.594513
Epoch 6.46: Loss = 0.542801
Epoch 6.47: Loss = 0.500656
Epoch 6.48: Loss = 0.487503
Epoch 6.49: Loss = 0.622803
Epoch 6.50: Loss = 0.452927
Epoch 6.51: Loss = 0.438644
Epoch 6.52: Loss = 0.473801
Epoch 6.53: Loss = 0.503357
Epoch 6.54: Loss = 0.428253
Epoch 6.55: Loss = 0.494675
Epoch 6.56: Loss = 0.553909
Epoch 6.57: Loss = 0.534409
Epoch 6.58: Loss = 0.530533
Epoch 6.59: Loss = 0.475098
Epoch 6.60: Loss = 0.489746
Epoch 6.61: Loss = 0.557571
Epoch 6.62: Loss = 0.571014
Epoch 6.63: Loss = 0.421722
Epoch 6.64: Loss = 0.567123
Epoch 6.65: Loss = 0.467957
Epoch 6.66: Loss = 0.423401
Epoch 6.67: Loss = 0.497986
Epoch 6.68: Loss = 0.454391
Epoch 6.69: Loss = 0.51915
Epoch 6.70: Loss = 0.420822
Epoch 6.71: Loss = 0.435394
Epoch 6.72: Loss = 0.524429
Epoch 6.73: Loss = 0.476791
Epoch 6.74: Loss = 0.508057
Epoch 6.75: Loss = 0.501816
Epoch 6.76: Loss = 0.493958
Epoch 6.77: Loss = 0.416519
Epoch 6.78: Loss = 0.516983
Epoch 6.79: Loss = 0.480804
Epoch 6.80: Loss = 0.447388
Epoch 6.81: Loss = 0.529526
Epoch 6.82: Loss = 0.559509
Epoch 6.83: Loss = 0.512695
Epoch 6.84: Loss = 0.382462
Epoch 6.85: Loss = 0.543655
Epoch 6.86: Loss = 0.52681
Epoch 6.87: Loss = 0.51889
Epoch 6.88: Loss = 0.558182
Epoch 6.89: Loss = 0.45195
Epoch 6.90: Loss = 0.548096
Epoch 6.91: Loss = 0.464249
Epoch 6.92: Loss = 0.426956
Epoch 6.93: Loss = 0.499969
Epoch 6.94: Loss = 0.501541
Epoch 6.95: Loss = 0.544724
Epoch 6.96: Loss = 0.432983
Epoch 6.97: Loss = 0.491638
Epoch 6.98: Loss = 0.554672
Epoch 6.99: Loss = 0.552917
Epoch 6.100: Loss = 0.557449
TRAIN LOSS = 0.497086
TRAIN ACC = 86.0397 % (51627/60000)
Loss = 0.53511
Loss = 0.624146
Loss = 0.731949
Loss = 0.743073
Loss = 0.466431
Loss = 0.522476
Loss = 0.674927
Loss = 0.600571
Loss = 0.412796
Loss = 0.390411
Loss = 0.331406
Loss = 0.345749
Loss = 0.28006
Loss = 0.414963
Loss = 0.119843
Loss = 0.304306
Loss = 0.843521
TEST LOSS = 0.483634
TEST ACC = 516.269 % (8628/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.512909
Epoch 7.2: Loss = 0.576157
Epoch 7.3: Loss = 0.44043
Epoch 7.4: Loss = 0.45961
Epoch 7.5: Loss = 0.435211
Epoch 7.6: Loss = 0.570618
Epoch 7.7: Loss = 0.440826
Epoch 7.8: Loss = 0.551086
Epoch 7.9: Loss = 0.51915
Epoch 7.10: Loss = 0.542313
Epoch 7.11: Loss = 0.48671
Epoch 7.12: Loss = 0.504379
Epoch 7.13: Loss = 0.543747
Epoch 7.14: Loss = 0.501343
Epoch 7.15: Loss = 0.45079
Epoch 7.16: Loss = 0.457916
Epoch 7.17: Loss = 0.546036
Epoch 7.18: Loss = 0.447205
Epoch 7.19: Loss = 0.547455
Epoch 7.20: Loss = 0.542084
Epoch 7.21: Loss = 0.640945
Epoch 7.22: Loss = 0.549194
Epoch 7.23: Loss = 0.505157
Epoch 7.24: Loss = 0.408813
Epoch 7.25: Loss = 0.467285
Epoch 7.26: Loss = 0.440216
Epoch 7.27: Loss = 0.547897
Epoch 7.28: Loss = 0.434784
Epoch 7.29: Loss = 0.440109
Epoch 7.30: Loss = 0.495285
Epoch 7.31: Loss = 0.460419
Epoch 7.32: Loss = 0.414597
Epoch 7.33: Loss = 0.45401
Epoch 7.34: Loss = 0.475952
Epoch 7.35: Loss = 0.586182
Epoch 7.36: Loss = 0.500549
Epoch 7.37: Loss = 0.472992
Epoch 7.38: Loss = 0.547638
Epoch 7.39: Loss = 0.410858
Epoch 7.40: Loss = 0.483704
Epoch 7.41: Loss = 0.584412
Epoch 7.42: Loss = 0.508591
Epoch 7.43: Loss = 0.499084
Epoch 7.44: Loss = 0.436646
Epoch 7.45: Loss = 0.474533
Epoch 7.46: Loss = 0.493088
Epoch 7.47: Loss = 0.443863
Epoch 7.48: Loss = 0.49411
Epoch 7.49: Loss = 0.507584
Epoch 7.50: Loss = 0.597183
Epoch 7.51: Loss = 0.44809
Epoch 7.52: Loss = 0.436005
Epoch 7.53: Loss = 0.591125
Epoch 7.54: Loss = 0.555344
Epoch 7.55: Loss = 0.535599
Epoch 7.56: Loss = 0.608963
Epoch 7.57: Loss = 0.534042
Epoch 7.58: Loss = 0.538315
Epoch 7.59: Loss = 0.537186
Epoch 7.60: Loss = 0.602371
Epoch 7.61: Loss = 0.595779
Epoch 7.62: Loss = 0.588516
Epoch 7.63: Loss = 0.528564
Epoch 7.64: Loss = 0.490875
Epoch 7.65: Loss = 0.444046
Epoch 7.66: Loss = 0.490707
Epoch 7.67: Loss = 0.519653
Epoch 7.68: Loss = 0.519669
Epoch 7.69: Loss = 0.52005
Epoch 7.70: Loss = 0.448425
Epoch 7.71: Loss = 0.500504
Epoch 7.72: Loss = 0.496506
Epoch 7.73: Loss = 0.471817
Epoch 7.74: Loss = 0.676041
Epoch 7.75: Loss = 0.50589
Epoch 7.76: Loss = 0.601593
Epoch 7.77: Loss = 0.569901
Epoch 7.78: Loss = 0.458481
Epoch 7.79: Loss = 0.464371
Epoch 7.80: Loss = 0.57431
Epoch 7.81: Loss = 0.496033
Epoch 7.82: Loss = 0.519226
Epoch 7.83: Loss = 0.674072
Epoch 7.84: Loss = 0.577347
Epoch 7.85: Loss = 0.455215
Epoch 7.86: Loss = 0.418732
Epoch 7.87: Loss = 0.49736
Epoch 7.88: Loss = 0.448532
Epoch 7.89: Loss = 0.520767
Epoch 7.90: Loss = 0.543427
Epoch 7.91: Loss = 0.449799
Epoch 7.92: Loss = 0.540634
Epoch 7.93: Loss = 0.591934
Epoch 7.94: Loss = 0.453613
Epoch 7.95: Loss = 0.621277
Epoch 7.96: Loss = 0.53241
Epoch 7.97: Loss = 0.496384
Epoch 7.98: Loss = 0.523544
Epoch 7.99: Loss = 0.395889
Epoch 7.100: Loss = 0.562317
TRAIN LOSS = 0.510223
TRAIN ACC = 86.0962 % (51660/60000)
Loss = 0.539948
Loss = 0.651764
Loss = 0.744827
Loss = 0.775848
Loss = 0.443542
Loss = 0.523697
Loss = 0.705154
Loss = 0.604797
Loss = 0.394958
Loss = 0.422913
Loss = 0.343613
Loss = 0.309402
Loss = 0.26889
Loss = 0.403412
Loss = 0.111725
Loss = 0.287659
Loss = 0.819077
TEST LOSS = 0.484692
TEST ACC = 516.599 % (8666/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.503204
Epoch 8.2: Loss = 0.414154
Epoch 8.3: Loss = 0.445755
Epoch 8.4: Loss = 0.482773
Epoch 8.5: Loss = 0.469193
Epoch 8.6: Loss = 0.521591
Epoch 8.7: Loss = 0.543945
Epoch 8.8: Loss = 0.521408
Epoch 8.9: Loss = 0.449158
Epoch 8.10: Loss = 0.528015
Epoch 8.11: Loss = 0.490677
Epoch 8.12: Loss = 0.573822
Epoch 8.13: Loss = 0.474213
Epoch 8.14: Loss = 0.485764
Epoch 8.15: Loss = 0.544052
Epoch 8.16: Loss = 0.561935
Epoch 8.17: Loss = 0.455124
Epoch 8.18: Loss = 0.470749
Epoch 8.19: Loss = 0.515747
Epoch 8.20: Loss = 0.588089
Epoch 8.21: Loss = 0.596695
Epoch 8.22: Loss = 0.508591
Epoch 8.23: Loss = 0.569122
Epoch 8.24: Loss = 0.524582
Epoch 8.25: Loss = 0.529587
Epoch 8.26: Loss = 0.573914
Epoch 8.27: Loss = 0.339584
Epoch 8.28: Loss = 0.461212
Epoch 8.29: Loss = 0.481262
Epoch 8.30: Loss = 0.463898
Epoch 8.31: Loss = 0.489761
Epoch 8.32: Loss = 0.44751
Epoch 8.33: Loss = 0.513412
Epoch 8.34: Loss = 0.590149
Epoch 8.35: Loss = 0.520905
Epoch 8.36: Loss = 0.557144
Epoch 8.37: Loss = 0.48674
Epoch 8.38: Loss = 0.520691
Epoch 8.39: Loss = 0.448944
Epoch 8.40: Loss = 0.442917
Epoch 8.41: Loss = 0.44043
Epoch 8.42: Loss = 0.492935
Epoch 8.43: Loss = 0.481064
Epoch 8.44: Loss = 0.562744
Epoch 8.45: Loss = 0.568344
Epoch 8.46: Loss = 0.466919
Epoch 8.47: Loss = 0.564651
Epoch 8.48: Loss = 0.522629
Epoch 8.49: Loss = 0.489471
Epoch 8.50: Loss = 0.534775
Epoch 8.51: Loss = 0.431992
Epoch 8.52: Loss = 0.477081
Epoch 8.53: Loss = 0.502274
Epoch 8.54: Loss = 0.430695
Epoch 8.55: Loss = 0.540543
Epoch 8.56: Loss = 0.496033
Epoch 8.57: Loss = 0.451645
Epoch 8.58: Loss = 0.545517
Epoch 8.59: Loss = 0.499344
Epoch 8.60: Loss = 0.522308
Epoch 8.61: Loss = 0.524628
Epoch 8.62: Loss = 0.502197
Epoch 8.63: Loss = 0.471924
Epoch 8.64: Loss = 0.515594
Epoch 8.65: Loss = 0.473267
Epoch 8.66: Loss = 0.49379
Epoch 8.67: Loss = 0.484573
Epoch 8.68: Loss = 0.529556
Epoch 8.69: Loss = 0.565262
Epoch 8.70: Loss = 0.501144
Epoch 8.71: Loss = 0.587662
Epoch 8.72: Loss = 0.38588
Epoch 8.73: Loss = 0.500351
Epoch 8.74: Loss = 0.530365
Epoch 8.75: Loss = 0.435318
Epoch 8.76: Loss = 0.549942
Epoch 8.77: Loss = 0.498932
Epoch 8.78: Loss = 0.442429
Epoch 8.79: Loss = 0.551926
Epoch 8.80: Loss = 0.553528
Epoch 8.81: Loss = 0.398315
Epoch 8.82: Loss = 0.521606
Epoch 8.83: Loss = 0.526016
Epoch 8.84: Loss = 0.565674
Epoch 8.85: Loss = 0.448944
Epoch 8.86: Loss = 0.48288
Epoch 8.87: Loss = 0.549011
Epoch 8.88: Loss = 0.464859
Epoch 8.89: Loss = 0.584915
Epoch 8.90: Loss = 0.456879
Epoch 8.91: Loss = 0.567383
Epoch 8.92: Loss = 0.511322
Epoch 8.93: Loss = 0.557602
Epoch 8.94: Loss = 0.432083
Epoch 8.95: Loss = 0.406174
Epoch 8.96: Loss = 0.579422
Epoch 8.97: Loss = 0.488495
Epoch 8.98: Loss = 0.48645
Epoch 8.99: Loss = 0.537109
Epoch 8.100: Loss = 0.513412
TRAIN LOSS = 0.503036
TRAIN ACC = 86.4655 % (51881/60000)
Loss = 0.519135
Loss = 0.655304
Loss = 0.755005
Loss = 0.810043
Loss = 0.425323
Loss = 0.514679
Loss = 0.703476
Loss = 0.603668
Loss = 0.420197
Loss = 0.400253
Loss = 0.395813
Loss = 0.332367
Loss = 0.285233
Loss = 0.400528
Loss = 0.124084
Loss = 0.266815
Loss = 0.804504
TEST LOSS = 0.488895
TEST ACC = 518.81 % (8680/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.598373
Epoch 9.2: Loss = 0.488266
Epoch 9.3: Loss = 0.427429
Epoch 9.4: Loss = 0.471878
Epoch 9.5: Loss = 0.486618
Epoch 9.6: Loss = 0.470871
Epoch 9.7: Loss = 0.580978
Epoch 9.8: Loss = 0.596588
Epoch 9.9: Loss = 0.50972
Epoch 9.10: Loss = 0.54422
Epoch 9.11: Loss = 0.511566
Epoch 9.12: Loss = 0.445343
Epoch 9.13: Loss = 0.410965
Epoch 9.14: Loss = 0.553909
Epoch 9.15: Loss = 0.483887
Epoch 9.16: Loss = 0.433105
Epoch 9.17: Loss = 0.63858
Epoch 9.18: Loss = 0.444748
Epoch 9.19: Loss = 0.481262
Epoch 9.20: Loss = 0.463028
Epoch 9.21: Loss = 0.491028
Epoch 9.22: Loss = 0.42868
Epoch 9.23: Loss = 0.498108
Epoch 9.24: Loss = 0.531067
Epoch 9.25: Loss = 0.522797
Epoch 9.26: Loss = 0.661011
Epoch 9.27: Loss = 0.503265
Epoch 9.28: Loss = 0.400513
Epoch 9.29: Loss = 0.492401
Epoch 9.30: Loss = 0.525803
Epoch 9.31: Loss = 0.526474
Epoch 9.32: Loss = 0.527634
Epoch 9.33: Loss = 0.499054
Epoch 9.34: Loss = 0.437149
Epoch 9.35: Loss = 0.522446
Epoch 9.36: Loss = 0.517731
Epoch 9.37: Loss = 0.51503
Epoch 9.38: Loss = 0.460663
Epoch 9.39: Loss = 0.47876
Epoch 9.40: Loss = 0.57045
Epoch 9.41: Loss = 0.548019
Epoch 9.42: Loss = 0.513367
Epoch 9.43: Loss = 0.480179
Epoch 9.44: Loss = 0.5392
Epoch 9.45: Loss = 0.55368
Epoch 9.46: Loss = 0.579559
Epoch 9.47: Loss = 0.486252
Epoch 9.48: Loss = 0.438553
Epoch 9.49: Loss = 0.472595
Epoch 9.50: Loss = 0.484863
Epoch 9.51: Loss = 0.435425
Epoch 9.52: Loss = 0.462326
Epoch 9.53: Loss = 0.446213
Epoch 9.54: Loss = 0.414993
Epoch 9.55: Loss = 0.532089
Epoch 9.56: Loss = 0.450912
Epoch 9.57: Loss = 0.50798
Epoch 9.58: Loss = 0.464981
Epoch 9.59: Loss = 0.497345
Epoch 9.60: Loss = 0.503494
Epoch 9.61: Loss = 0.500565
Epoch 9.62: Loss = 0.460098
Epoch 9.63: Loss = 0.498672
Epoch 9.64: Loss = 0.403168
Epoch 9.65: Loss = 0.514175
Epoch 9.66: Loss = 0.510132
Epoch 9.67: Loss = 0.477463
Epoch 9.68: Loss = 0.44902
Epoch 9.69: Loss = 0.454666
Epoch 9.70: Loss = 0.454559
Epoch 9.71: Loss = 0.522629
Epoch 9.72: Loss = 0.474487
Epoch 9.73: Loss = 0.514771
Epoch 9.74: Loss = 0.419983
Epoch 9.75: Loss = 0.532669
Epoch 9.76: Loss = 0.440659
Epoch 9.77: Loss = 0.620407
Epoch 9.78: Loss = 0.51947
Epoch 9.79: Loss = 0.518494
Epoch 9.80: Loss = 0.468201
Epoch 9.81: Loss = 0.492371
Epoch 9.82: Loss = 0.427017
Epoch 9.83: Loss = 0.442337
Epoch 9.84: Loss = 0.55954
Epoch 9.85: Loss = 0.509811
Epoch 9.86: Loss = 0.520538
Epoch 9.87: Loss = 0.547485
Epoch 9.88: Loss = 0.497696
Epoch 9.89: Loss = 0.508362
Epoch 9.90: Loss = 0.613358
Epoch 9.91: Loss = 0.440002
Epoch 9.92: Loss = 0.384293
Epoch 9.93: Loss = 0.554352
Epoch 9.94: Loss = 0.586761
Epoch 9.95: Loss = 0.537491
Epoch 9.96: Loss = 0.49614
Epoch 9.97: Loss = 0.455826
Epoch 9.98: Loss = 0.460602
Epoch 9.99: Loss = 0.617447
Epoch 9.100: Loss = 0.504959
TRAIN LOSS = 0.498749
TRAIN ACC = 86.7401 % (52047/60000)
Loss = 0.525635
Loss = 0.629028
Loss = 0.748306
Loss = 0.789352
Loss = 0.44342
Loss = 0.514832
Loss = 0.661972
Loss = 0.550323
Loss = 0.397736
Loss = 0.394577
Loss = 0.394058
Loss = 0.354797
Loss = 0.291672
Loss = 0.40451
Loss = 0.116608
Loss = 0.274185
Loss = 0.785049
TEST LOSS = 0.480863
TEST ACC = 520.47 % (8678/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.494995
Epoch 10.2: Loss = 0.36087
Epoch 10.3: Loss = 0.566147
Epoch 10.4: Loss = 0.472855
Epoch 10.5: Loss = 0.478683
Epoch 10.6: Loss = 0.521317
Epoch 10.7: Loss = 0.375015
Epoch 10.8: Loss = 0.490707
Epoch 10.9: Loss = 0.603043
Epoch 10.10: Loss = 0.467773
Epoch 10.11: Loss = 0.374252
Epoch 10.12: Loss = 0.539734
Epoch 10.13: Loss = 0.44754
Epoch 10.14: Loss = 0.489441
Epoch 10.15: Loss = 0.491302
Epoch 10.16: Loss = 0.411209
Epoch 10.17: Loss = 0.528534
Epoch 10.18: Loss = 0.493866
Epoch 10.19: Loss = 0.572479
Epoch 10.20: Loss = 0.540695
Epoch 10.21: Loss = 0.473206
Epoch 10.22: Loss = 0.465698
Epoch 10.23: Loss = 0.534393
Epoch 10.24: Loss = 0.573898
Epoch 10.25: Loss = 0.504318
Epoch 10.26: Loss = 0.500732
Epoch 10.27: Loss = 0.509628
Epoch 10.28: Loss = 0.451065
Epoch 10.29: Loss = 0.655716
Epoch 10.30: Loss = 0.568222
Epoch 10.31: Loss = 0.461166
Epoch 10.32: Loss = 0.571609
Epoch 10.33: Loss = 0.511887
Epoch 10.34: Loss = 0.492554
Epoch 10.35: Loss = 0.451233
Epoch 10.36: Loss = 0.470154
Epoch 10.37: Loss = 0.442642
Epoch 10.38: Loss = 0.509033
Epoch 10.39: Loss = 0.523865
Epoch 10.40: Loss = 0.480011
Epoch 10.41: Loss = 0.540039
Epoch 10.42: Loss = 0.434692
Epoch 10.43: Loss = 0.54686
Epoch 10.44: Loss = 0.511765
Epoch 10.45: Loss = 0.496719
Epoch 10.46: Loss = 0.491501
Epoch 10.47: Loss = 0.5
Epoch 10.48: Loss = 0.489487
Epoch 10.49: Loss = 0.48938
Epoch 10.50: Loss = 0.477524
Epoch 10.51: Loss = 0.455063
Epoch 10.52: Loss = 0.50412
Epoch 10.53: Loss = 0.456879
Epoch 10.54: Loss = 0.573212
Epoch 10.55: Loss = 0.529434
Epoch 10.56: Loss = 0.482788
Epoch 10.57: Loss = 0.533951
Epoch 10.58: Loss = 0.460678
Epoch 10.59: Loss = 0.561417
Epoch 10.60: Loss = 0.372238
Epoch 10.61: Loss = 0.409821
Epoch 10.62: Loss = 0.561539
Epoch 10.63: Loss = 0.530212
Epoch 10.64: Loss = 0.64682
Epoch 10.65: Loss = 0.519623
Epoch 10.66: Loss = 0.521393
Epoch 10.67: Loss = 0.399185
Epoch 10.68: Loss = 0.455093
Epoch 10.69: Loss = 0.523087
Epoch 10.70: Loss = 0.539948
Epoch 10.71: Loss = 0.482025
Epoch 10.72: Loss = 0.511124
Epoch 10.73: Loss = 0.526352
Epoch 10.74: Loss = 0.492691
Epoch 10.75: Loss = 0.500305
Epoch 10.76: Loss = 0.556808
Epoch 10.77: Loss = 0.433685
Epoch 10.78: Loss = 0.431396
Epoch 10.79: Loss = 0.480881
Epoch 10.80: Loss = 0.54451
Epoch 10.81: Loss = 0.442062
Epoch 10.82: Loss = 0.510056
Epoch 10.83: Loss = 0.581238
Epoch 10.84: Loss = 0.512604
Epoch 10.85: Loss = 0.534225
Epoch 10.86: Loss = 0.537582
Epoch 10.87: Loss = 0.525909
Epoch 10.88: Loss = 0.584946
Epoch 10.89: Loss = 0.548599
Epoch 10.90: Loss = 0.375778
Epoch 10.91: Loss = 0.423981
Epoch 10.92: Loss = 0.423889
Epoch 10.93: Loss = 0.373184
Epoch 10.94: Loss = 0.477417
Epoch 10.95: Loss = 0.662003
Epoch 10.96: Loss = 0.596664
Epoch 10.97: Loss = 0.674759
Epoch 10.98: Loss = 0.504578
Epoch 10.99: Loss = 0.439377
Epoch 10.100: Loss = 0.623337
TRAIN LOSS = 0.501968
TRAIN ACC = 86.9324 % (52162/60000)
Loss = 0.527161
Loss = 0.600159
Loss = 0.76564
Loss = 0.775818
Loss = 0.447601
Loss = 0.496552
Loss = 0.67038
Loss = 0.562027
Loss = 0.430069
Loss = 0.415482
Loss = 0.406769
Loss = 0.361389
Loss = 0.315308
Loss = 0.417511
Loss = 0.113647
Loss = 0.274109
Loss = 0.817688
TEST LOSS = 0.487485
TEST ACC = 521.619 % (8712/10000)
The following benchmarks are including preprocessing (offline phase).
Time = 40223.8 seconds 
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
