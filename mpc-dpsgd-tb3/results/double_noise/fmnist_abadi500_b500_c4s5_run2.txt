Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.56247
Epoch 1.2: Loss = 2.39592
Epoch 1.3: Loss = 2.26688
Epoch 1.4: Loss = 2.12874
Epoch 1.5: Loss = 2.0461
Epoch 1.6: Loss = 2.01964
Epoch 1.7: Loss = 1.91751
Epoch 1.8: Loss = 1.87425
Epoch 1.9: Loss = 1.82777
Epoch 1.10: Loss = 1.72952
Epoch 1.11: Loss = 1.77464
Epoch 1.12: Loss = 1.68498
Epoch 1.13: Loss = 1.62006
Epoch 1.14: Loss = 1.59264
Epoch 1.15: Loss = 1.52077
Epoch 1.16: Loss = 1.53795
Epoch 1.17: Loss = 1.47035
Epoch 1.18: Loss = 1.41644
Epoch 1.19: Loss = 1.37244
Epoch 1.20: Loss = 1.39764
Epoch 1.21: Loss = 1.34926
Epoch 1.22: Loss = 1.31403
Epoch 1.23: Loss = 1.29517
Epoch 1.24: Loss = 1.35855
Epoch 1.25: Loss = 1.29764
Epoch 1.26: Loss = 1.2469
Epoch 1.27: Loss = 1.19109
Epoch 1.28: Loss = 1.21742
Epoch 1.29: Loss = 1.20238
Epoch 1.30: Loss = 1.18437
Epoch 1.31: Loss = 1.20651
Epoch 1.32: Loss = 1.2195
Epoch 1.33: Loss = 1.09579
Epoch 1.34: Loss = 1.19084
Epoch 1.35: Loss = 1.20689
Epoch 1.36: Loss = 1.1783
Epoch 1.37: Loss = 1.14194
Epoch 1.38: Loss = 1.10245
Epoch 1.39: Loss = 1.09824
Epoch 1.40: Loss = 1.09889
Epoch 1.41: Loss = 1.1174
Epoch 1.42: Loss = 1.06667
Epoch 1.43: Loss = 1.03496
Epoch 1.44: Loss = 1.01285
Epoch 1.45: Loss = 1.06554
Epoch 1.46: Loss = 1.08563
Epoch 1.47: Loss = 1.03506
Epoch 1.48: Loss = 0.990646
Epoch 1.49: Loss = 1.04483
Epoch 1.50: Loss = 1.02205
Epoch 1.51: Loss = 0.952423
Epoch 1.52: Loss = 1.06009
Epoch 1.53: Loss = 1.03696
Epoch 1.54: Loss = 0.905533
Epoch 1.55: Loss = 1.00475
Epoch 1.56: Loss = 1.00659
Epoch 1.57: Loss = 1.00394
Epoch 1.58: Loss = 0.978821
Epoch 1.59: Loss = 0.975067
Epoch 1.60: Loss = 1.00482
Epoch 1.61: Loss = 0.915924
Epoch 1.62: Loss = 1.05502
Epoch 1.63: Loss = 0.853271
Epoch 1.64: Loss = 0.922165
Epoch 1.65: Loss = 0.89473
Epoch 1.66: Loss = 0.932083
Epoch 1.67: Loss = 0.830154
Epoch 1.68: Loss = 0.965073
Epoch 1.69: Loss = 0.877792
Epoch 1.70: Loss = 0.88028
Epoch 1.71: Loss = 0.835037
Epoch 1.72: Loss = 0.87468
Epoch 1.73: Loss = 0.968109
Epoch 1.74: Loss = 0.956345
Epoch 1.75: Loss = 0.84346
Epoch 1.76: Loss = 0.863556
Epoch 1.77: Loss = 0.845947
Epoch 1.78: Loss = 0.857758
Epoch 1.79: Loss = 0.781616
Epoch 1.80: Loss = 0.868469
Epoch 1.81: Loss = 0.825531
Epoch 1.82: Loss = 0.833939
Epoch 1.83: Loss = 0.891968
Epoch 1.84: Loss = 0.824112
Epoch 1.85: Loss = 0.823563
Epoch 1.86: Loss = 0.910156
Epoch 1.87: Loss = 0.88092
Epoch 1.88: Loss = 0.745911
Epoch 1.89: Loss = 0.922485
Epoch 1.90: Loss = 0.82756
Epoch 1.91: Loss = 0.880173
Epoch 1.92: Loss = 0.848358
Epoch 1.93: Loss = 0.854828
Epoch 1.94: Loss = 0.810028
Epoch 1.95: Loss = 0.858398
Epoch 1.96: Loss = 0.808685
Epoch 1.97: Loss = 0.720337
Epoch 1.98: Loss = 0.811996
Epoch 1.99: Loss = 0.820328
Epoch 1.100: Loss = 0.757813
Epoch 1.101: Loss = 0.861618
Epoch 1.102: Loss = 0.87648
Epoch 1.103: Loss = 0.812866
Epoch 1.104: Loss = 0.785645
Epoch 1.105: Loss = 0.749771
Epoch 1.106: Loss = 0.942612
Epoch 1.107: Loss = 0.829193
Epoch 1.108: Loss = 0.839279
Epoch 1.109: Loss = 0.820145
Epoch 1.110: Loss = 0.821701
Epoch 1.111: Loss = 0.747238
Epoch 1.112: Loss = 0.74295
Epoch 1.113: Loss = 0.803329
Epoch 1.114: Loss = 0.793488
Epoch 1.115: Loss = 0.796112
Epoch 1.116: Loss = 0.737411
Epoch 1.117: Loss = 0.847397
Epoch 1.118: Loss = 0.723984
Epoch 1.119: Loss = 0.757278
Epoch 1.120: Loss = 0.739609
TRAIN LOSS = 1.09387
TRAIN ACC = 63.2782 % (37969/60000)
Loss = 0.705765
Loss = 0.827408
Loss = 0.806488
Loss = 0.725525
Loss = 0.724655
Loss = 0.861557
Loss = 0.903748
Loss = 0.866592
Loss = 0.800125
Loss = 0.734695
Loss = 0.876877
Loss = 0.807114
Loss = 0.819534
Loss = 0.817169
Loss = 0.782822
Loss = 0.817215
Loss = 0.759521
Loss = 0.790207
Loss = 0.865219
Loss = 0.785873
TEST LOSS = 0.803905
TEST ACC = 379.689 % (7170/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.812088
Epoch 2.2: Loss = 0.737503
Epoch 2.3: Loss = 0.807571
Epoch 2.4: Loss = 0.698273
Epoch 2.5: Loss = 0.767441
Epoch 2.6: Loss = 0.83226
Epoch 2.7: Loss = 0.750702
Epoch 2.8: Loss = 0.786819
Epoch 2.9: Loss = 0.664505
Epoch 2.10: Loss = 0.611755
Epoch 2.11: Loss = 0.854935
Epoch 2.12: Loss = 0.791824
Epoch 2.13: Loss = 0.737488
Epoch 2.14: Loss = 0.780472
Epoch 2.15: Loss = 0.761948
Epoch 2.16: Loss = 0.822037
Epoch 2.17: Loss = 0.71608
Epoch 2.18: Loss = 0.790375
Epoch 2.19: Loss = 0.724045
Epoch 2.20: Loss = 0.837982
Epoch 2.21: Loss = 0.705124
Epoch 2.22: Loss = 0.673889
Epoch 2.23: Loss = 0.757813
Epoch 2.24: Loss = 0.809372
Epoch 2.25: Loss = 0.738556
Epoch 2.26: Loss = 0.69252
Epoch 2.27: Loss = 0.713745
Epoch 2.28: Loss = 0.745117
Epoch 2.29: Loss = 0.760864
Epoch 2.30: Loss = 0.722656
Epoch 2.31: Loss = 0.784836
Epoch 2.32: Loss = 0.735245
Epoch 2.33: Loss = 0.655243
Epoch 2.34: Loss = 0.792953
Epoch 2.35: Loss = 0.768982
Epoch 2.36: Loss = 0.818741
Epoch 2.37: Loss = 0.768219
Epoch 2.38: Loss = 0.766434
Epoch 2.39: Loss = 0.786758
Epoch 2.40: Loss = 0.742035
Epoch 2.41: Loss = 0.751053
Epoch 2.42: Loss = 0.746109
Epoch 2.43: Loss = 0.751709
Epoch 2.44: Loss = 0.675919
Epoch 2.45: Loss = 0.79863
Epoch 2.46: Loss = 0.86412
Epoch 2.47: Loss = 0.757385
Epoch 2.48: Loss = 0.677658
Epoch 2.49: Loss = 0.776016
Epoch 2.50: Loss = 0.750244
Epoch 2.51: Loss = 0.66333
Epoch 2.52: Loss = 0.787628
Epoch 2.53: Loss = 0.831589
Epoch 2.54: Loss = 0.6483
Epoch 2.55: Loss = 0.760895
Epoch 2.56: Loss = 0.762711
Epoch 2.57: Loss = 0.788986
Epoch 2.58: Loss = 0.765182
Epoch 2.59: Loss = 0.742676
Epoch 2.60: Loss = 0.770538
Epoch 2.61: Loss = 0.687393
Epoch 2.62: Loss = 0.84938
Epoch 2.63: Loss = 0.630219
Epoch 2.64: Loss = 0.656174
Epoch 2.65: Loss = 0.72905
Epoch 2.66: Loss = 0.727997
Epoch 2.67: Loss = 0.702545
Epoch 2.68: Loss = 0.848663
Epoch 2.69: Loss = 0.739914
Epoch 2.70: Loss = 0.753662
Epoch 2.71: Loss = 0.639084
Epoch 2.72: Loss = 0.724319
Epoch 2.73: Loss = 0.841156
Epoch 2.74: Loss = 0.785248
Epoch 2.75: Loss = 0.705704
Epoch 2.76: Loss = 0.737061
Epoch 2.77: Loss = 0.719711
Epoch 2.78: Loss = 0.743927
Epoch 2.79: Loss = 0.690964
Epoch 2.80: Loss = 0.696701
Epoch 2.81: Loss = 0.694077
Epoch 2.82: Loss = 0.654678
Epoch 2.83: Loss = 0.773621
Epoch 2.84: Loss = 0.680542
Epoch 2.85: Loss = 0.685684
Epoch 2.86: Loss = 0.773468
Epoch 2.87: Loss = 0.740112
Epoch 2.88: Loss = 0.635544
Epoch 2.89: Loss = 0.829407
Epoch 2.90: Loss = 0.733414
Epoch 2.91: Loss = 0.789963
Epoch 2.92: Loss = 0.701401
Epoch 2.93: Loss = 0.768738
Epoch 2.94: Loss = 0.691269
Epoch 2.95: Loss = 0.76825
Epoch 2.96: Loss = 0.691315
Epoch 2.97: Loss = 0.639008
Epoch 2.98: Loss = 0.735046
Epoch 2.99: Loss = 0.734772
Epoch 2.100: Loss = 0.732376
Epoch 2.101: Loss = 0.796448
Epoch 2.102: Loss = 0.76091
Epoch 2.103: Loss = 0.747498
Epoch 2.104: Loss = 0.701385
Epoch 2.105: Loss = 0.653717
Epoch 2.106: Loss = 0.830612
Epoch 2.107: Loss = 0.769287
Epoch 2.108: Loss = 0.810364
Epoch 2.109: Loss = 0.77388
Epoch 2.110: Loss = 0.781036
Epoch 2.111: Loss = 0.648041
Epoch 2.112: Loss = 0.675461
Epoch 2.113: Loss = 0.745193
Epoch 2.114: Loss = 0.73642
Epoch 2.115: Loss = 0.779053
Epoch 2.116: Loss = 0.656036
Epoch 2.117: Loss = 0.787872
Epoch 2.118: Loss = 0.667297
Epoch 2.119: Loss = 0.707092
Epoch 2.120: Loss = 0.681305
TRAIN LOSS = 0.742233
TRAIN ACC = 75.2075 % (45127/60000)
Loss = 0.700531
Loss = 0.785767
Loss = 0.773865
Loss = 0.666031
Loss = 0.669037
Loss = 0.869354
Loss = 0.891281
Loss = 0.847717
Loss = 0.777176
Loss = 0.671738
Loss = 0.848709
Loss = 0.806931
Loss = 0.793747
Loss = 0.800751
Loss = 0.768234
Loss = 0.783829
Loss = 0.717575
Loss = 0.792953
Loss = 0.8358
Loss = 0.750183
TEST LOSS = 0.77756
TEST ACC = 451.27 % (7550/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.709641
Epoch 3.2: Loss = 0.722519
Epoch 3.3: Loss = 0.751816
Epoch 3.4: Loss = 0.644226
Epoch 3.5: Loss = 0.74324
Epoch 3.6: Loss = 0.822784
Epoch 3.7: Loss = 0.773712
Epoch 3.8: Loss = 0.804016
Epoch 3.9: Loss = 0.620193
Epoch 3.10: Loss = 0.557816
Epoch 3.11: Loss = 0.853149
Epoch 3.12: Loss = 0.739349
Epoch 3.13: Loss = 0.777969
Epoch 3.14: Loss = 0.736649
Epoch 3.15: Loss = 0.755188
Epoch 3.16: Loss = 0.84375
Epoch 3.17: Loss = 0.681671
Epoch 3.18: Loss = 0.775406
Epoch 3.19: Loss = 0.714355
Epoch 3.20: Loss = 0.847809
Epoch 3.21: Loss = 0.677795
Epoch 3.22: Loss = 0.660172
Epoch 3.23: Loss = 0.736603
Epoch 3.24: Loss = 0.803543
Epoch 3.25: Loss = 0.677032
Epoch 3.26: Loss = 0.656708
Epoch 3.27: Loss = 0.709686
Epoch 3.28: Loss = 0.70697
Epoch 3.29: Loss = 0.762558
Epoch 3.30: Loss = 0.718399
Epoch 3.31: Loss = 0.768646
Epoch 3.32: Loss = 0.766693
Epoch 3.33: Loss = 0.646057
Epoch 3.34: Loss = 0.854355
Epoch 3.35: Loss = 0.775024
Epoch 3.36: Loss = 0.822235
Epoch 3.37: Loss = 0.804504
Epoch 3.38: Loss = 0.722244
Epoch 3.39: Loss = 0.792465
Epoch 3.40: Loss = 0.745224
Epoch 3.41: Loss = 0.803833
Epoch 3.42: Loss = 0.731064
Epoch 3.43: Loss = 0.71875
Epoch 3.44: Loss = 0.640244
Epoch 3.45: Loss = 0.777573
Epoch 3.46: Loss = 0.873138
Epoch 3.47: Loss = 0.765762
Epoch 3.48: Loss = 0.65921
Epoch 3.49: Loss = 0.808563
Epoch 3.50: Loss = 0.741592
Epoch 3.51: Loss = 0.614349
Epoch 3.52: Loss = 0.782883
Epoch 3.53: Loss = 0.781052
Epoch 3.54: Loss = 0.603134
Epoch 3.55: Loss = 0.740234
Epoch 3.56: Loss = 0.819748
Epoch 3.57: Loss = 0.783539
Epoch 3.58: Loss = 0.724701
Epoch 3.59: Loss = 0.770065
Epoch 3.60: Loss = 0.723938
Epoch 3.61: Loss = 0.660004
Epoch 3.62: Loss = 0.772339
Epoch 3.63: Loss = 0.615723
Epoch 3.64: Loss = 0.634872
Epoch 3.65: Loss = 0.73407
Epoch 3.66: Loss = 0.703796
Epoch 3.67: Loss = 0.665451
Epoch 3.68: Loss = 0.839127
Epoch 3.69: Loss = 0.76474
Epoch 3.70: Loss = 0.737946
Epoch 3.71: Loss = 0.622772
Epoch 3.72: Loss = 0.728729
Epoch 3.73: Loss = 0.853378
Epoch 3.74: Loss = 0.803101
Epoch 3.75: Loss = 0.706375
Epoch 3.76: Loss = 0.726517
Epoch 3.77: Loss = 0.680908
Epoch 3.78: Loss = 0.739578
Epoch 3.79: Loss = 0.709656
Epoch 3.80: Loss = 0.69072
Epoch 3.81: Loss = 0.763474
Epoch 3.82: Loss = 0.712753
Epoch 3.83: Loss = 0.761322
Epoch 3.84: Loss = 0.698837
Epoch 3.85: Loss = 0.719055
Epoch 3.86: Loss = 0.756424
Epoch 3.87: Loss = 0.725189
Epoch 3.88: Loss = 0.665054
Epoch 3.89: Loss = 0.833496
Epoch 3.90: Loss = 0.777145
Epoch 3.91: Loss = 0.821136
Epoch 3.92: Loss = 0.74559
Epoch 3.93: Loss = 0.784286
Epoch 3.94: Loss = 0.782593
Epoch 3.95: Loss = 0.747391
Epoch 3.96: Loss = 0.706238
Epoch 3.97: Loss = 0.645477
Epoch 3.98: Loss = 0.74823
Epoch 3.99: Loss = 0.720108
Epoch 3.100: Loss = 0.709732
Epoch 3.101: Loss = 0.80899
Epoch 3.102: Loss = 0.773712
Epoch 3.103: Loss = 0.712494
Epoch 3.104: Loss = 0.629425
Epoch 3.105: Loss = 0.64444
Epoch 3.106: Loss = 0.787689
Epoch 3.107: Loss = 0.763565
Epoch 3.108: Loss = 0.786819
Epoch 3.109: Loss = 0.782181
Epoch 3.110: Loss = 0.772293
Epoch 3.111: Loss = 0.694077
Epoch 3.112: Loss = 0.702744
Epoch 3.113: Loss = 0.759201
Epoch 3.114: Loss = 0.714569
Epoch 3.115: Loss = 0.770874
Epoch 3.116: Loss = 0.621948
Epoch 3.117: Loss = 0.782547
Epoch 3.118: Loss = 0.704163
Epoch 3.119: Loss = 0.737686
Epoch 3.120: Loss = 0.707352
TRAIN LOSS = 0.736816
TRAIN ACC = 76.9913 % (46197/60000)
Loss = 0.712616
Loss = 0.783783
Loss = 0.710373
Loss = 0.673752
Loss = 0.691666
Loss = 0.839355
Loss = 0.882797
Loss = 0.83194
Loss = 0.723358
Loss = 0.704468
Loss = 0.876373
Loss = 0.849991
Loss = 0.790298
Loss = 0.820221
Loss = 0.771774
Loss = 0.765305
Loss = 0.753372
Loss = 0.808701
Loss = 0.791702
Loss = 0.783676
TEST LOSS = 0.778276
TEST ACC = 461.969 % (7667/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.719452
Epoch 4.2: Loss = 0.738358
Epoch 4.3: Loss = 0.722183
Epoch 4.4: Loss = 0.640503
Epoch 4.5: Loss = 0.74707
Epoch 4.6: Loss = 0.793701
Epoch 4.7: Loss = 0.773087
Epoch 4.8: Loss = 0.828827
Epoch 4.9: Loss = 0.611725
Epoch 4.10: Loss = 0.567276
Epoch 4.11: Loss = 0.880737
Epoch 4.12: Loss = 0.743423
Epoch 4.13: Loss = 0.818649
Epoch 4.14: Loss = 0.765488
Epoch 4.15: Loss = 0.764984
Epoch 4.16: Loss = 0.843338
Epoch 4.17: Loss = 0.685623
Epoch 4.18: Loss = 0.787613
Epoch 4.19: Loss = 0.732178
Epoch 4.20: Loss = 0.839661
Epoch 4.21: Loss = 0.654984
Epoch 4.22: Loss = 0.627472
Epoch 4.23: Loss = 0.737671
Epoch 4.24: Loss = 0.855743
Epoch 4.25: Loss = 0.71994
Epoch 4.26: Loss = 0.671844
Epoch 4.27: Loss = 0.726044
Epoch 4.28: Loss = 0.760834
Epoch 4.29: Loss = 0.720581
Epoch 4.30: Loss = 0.771835
Epoch 4.31: Loss = 0.825821
Epoch 4.32: Loss = 0.701157
Epoch 4.33: Loss = 0.660385
Epoch 4.34: Loss = 0.852203
Epoch 4.35: Loss = 0.800461
Epoch 4.36: Loss = 0.900574
Epoch 4.37: Loss = 0.76683
Epoch 4.38: Loss = 0.724731
Epoch 4.39: Loss = 0.762314
Epoch 4.40: Loss = 0.708054
Epoch 4.41: Loss = 0.813187
Epoch 4.42: Loss = 0.683044
Epoch 4.43: Loss = 0.740051
Epoch 4.44: Loss = 0.623138
Epoch 4.45: Loss = 0.806137
Epoch 4.46: Loss = 0.832901
Epoch 4.47: Loss = 0.759933
Epoch 4.48: Loss = 0.642258
Epoch 4.49: Loss = 0.83046
Epoch 4.50: Loss = 0.764206
Epoch 4.51: Loss = 0.622208
Epoch 4.52: Loss = 0.821411
Epoch 4.53: Loss = 0.829147
Epoch 4.54: Loss = 0.591766
Epoch 4.55: Loss = 0.734238
Epoch 4.56: Loss = 0.809219
Epoch 4.57: Loss = 0.773453
Epoch 4.58: Loss = 0.73259
Epoch 4.59: Loss = 0.802612
Epoch 4.60: Loss = 0.759521
Epoch 4.61: Loss = 0.64006
Epoch 4.62: Loss = 0.785736
Epoch 4.63: Loss = 0.629913
Epoch 4.64: Loss = 0.607361
Epoch 4.65: Loss = 0.759598
Epoch 4.66: Loss = 0.720398
Epoch 4.67: Loss = 0.737183
Epoch 4.68: Loss = 0.834488
Epoch 4.69: Loss = 0.75412
Epoch 4.70: Loss = 0.761307
Epoch 4.71: Loss = 0.6315
Epoch 4.72: Loss = 0.744583
Epoch 4.73: Loss = 0.882767
Epoch 4.74: Loss = 0.82045
Epoch 4.75: Loss = 0.698547
Epoch 4.76: Loss = 0.7267
Epoch 4.77: Loss = 0.73822
Epoch 4.78: Loss = 0.763184
Epoch 4.79: Loss = 0.697144
Epoch 4.80: Loss = 0.698105
Epoch 4.81: Loss = 0.735855
Epoch 4.82: Loss = 0.694748
Epoch 4.83: Loss = 0.745209
Epoch 4.84: Loss = 0.685699
Epoch 4.85: Loss = 0.70787
Epoch 4.86: Loss = 0.730408
Epoch 4.87: Loss = 0.735931
Epoch 4.88: Loss = 0.665192
Epoch 4.89: Loss = 0.856155
Epoch 4.90: Loss = 0.757294
Epoch 4.91: Loss = 0.819412
Epoch 4.92: Loss = 0.724579
Epoch 4.93: Loss = 0.721786
Epoch 4.94: Loss = 0.7603
Epoch 4.95: Loss = 0.75531
Epoch 4.96: Loss = 0.660477
Epoch 4.97: Loss = 0.59967
Epoch 4.98: Loss = 0.781555
Epoch 4.99: Loss = 0.786514
Epoch 4.100: Loss = 0.733719
Epoch 4.101: Loss = 0.801254
Epoch 4.102: Loss = 0.781784
Epoch 4.103: Loss = 0.794632
Epoch 4.104: Loss = 0.619156
Epoch 4.105: Loss = 0.695358
Epoch 4.106: Loss = 0.819656
Epoch 4.107: Loss = 0.75824
Epoch 4.108: Loss = 0.855438
Epoch 4.109: Loss = 0.826859
Epoch 4.110: Loss = 0.833923
Epoch 4.111: Loss = 0.690323
Epoch 4.112: Loss = 0.734711
Epoch 4.113: Loss = 0.784592
Epoch 4.114: Loss = 0.798904
Epoch 4.115: Loss = 0.806458
Epoch 4.116: Loss = 0.665558
Epoch 4.117: Loss = 0.782608
Epoch 4.118: Loss = 0.716232
Epoch 4.119: Loss = 0.76152
Epoch 4.120: Loss = 0.697891
TRAIN LOSS = 0.745346
TRAIN ACC = 77.4948 % (46499/60000)
Loss = 0.719971
Loss = 0.813705
Loss = 0.742325
Loss = 0.720245
Loss = 0.759003
Loss = 0.878052
Loss = 0.987259
Loss = 0.860504
Loss = 0.761612
Loss = 0.7556
Loss = 0.992035
Loss = 0.895218
Loss = 0.875992
Loss = 0.821533
Loss = 0.794205
Loss = 0.834824
Loss = 0.765091
Loss = 0.831726
Loss = 0.815002
Loss = 0.820007
TEST LOSS = 0.822195
TEST ACC = 464.989 % (7713/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.788452
Epoch 5.2: Loss = 0.716827
Epoch 5.3: Loss = 0.760712
Epoch 5.4: Loss = 0.686478
Epoch 5.5: Loss = 0.759293
Epoch 5.6: Loss = 0.813492
Epoch 5.7: Loss = 0.773026
Epoch 5.8: Loss = 0.855148
Epoch 5.9: Loss = 0.598511
Epoch 5.10: Loss = 0.56134
Epoch 5.11: Loss = 0.961166
Epoch 5.12: Loss = 0.76976
Epoch 5.13: Loss = 0.838806
Epoch 5.14: Loss = 0.802521
Epoch 5.15: Loss = 0.81369
Epoch 5.16: Loss = 0.860489
Epoch 5.17: Loss = 0.721603
Epoch 5.18: Loss = 0.822769
Epoch 5.19: Loss = 0.715866
Epoch 5.20: Loss = 0.915924
Epoch 5.21: Loss = 0.646515
Epoch 5.22: Loss = 0.62999
Epoch 5.23: Loss = 0.786575
Epoch 5.24: Loss = 0.85318
Epoch 5.25: Loss = 0.753128
Epoch 5.26: Loss = 0.672729
Epoch 5.27: Loss = 0.797333
Epoch 5.28: Loss = 0.777786
Epoch 5.29: Loss = 0.715088
Epoch 5.30: Loss = 0.714081
Epoch 5.31: Loss = 0.881042
Epoch 5.32: Loss = 0.781952
Epoch 5.33: Loss = 0.734039
Epoch 5.34: Loss = 0.896667
Epoch 5.35: Loss = 0.813232
Epoch 5.36: Loss = 0.902451
Epoch 5.37: Loss = 0.755478
Epoch 5.38: Loss = 0.806046
Epoch 5.39: Loss = 0.859421
Epoch 5.40: Loss = 0.741043
Epoch 5.41: Loss = 0.845535
Epoch 5.42: Loss = 0.734131
Epoch 5.43: Loss = 0.753448
Epoch 5.44: Loss = 0.674011
Epoch 5.45: Loss = 0.804596
Epoch 5.46: Loss = 0.860321
Epoch 5.47: Loss = 0.821915
Epoch 5.48: Loss = 0.699615
Epoch 5.49: Loss = 0.860184
Epoch 5.50: Loss = 0.763855
Epoch 5.51: Loss = 0.613678
Epoch 5.52: Loss = 0.818619
Epoch 5.53: Loss = 0.902634
Epoch 5.54: Loss = 0.628357
Epoch 5.55: Loss = 0.815033
Epoch 5.56: Loss = 0.928497
Epoch 5.57: Loss = 0.807587
Epoch 5.58: Loss = 0.774033
Epoch 5.59: Loss = 0.862213
Epoch 5.60: Loss = 0.857468
Epoch 5.61: Loss = 0.74437
Epoch 5.62: Loss = 0.841446
Epoch 5.63: Loss = 0.684174
Epoch 5.64: Loss = 0.609344
Epoch 5.65: Loss = 0.797806
Epoch 5.66: Loss = 0.745529
Epoch 5.67: Loss = 0.754593
Epoch 5.68: Loss = 0.917374
Epoch 5.69: Loss = 0.781357
Epoch 5.70: Loss = 0.754623
Epoch 5.71: Loss = 0.712585
Epoch 5.72: Loss = 0.803787
Epoch 5.73: Loss = 0.906403
Epoch 5.74: Loss = 0.865341
Epoch 5.75: Loss = 0.72467
Epoch 5.76: Loss = 0.758041
Epoch 5.77: Loss = 0.76767
Epoch 5.78: Loss = 0.824982
Epoch 5.79: Loss = 0.737518
Epoch 5.80: Loss = 0.721603
Epoch 5.81: Loss = 0.74086
Epoch 5.82: Loss = 0.716156
Epoch 5.83: Loss = 0.763763
Epoch 5.84: Loss = 0.710571
Epoch 5.85: Loss = 0.79422
Epoch 5.86: Loss = 0.810455
Epoch 5.87: Loss = 0.756302
Epoch 5.88: Loss = 0.717346
Epoch 5.89: Loss = 0.864487
Epoch 5.90: Loss = 0.70369
Epoch 5.91: Loss = 0.840103
Epoch 5.92: Loss = 0.723572
Epoch 5.93: Loss = 0.718689
Epoch 5.94: Loss = 0.752655
Epoch 5.95: Loss = 0.760483
Epoch 5.96: Loss = 0.696609
Epoch 5.97: Loss = 0.637207
Epoch 5.98: Loss = 0.743134
Epoch 5.99: Loss = 0.77626
Epoch 5.100: Loss = 0.749985
Epoch 5.101: Loss = 0.793533
Epoch 5.102: Loss = 0.781219
Epoch 5.103: Loss = 0.758423
Epoch 5.104: Loss = 0.653778
Epoch 5.105: Loss = 0.679153
Epoch 5.106: Loss = 0.814026
Epoch 5.107: Loss = 0.814178
Epoch 5.108: Loss = 0.942429
Epoch 5.109: Loss = 0.886841
Epoch 5.110: Loss = 0.836182
Epoch 5.111: Loss = 0.695557
Epoch 5.112: Loss = 0.730331
Epoch 5.113: Loss = 0.863983
Epoch 5.114: Loss = 0.791519
Epoch 5.115: Loss = 0.715179
Epoch 5.116: Loss = 0.664078
Epoch 5.117: Loss = 0.712738
Epoch 5.118: Loss = 0.752808
Epoch 5.119: Loss = 0.729401
Epoch 5.120: Loss = 0.698669
TRAIN LOSS = 0.771866
TRAIN ACC = 77.9251 % (46757/60000)
Loss = 0.734711
Loss = 0.820908
Loss = 0.738235
Loss = 0.709991
Loss = 0.777985
Loss = 0.875305
Loss = 0.964691
Loss = 0.839218
Loss = 0.770554
Loss = 0.717407
Loss = 1.0161
Loss = 0.868896
Loss = 0.864212
Loss = 0.826096
Loss = 0.769363
Loss = 0.765717
Loss = 0.741531
Loss = 0.840988
Loss = 0.79216
Loss = 0.791199
TEST LOSS = 0.811263
TEST ACC = 467.569 % (7796/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.725525
Epoch 6.2: Loss = 0.733887
Epoch 6.3: Loss = 0.761322
Epoch 6.4: Loss = 0.6595
Epoch 6.5: Loss = 0.809509
Epoch 6.6: Loss = 0.880463
Epoch 6.7: Loss = 0.73526
Epoch 6.8: Loss = 0.86644
Epoch 6.9: Loss = 0.582031
Epoch 6.10: Loss = 0.610275
Epoch 6.11: Loss = 0.982544
Epoch 6.12: Loss = 0.748184
Epoch 6.13: Loss = 0.875519
Epoch 6.14: Loss = 0.773972
Epoch 6.15: Loss = 0.79866
Epoch 6.16: Loss = 0.870239
Epoch 6.17: Loss = 0.728287
Epoch 6.18: Loss = 0.833969
Epoch 6.19: Loss = 0.682632
Epoch 6.20: Loss = 0.855255
Epoch 6.21: Loss = 0.63504
Epoch 6.22: Loss = 0.58403
Epoch 6.23: Loss = 0.728455
Epoch 6.24: Loss = 0.826935
Epoch 6.25: Loss = 0.809662
Epoch 6.26: Loss = 0.627045
Epoch 6.27: Loss = 0.783447
Epoch 6.28: Loss = 0.71933
Epoch 6.29: Loss = 0.664352
Epoch 6.30: Loss = 0.743698
Epoch 6.31: Loss = 0.834198
Epoch 6.32: Loss = 0.783859
Epoch 6.33: Loss = 0.673767
Epoch 6.34: Loss = 0.818481
Epoch 6.35: Loss = 0.782745
Epoch 6.36: Loss = 0.854996
Epoch 6.37: Loss = 0.808075
Epoch 6.38: Loss = 0.799515
Epoch 6.39: Loss = 0.830154
Epoch 6.40: Loss = 0.70401
Epoch 6.41: Loss = 0.814392
Epoch 6.42: Loss = 0.776474
Epoch 6.43: Loss = 0.716385
Epoch 6.44: Loss = 0.670822
Epoch 6.45: Loss = 0.808121
Epoch 6.46: Loss = 0.914551
Epoch 6.47: Loss = 0.834961
Epoch 6.48: Loss = 0.658554
Epoch 6.49: Loss = 0.875366
Epoch 6.50: Loss = 0.789795
Epoch 6.51: Loss = 0.56485
Epoch 6.52: Loss = 0.901077
Epoch 6.53: Loss = 0.892166
Epoch 6.54: Loss = 0.626343
Epoch 6.55: Loss = 0.844193
Epoch 6.56: Loss = 0.904022
Epoch 6.57: Loss = 0.863113
Epoch 6.58: Loss = 0.740967
Epoch 6.59: Loss = 0.870926
Epoch 6.60: Loss = 0.805283
Epoch 6.61: Loss = 0.77327
Epoch 6.62: Loss = 0.80864
Epoch 6.63: Loss = 0.700928
Epoch 6.64: Loss = 0.636078
Epoch 6.65: Loss = 0.742523
Epoch 6.66: Loss = 0.740341
Epoch 6.67: Loss = 0.811462
Epoch 6.68: Loss = 0.982346
Epoch 6.69: Loss = 0.818634
Epoch 6.70: Loss = 0.823807
Epoch 6.71: Loss = 0.705063
Epoch 6.72: Loss = 0.862076
Epoch 6.73: Loss = 0.961609
Epoch 6.74: Loss = 0.879211
Epoch 6.75: Loss = 0.7388
Epoch 6.76: Loss = 0.825729
Epoch 6.77: Loss = 0.770126
Epoch 6.78: Loss = 0.881287
Epoch 6.79: Loss = 0.765381
Epoch 6.80: Loss = 0.740448
Epoch 6.81: Loss = 0.811691
Epoch 6.82: Loss = 0.746811
Epoch 6.83: Loss = 0.81839
Epoch 6.84: Loss = 0.742996
Epoch 6.85: Loss = 0.813354
Epoch 6.86: Loss = 0.837921
Epoch 6.87: Loss = 0.798309
Epoch 6.88: Loss = 0.759979
Epoch 6.89: Loss = 0.968674
Epoch 6.90: Loss = 0.743195
Epoch 6.91: Loss = 0.862198
Epoch 6.92: Loss = 0.768173
Epoch 6.93: Loss = 0.746307
Epoch 6.94: Loss = 0.835297
Epoch 6.95: Loss = 0.8815
Epoch 6.96: Loss = 0.768646
Epoch 6.97: Loss = 0.6521
Epoch 6.98: Loss = 0.765244
Epoch 6.99: Loss = 0.797455
Epoch 6.100: Loss = 0.798538
Epoch 6.101: Loss = 0.815567
Epoch 6.102: Loss = 0.918884
Epoch 6.103: Loss = 0.841187
Epoch 6.104: Loss = 0.697662
Epoch 6.105: Loss = 0.727493
Epoch 6.106: Loss = 0.827637
Epoch 6.107: Loss = 0.918533
Epoch 6.108: Loss = 1.01324
Epoch 6.109: Loss = 0.950729
Epoch 6.110: Loss = 0.868256
Epoch 6.111: Loss = 0.840759
Epoch 6.112: Loss = 0.762085
Epoch 6.113: Loss = 0.856079
Epoch 6.114: Loss = 0.832657
Epoch 6.115: Loss = 0.818924
Epoch 6.116: Loss = 0.723862
Epoch 6.117: Loss = 0.825958
Epoch 6.118: Loss = 0.764938
Epoch 6.119: Loss = 0.76329
Epoch 6.120: Loss = 0.729095
TRAIN LOSS = 0.789841
TRAIN ACC = 78.4637 % (47080/60000)
Loss = 0.81485
Loss = 0.877594
Loss = 0.800232
Loss = 0.798859
Loss = 0.837936
Loss = 0.988724
Loss = 1.03561
Loss = 0.958511
Loss = 0.843002
Loss = 0.791794
Loss = 1.10858
Loss = 0.971298
Loss = 0.898026
Loss = 0.879349
Loss = 0.87439
Loss = 0.816452
Loss = 0.758072
Loss = 0.866821
Loss = 0.825607
Loss = 0.837952
TEST LOSS = 0.879183
TEST ACC = 470.799 % (7810/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.797012
Epoch 7.2: Loss = 0.813782
Epoch 7.3: Loss = 0.859055
Epoch 7.4: Loss = 0.740128
Epoch 7.5: Loss = 0.878571
Epoch 7.6: Loss = 0.970322
Epoch 7.7: Loss = 0.857803
Epoch 7.8: Loss = 0.977127
Epoch 7.9: Loss = 0.636337
Epoch 7.10: Loss = 0.67247
Epoch 7.11: Loss = 1.05482
Epoch 7.12: Loss = 0.862518
Epoch 7.13: Loss = 0.95639
Epoch 7.14: Loss = 0.763748
Epoch 7.15: Loss = 0.887634
Epoch 7.16: Loss = 0.983826
Epoch 7.17: Loss = 0.873093
Epoch 7.18: Loss = 0.92804
Epoch 7.19: Loss = 0.749786
Epoch 7.20: Loss = 0.948044
Epoch 7.21: Loss = 0.701645
Epoch 7.22: Loss = 0.701141
Epoch 7.23: Loss = 0.754196
Epoch 7.24: Loss = 1.00407
Epoch 7.25: Loss = 0.809631
Epoch 7.26: Loss = 0.669296
Epoch 7.27: Loss = 0.891983
Epoch 7.28: Loss = 0.747849
Epoch 7.29: Loss = 0.75029
Epoch 7.30: Loss = 0.774384
Epoch 7.31: Loss = 0.877167
Epoch 7.32: Loss = 0.852859
Epoch 7.33: Loss = 0.751053
Epoch 7.34: Loss = 0.868515
Epoch 7.35: Loss = 0.854095
Epoch 7.36: Loss = 0.973145
Epoch 7.37: Loss = 0.903366
Epoch 7.38: Loss = 0.813599
Epoch 7.39: Loss = 0.918396
Epoch 7.40: Loss = 0.809525
Epoch 7.41: Loss = 0.915009
Epoch 7.42: Loss = 0.850876
Epoch 7.43: Loss = 0.7827
Epoch 7.44: Loss = 0.771454
Epoch 7.45: Loss = 0.855896
Epoch 7.46: Loss = 1.01146
Epoch 7.47: Loss = 0.859665
Epoch 7.48: Loss = 0.686859
Epoch 7.49: Loss = 0.877518
Epoch 7.50: Loss = 0.893372
Epoch 7.51: Loss = 0.581039
Epoch 7.52: Loss = 0.988663
Epoch 7.53: Loss = 0.940186
Epoch 7.54: Loss = 0.692535
Epoch 7.55: Loss = 0.832077
Epoch 7.56: Loss = 0.951721
Epoch 7.57: Loss = 0.955643
Epoch 7.58: Loss = 0.818405
Epoch 7.59: Loss = 0.962738
Epoch 7.60: Loss = 0.835373
Epoch 7.61: Loss = 0.83226
Epoch 7.62: Loss = 0.818405
Epoch 7.63: Loss = 0.721603
Epoch 7.64: Loss = 0.728348
Epoch 7.65: Loss = 0.725266
Epoch 7.66: Loss = 0.773575
Epoch 7.67: Loss = 0.880234
Epoch 7.68: Loss = 1.10204
Epoch 7.69: Loss = 0.816788
Epoch 7.70: Loss = 0.873657
Epoch 7.71: Loss = 0.771393
Epoch 7.72: Loss = 0.891495
Epoch 7.73: Loss = 1.05672
Epoch 7.74: Loss = 0.933762
Epoch 7.75: Loss = 0.766586
Epoch 7.76: Loss = 0.872437
Epoch 7.77: Loss = 0.807678
Epoch 7.78: Loss = 0.888016
Epoch 7.79: Loss = 0.848358
Epoch 7.80: Loss = 0.846054
Epoch 7.81: Loss = 0.847214
Epoch 7.82: Loss = 0.817703
Epoch 7.83: Loss = 0.875427
Epoch 7.84: Loss = 0.796417
Epoch 7.85: Loss = 0.800049
Epoch 7.86: Loss = 0.813599
Epoch 7.87: Loss = 0.823044
Epoch 7.88: Loss = 0.813599
Epoch 7.89: Loss = 0.975815
Epoch 7.90: Loss = 0.83902
Epoch 7.91: Loss = 0.940628
Epoch 7.92: Loss = 0.829697
Epoch 7.93: Loss = 0.863785
Epoch 7.94: Loss = 0.884537
Epoch 7.95: Loss = 0.947418
Epoch 7.96: Loss = 0.864822
Epoch 7.97: Loss = 0.683975
Epoch 7.98: Loss = 0.815506
Epoch 7.99: Loss = 0.824463
Epoch 7.100: Loss = 0.814011
Epoch 7.101: Loss = 0.923492
Epoch 7.102: Loss = 0.966171
Epoch 7.103: Loss = 0.911942
Epoch 7.104: Loss = 0.788681
Epoch 7.105: Loss = 0.764114
Epoch 7.106: Loss = 0.937454
Epoch 7.107: Loss = 0.910233
Epoch 7.108: Loss = 1.08119
Epoch 7.109: Loss = 1.01128
Epoch 7.110: Loss = 0.863525
Epoch 7.111: Loss = 0.883072
Epoch 7.112: Loss = 0.777206
Epoch 7.113: Loss = 0.870926
Epoch 7.114: Loss = 0.884781
Epoch 7.115: Loss = 0.904037
Epoch 7.116: Loss = 0.769348
Epoch 7.117: Loss = 0.849686
Epoch 7.118: Loss = 0.818192
Epoch 7.119: Loss = 0.7798
Epoch 7.120: Loss = 0.805267
TRAIN LOSS = 0.850235
TRAIN ACC = 78.6209 % (47175/60000)
Loss = 0.847
Loss = 0.992004
Loss = 0.875244
Loss = 0.826263
Loss = 0.888382
Loss = 1.09381
Loss = 1.08073
Loss = 1.04283
Loss = 0.863892
Loss = 0.836456
Loss = 1.10695
Loss = 1.00851
Loss = 1.00166
Loss = 0.987427
Loss = 0.949799
Loss = 0.856689
Loss = 0.944107
Loss = 0.98645
Loss = 0.937531
Loss = 0.894577
TEST LOSS = 0.951016
TEST ACC = 471.75 % (7805/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.85498
Epoch 8.2: Loss = 0.861511
Epoch 8.3: Loss = 0.947403
Epoch 8.4: Loss = 0.825119
Epoch 8.5: Loss = 0.923386
Epoch 8.6: Loss = 0.998108
Epoch 8.7: Loss = 0.817902
Epoch 8.8: Loss = 0.979782
Epoch 8.9: Loss = 0.672272
Epoch 8.10: Loss = 0.682816
Epoch 8.11: Loss = 1.07607
Epoch 8.12: Loss = 0.900909
Epoch 8.13: Loss = 0.970551
Epoch 8.14: Loss = 0.778961
Epoch 8.15: Loss = 0.932007
Epoch 8.16: Loss = 1.03612
Epoch 8.17: Loss = 0.850723
Epoch 8.18: Loss = 1.01271
Epoch 8.19: Loss = 0.791931
Epoch 8.20: Loss = 1.03545
Epoch 8.21: Loss = 0.757156
Epoch 8.22: Loss = 0.73613
Epoch 8.23: Loss = 0.804642
Epoch 8.24: Loss = 0.989395
Epoch 8.25: Loss = 0.868469
Epoch 8.26: Loss = 0.647888
Epoch 8.27: Loss = 0.871872
Epoch 8.28: Loss = 0.808014
Epoch 8.29: Loss = 0.745758
Epoch 8.30: Loss = 0.790741
Epoch 8.31: Loss = 0.847031
Epoch 8.32: Loss = 0.830124
Epoch 8.33: Loss = 0.766251
Epoch 8.34: Loss = 0.906601
Epoch 8.35: Loss = 0.942673
Epoch 8.36: Loss = 0.968002
Epoch 8.37: Loss = 0.973434
Epoch 8.38: Loss = 0.858292
Epoch 8.39: Loss = 0.911911
Epoch 8.40: Loss = 0.888565
Epoch 8.41: Loss = 0.995804
Epoch 8.42: Loss = 0.862885
Epoch 8.43: Loss = 0.78923
Epoch 8.44: Loss = 0.820206
Epoch 8.45: Loss = 0.935989
Epoch 8.46: Loss = 1.11517
Epoch 8.47: Loss = 0.837845
Epoch 8.48: Loss = 0.742828
Epoch 8.49: Loss = 0.874359
Epoch 8.50: Loss = 0.931854
Epoch 8.51: Loss = 0.647339
Epoch 8.52: Loss = 0.98378
Epoch 8.53: Loss = 0.990387
Epoch 8.54: Loss = 0.798111
Epoch 8.55: Loss = 0.919327
Epoch 8.56: Loss = 0.964554
Epoch 8.57: Loss = 0.881561
Epoch 8.58: Loss = 0.921753
Epoch 8.59: Loss = 0.981964
Epoch 8.60: Loss = 0.88588
Epoch 8.61: Loss = 0.76149
Epoch 8.62: Loss = 0.858658
Epoch 8.63: Loss = 0.725372
Epoch 8.64: Loss = 0.755585
Epoch 8.65: Loss = 0.789963
Epoch 8.66: Loss = 0.770599
Epoch 8.67: Loss = 0.953476
Epoch 8.68: Loss = 1.19246
Epoch 8.69: Loss = 0.844421
Epoch 8.70: Loss = 0.876785
Epoch 8.71: Loss = 0.851471
Epoch 8.72: Loss = 0.973389
Epoch 8.73: Loss = 1.06021
Epoch 8.74: Loss = 0.934982
Epoch 8.75: Loss = 0.823502
Epoch 8.76: Loss = 0.894943
Epoch 8.77: Loss = 0.843948
Epoch 8.78: Loss = 0.986679
Epoch 8.79: Loss = 0.865631
Epoch 8.80: Loss = 0.878815
Epoch 8.81: Loss = 0.890884
Epoch 8.82: Loss = 0.860107
Epoch 8.83: Loss = 0.856369
Epoch 8.84: Loss = 0.823059
Epoch 8.85: Loss = 0.883942
Epoch 8.86: Loss = 0.914459
Epoch 8.87: Loss = 0.835037
Epoch 8.88: Loss = 0.802032
Epoch 8.89: Loss = 1.00359
Epoch 8.90: Loss = 0.89946
Epoch 8.91: Loss = 1.01149
Epoch 8.92: Loss = 0.879532
Epoch 8.93: Loss = 0.894562
Epoch 8.94: Loss = 0.880798
Epoch 8.95: Loss = 0.947739
Epoch 8.96: Loss = 0.913193
Epoch 8.97: Loss = 0.708435
Epoch 8.98: Loss = 0.903305
Epoch 8.99: Loss = 0.866943
Epoch 8.100: Loss = 0.854782
Epoch 8.101: Loss = 0.959488
Epoch 8.102: Loss = 0.973618
Epoch 8.103: Loss = 0.898575
Epoch 8.104: Loss = 0.788757
Epoch 8.105: Loss = 0.816772
Epoch 8.106: Loss = 0.984818
Epoch 8.107: Loss = 0.923645
Epoch 8.108: Loss = 1.05394
Epoch 8.109: Loss = 1.04211
Epoch 8.110: Loss = 0.930466
Epoch 8.111: Loss = 0.873947
Epoch 8.112: Loss = 0.883972
Epoch 8.113: Loss = 0.964371
Epoch 8.114: Loss = 0.986847
Epoch 8.115: Loss = 0.871246
Epoch 8.116: Loss = 0.790695
Epoch 8.117: Loss = 0.883057
Epoch 8.118: Loss = 0.791153
Epoch 8.119: Loss = 0.81572
Epoch 8.120: Loss = 0.813126
TRAIN LOSS = 0.88385
TRAIN ACC = 78.7186 % (47233/60000)
Loss = 0.872803
Loss = 0.994827
Loss = 0.918243
Loss = 0.81221
Loss = 0.856522
Loss = 1.06558
Loss = 1.16573
Loss = 1.01593
Loss = 0.888351
Loss = 0.882294
Loss = 1.14383
Loss = 1.04178
Loss = 1.01752
Loss = 0.973389
Loss = 0.973572
Loss = 0.838654
Loss = 0.858902
Loss = 0.954559
Loss = 0.91362
Loss = 0.886581
TEST LOSS = 0.953744
TEST ACC = 472.33 % (7764/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.807297
Epoch 9.2: Loss = 0.915009
Epoch 9.3: Loss = 1.03989
Epoch 9.4: Loss = 0.802689
Epoch 9.5: Loss = 0.919235
Epoch 9.6: Loss = 1.00053
Epoch 9.7: Loss = 0.907669
Epoch 9.8: Loss = 0.978882
Epoch 9.9: Loss = 0.72435
Epoch 9.10: Loss = 0.624969
Epoch 9.11: Loss = 1.12032
Epoch 9.12: Loss = 0.859573
Epoch 9.13: Loss = 0.972809
Epoch 9.14: Loss = 0.830704
Epoch 9.15: Loss = 0.927353
Epoch 9.16: Loss = 0.997803
Epoch 9.17: Loss = 0.806992
Epoch 9.18: Loss = 0.969833
Epoch 9.19: Loss = 0.849686
Epoch 9.20: Loss = 1.04965
Epoch 9.21: Loss = 0.831161
Epoch 9.22: Loss = 0.770828
Epoch 9.23: Loss = 0.832779
Epoch 9.24: Loss = 1.02974
Epoch 9.25: Loss = 0.877731
Epoch 9.26: Loss = 0.685349
Epoch 9.27: Loss = 0.908249
Epoch 9.28: Loss = 0.784409
Epoch 9.29: Loss = 0.814377
Epoch 9.30: Loss = 0.897842
Epoch 9.31: Loss = 0.962555
Epoch 9.32: Loss = 0.919083
Epoch 9.33: Loss = 0.804535
Epoch 9.34: Loss = 1.02617
Epoch 9.35: Loss = 1.01968
Epoch 9.36: Loss = 1.03839
Epoch 9.37: Loss = 1.07137
Epoch 9.38: Loss = 0.906464
Epoch 9.39: Loss = 1.02888
Epoch 9.40: Loss = 0.93335
Epoch 9.41: Loss = 1.0715
Epoch 9.42: Loss = 0.917816
Epoch 9.43: Loss = 0.849548
Epoch 9.44: Loss = 0.832993
Epoch 9.45: Loss = 1.00591
Epoch 9.46: Loss = 1.13243
Epoch 9.47: Loss = 0.899094
Epoch 9.48: Loss = 0.863602
Epoch 9.49: Loss = 0.95813
Epoch 9.50: Loss = 0.970352
Epoch 9.51: Loss = 0.676056
Epoch 9.52: Loss = 1.00565
Epoch 9.53: Loss = 1.03761
Epoch 9.54: Loss = 0.791138
Epoch 9.55: Loss = 0.88176
Epoch 9.56: Loss = 1.00977
Epoch 9.57: Loss = 0.997025
Epoch 9.58: Loss = 0.993271
Epoch 9.59: Loss = 1.03616
Epoch 9.60: Loss = 1.0265
Epoch 9.61: Loss = 0.832886
Epoch 9.62: Loss = 0.967163
Epoch 9.63: Loss = 0.817459
Epoch 9.64: Loss = 0.872299
Epoch 9.65: Loss = 0.880493
Epoch 9.66: Loss = 0.822052
Epoch 9.67: Loss = 1.06555
Epoch 9.68: Loss = 1.27498
Epoch 9.69: Loss = 0.90477
Epoch 9.70: Loss = 0.941162
Epoch 9.71: Loss = 0.976822
Epoch 9.72: Loss = 0.989029
Epoch 9.73: Loss = 1.13249
Epoch 9.74: Loss = 0.979599
Epoch 9.75: Loss = 0.873825
Epoch 9.76: Loss = 0.966385
Epoch 9.77: Loss = 0.942657
Epoch 9.78: Loss = 1.11658
Epoch 9.79: Loss = 0.878876
Epoch 9.80: Loss = 0.891083
Epoch 9.81: Loss = 0.868576
Epoch 9.82: Loss = 0.820068
Epoch 9.83: Loss = 0.956238
Epoch 9.84: Loss = 0.906647
Epoch 9.85: Loss = 0.988419
Epoch 9.86: Loss = 0.979904
Epoch 9.87: Loss = 0.856277
Epoch 9.88: Loss = 0.893372
Epoch 9.89: Loss = 1.09337
Epoch 9.90: Loss = 0.948059
Epoch 9.91: Loss = 1.06015
Epoch 9.92: Loss = 0.899078
Epoch 9.93: Loss = 0.926041
Epoch 9.94: Loss = 0.942123
Epoch 9.95: Loss = 1.01683
Epoch 9.96: Loss = 0.928055
Epoch 9.97: Loss = 0.776993
Epoch 9.98: Loss = 0.941879
Epoch 9.99: Loss = 0.965942
Epoch 9.100: Loss = 0.93512
Epoch 9.101: Loss = 1.08479
Epoch 9.102: Loss = 1.02507
Epoch 9.103: Loss = 0.96756
Epoch 9.104: Loss = 0.831039
Epoch 9.105: Loss = 0.890396
Epoch 9.106: Loss = 1.03777
Epoch 9.107: Loss = 0.94693
Epoch 9.108: Loss = 1.25725
Epoch 9.109: Loss = 1.03517
Epoch 9.110: Loss = 0.998581
Epoch 9.111: Loss = 0.951752
Epoch 9.112: Loss = 0.954086
Epoch 9.113: Loss = 0.997711
Epoch 9.114: Loss = 1.01181
Epoch 9.115: Loss = 1.02928
Epoch 9.116: Loss = 0.811005
Epoch 9.117: Loss = 0.960449
Epoch 9.118: Loss = 0.802521
Epoch 9.119: Loss = 0.869003
Epoch 9.120: Loss = 0.859451
TRAIN LOSS = 0.93631
TRAIN ACC = 78.4424 % (47067/60000)
Loss = 0.886734
Loss = 1.07115
Loss = 0.960724
Loss = 0.822647
Loss = 0.901779
Loss = 1.17038
Loss = 1.29564
Loss = 1.10846
Loss = 1.00282
Loss = 0.946701
Loss = 1.16052
Loss = 1.20853
Loss = 1.11021
Loss = 1.08321
Loss = 1.08025
Loss = 0.931595
Loss = 0.921951
Loss = 1.04738
Loss = 1.10281
Loss = 0.967331
TEST LOSS = 1.03904
TEST ACC = 470.67 % (7735/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.97995
Epoch 10.2: Loss = 0.981674
Epoch 10.3: Loss = 1.04285
Epoch 10.4: Loss = 0.868347
Epoch 10.5: Loss = 1.01892
Epoch 10.6: Loss = 1.22731
Epoch 10.7: Loss = 1.02475
Epoch 10.8: Loss = 1.2146
Epoch 10.9: Loss = 0.755463
Epoch 10.10: Loss = 0.802353
Epoch 10.11: Loss = 1.23433
Epoch 10.12: Loss = 1.03395
Epoch 10.13: Loss = 1.10939
Epoch 10.14: Loss = 0.871719
Epoch 10.15: Loss = 1.02472
Epoch 10.16: Loss = 1.14174
Epoch 10.17: Loss = 0.894119
Epoch 10.18: Loss = 1.07898
Epoch 10.19: Loss = 0.969513
Epoch 10.20: Loss = 1.13326
Epoch 10.21: Loss = 0.938797
Epoch 10.22: Loss = 0.896912
Epoch 10.23: Loss = 0.898315
Epoch 10.24: Loss = 1.08205
Epoch 10.25: Loss = 1.04114
Epoch 10.26: Loss = 0.798828
Epoch 10.27: Loss = 1.05923
Epoch 10.28: Loss = 0.88179
Epoch 10.29: Loss = 0.922211
Epoch 10.30: Loss = 1.00696
Epoch 10.31: Loss = 1.17351
Epoch 10.32: Loss = 0.954178
Epoch 10.33: Loss = 0.829544
Epoch 10.34: Loss = 1.11794
Epoch 10.35: Loss = 1.03989
Epoch 10.36: Loss = 1.0748
Epoch 10.37: Loss = 1.13611
Epoch 10.38: Loss = 1.00717
Epoch 10.39: Loss = 1.0993
Epoch 10.40: Loss = 1.05516
Epoch 10.41: Loss = 1.15558
Epoch 10.42: Loss = 1.07381
Epoch 10.43: Loss = 0.884979
Epoch 10.44: Loss = 0.915329
Epoch 10.45: Loss = 1.12996
Epoch 10.46: Loss = 1.28905
Epoch 10.47: Loss = 0.965149
Epoch 10.48: Loss = 0.862823
Epoch 10.49: Loss = 1.01129
Epoch 10.50: Loss = 1.02399
Epoch 10.51: Loss = 0.828323
Epoch 10.52: Loss = 1.08086
Epoch 10.53: Loss = 1.20631
Epoch 10.54: Loss = 0.81543
Epoch 10.55: Loss = 0.943207
Epoch 10.56: Loss = 1.05925
Epoch 10.57: Loss = 1.12654
Epoch 10.58: Loss = 1.08215
Epoch 10.59: Loss = 1.0983
Epoch 10.60: Loss = 1.09128
Epoch 10.61: Loss = 0.902527
Epoch 10.62: Loss = 0.994095
Epoch 10.63: Loss = 0.82785
Epoch 10.64: Loss = 0.897141
Epoch 10.65: Loss = 1.05507
Epoch 10.66: Loss = 0.896484
Epoch 10.67: Loss = 1.12454
Epoch 10.68: Loss = 1.36804
Epoch 10.69: Loss = 0.930923
Epoch 10.70: Loss = 1.08624
Epoch 10.71: Loss = 1.01974
Epoch 10.72: Loss = 1.05891
Epoch 10.73: Loss = 1.23088
Epoch 10.74: Loss = 1.05054
Epoch 10.75: Loss = 0.968536
Epoch 10.76: Loss = 0.998474
Epoch 10.77: Loss = 1.00558
Epoch 10.78: Loss = 1.16402
Epoch 10.79: Loss = 1.01846
Epoch 10.80: Loss = 1.02509
Epoch 10.81: Loss = 0.940063
Epoch 10.82: Loss = 0.94783
Epoch 10.83: Loss = 1.14734
Epoch 10.84: Loss = 0.921661
Epoch 10.85: Loss = 1.0777
Epoch 10.86: Loss = 1.06694
Epoch 10.87: Loss = 0.951004
Epoch 10.88: Loss = 0.918991
Epoch 10.89: Loss = 1.18718
Epoch 10.90: Loss = 0.957779
Epoch 10.91: Loss = 1.095
Epoch 10.92: Loss = 0.869232
Epoch 10.93: Loss = 0.905991
Epoch 10.94: Loss = 0.918015
Epoch 10.95: Loss = 1.02809
Epoch 10.96: Loss = 0.986649
Epoch 10.97: Loss = 0.831589
Epoch 10.98: Loss = 0.996002
Epoch 10.99: Loss = 1.02095
Epoch 10.100: Loss = 1.01408
Epoch 10.101: Loss = 1.09886
Epoch 10.102: Loss = 1.05806
Epoch 10.103: Loss = 1.01062
Epoch 10.104: Loss = 0.903885
Epoch 10.105: Loss = 0.994827
Epoch 10.106: Loss = 1.07294
Epoch 10.107: Loss = 1.05898
Epoch 10.108: Loss = 1.36389
Epoch 10.109: Loss = 1.21104
Epoch 10.110: Loss = 1.00127
Epoch 10.111: Loss = 1.04959
Epoch 10.112: Loss = 0.95787
Epoch 10.113: Loss = 1.13545
Epoch 10.114: Loss = 1.06679
Epoch 10.115: Loss = 1.02681
Epoch 10.116: Loss = 0.918442
Epoch 10.117: Loss = 1.15379
Epoch 10.118: Loss = 0.848572
Epoch 10.119: Loss = 1.00224
Epoch 10.120: Loss = 0.935318
TRAIN LOSS = 1.01952
TRAIN ACC = 78.2059 % (46926/60000)
Loss = 0.909729
Loss = 1.16365
Loss = 0.960632
Loss = 0.908661
Loss = 0.952087
Loss = 1.27061
Loss = 1.4007
Loss = 1.11171
Loss = 1.00082
Loss = 1.03978
Loss = 1.27849
Loss = 1.22815
Loss = 1.10353
Loss = 1.10548
Loss = 1.17476
Loss = 1.049
Loss = 0.988266
Loss = 1.05693
Loss = 1.20219
Loss = 0.958832
TEST LOSS = 1.0932
TEST ACC = 469.26 % (7691/10000)
