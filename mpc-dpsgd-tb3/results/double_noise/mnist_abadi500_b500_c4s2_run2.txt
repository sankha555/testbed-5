Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.34018
Epoch 1.2: Loss = 2.34859
Epoch 1.3: Loss = 2.26224
Epoch 1.4: Loss = 2.22125
Epoch 1.5: Loss = 2.19905
Epoch 1.6: Loss = 2.1274
Epoch 1.7: Loss = 2.10373
Epoch 1.8: Loss = 2.03149
Epoch 1.9: Loss = 1.98445
Epoch 1.10: Loss = 1.90594
Epoch 1.11: Loss = 1.89619
Epoch 1.12: Loss = 1.88326
Epoch 1.13: Loss = 1.76425
Epoch 1.14: Loss = 1.82158
Epoch 1.15: Loss = 1.87502
Epoch 1.16: Loss = 1.75102
Epoch 1.17: Loss = 1.68202
Epoch 1.18: Loss = 1.65178
Epoch 1.19: Loss = 1.60863
Epoch 1.20: Loss = 1.56616
Epoch 1.21: Loss = 1.51817
Epoch 1.22: Loss = 1.49606
Epoch 1.23: Loss = 1.43065
Epoch 1.24: Loss = 1.5421
Epoch 1.25: Loss = 1.4402
Epoch 1.26: Loss = 1.47357
Epoch 1.27: Loss = 1.41508
Epoch 1.28: Loss = 1.40466
Epoch 1.29: Loss = 1.38077
Epoch 1.30: Loss = 1.43239
Epoch 1.31: Loss = 1.31277
Epoch 1.32: Loss = 1.35243
Epoch 1.33: Loss = 1.26364
Epoch 1.34: Loss = 1.28578
Epoch 1.35: Loss = 1.21754
Epoch 1.36: Loss = 1.33159
Epoch 1.37: Loss = 1.18951
Epoch 1.38: Loss = 1.11285
Epoch 1.39: Loss = 1.12784
Epoch 1.40: Loss = 1.05066
Epoch 1.41: Loss = 1.10989
Epoch 1.42: Loss = 1.118
Epoch 1.43: Loss = 1.03938
Epoch 1.44: Loss = 0.961426
Epoch 1.45: Loss = 1.09541
Epoch 1.46: Loss = 1.0312
Epoch 1.47: Loss = 0.960541
Epoch 1.48: Loss = 1.0363
Epoch 1.49: Loss = 0.960968
Epoch 1.50: Loss = 1.04391
Epoch 1.51: Loss = 0.880569
Epoch 1.52: Loss = 0.891602
Epoch 1.53: Loss = 0.92894
Epoch 1.54: Loss = 0.972092
Epoch 1.55: Loss = 0.940964
Epoch 1.56: Loss = 0.872147
Epoch 1.57: Loss = 0.792465
Epoch 1.58: Loss = 0.860794
Epoch 1.59: Loss = 0.85643
Epoch 1.60: Loss = 0.972214
Epoch 1.61: Loss = 0.920532
Epoch 1.62: Loss = 0.944122
Epoch 1.63: Loss = 0.948792
Epoch 1.64: Loss = 0.910172
Epoch 1.65: Loss = 0.948975
Epoch 1.66: Loss = 0.811417
Epoch 1.67: Loss = 0.817841
Epoch 1.68: Loss = 0.677811
Epoch 1.69: Loss = 0.74939
Epoch 1.70: Loss = 0.846848
Epoch 1.71: Loss = 0.764236
Epoch 1.72: Loss = 0.757095
Epoch 1.73: Loss = 0.779816
Epoch 1.74: Loss = 0.635422
Epoch 1.75: Loss = 0.782486
Epoch 1.76: Loss = 0.727417
Epoch 1.77: Loss = 0.714188
Epoch 1.78: Loss = 0.691757
Epoch 1.79: Loss = 0.687729
Epoch 1.80: Loss = 0.80336
Epoch 1.81: Loss = 0.682266
Epoch 1.82: Loss = 0.637787
Epoch 1.83: Loss = 0.802094
Epoch 1.84: Loss = 0.728104
Epoch 1.85: Loss = 0.780655
Epoch 1.86: Loss = 0.687302
Epoch 1.87: Loss = 0.634476
Epoch 1.88: Loss = 0.653961
Epoch 1.89: Loss = 0.760162
Epoch 1.90: Loss = 0.643509
Epoch 1.91: Loss = 0.684311
Epoch 1.92: Loss = 0.673203
Epoch 1.93: Loss = 0.717545
Epoch 1.94: Loss = 0.555161
Epoch 1.95: Loss = 0.664429
Epoch 1.96: Loss = 0.627487
Epoch 1.97: Loss = 0.491302
Epoch 1.98: Loss = 0.596558
Epoch 1.99: Loss = 0.676651
Epoch 1.100: Loss = 0.780746
Epoch 1.101: Loss = 0.666245
Epoch 1.102: Loss = 0.634491
Epoch 1.103: Loss = 0.562958
Epoch 1.104: Loss = 0.532425
Epoch 1.105: Loss = 0.666245
Epoch 1.106: Loss = 0.642929
Epoch 1.107: Loss = 0.534973
Epoch 1.108: Loss = 0.58577
Epoch 1.109: Loss = 0.565918
Epoch 1.110: Loss = 0.593979
Epoch 1.111: Loss = 0.488586
Epoch 1.112: Loss = 0.469162
Epoch 1.113: Loss = 0.555603
Epoch 1.114: Loss = 0.469803
Epoch 1.115: Loss = 0.565353
Epoch 1.116: Loss = 0.553619
Epoch 1.117: Loss = 0.437805
Epoch 1.118: Loss = 0.405212
Epoch 1.119: Loss = 0.407242
Epoch 1.120: Loss = 0.447525
TRAIN LOSS = 1.04428
TRAIN ACC = 72.9141 % (43750/60000)
Loss = 0.568222
Loss = 0.595505
Loss = 0.705017
Loss = 0.64978
Loss = 0.691574
Loss = 0.595535
Loss = 0.560349
Loss = 0.72554
Loss = 0.662491
Loss = 0.626617
Loss = 0.315567
Loss = 0.482239
Loss = 0.341965
Loss = 0.512207
Loss = 0.427322
Loss = 0.400406
Loss = 0.378433
Loss = 0.217377
Loss = 0.385361
Loss = 0.659073
TEST LOSS = 0.525029
TEST ACC = 437.5 % (8471/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.505188
Epoch 2.2: Loss = 0.651688
Epoch 2.3: Loss = 0.604446
Epoch 2.4: Loss = 0.479645
Epoch 2.5: Loss = 0.491913
Epoch 2.6: Loss = 0.484772
Epoch 2.7: Loss = 0.534973
Epoch 2.8: Loss = 0.514511
Epoch 2.9: Loss = 0.501999
Epoch 2.10: Loss = 0.505051
Epoch 2.11: Loss = 0.504898
Epoch 2.12: Loss = 0.495728
Epoch 2.13: Loss = 0.438141
Epoch 2.14: Loss = 0.478394
Epoch 2.15: Loss = 0.623245
Epoch 2.16: Loss = 0.560562
Epoch 2.17: Loss = 0.559753
Epoch 2.18: Loss = 0.647003
Epoch 2.19: Loss = 0.497391
Epoch 2.20: Loss = 0.444168
Epoch 2.21: Loss = 0.429092
Epoch 2.22: Loss = 0.435318
Epoch 2.23: Loss = 0.437622
Epoch 2.24: Loss = 0.61557
Epoch 2.25: Loss = 0.499084
Epoch 2.26: Loss = 0.599533
Epoch 2.27: Loss = 0.544357
Epoch 2.28: Loss = 0.553696
Epoch 2.29: Loss = 0.606705
Epoch 2.30: Loss = 0.667725
Epoch 2.31: Loss = 0.463989
Epoch 2.32: Loss = 0.586746
Epoch 2.33: Loss = 0.485077
Epoch 2.34: Loss = 0.557709
Epoch 2.35: Loss = 0.516205
Epoch 2.36: Loss = 0.600082
Epoch 2.37: Loss = 0.431625
Epoch 2.38: Loss = 0.406509
Epoch 2.39: Loss = 0.492111
Epoch 2.40: Loss = 0.44725
Epoch 2.41: Loss = 0.495392
Epoch 2.42: Loss = 0.566223
Epoch 2.43: Loss = 0.437073
Epoch 2.44: Loss = 0.387009
Epoch 2.45: Loss = 0.503357
Epoch 2.46: Loss = 0.526855
Epoch 2.47: Loss = 0.446243
Epoch 2.48: Loss = 0.507202
Epoch 2.49: Loss = 0.477615
Epoch 2.50: Loss = 0.564651
Epoch 2.51: Loss = 0.4254
Epoch 2.52: Loss = 0.41156
Epoch 2.53: Loss = 0.464676
Epoch 2.54: Loss = 0.554825
Epoch 2.55: Loss = 0.480652
Epoch 2.56: Loss = 0.432175
Epoch 2.57: Loss = 0.415359
Epoch 2.58: Loss = 0.468216
Epoch 2.59: Loss = 0.503464
Epoch 2.60: Loss = 0.562637
Epoch 2.61: Loss = 0.56752
Epoch 2.62: Loss = 0.559326
Epoch 2.63: Loss = 0.599686
Epoch 2.64: Loss = 0.564209
Epoch 2.65: Loss = 0.612686
Epoch 2.66: Loss = 0.466339
Epoch 2.67: Loss = 0.504822
Epoch 2.68: Loss = 0.326752
Epoch 2.69: Loss = 0.408813
Epoch 2.70: Loss = 0.558258
Epoch 2.71: Loss = 0.417648
Epoch 2.72: Loss = 0.439178
Epoch 2.73: Loss = 0.491104
Epoch 2.74: Loss = 0.351257
Epoch 2.75: Loss = 0.57196
Epoch 2.76: Loss = 0.461548
Epoch 2.77: Loss = 0.431793
Epoch 2.78: Loss = 0.425507
Epoch 2.79: Loss = 0.480881
Epoch 2.80: Loss = 0.527267
Epoch 2.81: Loss = 0.412231
Epoch 2.82: Loss = 0.379059
Epoch 2.83: Loss = 0.539978
Epoch 2.84: Loss = 0.460541
Epoch 2.85: Loss = 0.593414
Epoch 2.86: Loss = 0.476746
Epoch 2.87: Loss = 0.377472
Epoch 2.88: Loss = 0.43129
Epoch 2.89: Loss = 0.539383
Epoch 2.90: Loss = 0.399307
Epoch 2.91: Loss = 0.483795
Epoch 2.92: Loss = 0.491257
Epoch 2.93: Loss = 0.544174
Epoch 2.94: Loss = 0.371399
Epoch 2.95: Loss = 0.466888
Epoch 2.96: Loss = 0.494583
Epoch 2.97: Loss = 0.33812
Epoch 2.98: Loss = 0.409897
Epoch 2.99: Loss = 0.510437
Epoch 2.100: Loss = 0.602798
Epoch 2.101: Loss = 0.535599
Epoch 2.102: Loss = 0.460526
Epoch 2.103: Loss = 0.409851
Epoch 2.104: Loss = 0.362885
Epoch 2.105: Loss = 0.524673
Epoch 2.106: Loss = 0.521301
Epoch 2.107: Loss = 0.385468
Epoch 2.108: Loss = 0.453537
Epoch 2.109: Loss = 0.413528
Epoch 2.110: Loss = 0.455429
Epoch 2.111: Loss = 0.35318
Epoch 2.112: Loss = 0.337296
Epoch 2.113: Loss = 0.409332
Epoch 2.114: Loss = 0.334808
Epoch 2.115: Loss = 0.388641
Epoch 2.116: Loss = 0.3965
Epoch 2.117: Loss = 0.291351
Epoch 2.118: Loss = 0.248337
Epoch 2.119: Loss = 0.294754
Epoch 2.120: Loss = 0.332474
TRAIN LOSS = 0.479416
TRAIN ACC = 85.7697 % (51464/60000)
Loss = 0.423004
Loss = 0.47879
Loss = 0.568466
Loss = 0.541656
Loss = 0.582047
Loss = 0.456497
Loss = 0.419846
Loss = 0.618591
Loss = 0.535049
Loss = 0.51976
Loss = 0.213898
Loss = 0.345932
Loss = 0.267349
Loss = 0.390396
Loss = 0.267807
Loss = 0.299149
Loss = 0.263947
Loss = 0.114822
Loss = 0.265442
Loss = 0.567657
TEST LOSS = 0.407005
TEST ACC = 514.639 % (8782/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.392593
Epoch 3.2: Loss = 0.532028
Epoch 3.3: Loss = 0.489456
Epoch 3.4: Loss = 0.352158
Epoch 3.5: Loss = 0.374359
Epoch 3.6: Loss = 0.370667
Epoch 3.7: Loss = 0.388626
Epoch 3.8: Loss = 0.400192
Epoch 3.9: Loss = 0.382904
Epoch 3.10: Loss = 0.406128
Epoch 3.11: Loss = 0.415451
Epoch 3.12: Loss = 0.392609
Epoch 3.13: Loss = 0.336609
Epoch 3.14: Loss = 0.374176
Epoch 3.15: Loss = 0.473282
Epoch 3.16: Loss = 0.462814
Epoch 3.17: Loss = 0.483932
Epoch 3.18: Loss = 0.587097
Epoch 3.19: Loss = 0.413864
Epoch 3.20: Loss = 0.356857
Epoch 3.21: Loss = 0.344971
Epoch 3.22: Loss = 0.331848
Epoch 3.23: Loss = 0.353104
Epoch 3.24: Loss = 0.5336
Epoch 3.25: Loss = 0.433151
Epoch 3.26: Loss = 0.518768
Epoch 3.27: Loss = 0.469421
Epoch 3.28: Loss = 0.48436
Epoch 3.29: Loss = 0.54689
Epoch 3.30: Loss = 0.58197
Epoch 3.31: Loss = 0.374512
Epoch 3.32: Loss = 0.497498
Epoch 3.33: Loss = 0.389069
Epoch 3.34: Loss = 0.470917
Epoch 3.35: Loss = 0.429474
Epoch 3.36: Loss = 0.499985
Epoch 3.37: Loss = 0.334167
Epoch 3.38: Loss = 0.347427
Epoch 3.39: Loss = 0.407547
Epoch 3.40: Loss = 0.374084
Epoch 3.41: Loss = 0.403549
Epoch 3.42: Loss = 0.528091
Epoch 3.43: Loss = 0.361969
Epoch 3.44: Loss = 0.330734
Epoch 3.45: Loss = 0.425034
Epoch 3.46: Loss = 0.472336
Epoch 3.47: Loss = 0.401398
Epoch 3.48: Loss = 0.432739
Epoch 3.49: Loss = 0.406265
Epoch 3.50: Loss = 0.487457
Epoch 3.51: Loss = 0.348679
Epoch 3.52: Loss = 0.329315
