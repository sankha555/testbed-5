Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.33434
Epoch 1.2: Loss = 2.27032
Epoch 1.3: Loss = 2.22594
Epoch 1.4: Loss = 2.14282
Epoch 1.5: Loss = 2.10558
Epoch 1.6: Loss = 2.07278
Epoch 1.7: Loss = 1.99158
Epoch 1.8: Loss = 1.94237
Epoch 1.9: Loss = 1.90063
Epoch 1.10: Loss = 1.82957
Epoch 1.11: Loss = 1.8468
Epoch 1.12: Loss = 1.7831
Epoch 1.13: Loss = 1.71519
Epoch 1.14: Loss = 1.70438
Epoch 1.15: Loss = 1.64215
Epoch 1.16: Loss = 1.65596
Epoch 1.17: Loss = 1.60913
Epoch 1.18: Loss = 1.57724
Epoch 1.19: Loss = 1.52916
Epoch 1.20: Loss = 1.52931
Epoch 1.21: Loss = 1.47388
Epoch 1.22: Loss = 1.45564
Epoch 1.23: Loss = 1.40558
Epoch 1.24: Loss = 1.46169
Epoch 1.25: Loss = 1.38423
Epoch 1.26: Loss = 1.31117
Epoch 1.27: Loss = 1.25829
Epoch 1.28: Loss = 1.27953
Epoch 1.29: Loss = 1.27367
Epoch 1.30: Loss = 1.23622
Epoch 1.31: Loss = 1.25816
Epoch 1.32: Loss = 1.22226
Epoch 1.33: Loss = 1.12663
Epoch 1.34: Loss = 1.20023
Epoch 1.35: Loss = 1.22142
Epoch 1.36: Loss = 1.18985
Epoch 1.37: Loss = 1.14342
Epoch 1.38: Loss = 1.10599
Epoch 1.39: Loss = 1.09596
Epoch 1.40: Loss = 1.07568
Epoch 1.41: Loss = 1.12946
Epoch 1.42: Loss = 1.06013
Epoch 1.43: Loss = 1.05167
Epoch 1.44: Loss = 0.9823
Epoch 1.45: Loss = 1.03537
Epoch 1.46: Loss = 1.03191
Epoch 1.47: Loss = 0.997513
Epoch 1.48: Loss = 0.97023
Epoch 1.49: Loss = 0.996918
Epoch 1.50: Loss = 0.944
Epoch 1.51: Loss = 0.907486
Epoch 1.52: Loss = 1.01666
Epoch 1.53: Loss = 1.00797
Epoch 1.54: Loss = 0.873337
Epoch 1.55: Loss = 0.93602
Epoch 1.56: Loss = 0.955444
Epoch 1.57: Loss = 0.968796
Epoch 1.58: Loss = 0.934219
Epoch 1.59: Loss = 0.914856
Epoch 1.60: Loss = 0.967804
Epoch 1.61: Loss = 0.855911
Epoch 1.62: Loss = 0.954758
Epoch 1.63: Loss = 0.822708
Epoch 1.64: Loss = 0.84668
Epoch 1.65: Loss = 0.899567
Epoch 1.66: Loss = 0.918503
Epoch 1.67: Loss = 0.823776
Epoch 1.68: Loss = 0.938354
Epoch 1.69: Loss = 0.889389
Epoch 1.70: Loss = 0.866135
Epoch 1.71: Loss = 0.799042
Epoch 1.72: Loss = 0.82489
Epoch 1.73: Loss = 0.916077
Epoch 1.74: Loss = 0.908112
Epoch 1.75: Loss = 0.846909
Epoch 1.76: Loss = 0.861511
Epoch 1.77: Loss = 0.837708
Epoch 1.78: Loss = 0.855469
Epoch 1.79: Loss = 0.772949
Epoch 1.80: Loss = 0.839905
Epoch 1.81: Loss = 0.793655
Epoch 1.82: Loss = 0.838577
Epoch 1.83: Loss = 0.875259
Epoch 1.84: Loss = 0.814484
Epoch 1.85: Loss = 0.777969
Epoch 1.86: Loss = 0.883728
Epoch 1.87: Loss = 0.851822
Epoch 1.88: Loss = 0.749451
Epoch 1.89: Loss = 0.866364
Epoch 1.90: Loss = 0.776978
Epoch 1.91: Loss = 0.866333
Epoch 1.92: Loss = 0.819702
Epoch 1.93: Loss = 0.839111
Epoch 1.94: Loss = 0.799683
Epoch 1.95: Loss = 0.821167
Epoch 1.96: Loss = 0.764847
Epoch 1.97: Loss = 0.649948
Epoch 1.98: Loss = 0.77417
Epoch 1.99: Loss = 0.769608
Epoch 1.100: Loss = 0.7435
Epoch 1.101: Loss = 0.838287
Epoch 1.102: Loss = 0.840942
Epoch 1.103: Loss = 0.819672
Epoch 1.104: Loss = 0.763153
Epoch 1.105: Loss = 0.721146
Epoch 1.106: Loss = 0.869278
Epoch 1.107: Loss = 0.802536
Epoch 1.108: Loss = 0.804321
Epoch 1.109: Loss = 0.784485
Epoch 1.110: Loss = 0.789444
Epoch 1.111: Loss = 0.735931
Epoch 1.112: Loss = 0.702454
Epoch 1.113: Loss = 0.758133
Epoch 1.114: Loss = 0.738632
Epoch 1.115: Loss = 0.749634
Epoch 1.116: Loss = 0.696396
Epoch 1.117: Loss = 0.828934
Epoch 1.118: Loss = 0.68576
Epoch 1.119: Loss = 0.736694
Epoch 1.120: Loss = 0.735397
TRAIN LOSS = 1.09189
TRAIN ACC = 64.3234 % (38596/60000)
Loss = 0.7108
Loss = 0.804565
Loss = 0.837524
Loss = 0.728226
Loss = 0.726089
Loss = 0.912247
Loss = 0.897858
Loss = 0.843857
Loss = 0.787796
Loss = 0.714081
Loss = 0.851913
Loss = 0.823517
Loss = 0.828033
Loss = 0.83046
Loss = 0.787781
Loss = 0.825775
Loss = 0.726532
Loss = 0.800659
Loss = 0.851807
Loss = 0.762314
TEST LOSS = 0.802591
TEST ACC = 385.959 % (7276/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.790405
Epoch 2.2: Loss = 0.759094
Epoch 2.3: Loss = 0.822128
Epoch 2.4: Loss = 0.673965
Epoch 2.5: Loss = 0.757553
Epoch 2.6: Loss = 0.805603
Epoch 2.7: Loss = 0.731644
Epoch 2.8: Loss = 0.803131
Epoch 2.9: Loss = 0.681305
Epoch 2.10: Loss = 0.592773
Epoch 2.11: Loss = 0.831009
Epoch 2.12: Loss = 0.779358
Epoch 2.13: Loss = 0.763809
Epoch 2.14: Loss = 0.749588
Epoch 2.15: Loss = 0.750946
Epoch 2.16: Loss = 0.798416
Epoch 2.17: Loss = 0.70015
Epoch 2.18: Loss = 0.745834
Epoch 2.19: Loss = 0.722366
Epoch 2.20: Loss = 0.80574
Epoch 2.21: Loss = 0.682449
Epoch 2.22: Loss = 0.65535
Epoch 2.23: Loss = 0.766327
Epoch 2.24: Loss = 0.819733
Epoch 2.25: Loss = 0.75824
Epoch 2.26: Loss = 0.674927
Epoch 2.27: Loss = 0.717438
Epoch 2.28: Loss = 0.725983
Epoch 2.29: Loss = 0.742477
Epoch 2.30: Loss = 0.690063
Epoch 2.31: Loss = 0.807587
Epoch 2.32: Loss = 0.711533
Epoch 2.33: Loss = 0.644409
Epoch 2.34: Loss = 0.794678
Epoch 2.35: Loss = 0.77774
Epoch 2.36: Loss = 0.767624
Epoch 2.37: Loss = 0.767349
Epoch 2.38: Loss = 0.735062
Epoch 2.39: Loss = 0.781021
Epoch 2.40: Loss = 0.718292
Epoch 2.41: Loss = 0.768814
Epoch 2.42: Loss = 0.719666
Epoch 2.43: Loss = 0.797485
Epoch 2.44: Loss = 0.634308
Epoch 2.45: Loss = 0.76796
Epoch 2.46: Loss = 0.860703
Epoch 2.47: Loss = 0.69545
Epoch 2.48: Loss = 0.678619
Epoch 2.49: Loss = 0.8004
Epoch 2.50: Loss = 0.711166
Epoch 2.51: Loss = 0.611755
Epoch 2.52: Loss = 0.798294
Epoch 2.53: Loss = 0.830536
Epoch 2.54: Loss = 0.598419
Epoch 2.55: Loss = 0.75209
Epoch 2.56: Loss = 0.746964
Epoch 2.57: Loss = 0.772888
Epoch 2.58: Loss = 0.725296
Epoch 2.59: Loss = 0.746368
Epoch 2.60: Loss = 0.764862
Epoch 2.61: Loss = 0.696579
Epoch 2.62: Loss = 0.799133
Epoch 2.63: Loss = 0.641052
Epoch 2.64: Loss = 0.663437
Epoch 2.65: Loss = 0.739288
Epoch 2.66: Loss = 0.723068
Epoch 2.67: Loss = 0.690445
Epoch 2.68: Loss = 0.812653
Epoch 2.69: Loss = 0.730194
Epoch 2.70: Loss = 0.815155
Epoch 2.71: Loss = 0.640045
Epoch 2.72: Loss = 0.749985
Epoch 2.73: Loss = 0.788818
Epoch 2.74: Loss = 0.763168
Epoch 2.75: Loss = 0.685333
Epoch 2.76: Loss = 0.713501
Epoch 2.77: Loss = 0.706329
Epoch 2.78: Loss = 0.698776
Epoch 2.79: Loss = 0.674973
Epoch 2.80: Loss = 0.666397
Epoch 2.81: Loss = 0.684006
Epoch 2.82: Loss = 0.712997
Epoch 2.83: Loss = 0.779984
Epoch 2.84: Loss = 0.676407
Epoch 2.85: Loss = 0.692673
Epoch 2.86: Loss = 0.769531
Epoch 2.87: Loss = 0.742523
Epoch 2.88: Loss = 0.64122
Epoch 2.89: Loss = 0.833466
Epoch 2.90: Loss = 0.734192
Epoch 2.91: Loss = 0.78476
Epoch 2.92: Loss = 0.735092
Epoch 2.93: Loss = 0.779037
Epoch 2.94: Loss = 0.71405
Epoch 2.95: Loss = 0.735855
Epoch 2.96: Loss = 0.71312
Epoch 2.97: Loss = 0.607925
Epoch 2.98: Loss = 0.718719
Epoch 2.99: Loss = 0.729507
Epoch 2.100: Loss = 0.700241
Epoch 2.101: Loss = 0.790207
Epoch 2.102: Loss = 0.781754
Epoch 2.103: Loss = 0.725632
Epoch 2.104: Loss = 0.696075
Epoch 2.105: Loss = 0.638229
Epoch 2.106: Loss = 0.802078
Epoch 2.107: Loss = 0.740479
Epoch 2.108: Loss = 0.78891
Epoch 2.109: Loss = 0.764374
Epoch 2.110: Loss = 0.74057
Epoch 2.111: Loss = 0.658951
Epoch 2.112: Loss = 0.70575
Epoch 2.113: Loss = 0.716263
Epoch 2.114: Loss = 0.70079
Epoch 2.115: Loss = 0.733749
Epoch 2.116: Loss = 0.658844
Epoch 2.117: Loss = 0.7854
Epoch 2.118: Loss = 0.627151
Epoch 2.119: Loss = 0.690781
Epoch 2.120: Loss = 0.682037
TRAIN LOSS = 0.732498
TRAIN ACC = 75.5646 % (45341/60000)
Loss = 0.638046
Loss = 0.725204
Loss = 0.720505
Loss = 0.625793
Loss = 0.715836
Loss = 0.887863
Loss = 0.879822
Loss = 0.76091
Loss = 0.746872
Loss = 0.650009
Loss = 0.803391
Loss = 0.813812
Loss = 0.774902
Loss = 0.754349
Loss = 0.732147
Loss = 0.77417
Loss = 0.649185
Loss = 0.764313
Loss = 0.843079
Loss = 0.70755
TEST LOSS = 0.748388
TEST ACC = 453.409 % (7593/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.74028
Epoch 3.2: Loss = 0.730804
Epoch 3.3: Loss = 0.752396
Epoch 3.4: Loss = 0.583557
Epoch 3.5: Loss = 0.683243
Epoch 3.6: Loss = 0.767151
Epoch 3.7: Loss = 0.694794
Epoch 3.8: Loss = 0.76413
Epoch 3.9: Loss = 0.615021
Epoch 3.10: Loss = 0.539871
Epoch 3.11: Loss = 0.760971
Epoch 3.12: Loss = 0.73024
Epoch 3.13: Loss = 0.779816
Epoch 3.14: Loss = 0.711609
Epoch 3.15: Loss = 0.731979
Epoch 3.16: Loss = 0.814865
Epoch 3.17: Loss = 0.632996
Epoch 3.18: Loss = 0.74794
Epoch 3.19: Loss = 0.675629
Epoch 3.20: Loss = 0.796982
Epoch 3.21: Loss = 0.622314
Epoch 3.22: Loss = 0.588593
Epoch 3.23: Loss = 0.775085
Epoch 3.24: Loss = 0.800507
Epoch 3.25: Loss = 0.732605
Epoch 3.26: Loss = 0.581238
Epoch 3.27: Loss = 0.707092
Epoch 3.28: Loss = 0.694458
Epoch 3.29: Loss = 0.706543
Epoch 3.30: Loss = 0.690002
Epoch 3.31: Loss = 0.796982
Epoch 3.32: Loss = 0.651825
Epoch 3.33: Loss = 0.619995
Epoch 3.34: Loss = 0.792892
Epoch 3.35: Loss = 0.786194
Epoch 3.36: Loss = 0.744308
Epoch 3.37: Loss = 0.756927
Epoch 3.38: Loss = 0.747177
Epoch 3.39: Loss = 0.769852
Epoch 3.40: Loss = 0.653824
Epoch 3.41: Loss = 0.741669
Epoch 3.42: Loss = 0.662048
Epoch 3.43: Loss = 0.785583
Epoch 3.44: Loss = 0.612167
Epoch 3.45: Loss = 0.75145
Epoch 3.46: Loss = 0.859497
Epoch 3.47: Loss = 0.646652
Epoch 3.48: Loss = 0.633179
Epoch 3.49: Loss = 0.726501
Epoch 3.50: Loss = 0.743332
Epoch 3.51: Loss = 0.564453
Epoch 3.52: Loss = 0.843765
Epoch 3.53: Loss = 0.833282
Epoch 3.54: Loss = 0.572891
Epoch 3.55: Loss = 0.735489
Epoch 3.56: Loss = 0.740707
Epoch 3.57: Loss = 0.778519
Epoch 3.58: Loss = 0.712646
Epoch 3.59: Loss = 0.727814
Epoch 3.60: Loss = 0.741302
Epoch 3.61: Loss = 0.680634
Epoch 3.62: Loss = 0.802994
Epoch 3.63: Loss = 0.644974
Epoch 3.64: Loss = 0.62413
Epoch 3.65: Loss = 0.811401
Epoch 3.66: Loss = 0.718246
Epoch 3.67: Loss = 0.691742
Epoch 3.68: Loss = 0.812668
Epoch 3.69: Loss = 0.720642
Epoch 3.70: Loss = 0.806198
Epoch 3.71: Loss = 0.604996
Epoch 3.72: Loss = 0.733505
Epoch 3.73: Loss = 0.77533
Epoch 3.74: Loss = 0.758087
Epoch 3.75: Loss = 0.695129
Epoch 3.76: Loss = 0.76442
Epoch 3.77: Loss = 0.716751
Epoch 3.78: Loss = 0.736649
Epoch 3.79: Loss = 0.650009
Epoch 3.80: Loss = 0.689453
Epoch 3.81: Loss = 0.698715
Epoch 3.82: Loss = 0.734695
Epoch 3.83: Loss = 0.763916
Epoch 3.84: Loss = 0.678741
Epoch 3.85: Loss = 0.695541
Epoch 3.86: Loss = 0.763397
Epoch 3.87: Loss = 0.743576
Epoch 3.88: Loss = 0.606873
Epoch 3.89: Loss = 0.873367
Epoch 3.90: Loss = 0.741745
Epoch 3.91: Loss = 0.812119
Epoch 3.92: Loss = 0.689987
Epoch 3.93: Loss = 0.776291
Epoch 3.94: Loss = 0.720291
Epoch 3.95: Loss = 0.734039
Epoch 3.96: Loss = 0.688751
Epoch 3.97: Loss = 0.580276
Epoch 3.98: Loss = 0.751953
Epoch 3.99: Loss = 0.712311
Epoch 3.100: Loss = 0.698959
Epoch 3.101: Loss = 0.764725
Epoch 3.102: Loss = 0.750961
Epoch 3.103: Loss = 0.705704
Epoch 3.104: Loss = 0.64389
Epoch 3.105: Loss = 0.61937
Epoch 3.106: Loss = 0.799484
Epoch 3.107: Loss = 0.803116
Epoch 3.108: Loss = 0.763245
Epoch 3.109: Loss = 0.763977
Epoch 3.110: Loss = 0.783722
Epoch 3.111: Loss = 0.706482
Epoch 3.112: Loss = 0.764191
Epoch 3.113: Loss = 0.669144
Epoch 3.114: Loss = 0.739578
Epoch 3.115: Loss = 0.79744
Epoch 3.116: Loss = 0.718552
Epoch 3.117: Loss = 0.812531
Epoch 3.118: Loss = 0.669235
Epoch 3.119: Loss = 0.747528
Epoch 3.120: Loss = 0.684631
TRAIN LOSS = 0.720825
TRAIN ACC = 77.4124 % (46449/60000)
Loss = 0.707626
Loss = 0.80719
Loss = 0.737823
Loss = 0.652206
Loss = 0.749573
Loss = 0.920074
Loss = 0.890518
Loss = 0.8022
Loss = 0.801361
Loss = 0.69455
Loss = 0.861862
Loss = 0.847397
Loss = 0.80278
Loss = 0.835571
Loss = 0.807648
Loss = 0.813492
Loss = 0.65535
Loss = 0.789948
Loss = 0.863266
Loss = 0.714218
TEST LOSS = 0.787732
TEST ACC = 464.49 % (7648/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.747437
Epoch 4.2: Loss = 0.78801
Epoch 4.3: Loss = 0.768051
Epoch 4.4: Loss = 0.620178
Epoch 4.5: Loss = 0.713165
Epoch 4.6: Loss = 0.833679
Epoch 4.7: Loss = 0.767395
Epoch 4.8: Loss = 0.82489
Epoch 4.9: Loss = 0.629776
Epoch 4.10: Loss = 0.540176
Epoch 4.11: Loss = 0.779114
Epoch 4.12: Loss = 0.733521
Epoch 4.13: Loss = 0.826065
Epoch 4.14: Loss = 0.720276
Epoch 4.15: Loss = 0.739716
Epoch 4.16: Loss = 0.837845
Epoch 4.17: Loss = 0.63707
Epoch 4.18: Loss = 0.774704
Epoch 4.19: Loss = 0.651047
Epoch 4.20: Loss = 0.897552
Epoch 4.21: Loss = 0.648178
Epoch 4.22: Loss = 0.57933
Epoch 4.23: Loss = 0.801743
Epoch 4.24: Loss = 0.824646
Epoch 4.25: Loss = 0.72052
Epoch 4.26: Loss = 0.614868
Epoch 4.27: Loss = 0.753082
Epoch 4.28: Loss = 0.738983
Epoch 4.29: Loss = 0.723282
Epoch 4.30: Loss = 0.728256
Epoch 4.31: Loss = 0.859375
Epoch 4.32: Loss = 0.704727
Epoch 4.33: Loss = 0.589478
Epoch 4.34: Loss = 0.836716
Epoch 4.35: Loss = 0.773544
Epoch 4.36: Loss = 0.766571
Epoch 4.37: Loss = 0.722916
Epoch 4.38: Loss = 0.729401
Epoch 4.39: Loss = 0.786407
Epoch 4.40: Loss = 0.731186
Epoch 4.41: Loss = 0.773819
Epoch 4.42: Loss = 0.686768
Epoch 4.43: Loss = 0.765472
Epoch 4.44: Loss = 0.624115
Epoch 4.45: Loss = 0.775726
Epoch 4.46: Loss = 0.964737
Epoch 4.47: Loss = 0.651459
Epoch 4.48: Loss = 0.666428
Epoch 4.49: Loss = 0.702393
Epoch 4.50: Loss = 0.780029
Epoch 4.51: Loss = 0.613419
Epoch 4.52: Loss = 0.843964
Epoch 4.53: Loss = 0.892273
Epoch 4.54: Loss = 0.618759
Epoch 4.55: Loss = 0.736649
Epoch 4.56: Loss = 0.761276
Epoch 4.57: Loss = 0.777161
Epoch 4.58: Loss = 0.700729
Epoch 4.59: Loss = 0.750092
Epoch 4.60: Loss = 0.810028
Epoch 4.61: Loss = 0.754929
Epoch 4.62: Loss = 0.836441
Epoch 4.63: Loss = 0.732498
Epoch 4.64: Loss = 0.648834
Epoch 4.65: Loss = 0.790253
Epoch 4.66: Loss = 0.658981
Epoch 4.67: Loss = 0.703552
Epoch 4.68: Loss = 0.893097
Epoch 4.69: Loss = 0.805634
Epoch 4.70: Loss = 0.831467
Epoch 4.71: Loss = 0.683472
Epoch 4.72: Loss = 0.715225
Epoch 4.73: Loss = 0.83017
Epoch 4.74: Loss = 0.789413
Epoch 4.75: Loss = 0.67601
Epoch 4.76: Loss = 0.739792
Epoch 4.77: Loss = 0.736679
Epoch 4.78: Loss = 0.755768
Epoch 4.79: Loss = 0.705383
Epoch 4.80: Loss = 0.694916
Epoch 4.81: Loss = 0.716583
Epoch 4.82: Loss = 0.753784
Epoch 4.83: Loss = 0.802597
Epoch 4.84: Loss = 0.689178
Epoch 4.85: Loss = 0.777496
Epoch 4.86: Loss = 0.804474
Epoch 4.87: Loss = 0.72464
Epoch 4.88: Loss = 0.645508
Epoch 4.89: Loss = 0.930283
Epoch 4.90: Loss = 0.766693
Epoch 4.91: Loss = 0.797684
Epoch 4.92: Loss = 0.763382
Epoch 4.93: Loss = 0.774506
Epoch 4.94: Loss = 0.736145
Epoch 4.95: Loss = 0.811188
Epoch 4.96: Loss = 0.748978
Epoch 4.97: Loss = 0.577347
Epoch 4.98: Loss = 0.764069
Epoch 4.99: Loss = 0.72879
Epoch 4.100: Loss = 0.745895
Epoch 4.101: Loss = 0.884384
Epoch 4.102: Loss = 0.834808
Epoch 4.103: Loss = 0.739349
Epoch 4.104: Loss = 0.704407
Epoch 4.105: Loss = 0.635818
Epoch 4.106: Loss = 0.799622
Epoch 4.107: Loss = 0.881378
Epoch 4.108: Loss = 0.812805
Epoch 4.109: Loss = 0.805496
Epoch 4.110: Loss = 0.787018
Epoch 4.111: Loss = 0.772232
Epoch 4.112: Loss = 0.818558
Epoch 4.113: Loss = 0.772858
Epoch 4.114: Loss = 0.738693
Epoch 4.115: Loss = 0.758392
Epoch 4.116: Loss = 0.69426
Epoch 4.117: Loss = 0.880905
Epoch 4.118: Loss = 0.669571
Epoch 4.119: Loss = 0.726532
Epoch 4.120: Loss = 0.742798
TRAIN LOSS = 0.74884
TRAIN ACC = 78.3615 % (47019/60000)
Loss = 0.733612
Loss = 0.8582
Loss = 0.780762
Loss = 0.747162
Loss = 0.82811
Loss = 0.980072
Loss = 0.970154
Loss = 0.867798
Loss = 0.839371
Loss = 0.718445
Loss = 0.932663
Loss = 0.897095
Loss = 0.849045
Loss = 0.876862
Loss = 0.868393
Loss = 0.839691
Loss = 0.748734
Loss = 0.858414
Loss = 0.920563
Loss = 0.824661
TEST LOSS = 0.84699
TEST ACC = 470.189 % (7749/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.742996
Epoch 5.2: Loss = 0.844803
Epoch 5.3: Loss = 0.789536
Epoch 5.4: Loss = 0.654175
Epoch 5.5: Loss = 0.784164
Epoch 5.6: Loss = 0.862946
Epoch 5.7: Loss = 0.765289
Epoch 5.8: Loss = 0.88324
Epoch 5.9: Loss = 0.652695
Epoch 5.10: Loss = 0.591217
Epoch 5.11: Loss = 0.796738
Epoch 5.12: Loss = 0.766556
Epoch 5.13: Loss = 0.857361
Epoch 5.14: Loss = 0.794815
Epoch 5.15: Loss = 0.809692
Epoch 5.16: Loss = 0.883438
Epoch 5.17: Loss = 0.674713
Epoch 5.18: Loss = 0.752838
Epoch 5.19: Loss = 0.71814
Epoch 5.20: Loss = 0.926224
Epoch 5.21: Loss = 0.645737
Epoch 5.22: Loss = 0.650345
Epoch 5.23: Loss = 0.790329
Epoch 5.24: Loss = 0.826126
Epoch 5.25: Loss = 0.741547
Epoch 5.26: Loss = 0.646454
Epoch 5.27: Loss = 0.824066
Epoch 5.28: Loss = 0.797241
Epoch 5.29: Loss = 0.778214
Epoch 5.30: Loss = 0.743958
Epoch 5.31: Loss = 0.863388
Epoch 5.32: Loss = 0.733017
Epoch 5.33: Loss = 0.648651
Epoch 5.34: Loss = 0.847183
Epoch 5.35: Loss = 0.816345
Epoch 5.36: Loss = 0.791809
Epoch 5.37: Loss = 0.82338
Epoch 5.38: Loss = 0.795059
Epoch 5.39: Loss = 0.809189
Epoch 5.40: Loss = 0.731491
Epoch 5.41: Loss = 0.905701
Epoch 5.42: Loss = 0.746017
Epoch 5.43: Loss = 0.83696
Epoch 5.44: Loss = 0.678085
Epoch 5.45: Loss = 0.794907
Epoch 5.46: Loss = 0.916916
Epoch 5.47: Loss = 0.760284
Epoch 5.48: Loss = 0.740753
Epoch 5.49: Loss = 0.793137
Epoch 5.50: Loss = 0.815674
Epoch 5.51: Loss = 0.629059
Epoch 5.52: Loss = 0.885254
Epoch 5.53: Loss = 0.946396
Epoch 5.54: Loss = 0.711716
Epoch 5.55: Loss = 0.872116
Epoch 5.56: Loss = 0.883667
Epoch 5.57: Loss = 0.868668
Epoch 5.58: Loss = 0.800537
Epoch 5.59: Loss = 0.856827
Epoch 5.60: Loss = 0.837677
Epoch 5.61: Loss = 0.802963
Epoch 5.62: Loss = 0.882721
Epoch 5.63: Loss = 0.724213
Epoch 5.64: Loss = 0.685333
Epoch 5.65: Loss = 0.907623
Epoch 5.66: Loss = 0.734055
Epoch 5.67: Loss = 0.803146
Epoch 5.68: Loss = 1.04053
Epoch 5.69: Loss = 0.86908
Epoch 5.70: Loss = 0.836288
Epoch 5.71: Loss = 0.693802
Epoch 5.72: Loss = 0.781708
Epoch 5.73: Loss = 0.922897
Epoch 5.74: Loss = 0.848236
Epoch 5.75: Loss = 0.756683
Epoch 5.76: Loss = 0.896149
Epoch 5.77: Loss = 0.891006
Epoch 5.78: Loss = 0.874191
Epoch 5.79: Loss = 0.80571
Epoch 5.80: Loss = 0.773071
Epoch 5.81: Loss = 0.771561
Epoch 5.82: Loss = 0.825867
Epoch 5.83: Loss = 0.939941
Epoch 5.84: Loss = 0.760437
Epoch 5.85: Loss = 0.879868
Epoch 5.86: Loss = 0.902802
Epoch 5.87: Loss = 0.762115
Epoch 5.88: Loss = 0.770142
Epoch 5.89: Loss = 1.00491
Epoch 5.90: Loss = 0.852234
Epoch 5.91: Loss = 0.868073
Epoch 5.92: Loss = 0.829819
Epoch 5.93: Loss = 0.853027
Epoch 5.94: Loss = 0.850464
Epoch 5.95: Loss = 0.860611
Epoch 5.96: Loss = 0.768951
Epoch 5.97: Loss = 0.593704
Epoch 5.98: Loss = 0.760391
Epoch 5.99: Loss = 0.755295
Epoch 5.100: Loss = 0.851242
Epoch 5.101: Loss = 0.877762
Epoch 5.102: Loss = 0.854355
Epoch 5.103: Loss = 0.820572
Epoch 5.104: Loss = 0.742859
Epoch 5.105: Loss = 0.72551
Epoch 5.106: Loss = 0.895905
Epoch 5.107: Loss = 0.940964
Epoch 5.108: Loss = 0.941757
Epoch 5.109: Loss = 0.8703
Epoch 5.110: Loss = 0.967499
Epoch 5.111: Loss = 0.878357
Epoch 5.112: Loss = 0.924515
Epoch 5.113: Loss = 0.801514
Epoch 5.114: Loss = 0.857132
Epoch 5.115: Loss = 0.8302
Epoch 5.116: Loss = 0.774872
Epoch 5.117: Loss = 0.921158
Epoch 5.118: Loss = 0.792465
Epoch 5.119: Loss = 0.849075
Epoch 5.120: Loss = 0.860657
TRAIN LOSS = 0.810471
TRAIN ACC = 78.6148 % (47171/60000)
Loss = 0.766968
Loss = 0.904007
Loss = 0.79985
Loss = 0.840729
Loss = 0.867096
Loss = 1.07034
Loss = 0.997971
Loss = 0.955582
Loss = 0.89801
Loss = 0.791519
Loss = 1.0136
Loss = 0.980484
Loss = 0.890961
Loss = 0.923294
Loss = 0.905472
Loss = 0.898117
Loss = 0.779236
Loss = 0.889557
Loss = 0.930954
Loss = 0.899796
TEST LOSS = 0.900177
TEST ACC = 471.709 % (7820/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.803864
Epoch 6.2: Loss = 0.929184
Epoch 6.3: Loss = 0.927307
Epoch 6.4: Loss = 0.739243
Epoch 6.5: Loss = 0.872543
Epoch 6.6: Loss = 0.97467
Epoch 6.7: Loss = 0.843674
Epoch 6.8: Loss = 0.949432
Epoch 6.9: Loss = 0.741867
Epoch 6.10: Loss = 0.6185
Epoch 6.11: Loss = 0.908936
Epoch 6.12: Loss = 0.828735
Epoch 6.13: Loss = 0.944046
Epoch 6.14: Loss = 0.778839
Epoch 6.15: Loss = 0.857956
Epoch 6.16: Loss = 0.963608
Epoch 6.17: Loss = 0.733719
Epoch 6.18: Loss = 0.842392
Epoch 6.19: Loss = 0.750137
Epoch 6.20: Loss = 1.01657
Epoch 6.21: Loss = 0.706467
Epoch 6.22: Loss = 0.668396
Epoch 6.23: Loss = 0.779678
Epoch 6.24: Loss = 0.97554
Epoch 6.25: Loss = 0.833115
Epoch 6.26: Loss = 0.721085
Epoch 6.27: Loss = 0.826981
Epoch 6.28: Loss = 0.865616
Epoch 6.29: Loss = 0.796936
Epoch 6.30: Loss = 0.774796
Epoch 6.31: Loss = 0.944168
Epoch 6.32: Loss = 0.768509
Epoch 6.33: Loss = 0.721848
Epoch 6.34: Loss = 0.88765
Epoch 6.35: Loss = 0.928467
Epoch 6.36: Loss = 0.833496
Epoch 6.37: Loss = 0.925095
Epoch 6.38: Loss = 0.954346
Epoch 6.39: Loss = 0.92276
Epoch 6.40: Loss = 0.828018
Epoch 6.41: Loss = 0.943909
Epoch 6.42: Loss = 0.795486
Epoch 6.43: Loss = 0.921494
Epoch 6.44: Loss = 0.717758
Epoch 6.45: Loss = 0.924652
Epoch 6.46: Loss = 1.03392
Epoch 6.47: Loss = 0.862595
Epoch 6.48: Loss = 0.759979
Epoch 6.49: Loss = 0.887146
Epoch 6.50: Loss = 0.845169
Epoch 6.51: Loss = 0.639114
Epoch 6.52: Loss = 0.913086
Epoch 6.53: Loss = 0.952988
Epoch 6.54: Loss = 0.750046
Epoch 6.55: Loss = 0.901459
Epoch 6.56: Loss = 0.971252
Epoch 6.57: Loss = 1.0275
Epoch 6.58: Loss = 0.871277
Epoch 6.59: Loss = 0.953568
Epoch 6.60: Loss = 0.903595
Epoch 6.61: Loss = 0.79425
Epoch 6.62: Loss = 0.879227
Epoch 6.63: Loss = 0.701126
Epoch 6.64: Loss = 0.795868
Epoch 6.65: Loss = 0.959808
Epoch 6.66: Loss = 0.944107
Epoch 6.67: Loss = 34.1724
Epoch 6.68: Loss = 82.5589
Epoch 6.69: Loss = 17.3266
Epoch 6.70: Loss = 10.1907
Epoch 6.71: Loss = 8.65764
Epoch 6.72: Loss = 7.55556
Epoch 6.73: Loss = 6.87436
Epoch 6.74: Loss = 6.67888
Epoch 6.75: Loss = 5.4809
Epoch 6.76: Loss = 5.9747
Epoch 6.77: Loss = 5.24113
Epoch 6.78: Loss = 4.45639
Epoch 6.79: Loss = 4.33687
Epoch 6.80: Loss = 4.11757
Epoch 6.81: Loss = 3.83182
Epoch 6.82: Loss = 2.65669
Epoch 6.83: Loss = 2.68037
Epoch 6.84: Loss = 1.98872
Epoch 6.85: Loss = 2.18201
Epoch 6.86: Loss = 1.4771
Epoch 6.87: Loss = 1.76195
Epoch 6.88: Loss = 1.42975
Epoch 6.89: Loss = 1.29842
Epoch 6.90: Loss = 1.05763
Epoch 6.91: Loss = 1.06493
Epoch 6.92: Loss = 0.832672
Epoch 6.93: Loss = 0.882202
Epoch 6.94: Loss = 0.904495
Epoch 6.95: Loss = 0.935745
Epoch 6.96: Loss = 0.882904
Epoch 6.97: Loss = 0.779144
Epoch 6.98: Loss = 0.806244
Epoch 6.99: Loss = 0.83493
Epoch 6.100: Loss = 0.906631
Epoch 6.101: Loss = 0.963348
Epoch 6.102: Loss = 0.951645
Epoch 6.103: Loss = 0.831039
Epoch 6.104: Loss = 0.716248
Epoch 6.105: Loss = 0.747818
Epoch 6.106: Loss = 0.925095
Epoch 6.107: Loss = 0.954544
Epoch 6.108: Loss = 0.926575
Epoch 6.109: Loss = 0.96254
Epoch 6.110: Loss = 0.896027
Epoch 6.111: Loss = 0.872086
Epoch 6.112: Loss = 0.917709
Epoch 6.113: Loss = 0.836639
Epoch 6.114: Loss = 0.851196
Epoch 6.115: Loss = 0.820938
Epoch 6.116: Loss = 0.770844
Epoch 6.117: Loss = 1.01561
Epoch 6.118: Loss = 0.789276
Epoch 6.119: Loss = 0.831635
Epoch 6.120: Loss = 0.891556
TRAIN LOSS = 2.55528
TRAIN ACC = 73.9197 % (44354/60000)
Loss = 0.813232
Loss = 0.928513
Loss = 0.807571
Loss = 0.806458
Loss = 0.888885
Loss = 1.11334
Loss = 1.12491
Loss = 1.07512
Loss = 1.00037
Loss = 0.859787
Loss = 1.0948
Loss = 1.04756
Loss = 1.01231
Loss = 1.02901
Loss = 0.888489
Loss = 0.919815
Loss = 0.808533
Loss = 0.952499
Loss = 0.989288
Loss = 0.933136
TEST LOSS = 0.954681
TEST ACC = 443.539 % (7606/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.798538
Epoch 7.2: Loss = 0.869965
Epoch 7.3: Loss = 0.962402
Epoch 7.4: Loss = 0.772079
Epoch 7.5: Loss = 0.850403
Epoch 7.6: Loss = 1.01382
Epoch 7.7: Loss = 0.833267
Epoch 7.8: Loss = 0.981461
Epoch 7.9: Loss = 0.786942
Epoch 7.10: Loss = 0.683533
Epoch 7.11: Loss = 0.940781
Epoch 7.12: Loss = 0.911301
Epoch 7.13: Loss = 1.00067
Epoch 7.14: Loss = 0.843781
Epoch 7.15: Loss = 0.890503
Epoch 7.16: Loss = 0.971664
Epoch 7.17: Loss = 0.839859
Epoch 7.18: Loss = 0.871826
Epoch 7.19: Loss = 0.804367
Epoch 7.20: Loss = 0.988754
Epoch 7.21: Loss = 0.707764
Epoch 7.22: Loss = 0.734695
Epoch 7.23: Loss = 0.870972
Epoch 7.24: Loss = 0.935394
Epoch 7.25: Loss = 0.961319
Epoch 7.26: Loss = 0.732178
Epoch 7.27: Loss = 0.922409
Epoch 7.28: Loss = 0.916214
Epoch 7.29: Loss = 0.799774
Epoch 7.30: Loss = 0.886871
Epoch 7.31: Loss = 1.02512
Epoch 7.32: Loss = 0.8965
Epoch 7.33: Loss = 0.793381
Epoch 7.34: Loss = 1.03261
Epoch 7.35: Loss = 0.92955
Epoch 7.36: Loss = 0.896179
Epoch 7.37: Loss = 0.962128
Epoch 7.38: Loss = 0.890503
Epoch 7.39: Loss = 0.929901
Epoch 7.40: Loss = 0.899948
Epoch 7.41: Loss = 0.964111
Epoch 7.42: Loss = 0.821259
Epoch 7.43: Loss = 0.930237
Epoch 7.44: Loss = 0.821518
Epoch 7.45: Loss = 0.945389
Epoch 7.46: Loss = 1.08318
Epoch 7.47: Loss = 0.918549
Epoch 7.48: Loss = 0.815552
Epoch 7.49: Loss = 0.853271
Epoch 7.50: Loss = 0.965057
Epoch 7.51: Loss = 0.692917
Epoch 7.52: Loss = 0.995956
Epoch 7.53: Loss = 1.00278
Epoch 7.54: Loss = 0.743652
Epoch 7.55: Loss = 0.932404
Epoch 7.56: Loss = 0.975174
Epoch 7.57: Loss = 0.917358
Epoch 7.58: Loss = 0.840378
Epoch 7.59: Loss = 0.89502
Epoch 7.60: Loss = 0.908478
Epoch 7.61: Loss = 0.811111
Epoch 7.62: Loss = 0.957367
Epoch 7.63: Loss = 0.73616
Epoch 7.64: Loss = 0.686508
Epoch 7.65: Loss = 0.977356
Epoch 7.66: Loss = 0.783676
Epoch 7.67: Loss = 0.810486
Epoch 7.68: Loss = 1.01157
Epoch 7.69: Loss = 0.892487
Epoch 7.70: Loss = 0.851974
Epoch 7.71: Loss = 0.822296
Epoch 7.72: Loss = 0.839584
Epoch 7.73: Loss = 0.982971
Epoch 7.74: Loss = 0.928513
Epoch 7.75: Loss = 0.754486
Epoch 7.76: Loss = 0.991226
Epoch 7.77: Loss = 0.922699
Epoch 7.78: Loss = 0.946014
Epoch 7.79: Loss = 0.815018
Epoch 7.80: Loss = 0.790878
Epoch 7.81: Loss = 0.887207
Epoch 7.82: Loss = 0.896973
Epoch 7.83: Loss = 0.986984
Epoch 7.84: Loss = 0.83963
Epoch 7.85: Loss = 0.862823
Epoch 7.86: Loss = 0.935959
Epoch 7.87: Loss = 0.818619
Epoch 7.88: Loss = 0.869553
Epoch 7.89: Loss = 1.03873
Epoch 7.90: Loss = 0.864563
Epoch 7.91: Loss = 0.97052
Epoch 7.92: Loss = 0.918076
Epoch 7.93: Loss = 0.844315
Epoch 7.94: Loss = 0.882874
Epoch 7.95: Loss = 0.897675
Epoch 7.96: Loss = 0.867554
Epoch 7.97: Loss = 0.727966
Epoch 7.98: Loss = 0.824554
Epoch 7.99: Loss = 0.905991
Epoch 7.100: Loss = 0.953903
Epoch 7.101: Loss = 0.898651
Epoch 7.102: Loss = 0.900528
Epoch 7.103: Loss = 0.867523
Epoch 7.104: Loss = 0.716812
Epoch 7.105: Loss = 0.812103
Epoch 7.106: Loss = 0.973373
Epoch 7.107: Loss = 0.894348
Epoch 7.108: Loss = 0.914886
Epoch 7.109: Loss = 1.00143
Epoch 7.110: Loss = 0.877274
Epoch 7.111: Loss = 0.881683
Epoch 7.112: Loss = 0.857498
Epoch 7.113: Loss = 0.830811
Epoch 7.114: Loss = 0.875336
Epoch 7.115: Loss = 0.879684
Epoch 7.116: Loss = 0.831558
Epoch 7.117: Loss = 1.06693
Epoch 7.118: Loss = 0.77713
Epoch 7.119: Loss = 1.10117
Epoch 7.120: Loss = 1.04507
TRAIN LOSS = 0.887283
TRAIN ACC = 77.3575 % (46417/60000)
Loss = 0.751831
Loss = 0.961914
Loss = 0.79892
Loss = 0.855072
Loss = 0.834137
Loss = 1.09186
Loss = 1.05309
Loss = 1.06757
Loss = 0.920883
Loss = 0.785263
Loss = 1.1324
Loss = 1.01555
Loss = 0.941101
Loss = 0.884781
Loss = 0.889908
Loss = 0.934036
Loss = 0.791977
Loss = 0.988052
Loss = 0.949661
Loss = 0.950897
TEST LOSS = 0.929944
TEST ACC = 464.169 % (7672/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.798706
Epoch 8.2: Loss = 0.931168
Epoch 8.3: Loss = 0.912155
Epoch 8.4: Loss = 0.795181
Epoch 8.5: Loss = 0.840302
Epoch 8.6: Loss = 1.00717
Epoch 8.7: Loss = 0.781998
Epoch 8.8: Loss = 0.923904
Epoch 8.9: Loss = 0.754532
Epoch 8.10: Loss = 0.733658
Epoch 8.11: Loss = 1.00983
Epoch 8.12: Loss = 0.921677
Epoch 8.13: Loss = 0.926895
Epoch 8.14: Loss = 0.87384
Epoch 8.15: Loss = 0.993057
Epoch 8.16: Loss = 1.02034
Epoch 8.17: Loss = 0.780365
Epoch 8.18: Loss = 0.912308
Epoch 8.19: Loss = 0.761169
Epoch 8.20: Loss = 1.07103
Epoch 8.21: Loss = 0.752548
Epoch 8.22: Loss = 0.750763
Epoch 8.23: Loss = 0.967728
Epoch 8.24: Loss = 0.995132
Epoch 8.25: Loss = 1.01349
Epoch 8.26: Loss = 748.92
Epoch 8.27: Loss = 1068.08
Epoch 8.28: Loss = -592.03
Epoch 8.29: Loss = 1411.53
Epoch 8.30: Loss = 730.377
Epoch 8.31: Loss = 1969.38
Epoch 8.32: Loss = 491.895
Epoch 8.33: Loss = -1227.55
Epoch 8.34: Loss = -335.352
Epoch 8.35: Loss = 1811.04
Epoch 8.36: Loss = 13.7842
Epoch 8.37: Loss = -33.3341
Epoch 8.38: Loss = 434.37
Epoch 8.39: Loss = -1537.85
Epoch 8.40: Loss = 1708.85
Epoch 8.41: Loss = -1051.9
Epoch 8.42: Loss = 647.661
Epoch 8.43: Loss = 1360.81
Epoch 8.44: Loss = 1663.66
Epoch 8.45: Loss = 363.041
Epoch 8.46: Loss = -1178.79
Epoch 8.47: Loss = 1971.45
Epoch 8.48: Loss = 1125.29
Epoch 8.49: Loss = 463.446
Epoch 8.50: Loss = 446.963
Epoch 8.51: Loss = -1326.14
Epoch 8.52: Loss = -733.166
Epoch 8.53: Loss = -642.024
Epoch 8.54: Loss = 1464.72
Epoch 8.55: Loss = 405.62
Epoch 8.56: Loss = 1144.23
Epoch 8.57: Loss = 280.215
Epoch 8.58: Loss = 1426.1
Epoch 8.59: Loss = -1655.14
Epoch 8.60: Loss = -384.256
Epoch 8.61: Loss = -793.335
Epoch 8.62: Loss = 2272.14
Epoch 8.63: Loss = 146.964
Epoch 8.64: Loss = -728.43
Epoch 8.65: Loss = 2045.51
Epoch 8.66: Loss = 2472.71
Epoch 8.67: Loss = -2597.85
Epoch 8.68: Loss = 2296.01
Epoch 8.69: Loss = 1099.79
Epoch 8.70: Loss = 698.428
Epoch 8.71: Loss = 655.986
Epoch 8.72: Loss = 1703.5
Epoch 8.73: Loss = 203.995
Epoch 8.74: Loss = -1034.82
Epoch 8.75: Loss = 1671.13
Epoch 8.76: Loss = 1062.29
Epoch 8.77: Loss = -714.729
Epoch 8.78: Loss = -807.807
Epoch 8.79: Loss = -1139.43
Epoch 8.80: Loss = 700.73
Epoch 8.81: Loss = -1613.73
Epoch 8.82: Loss = 390.702
Epoch 8.83: Loss = -628.51
Epoch 8.84: Loss = 1327.63
Epoch 8.85: Loss = 35.8116
Epoch 8.86: Loss = -612.035
Epoch 8.87: Loss = -26.0349
Epoch 8.88: Loss = -903.506
Epoch 8.89: Loss = -1397.96
Epoch 8.90: Loss = 676.234
Epoch 8.91: Loss = -2608.88
Epoch 8.92: Loss = -11.4829
Epoch 8.93: Loss = -795.97
Epoch 8.94: Loss = -428.14
Epoch 8.95: Loss = 277.229
Epoch 8.96: Loss = 402.612
Epoch 8.97: Loss = 711.503
Epoch 8.98: Loss = -1104.24
Epoch 8.99: Loss = -1325.52
Epoch 8.100: Loss = -415.314
Epoch 8.101: Loss = -2365.03
Epoch 8.102: Loss = -697.56
Epoch 8.103: Loss = -777.461
Epoch 8.104: Loss = -2162.08
Epoch 8.105: Loss = -1196.56
Epoch 8.106: Loss = -1300.21
Epoch 8.107: Loss = -158.716
Epoch 8.108: Loss = -619.13
Epoch 8.109: Loss = 3.70932
Epoch 8.110: Loss = -703.837
Epoch 8.111: Loss = -200.351
Epoch 8.112: Loss = 539.83
Epoch 8.113: Loss = 933.533
Epoch 8.114: Loss = -578.045
Epoch 8.115: Loss = -169.268
Epoch 8.116: Loss = -1312.09
Epoch 8.117: Loss = -635.882
Epoch 8.118: Loss = -37.5338
Epoch 8.119: Loss = -1202.61
Epoch 8.120: Loss = -935.24
TRAIN LOSS = 0.539627
TRAIN ACC = 23.9655 % (14380/60000)
Loss = 282.895
Loss = -1167.24
Loss = -1655.89
Loss = -316.454
Loss = -929.245
Loss = -556.77
Loss = -712.509
Loss = 5.82028
Loss = -547.784
Loss = 528.355
Loss = -1443.02
Loss = 944.368
Loss = 929.655
Loss = -962.284
Loss = 576.769
Loss = 1526.41
Loss = 1062.28
Loss = 1486.51
Loss = 503.389
Loss = 1310.18
TEST LOSS = 3.95045
TEST ACC = 143.8 % (1027/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = -2674.32
Epoch 9.2: Loss = -410.291
Epoch 9.3: Loss = 3016.95
Epoch 9.4: Loss = 986.469
Epoch 9.5: Loss = -139.216
Epoch 9.6: Loss = 322.348
Epoch 9.7: Loss = 783.134
Epoch 9.8: Loss = 32.1123
Epoch 9.9: Loss = 880.826
Epoch 9.10: Loss = -1894.88
Epoch 9.11: Loss = 2184.19
Epoch 9.12: Loss = -1744.65
Epoch 9.13: Loss = -38.2837
Epoch 9.14: Loss = 3001.14
Epoch 9.15: Loss = -271.475
Epoch 9.16: Loss = -81.6668
Epoch 9.17: Loss = -2949.66
Epoch 9.18: Loss = -383.326
Epoch 9.19: Loss = 1915.54
Epoch 9.20: Loss = -129.118
Epoch 9.21: Loss = 1473.86
Epoch 9.22: Loss = 2113.07
Epoch 9.23: Loss = 1106.42
Epoch 9.24: Loss = -1690.33
Epoch 9.25: Loss = -207.189
Epoch 9.26: Loss = -653.525
Epoch 9.27: Loss = -792.807
Epoch 9.28: Loss = 668.076
Epoch 9.29: Loss = 1843.06
Epoch 9.30: Loss = -366.049
Epoch 9.31: Loss = -305.816
Epoch 9.32: Loss = 752.705
Epoch 9.33: Loss = 572.174
Epoch 9.34: Loss = 2117.62
Epoch 9.35: Loss = 989.107
Epoch 9.36: Loss = 1303.98
Epoch 9.37: Loss = -483.564
Epoch 9.38: Loss = 1040.7
Epoch 9.39: Loss = -3660.52
Epoch 9.40: Loss = -683.573
Epoch 9.41: Loss = 397.667
Epoch 9.42: Loss = 2345.91
Epoch 9.43: Loss = -373.784
Epoch 9.44: Loss = -1638.08
Epoch 9.45: Loss = -1429.81
Epoch 9.46: Loss = -1112.93
Epoch 9.47: Loss = 2098.22
Epoch 9.48: Loss = 1130.75
Epoch 9.49: Loss = -720.396
Epoch 9.50: Loss = -1834.55
Epoch 9.51: Loss = -921.778
Epoch 9.52: Loss = -2105.51
Epoch 9.53: Loss = 1018.55
Epoch 9.54: Loss = -96.6668
Epoch 9.55: Loss = 98.9604
Epoch 9.56: Loss = -804.203
Epoch 9.57: Loss = 498.922
Epoch 9.58: Loss = -1493.33
Epoch 9.59: Loss = -1041.11
Epoch 9.60: Loss = 431.868
Epoch 9.61: Loss = 864.883
Epoch 9.62: Loss = 210.47
Epoch 9.63: Loss = -1005.47
Epoch 9.64: Loss = -1296.42
Epoch 9.65: Loss = 483.666
Epoch 9.66: Loss = 2174.98
Epoch 9.67: Loss = 905.74
Epoch 9.68: Loss = -74.4565
Epoch 9.69: Loss = 1742.62
Epoch 9.70: Loss = 669.002
Epoch 9.71: Loss = -125.137
Epoch 9.72: Loss = -410.327
Epoch 9.73: Loss = -195.714
Epoch 9.74: Loss = 3750.71
Epoch 9.75: Loss = 135.87
Epoch 9.76: Loss = -673.249
Epoch 9.77: Loss = 1323.55
Epoch 9.78: Loss = 576.327
Epoch 9.79: Loss = 192.692
Epoch 9.80: Loss = 96.2031
Epoch 9.81: Loss = -1244.99
Epoch 9.82: Loss = 372.439
Epoch 9.83: Loss = 710.748
Epoch 9.84: Loss = -416.603
Epoch 9.85: Loss = 1630.42
Epoch 9.86: Loss = 1114.71
Epoch 9.87: Loss = 177.986
Epoch 9.88: Loss = -925.731
Epoch 9.89: Loss = -1660.74
Epoch 9.90: Loss = -1010.62
Epoch 9.91: Loss = 1230.45
Epoch 9.92: Loss = -2329.18
Epoch 9.93: Loss = 1133.98
Epoch 9.94: Loss = 1065.54
Epoch 9.95: Loss = 1319.6
Epoch 9.96: Loss = -1413.82
Epoch 9.97: Loss = -136.247
Epoch 9.98: Loss = 309.502
Epoch 9.99: Loss = 651.924
Epoch 9.100: Loss = 951.633
Epoch 9.101: Loss = -2827.6
Epoch 9.102: Loss = 1002.12
Epoch 9.103: Loss = 1251.95
Epoch 9.104: Loss = -2380.8
Epoch 9.105: Loss = -829.699
Epoch 9.106: Loss = 215.872
Epoch 9.107: Loss = -784.819
Epoch 9.108: Loss = 1677.9
Epoch 9.109: Loss = 355.763
Epoch 9.110: Loss = -1935.51
Epoch 9.111: Loss = 425.865
Epoch 9.112: Loss = -1500.99
Epoch 9.113: Loss = -21.9862
Epoch 9.114: Loss = -245.451
Epoch 9.115: Loss = 943.598
Epoch 9.116: Loss = -700.206
Epoch 9.117: Loss = 324.83
Epoch 9.118: Loss = -940.448
Epoch 9.119: Loss = 702.903
Epoch 9.120: Loss = 1730.47
TRAIN LOSS = 77.773
TRAIN ACC = 10.0708 % (6043/60000)
Loss = 69.8735
Loss = 1405.39
Loss = 2082.78
Loss = -1847.08
Loss = -650.092
Loss = 1394.58
Loss = -259.86
Loss = -1961.94
Loss = -103.233
Loss = -150.673
Loss = 2756
Loss = 25.1764
Loss = 388.659
Loss = 296.015
Loss = 1089.02
Loss = -1297.3
Loss = -1275.62
Loss = 3347.54
Loss = -544.866
Loss = 280.846
TEST LOSS = 3.22403
TEST ACC = 60.4294 % (968/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 417.204
Epoch 10.2: Loss = -2808.51
Epoch 10.3: Loss = 949.759
Epoch 10.4: Loss = 2233.24
Epoch 10.5: Loss = 2738.11
Epoch 10.6: Loss = 1484.71
Epoch 10.7: Loss = -11.4431
Epoch 10.8: Loss = -1449.18
Epoch 10.9: Loss = -97.8817
Epoch 10.10: Loss = -1091.3
Epoch 10.11: Loss = 1045.87
Epoch 10.12: Loss = 2878.68
Epoch 10.13: Loss = 582.367
Epoch 10.14: Loss = -807.766
Epoch 10.15: Loss = -1260.74
Epoch 10.16: Loss = -970.504
Epoch 10.17: Loss = -498.717
Epoch 10.18: Loss = -943.597
Epoch 10.19: Loss = 2760.93
Epoch 10.20: Loss = -107.099
Epoch 10.21: Loss = -1313.84
Epoch 10.22: Loss = -1017.04
Epoch 10.23: Loss = 2234.9
Epoch 10.24: Loss = 603.714
Epoch 10.25: Loss = 375.531
Epoch 10.26: Loss = -496.591
Epoch 10.27: Loss = 777.42
Epoch 10.28: Loss = -197.057
Epoch 10.29: Loss = -1393.57
Epoch 10.30: Loss = -843.106
Epoch 10.31: Loss = -70.7745
Epoch 10.32: Loss = -34.0744
Epoch 10.33: Loss = 2797.63
Epoch 10.34: Loss = -846.938
Epoch 10.35: Loss = 556.169
Epoch 10.36: Loss = -574.602
Epoch 10.37: Loss = -553.736
Epoch 10.38: Loss = -461.791
Epoch 10.39: Loss = 721.366
Epoch 10.40: Loss = -1400.77
Epoch 10.41: Loss = -1133.51
Epoch 10.42: Loss = -354.128
Epoch 10.43: Loss = 741.986
Epoch 10.44: Loss = -288.249
Epoch 10.45: Loss = 1648.38
Epoch 10.46: Loss = -675.671
Epoch 10.47: Loss = 719.674
Epoch 10.48: Loss = 354.652
Epoch 10.49: Loss = 2137.11
Epoch 10.50: Loss = 1202.73
Epoch 10.51: Loss = -1730.3
Epoch 10.52: Loss = 1660.46
Epoch 10.53: Loss = 78.6726
Epoch 10.54: Loss = 930.344
Epoch 10.55: Loss = 1816.63
Epoch 10.56: Loss = -919.942
Epoch 10.57: Loss = -3258.15
Epoch 10.58: Loss = -946.425
Epoch 10.59: Loss = 1043.86
Epoch 10.60: Loss = 1914.8
Epoch 10.61: Loss = 477.818
Epoch 10.62: Loss = -1.02739
Epoch 10.63: Loss = -203.265
Epoch 10.64: Loss = 1596.1
Epoch 10.65: Loss = 901.986
Epoch 10.66: Loss = 203.446
Epoch 10.67: Loss = -150.798
Epoch 10.68: Loss = 116.075
Epoch 10.69: Loss = 1174.4
Epoch 10.70: Loss = -137.348
Epoch 10.71: Loss = -689.752
Epoch 10.72: Loss = 1502.28
Epoch 10.73: Loss = -692.544
Epoch 10.74: Loss = 369.195
Epoch 10.75: Loss = 2041.78
Epoch 10.76: Loss = 209.334
Epoch 10.77: Loss = 808.782
Epoch 10.78: Loss = -734.973
Epoch 10.79: Loss = -1692.69
Epoch 10.80: Loss = -2473.55
Epoch 10.81: Loss = 944.008
Epoch 10.82: Loss = 795.112
Epoch 10.83: Loss = -1645.68
Epoch 10.84: Loss = 2954.62
Epoch 10.85: Loss = -1110.81
Epoch 10.86: Loss = -1103.99
Epoch 10.87: Loss = 2558.58
Epoch 10.88: Loss = 1444.91
Epoch 10.89: Loss = 2712.49
Epoch 10.90: Loss = -1157.83
Epoch 10.91: Loss = -2466.45
Epoch 10.92: Loss = -321.243
Epoch 10.93: Loss = 341.146
Epoch 10.94: Loss = -545.864
Epoch 10.95: Loss = 1116.44
Epoch 10.96: Loss = 1653.74
Epoch 10.97: Loss = -928.644
Epoch 10.98: Loss = -561.338
Epoch 10.99: Loss = -327.914
Epoch 10.100: Loss = -514.536
Epoch 10.101: Loss = 253.686
Epoch 10.102: Loss = 1817.58
Epoch 10.103: Loss = -398.593
Epoch 10.104: Loss = -636.975
Epoch 10.105: Loss = -477.763
Epoch 10.106: Loss = 599.31
Epoch 10.107: Loss = -852.737
Epoch 10.108: Loss = -420.63
Epoch 10.109: Loss = 2853.7
Epoch 10.110: Loss = 1770.36
Epoch 10.111: Loss = 2784.86
Epoch 10.112: Loss = -2136.05
Epoch 10.113: Loss = -1191.92
Epoch 10.114: Loss = 560.249
Epoch 10.115: Loss = -258.182
Epoch 10.116: Loss = 1822.58
Epoch 10.117: Loss = -163.367
Epoch 10.118: Loss = 840.489
Epoch 10.119: Loss = 509.668
Epoch 10.120: Loss = -1451.75
TRAIN LOSS = 176.139
TRAIN ACC = 9.97772 % (5987/60000)
Loss = 1603.73
Loss = -1573.43
Loss = 1712.89
Loss = -576.624
Loss = 1401.22
Loss = 1442.65
Loss = -1276.54
Loss = -845.528
Loss = -111.004
Loss = 798.402
Loss = -175.323
Loss = 2637.35
Loss = -653.835
Loss = 1739.33
Loss = 3442.1
Loss = -2152.73
Loss = -207.073
Loss = -1109.59
Loss = 880.144
Loss = -1127.29
TEST LOSS = 0.0813985
TEST ACC = 59.8694 % (1015/10000)
