Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.34671
Epoch 1.2: Loss = 2.31142
Epoch 1.3: Loss = 2.29013
Epoch 1.4: Loss = 2.24852
Epoch 1.5: Loss = 2.21396
Epoch 1.6: Loss = 2.18536
Epoch 1.7: Loss = 2.16673
Epoch 1.8: Loss = 2.12878
Epoch 1.9: Loss = 2.10387
Epoch 1.10: Loss = 2.08267
Epoch 1.11: Loss = 2.00842
Epoch 1.12: Loss = 1.9991
Epoch 1.13: Loss = 1.93187
Epoch 1.14: Loss = 1.94562
Epoch 1.15: Loss = 1.95799
Epoch 1.16: Loss = 1.87408
Epoch 1.17: Loss = 1.82971
Epoch 1.18: Loss = 1.8197
Epoch 1.19: Loss = 1.77449
Epoch 1.20: Loss = 1.7269
Epoch 1.21: Loss = 1.65039
Epoch 1.22: Loss = 1.65962
Epoch 1.23: Loss = 1.58546
Epoch 1.24: Loss = 1.68611
Epoch 1.25: Loss = 1.55537
Epoch 1.26: Loss = 1.58597
Epoch 1.27: Loss = 1.5499
Epoch 1.28: Loss = 1.53412
Epoch 1.29: Loss = 1.49562
Epoch 1.30: Loss = 1.54797
Epoch 1.31: Loss = 1.41933
Epoch 1.32: Loss = 1.45802
Epoch 1.33: Loss = 1.40921
Epoch 1.34: Loss = 1.38475
Epoch 1.35: Loss = 1.3065
Epoch 1.36: Loss = 1.42969
Epoch 1.37: Loss = 1.29796
Epoch 1.38: Loss = 1.26004
Epoch 1.39: Loss = 1.23517
Epoch 1.40: Loss = 1.15178
Epoch 1.41: Loss = 1.18877
Epoch 1.42: Loss = 1.19078
Epoch 1.43: Loss = 1.13434
Epoch 1.44: Loss = 1.04292
Epoch 1.45: Loss = 1.16542
Epoch 1.46: Loss = 1.09859
Epoch 1.47: Loss = 1.06128
Epoch 1.48: Loss = 1.12077
Epoch 1.49: Loss = 1.03998
Epoch 1.50: Loss = 1.10635
Epoch 1.51: Loss = 0.953949
Epoch 1.52: Loss = 0.980255
Epoch 1.53: Loss = 1.00363
Epoch 1.54: Loss = 1.04201
Epoch 1.55: Loss = 1.00496
Epoch 1.56: Loss = 0.926498
Epoch 1.57: Loss = 0.859283
Epoch 1.58: Loss = 0.897614
Epoch 1.59: Loss = 0.912247
Epoch 1.60: Loss = 1.03282
Epoch 1.61: Loss = 0.952866
Epoch 1.62: Loss = 0.988846
Epoch 1.63: Loss = 1.00967
Epoch 1.64: Loss = 0.959641
Epoch 1.65: Loss = 1.03223
Epoch 1.66: Loss = 0.861298
Epoch 1.67: Loss = 0.845856
Epoch 1.68: Loss = 0.708191
Epoch 1.69: Loss = 0.795166
Epoch 1.70: Loss = 0.870331
Epoch 1.71: Loss = 0.799942
Epoch 1.72: Loss = 0.792679
Epoch 1.73: Loss = 0.817673
Epoch 1.74: Loss = 0.69664
Epoch 1.75: Loss = 0.813248
Epoch 1.76: Loss = 0.805878
Epoch 1.77: Loss = 0.767731
Epoch 1.78: Loss = 0.725082
Epoch 1.79: Loss = 0.721573
Epoch 1.80: Loss = 0.857712
Epoch 1.81: Loss = 0.679535
Epoch 1.82: Loss = 0.696365
Epoch 1.83: Loss = 0.839905
Epoch 1.84: Loss = 0.752869
Epoch 1.85: Loss = 0.834763
Epoch 1.86: Loss = 0.730164
Epoch 1.87: Loss = 0.679092
Epoch 1.88: Loss = 0.715561
Epoch 1.89: Loss = 0.76712
Epoch 1.90: Loss = 0.675095
Epoch 1.91: Loss = 0.740189
Epoch 1.92: Loss = 0.69339
Epoch 1.93: Loss = 0.742722
Epoch 1.94: Loss = 0.591461
Epoch 1.95: Loss = 0.713333
Epoch 1.96: Loss = 0.728226
Epoch 1.97: Loss = 0.554718
Epoch 1.98: Loss = 0.636932
Epoch 1.99: Loss = 0.735107
Epoch 1.100: Loss = 0.812622
Epoch 1.101: Loss = 0.745834
Epoch 1.102: Loss = 0.67485
Epoch 1.103: Loss = 0.624542
Epoch 1.104: Loss = 0.568466
Epoch 1.105: Loss = 0.713974
Epoch 1.106: Loss = 0.693939
Epoch 1.107: Loss = 0.596558
Epoch 1.108: Loss = 0.662369
Epoch 1.109: Loss = 0.618286
Epoch 1.110: Loss = 0.642349
Epoch 1.111: Loss = 0.5336
Epoch 1.112: Loss = 0.538239
Epoch 1.113: Loss = 0.595612
Epoch 1.114: Loss = 0.525467
Epoch 1.115: Loss = 0.602417
Epoch 1.116: Loss = 0.592804
Epoch 1.117: Loss = 0.493484
Epoch 1.118: Loss = 0.406601
Epoch 1.119: Loss = 0.436127
Epoch 1.120: Loss = 0.488205
TRAIN LOSS = 1.11507
TRAIN ACC = 68.4387 % (41065/60000)
Loss = 0.642731
Loss = 0.694336
Loss = 0.769455
Loss = 0.733994
Loss = 0.781189
Loss = 0.6418
Loss = 0.598099
Loss = 0.785019
Loss = 0.768707
Loss = 0.668472
Loss = 0.358017
Loss = 0.511551
Loss = 0.385315
Loss = 0.544617
Loss = 0.433792
Loss = 0.416718
Loss = 0.406265
Loss = 0.245346
Loss = 0.427597
Loss = 0.720032
TEST LOSS = 0.576652
TEST ACC = 410.649 % (8183/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.574768
Epoch 2.2: Loss = 0.64238
Epoch 2.3: Loss = 0.648422
Epoch 2.4: Loss = 0.535828
Epoch 2.5: Loss = 0.527893
Epoch 2.6: Loss = 0.50499
Epoch 2.7: Loss = 0.598633
Epoch 2.8: Loss = 0.552704
Epoch 2.9: Loss = 0.5466
Epoch 2.10: Loss = 0.546448
Epoch 2.11: Loss = 0.571121
Epoch 2.12: Loss = 0.545151
Epoch 2.13: Loss = 0.496796
Epoch 2.14: Loss = 0.518906
Epoch 2.15: Loss = 0.643509
Epoch 2.16: Loss = 0.570877
Epoch 2.17: Loss = 0.594711
Epoch 2.18: Loss = 0.678497
Epoch 2.19: Loss = 0.574905
Epoch 2.20: Loss = 0.509354
Epoch 2.21: Loss = 0.485764
Epoch 2.22: Loss = 0.497009
Epoch 2.23: Loss = 0.493774
Epoch 2.24: Loss = 0.705139
Epoch 2.25: Loss = 0.5159
Epoch 2.26: Loss = 0.677811
Epoch 2.27: Loss = 0.626541
Epoch 2.28: Loss = 0.584488
Epoch 2.29: Loss = 0.632492
Epoch 2.30: Loss = 0.725372
Epoch 2.31: Loss = 0.472931
Epoch 2.32: Loss = 0.621445
Epoch 2.33: Loss = 0.554108
Epoch 2.34: Loss = 0.616394
Epoch 2.35: Loss = 0.539597
Epoch 2.36: Loss = 0.638641
Epoch 2.37: Loss = 0.456451
Epoch 2.38: Loss = 0.488541
Epoch 2.39: Loss = 0.510727
Epoch 2.40: Loss = 0.457733
Epoch 2.41: Loss = 0.530197
Epoch 2.42: Loss = 0.602722
Epoch 2.43: Loss = 0.485336
Epoch 2.44: Loss = 0.41037
Epoch 2.45: Loss = 0.53241
Epoch 2.46: Loss = 0.562943
Epoch 2.47: Loss = 0.516464
Epoch 2.48: Loss = 0.570511
Epoch 2.49: Loss = 0.554276
Epoch 2.50: Loss = 0.605713
Epoch 2.51: Loss = 0.462692
Epoch 2.52: Loss = 0.469711
Epoch 2.53: Loss = 0.513687
Epoch 2.54: Loss = 0.615967
Epoch 2.55: Loss = 0.527466
Epoch 2.56: Loss = 0.462784
Epoch 2.57: Loss = 0.475296
Epoch 2.58: Loss = 0.508209
Epoch 2.59: Loss = 0.552002
Epoch 2.60: Loss = 0.624283
Epoch 2.61: Loss = 0.620621
Epoch 2.62: Loss = 0.616684
Epoch 2.63: Loss = 0.696182
Epoch 2.64: Loss = 0.609573
Epoch 2.65: Loss = 0.726547
Epoch 2.66: Loss = 0.528305
Epoch 2.67: Loss = 0.617752
Epoch 2.68: Loss = 0.379318
Epoch 2.69: Loss = 0.443115
Epoch 2.70: Loss = 0.626785
Epoch 2.71: Loss = 0.450897
Epoch 2.72: Loss = 0.440308
Epoch 2.73: Loss = 0.534317
Epoch 2.74: Loss = 0.421387
Epoch 2.75: Loss = 0.610809
Epoch 2.76: Loss = 0.570557
Epoch 2.77: Loss = 0.525146
Epoch 2.78: Loss = 0.484283
Epoch 2.79: Loss = 0.56604
Epoch 2.80: Loss = 0.61322
Epoch 2.81: Loss = 0.4702
Epoch 2.82: Loss = 0.441574
Epoch 2.83: Loss = 0.617462
Epoch 2.84: Loss = 0.539597
Epoch 2.85: Loss = 0.697678
Epoch 2.86: Loss = 0.55394
Epoch 2.87: Loss = 0.403793
Epoch 2.88: Loss = 0.456329
Epoch 2.89: Loss = 0.592255
Epoch 2.90: Loss = 0.432953
Epoch 2.91: Loss = 0.573425
Epoch 2.92: Loss = 0.502274
Epoch 2.93: Loss = 0.570847
Epoch 2.94: Loss = 0.428772
Epoch 2.95: Loss = 0.562027
Epoch 2.96: Loss = 0.601883
Epoch 2.97: Loss = 0.417877
Epoch 2.98: Loss = 0.513931
Epoch 2.99: Loss = 0.643433
Epoch 2.100: Loss = 0.695267
Epoch 2.101: Loss = 0.633102
Epoch 2.102: Loss = 0.521927
Epoch 2.103: Loss = 0.508423
Epoch 2.104: Loss = 0.427872
Epoch 2.105: Loss = 0.635452
Epoch 2.106: Loss = 0.612366
Epoch 2.107: Loss = 0.4646
Epoch 2.108: Loss = 0.5457
Epoch 2.109: Loss = 0.46402
Epoch 2.110: Loss = 0.5616
Epoch 2.111: Loss = 0.420685
Epoch 2.112: Loss = 0.47406
Epoch 2.113: Loss = 0.456085
Epoch 2.114: Loss = 0.38237
Epoch 2.115: Loss = 0.438751
Epoch 2.116: Loss = 0.501694
Epoch 2.117: Loss = 0.304306
Epoch 2.118: Loss = 0.278519
Epoch 2.119: Loss = 0.384399
Epoch 2.120: Loss = 0.380356
TRAIN LOSS = 0.537186
TRAIN ACC = 83.2153 % (49931/60000)
Loss = 0.489746
Loss = 0.637161
Loss = 0.679993
Loss = 0.649765
Loss = 0.716568
Loss = 0.494995
Loss = 0.47998
Loss = 0.687302
Loss = 0.677643
Loss = 0.610199
Loss = 0.260605
Loss = 0.412827
Loss = 0.331924
Loss = 0.468918
Loss = 0.332809
Loss = 0.404709
Loss = 0.312408
Loss = 0.13295
Loss = 0.310226
Loss = 0.683655
TEST LOSS = 0.488719
TEST ACC = 499.309 % (8519/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.511337
Epoch 3.2: Loss = 0.571411
Epoch 3.3: Loss = 0.593872
Epoch 3.4: Loss = 0.410339
Epoch 3.5: Loss = 0.432175
Epoch 3.6: Loss = 0.459778
Epoch 3.7: Loss = 0.481827
Epoch 3.8: Loss = 0.467789
Epoch 3.9: Loss = 0.448486
Epoch 3.10: Loss = 0.458649
Epoch 3.11: Loss = 0.543259
Epoch 3.12: Loss = 0.4431
Epoch 3.13: Loss = 0.417633
Epoch 3.14: Loss = 0.461761
Epoch 3.15: Loss = 0.530502
Epoch 3.16: Loss = 0.543411
Epoch 3.17: Loss = 0.564011
Epoch 3.18: Loss = 0.664398
Epoch 3.19: Loss = 0.493271
Epoch 3.20: Loss = 0.467102
Epoch 3.21: Loss = 0.423279
Epoch 3.22: Loss = 0.418015
Epoch 3.23: Loss = 0.434174
Epoch 3.24: Loss = 0.660614
Epoch 3.25: Loss = 0.495316
Epoch 3.26: Loss = 0.656662
Epoch 3.27: Loss = 0.615601
Epoch 3.28: Loss = 0.520798
Epoch 3.29: Loss = 0.600433
Epoch 3.30: Loss = 0.673874
Epoch 3.31: Loss = 0.406372
Epoch 3.32: Loss = 0.596695
Epoch 3.33: Loss = 0.507401
Epoch 3.34: Loss = 0.576157
Epoch 3.35: Loss = 0.533447
Epoch 3.36: Loss = 0.563324
Epoch 3.37: Loss = 0.430511
Epoch 3.38: Loss = 0.418732
Epoch 3.39: Loss = 0.494629
Epoch 3.40: Loss = 0.444168
Epoch 3.41: Loss = 0.515305
Epoch 3.42: Loss = 0.593903
Epoch 3.43: Loss = 0.38382
Epoch 3.44: Loss = 0.380707
Epoch 3.45: Loss = 0.48967
Epoch 3.46: Loss = 0.546021
Epoch 3.47: Loss = 0.486694
Epoch 3.48: Loss = 0.516144
Epoch 3.49: Loss = 0.519913
Epoch 3.50: Loss = 0.565536
Epoch 3.51: Loss = 0.439163
Epoch 3.52: Loss = 0.440536
Epoch 3.53: Loss = 0.475052
Epoch 3.54: Loss = 0.605652
Epoch 3.55: Loss = 0.476227
Epoch 3.56: Loss = 0.443253
Epoch 3.57: Loss = 0.452271
Epoch 3.58: Loss = 0.510437
Epoch 3.59: Loss = 0.553757
Epoch 3.60: Loss = 0.628387
Epoch 3.61: Loss = 0.603104
Epoch 3.62: Loss = 0.635788
Epoch 3.63: Loss = 0.706299
Epoch 3.64: Loss = 0.613037
Epoch 3.65: Loss = 0.758118
Epoch 3.66: Loss = 0.473328
Epoch 3.67: Loss = 0.565811
Epoch 3.68: Loss = 0.368622
Epoch 3.69: Loss = 0.462509
Epoch 3.70: Loss = 0.630264
Epoch 3.71: Loss = 0.451157
Epoch 3.72: Loss = 0.409592
Epoch 3.73: Loss = 0.549744
Epoch 3.74: Loss = 0.440536
Epoch 3.75: Loss = 0.650375
Epoch 3.76: Loss = 0.56395
Epoch 3.77: Loss = 0.497864
Epoch 3.78: Loss = 0.481049
Epoch 3.79: Loss = 0.604507
Epoch 3.80: Loss = 0.642059
Epoch 3.81: Loss = 0.465698
Epoch 3.82: Loss = 0.438812
Epoch 3.83: Loss = 0.579071
Epoch 3.84: Loss = 0.474075
Epoch 3.85: Loss = 0.689728
Epoch 3.86: Loss = 0.577377
Epoch 3.87: Loss = 0.369415
Epoch 3.88: Loss = 0.504623
Epoch 3.89: Loss = 0.557312
Epoch 3.90: Loss = 0.41301
Epoch 3.91: Loss = 0.573898
Epoch 3.92: Loss = 0.529068
Epoch 3.93: Loss = 0.617126
Epoch 3.94: Loss = 0.453247
Epoch 3.95: Loss = 0.525635
Epoch 3.96: Loss = 0.640594
Epoch 3.97: Loss = 0.467422
Epoch 3.98: Loss = 0.52092
Epoch 3.99: Loss = 0.572586
Epoch 3.100: Loss = 0.667999
Epoch 3.101: Loss = 0.665054
Epoch 3.102: Loss = 0.535065
Epoch 3.103: Loss = 0.482315
Epoch 3.104: Loss = 0.417496
Epoch 3.105: Loss = 0.643921
Epoch 3.106: Loss = 0.612061
Epoch 3.107: Loss = 0.46463
Epoch 3.108: Loss = 0.542404
Epoch 3.109: Loss = 0.455353
Epoch 3.110: Loss = 0.52449
Epoch 3.111: Loss = 0.461853
Epoch 3.112: Loss = 0.470261
Epoch 3.113: Loss = 0.497375
Epoch 3.114: Loss = 0.387405
Epoch 3.115: Loss = 0.425278
Epoch 3.116: Loss = 0.5177
Epoch 3.117: Loss = 0.312454
Epoch 3.118: Loss = 0.241638
Epoch 3.119: Loss = 0.443512
Epoch 3.120: Loss = 0.397354
TRAIN LOSS = 0.514175
TRAIN ACC = 84.9121 % (50949/60000)
Loss = 0.521622
Loss = 0.671677
Loss = 0.755051
Loss = 0.656937
Loss = 0.762894
Loss = 0.488907
Loss = 0.507874
Loss = 0.704483
Loss = 0.739746
Loss = 0.629517
Loss = 0.230637
Loss = 0.499496
Loss = 0.406464
Loss = 0.47551
Loss = 0.316315
Loss = 0.373901
Loss = 0.335754
Loss = 0.112793
Loss = 0.342285
Loss = 0.701248
TEST LOSS = 0.511655
TEST ACC = 509.489 % (8534/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.474045
Epoch 4.2: Loss = 0.612701
Epoch 4.3: Loss = 0.593887
Epoch 4.4: Loss = 0.423859
Epoch 4.5: Loss = 0.43924
Epoch 4.6: Loss = 0.442627
Epoch 4.7: Loss = 0.489639
Epoch 4.8: Loss = 0.484512
Epoch 4.9: Loss = 0.471222
Epoch 4.10: Loss = 0.518768
Epoch 4.11: Loss = 0.525055
Epoch 4.12: Loss = 0.46579
Epoch 4.13: Loss = 0.430527
Epoch 4.14: Loss = 0.506851
Epoch 4.15: Loss = 0.520126
Epoch 4.16: Loss = 0.578873
Epoch 4.17: Loss = 0.58255
Epoch 4.18: Loss = 0.707397
Epoch 4.19: Loss = 0.528397
Epoch 4.20: Loss = 0.460663
Epoch 4.21: Loss = 0.440369
Epoch 4.22: Loss = 0.440033
Epoch 4.23: Loss = 0.414642
Epoch 4.24: Loss = 0.751099
Epoch 4.25: Loss = 0.537277
Epoch 4.26: Loss = 0.65097
Epoch 4.27: Loss = 0.68129
Epoch 4.28: Loss = 0.558929
Epoch 4.29: Loss = 0.714706
Epoch 4.30: Loss = 0.712784
Epoch 4.31: Loss = 0.475708
Epoch 4.32: Loss = 0.580566
Epoch 4.33: Loss = 0.509903
Epoch 4.34: Loss = 0.602737
Epoch 4.35: Loss = 0.54277
Epoch 4.36: Loss = 0.573166
Epoch 4.37: Loss = 0.429718
Epoch 4.38: Loss = 0.44191
Epoch 4.39: Loss = 0.480728
Epoch 4.40: Loss = 0.477905
Epoch 4.41: Loss = 0.503281
Epoch 4.42: Loss = 0.705353
Epoch 4.43: Loss = 0.374146
Epoch 4.44: Loss = 0.356308
Epoch 4.45: Loss = 0.575912
Epoch 4.46: Loss = 0.578003
Epoch 4.47: Loss = 0.543777
Epoch 4.48: Loss = 0.545074
Epoch 4.49: Loss = 0.568207
Epoch 4.50: Loss = 0.627411
Epoch 4.51: Loss = 0.439499
Epoch 4.52: Loss = 0.464096
Epoch 4.53: Loss = 0.50946
Epoch 4.54: Loss = 0.623306
Epoch 4.55: Loss = 0.518127
Epoch 4.56: Loss = 0.497208
Epoch 4.57: Loss = 0.484741
Epoch 4.58: Loss = 0.544159
Epoch 4.59: Loss = 0.55954
Epoch 4.60: Loss = 0.629227
Epoch 4.61: Loss = 0.625229
Epoch 4.62: Loss = 0.675522
Epoch 4.63: Loss = 0.797836
Epoch 4.64: Loss = 0.61232
Epoch 4.65: Loss = 0.79657
Epoch 4.66: Loss = 0.518448
Epoch 4.67: Loss = 0.591721
Epoch 4.68: Loss = 0.353928
Epoch 4.69: Loss = 0.511993
Epoch 4.70: Loss = 0.744797
Epoch 4.71: Loss = 0.470398
Epoch 4.72: Loss = 0.442413
Epoch 4.73: Loss = 0.588318
Epoch 4.74: Loss = 0.441437
Epoch 4.75: Loss = 0.741882
Epoch 4.76: Loss = 0.595047
Epoch 4.77: Loss = 0.470779
Epoch 4.78: Loss = 0.507431
Epoch 4.79: Loss = 0.683441
Epoch 4.80: Loss = 0.684082
Epoch 4.81: Loss = 0.449203
Epoch 4.82: Loss = 0.442139
Epoch 4.83: Loss = 0.560394
Epoch 4.84: Loss = 0.523331
Epoch 4.85: Loss = 0.783478
Epoch 4.86: Loss = 0.656693
Epoch 4.87: Loss = 0.428055
Epoch 4.88: Loss = 0.566422
Epoch 4.89: Loss = 0.62439
Epoch 4.90: Loss = 0.438431
Epoch 4.91: Loss = 0.592468
Epoch 4.92: Loss = 0.661453
Epoch 4.93: Loss = 0.721176
Epoch 4.94: Loss = 0.471039
Epoch 4.95: Loss = 0.578659
Epoch 4.96: Loss = 0.684814
Epoch 4.97: Loss = 0.495377
Epoch 4.98: Loss = 0.56102
Epoch 4.99: Loss = 0.661438
Epoch 4.100: Loss = 0.744873
Epoch 4.101: Loss = 0.724945
Epoch 4.102: Loss = 0.486084
Epoch 4.103: Loss = 0.534454
Epoch 4.104: Loss = 0.485916
Epoch 4.105: Loss = 0.75238
Epoch 4.106: Loss = 0.598755
Epoch 4.107: Loss = 0.490524
Epoch 4.108: Loss = 0.596909
Epoch 4.109: Loss = 0.453262
Epoch 4.110: Loss = 0.497391
Epoch 4.111: Loss = 0.493958
Epoch 4.112: Loss = 0.449844
Epoch 4.113: Loss = 0.490723
Epoch 4.114: Loss = 0.399002
Epoch 4.115: Loss = 0.430038
Epoch 4.116: Loss = 0.535095
Epoch 4.117: Loss = 0.308487
Epoch 4.118: Loss = 0.237549
Epoch 4.119: Loss = 0.425369
Epoch 4.120: Loss = 0.427414
TRAIN LOSS = 0.543671
TRAIN ACC = 85.1196 % (51074/60000)
Loss = 0.557297
Loss = 0.729538
Loss = 0.810181
Loss = 0.722137
Loss = 0.799911
Loss = 0.515045
Loss = 0.552582
Loss = 0.752487
Loss = 0.801102
Loss = 0.66774
Loss = 0.233139
Loss = 0.535141
Loss = 0.440002
Loss = 0.465881
Loss = 0.335724
Loss = 0.41362
Loss = 0.260574
Loss = 0.107162
Loss = 0.332092
Loss = 0.733414
TEST LOSS = 0.538238
TEST ACC = 510.739 % (8545/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.498413
Epoch 5.2: Loss = 0.64888
Epoch 5.3: Loss = 0.655807
Epoch 5.4: Loss = 0.417374
Epoch 5.5: Loss = 0.456848
Epoch 5.6: Loss = 0.477295
Epoch 5.7: Loss = 0.48822
Epoch 5.8: Loss = 0.475372
Epoch 5.9: Loss = 0.505035
Epoch 5.10: Loss = 0.556854
Epoch 5.11: Loss = 0.60759
Epoch 5.12: Loss = 0.536926
Epoch 5.13: Loss = 0.467728
Epoch 5.14: Loss = 0.580048
Epoch 5.15: Loss = 0.546249
Epoch 5.16: Loss = 0.633469
Epoch 5.17: Loss = 0.641006
Epoch 5.18: Loss = 0.803177
Epoch 5.19: Loss = 0.5784
Epoch 5.20: Loss = 0.444016
Epoch 5.21: Loss = 0.448578
Epoch 5.22: Loss = 0.449173
Epoch 5.23: Loss = 0.439651
Epoch 5.24: Loss = 0.736343
Epoch 5.25: Loss = 0.595001
Epoch 5.26: Loss = 0.697403
Epoch 5.27: Loss = 0.642868
Epoch 5.28: Loss = 0.598541
Epoch 5.29: Loss = 0.695923
Epoch 5.30: Loss = 0.760086
Epoch 5.31: Loss = 0.477921
Epoch 5.32: Loss = 0.568832
Epoch 5.33: Loss = 0.540268
Epoch 5.34: Loss = 0.613861
Epoch 5.35: Loss = 0.5159
Epoch 5.36: Loss = 0.591736
Epoch 5.37: Loss = 0.454163
Epoch 5.38: Loss = 0.503967
Epoch 5.39: Loss = 0.497498
Epoch 5.40: Loss = 0.546143
Epoch 5.41: Loss = 0.547363
Epoch 5.42: Loss = 0.74939
Epoch 5.43: Loss = 0.424118
Epoch 5.44: Loss = 0.412186
Epoch 5.45: Loss = 0.559875
Epoch 5.46: Loss = 0.633286
Epoch 5.47: Loss = 0.595581
Epoch 5.48: Loss = 0.600433
Epoch 5.49: Loss = 0.490555
Epoch 5.50: Loss = 0.649551
Epoch 5.51: Loss = 0.465805
Epoch 5.52: Loss = 0.45813
Epoch 5.53: Loss = 0.539612
Epoch 5.54: Loss = 0.668488
Epoch 5.55: Loss = 0.60849
Epoch 5.56: Loss = 0.559402
Epoch 5.57: Loss = 0.535583
Epoch 5.58: Loss = 0.555176
Epoch 5.59: Loss = 0.628281
Epoch 5.60: Loss = 0.678345
Epoch 5.61: Loss = 0.679703
Epoch 5.62: Loss = 0.75116
Epoch 5.63: Loss = 0.869644
Epoch 5.64: Loss = 0.722275
Epoch 5.65: Loss = 0.849808
Epoch 5.66: Loss = 0.595901
Epoch 5.67: Loss = 0.636765
Epoch 5.68: Loss = 0.409073
Epoch 5.69: Loss = 0.546524
Epoch 5.70: Loss = 0.692566
Epoch 5.71: Loss = 0.476974
Epoch 5.72: Loss = 0.502182
Epoch 5.73: Loss = 0.620346
Epoch 5.74: Loss = 0.474564
Epoch 5.75: Loss = 0.792068
Epoch 5.76: Loss = 0.621277
Epoch 5.77: Loss = 0.507614
Epoch 5.78: Loss = 0.508362
Epoch 5.79: Loss = 0.676071
Epoch 5.80: Loss = 0.704132
Epoch 5.81: Loss = 0.497025
Epoch 5.82: Loss = 0.487259
Epoch 5.83: Loss = 0.628723
Epoch 5.84: Loss = 0.574219
Epoch 5.85: Loss = 0.752655
Epoch 5.86: Loss = 0.674637
Epoch 5.87: Loss = 0.45459
Epoch 5.88: Loss = 0.583389
Epoch 5.89: Loss = 0.659805
Epoch 5.90: Loss = 0.45079
Epoch 5.91: Loss = 0.635422
Epoch 5.92: Loss = 0.728958
Epoch 5.93: Loss = 0.720993
Epoch 5.94: Loss = 0.509399
Epoch 5.95: Loss = 0.647247
Epoch 5.96: Loss = 0.682068
Epoch 5.97: Loss = 0.504562
Epoch 5.98: Loss = 0.576614
Epoch 5.99: Loss = 0.723206
Epoch 5.100: Loss = 0.77562
Epoch 5.101: Loss = 0.812836
Epoch 5.102: Loss = 0.459061
Epoch 5.103: Loss = 0.556488
Epoch 5.104: Loss = 0.46347
Epoch 5.105: Loss = 0.730026
Epoch 5.106: Loss = 0.703171
Epoch 5.107: Loss = 0.516357
Epoch 5.108: Loss = 0.629501
Epoch 5.109: Loss = 0.529144
Epoch 5.110: Loss = 0.5802
Epoch 5.111: Loss = 0.537766
Epoch 5.112: Loss = 0.53891
Epoch 5.113: Loss = 0.521469
Epoch 5.114: Loss = 0.49939
Epoch 5.115: Loss = 0.478104
Epoch 5.116: Loss = 0.595261
Epoch 5.117: Loss = 0.316055
Epoch 5.118: Loss = 0.240402
Epoch 5.119: Loss = 0.356705
Epoch 5.120: Loss = 0.454834
TRAIN LOSS = 0.576477
TRAIN ACC = 85.2921 % (51178/60000)
Loss = 0.595154
Loss = 0.782928
Loss = 0.825165
Loss = 0.809692
Loss = 0.890152
Loss = 0.609985
Loss = 0.613861
Loss = 0.836807
Loss = 0.850037
Loss = 0.720474
Loss = 0.250198
Loss = 0.538406
Loss = 0.461456
Loss = 0.552048
Loss = 0.333221
Loss = 0.423157
Loss = 0.306717
Loss = 0.107346
Loss = 0.379028
Loss = 0.732697
TEST LOSS = 0.580926
TEST ACC = 511.78 % (8520/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.500854
Epoch 6.2: Loss = 0.712708
Epoch 6.3: Loss = 0.735458
Epoch 6.4: Loss = 0.382553
Epoch 6.5: Loss = 0.499374
Epoch 6.6: Loss = 0.512283
Epoch 6.7: Loss = 0.565887
Epoch 6.8: Loss = 0.546371
Epoch 6.9: Loss = 0.54248
Epoch 6.10: Loss = 0.657608
Epoch 6.11: Loss = 0.693329
Epoch 6.12: Loss = 0.608963
Epoch 6.13: Loss = 0.504105
Epoch 6.14: Loss = 0.667145
Epoch 6.15: Loss = 0.574265
Epoch 6.16: Loss = 0.642975
Epoch 6.17: Loss = 0.737885
Epoch 6.18: Loss = 0.95871
Epoch 6.19: Loss = 0.63501
Epoch 6.20: Loss = 0.500473
Epoch 6.21: Loss = 0.49971
Epoch 6.22: Loss = 0.493454
Epoch 6.23: Loss = 0.506149
Epoch 6.24: Loss = 0.877243
Epoch 6.25: Loss = 0.753571
Epoch 6.26: Loss = 0.750732
Epoch 6.27: Loss = 0.737411
Epoch 6.28: Loss = 0.673553
Epoch 6.29: Loss = 0.757019
Epoch 6.30: Loss = 0.800385
Epoch 6.31: Loss = 0.580338
Epoch 6.32: Loss = 0.693695
Epoch 6.33: Loss = 0.542038
Epoch 6.34: Loss = 0.757004
Epoch 6.35: Loss = 0.618713
Epoch 6.36: Loss = 0.73465
Epoch 6.37: Loss = 0.506683
Epoch 6.38: Loss = 0.552887
Epoch 6.39: Loss = 0.445313
Epoch 6.40: Loss = 0.558304
Epoch 6.41: Loss = 0.573792
Epoch 6.42: Loss = 0.821854
Epoch 6.43: Loss = 0.511124
Epoch 6.44: Loss = 0.422256
Epoch 6.45: Loss = 0.587769
Epoch 6.46: Loss = 0.7202
Epoch 6.47: Loss = 0.668167
Epoch 6.48: Loss = 0.610321
Epoch 6.49: Loss = 0.622849
Epoch 6.50: Loss = 0.74118
Epoch 6.51: Loss = 0.504883
Epoch 6.52: Loss = 0.535522
Epoch 6.53: Loss = 0.662247
Epoch 6.54: Loss = 0.758148
Epoch 6.55: Loss = 0.672501
Epoch 6.56: Loss = 0.535172
Epoch 6.57: Loss = 0.593063
Epoch 6.58: Loss = 0.620178
Epoch 6.59: Loss = 0.690857
Epoch 6.60: Loss = 0.72908
Epoch 6.61: Loss = 0.789597
Epoch 6.62: Loss = 0.838165
Epoch 6.63: Loss = 0.930695
Epoch 6.64: Loss = 0.784607
Epoch 6.65: Loss = 0.839447
Epoch 6.66: Loss = 0.666016
Epoch 6.67: Loss = 0.679077
Epoch 6.68: Loss = 0.49382
Epoch 6.69: Loss = 0.535995
Epoch 6.70: Loss = 0.801636
Epoch 6.71: Loss = 0.540588
Epoch 6.72: Loss = 0.63266
Epoch 6.73: Loss = 0.5952
Epoch 6.74: Loss = 0.489349
Epoch 6.75: Loss = 0.799484
Epoch 6.76: Loss = 0.654984
Epoch 6.77: Loss = 0.599899
Epoch 6.78: Loss = 0.562683
Epoch 6.79: Loss = 0.799484
Epoch 6.80: Loss = 0.857422
Epoch 6.81: Loss = 0.562317
Epoch 6.82: Loss = 0.550003
Epoch 6.83: Loss = 0.701065
Epoch 6.84: Loss = 0.566528
Epoch 6.85: Loss = 0.802139
Epoch 6.86: Loss = 0.728729
Epoch 6.87: Loss = 0.535172
Epoch 6.88: Loss = 0.591248
Epoch 6.89: Loss = 0.724136
Epoch 6.90: Loss = 0.503922
Epoch 6.91: Loss = 0.659668
Epoch 6.92: Loss = 0.734238
Epoch 6.93: Loss = 0.825012
Epoch 6.94: Loss = 0.556015
Epoch 6.95: Loss = 0.641495
Epoch 6.96: Loss = 0.87471
Epoch 6.97: Loss = 0.63797
Epoch 6.98: Loss = 0.619476
Epoch 6.99: Loss = 0.785522
Epoch 6.100: Loss = 0.915863
Epoch 6.101: Loss = 0.821716
Epoch 6.102: Loss = 0.593567
Epoch 6.103: Loss = 0.628174
Epoch 6.104: Loss = 0.478683
Epoch 6.105: Loss = 0.804306
Epoch 6.106: Loss = 0.776413
Epoch 6.107: Loss = 0.513077
Epoch 6.108: Loss = 0.714462
Epoch 6.109: Loss = 0.620697
Epoch 6.110: Loss = 0.683029
Epoch 6.111: Loss = 0.536987
Epoch 6.112: Loss = 0.566391
Epoch 6.113: Loss = 0.626236
Epoch 6.114: Loss = 0.51561
Epoch 6.115: Loss = 0.548599
Epoch 6.116: Loss = 0.663696
Epoch 6.117: Loss = 0.4086
Epoch 6.118: Loss = 0.262115
Epoch 6.119: Loss = 0.410141
Epoch 6.120: Loss = 0.472809
TRAIN LOSS = 0.638763
TRAIN ACC = 84.6008 % (50763/60000)
Loss = 0.667145
Loss = 0.781876
Loss = 0.981659
Loss = 0.895096
Loss = 0.937851
Loss = 0.625595
Loss = 0.667267
Loss = 0.875351
Loss = 0.915039
Loss = 0.815628
Loss = 0.271225
Loss = 0.555649
Loss = 0.581558
Loss = 0.592102
Loss = 0.339615
Loss = 0.437302
Loss = 0.354523
Loss = 0.103027
Loss = 0.37085
Loss = 0.879379
TEST LOSS = 0.632387
TEST ACC = 507.629 % (8542/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.521408
Epoch 7.2: Loss = 0.744614
Epoch 7.3: Loss = 0.730118
Epoch 7.4: Loss = 0.459061
Epoch 7.5: Loss = 0.521545
Epoch 7.6: Loss = 0.555618
Epoch 7.7: Loss = 0.585083
Epoch 7.8: Loss = 0.580734
Epoch 7.9: Loss = 0.63028
Epoch 7.10: Loss = 0.663528
Epoch 7.11: Loss = 0.676239
Epoch 7.12: Loss = 0.61557
Epoch 7.13: Loss = 0.553146
Epoch 7.14: Loss = 0.751434
Epoch 7.15: Loss = 0.656036
Epoch 7.16: Loss = 0.684082
Epoch 7.17: Loss = 0.699783
Epoch 7.18: Loss = 0.969009
Epoch 7.19: Loss = 0.732285
Epoch 7.20: Loss = 0.590057
Epoch 7.21: Loss = 0.576813
Epoch 7.22: Loss = 0.532562
Epoch 7.23: Loss = 0.471817
Epoch 7.24: Loss = 0.831985
Epoch 7.25: Loss = 0.622772
Epoch 7.26: Loss = 0.866409
Epoch 7.27: Loss = 0.759918
Epoch 7.28: Loss = 0.670471
Epoch 7.29: Loss = 0.770523
Epoch 7.30: Loss = 0.909393
Epoch 7.31: Loss = 0.629517
Epoch 7.32: Loss = 0.721222
Epoch 7.33: Loss = 0.569092
Epoch 7.34: Loss = 0.836411
Epoch 7.35: Loss = 0.640961
Epoch 7.36: Loss = 0.851929
Epoch 7.37: Loss = 0.528961
Epoch 7.38: Loss = 0.569061
Epoch 7.39: Loss = 0.576431
Epoch 7.40: Loss = 0.656891
Epoch 7.41: Loss = 0.620651
Epoch 7.42: Loss = 0.835129
Epoch 7.43: Loss = 0.523193
Epoch 7.44: Loss = 0.528336
Epoch 7.45: Loss = 0.571854
Epoch 7.46: Loss = 0.649857
Epoch 7.47: Loss = 0.693756
Epoch 7.48: Loss = 0.757996
Epoch 7.49: Loss = 0.61911
Epoch 7.50: Loss = 0.765533
Epoch 7.51: Loss = 0.543686
Epoch 7.52: Loss = 0.5448
Epoch 7.53: Loss = 0.649796
Epoch 7.54: Loss = 0.779327
Epoch 7.55: Loss = 0.669464
Epoch 7.56: Loss = 0.554535
Epoch 7.57: Loss = 0.594421
Epoch 7.58: Loss = 0.622208
Epoch 7.59: Loss = 0.721497
Epoch 7.60: Loss = 0.803574
Epoch 7.61: Loss = 0.847519
Epoch 7.62: Loss = 0.801849
Epoch 7.63: Loss = 0.984406
Epoch 7.64: Loss = 0.827286
Epoch 7.65: Loss = 0.885117
Epoch 7.66: Loss = 0.67778
Epoch 7.67: Loss = 0.703064
Epoch 7.68: Loss = 0.446854
Epoch 7.69: Loss = 0.508621
Epoch 7.70: Loss = 0.851273
Epoch 7.71: Loss = 0.546768
Epoch 7.72: Loss = 0.562225
Epoch 7.73: Loss = 0.675613
Epoch 7.74: Loss = 0.548218
Epoch 7.75: Loss = 0.883438
Epoch 7.76: Loss = 0.678055
Epoch 7.77: Loss = 0.583237
Epoch 7.78: Loss = 0.592407
Epoch 7.79: Loss = 0.809799
Epoch 7.80: Loss = 0.918427
Epoch 7.81: Loss = 0.6194
Epoch 7.82: Loss = 0.621353
Epoch 7.83: Loss = 0.778244
Epoch 7.84: Loss = 0.583893
Epoch 7.85: Loss = 0.788742
Epoch 7.86: Loss = 0.714615
Epoch 7.87: Loss = 0.5224
Epoch 7.88: Loss = 0.621307
Epoch 7.89: Loss = 0.781067
Epoch 7.90: Loss = 0.537643
Epoch 7.91: Loss = 0.730255
Epoch 7.92: Loss = 0.795151
Epoch 7.93: Loss = 0.850113
Epoch 7.94: Loss = 0.520279
Epoch 7.95: Loss = 0.666107
Epoch 7.96: Loss = 0.820694
Epoch 7.97: Loss = 0.588196
Epoch 7.98: Loss = 0.726868
Epoch 7.99: Loss = 0.868561
Epoch 7.100: Loss = 0.951767
Epoch 7.101: Loss = 0.837967
Epoch 7.102: Loss = 0.618835
Epoch 7.103: Loss = 0.645752
Epoch 7.104: Loss = 0.523056
Epoch 7.105: Loss = 0.817871
Epoch 7.106: Loss = 0.75798
Epoch 7.107: Loss = 0.539841
Epoch 7.108: Loss = 0.760025
Epoch 7.109: Loss = 0.608871
Epoch 7.110: Loss = 0.715256
Epoch 7.111: Loss = 0.539978
Epoch 7.112: Loss = 0.57579
Epoch 7.113: Loss = 0.573822
Epoch 7.114: Loss = 0.541061
Epoch 7.115: Loss = 0.600388
Epoch 7.116: Loss = 0.693069
Epoch 7.117: Loss = 0.400604
Epoch 7.118: Loss = 0.300217
Epoch 7.119: Loss = 0.500031
Epoch 7.120: Loss = 0.486923
TRAIN LOSS = 0.66684
TRAIN ACC = 84.5932 % (50758/60000)
Loss = 0.726532
Loss = 0.845383
Loss = 1.03577
Loss = 1.00943
Loss = 1.06375
Loss = 0.686859
Loss = 0.724045
Loss = 0.994171
Loss = 1.01871
Loss = 0.853653
Loss = 0.266647
Loss = 0.604141
Loss = 0.561249
Loss = 0.608047
Loss = 0.333893
Loss = 0.477356
Loss = 0.379272
Loss = 0.114639
Loss = 0.362396
Loss = 0.826279
TEST LOSS = 0.674611
TEST ACC = 507.579 % (8498/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.604172
Epoch 8.2: Loss = 0.743546
Epoch 8.3: Loss = 0.770248
Epoch 8.4: Loss = 0.519073
Epoch 8.5: Loss = 0.504944
Epoch 8.6: Loss = 0.634735
Epoch 8.7: Loss = 0.568359
Epoch 8.8: Loss = 0.58519
Epoch 8.9: Loss = 0.630753
Epoch 8.10: Loss = 0.625641
Epoch 8.11: Loss = 0.68399
Epoch 8.12: Loss = 0.628235
Epoch 8.13: Loss = 0.568237
Epoch 8.14: Loss = 0.602905
Epoch 8.15: Loss = 0.677322
Epoch 8.16: Loss = 0.700989
Epoch 8.17: Loss = 0.637421
Epoch 8.18: Loss = 0.973846
Epoch 8.19: Loss = 0.64035
Epoch 8.20: Loss = 0.595459
Epoch 8.21: Loss = 0.632355
Epoch 8.22: Loss = 0.480591
Epoch 8.23: Loss = 0.51535
Epoch 8.24: Loss = 0.882278
Epoch 8.25: Loss = 0.61084
Epoch 8.26: Loss = 0.811554
Epoch 8.27: Loss = 0.668655
Epoch 8.28: Loss = 0.700424
Epoch 8.29: Loss = 0.8116
Epoch 8.30: Loss = 0.814117
Epoch 8.31: Loss = 0.60434
Epoch 8.32: Loss = 0.672287
Epoch 8.33: Loss = 0.578766
Epoch 8.34: Loss = 0.889923
Epoch 8.35: Loss = 0.595581
Epoch 8.36: Loss = 0.745285
Epoch 8.37: Loss = 0.552216
Epoch 8.38: Loss = 0.611313
Epoch 8.39: Loss = 0.539413
Epoch 8.40: Loss = 0.723633
Epoch 8.41: Loss = 0.659851
Epoch 8.42: Loss = 0.905914
Epoch 8.43: Loss = 0.589783
Epoch 8.44: Loss = 0.563614
Epoch 8.45: Loss = 0.601837
Epoch 8.46: Loss = 0.716339
Epoch 8.47: Loss = 0.738205
Epoch 8.48: Loss = 0.743195
Epoch 8.49: Loss = 0.638657
Epoch 8.50: Loss = 0.841629
Epoch 8.51: Loss = 0.537933
Epoch 8.52: Loss = 0.607758
Epoch 8.53: Loss = 0.703186
Epoch 8.54: Loss = 0.837128
Epoch 8.55: Loss = 0.77771
Epoch 8.56: Loss = 0.619125
Epoch 8.57: Loss = 0.625305
Epoch 8.58: Loss = 0.586502
Epoch 8.59: Loss = 0.766678
Epoch 8.60: Loss = 0.87146
Epoch 8.61: Loss = 0.769135
Epoch 8.62: Loss = 0.857132
Epoch 8.63: Loss = 0.968781
Epoch 8.64: Loss = 0.81041
Epoch 8.65: Loss = 0.945267
Epoch 8.66: Loss = 0.645004
Epoch 8.67: Loss = 0.703873
Epoch 8.68: Loss = 0.437454
Epoch 8.69: Loss = 0.526306
Epoch 8.70: Loss = 0.842743
Epoch 8.71: Loss = 0.567917
Epoch 8.72: Loss = 0.563705
Epoch 8.73: Loss = 0.733627
Epoch 8.74: Loss = 0.463409
Epoch 8.75: Loss = 0.90065
Epoch 8.76: Loss = 0.716492
Epoch 8.77: Loss = 0.570694
Epoch 8.78: Loss = 0.584366
Epoch 8.79: Loss = 0.866791
Epoch 8.80: Loss = 1.00755
Epoch 8.81: Loss = 0.614532
Epoch 8.82: Loss = 0.653839
Epoch 8.83: Loss = 0.779266
Epoch 8.84: Loss = 0.695786
Epoch 8.85: Loss = 0.872833
Epoch 8.86: Loss = 0.839859
Epoch 8.87: Loss = 0.63327
Epoch 8.88: Loss = 0.713318
Epoch 8.89: Loss = 0.787766
Epoch 8.90: Loss = 0.580383
Epoch 8.91: Loss = 0.81105
Epoch 8.92: Loss = 0.751556
Epoch 8.93: Loss = 0.857437
Epoch 8.94: Loss = 0.568924
Epoch 8.95: Loss = 0.70845
Epoch 8.96: Loss = 0.865509
Epoch 8.97: Loss = 0.678192
Epoch 8.98: Loss = 0.747147
Epoch 8.99: Loss = 0.965561
Epoch 8.100: Loss = 1.08473
Epoch 8.101: Loss = 0.887543
Epoch 8.102: Loss = 0.663254
Epoch 8.103: Loss = 0.769547
Epoch 8.104: Loss = 0.489166
Epoch 8.105: Loss = 0.968613
Epoch 8.106: Loss = 0.814743
Epoch 8.107: Loss = 0.625839
Epoch 8.108: Loss = 0.776108
Epoch 8.109: Loss = 0.622467
Epoch 8.110: Loss = 0.762848
Epoch 8.111: Loss = 0.599091
Epoch 8.112: Loss = 0.645828
Epoch 8.113: Loss = 0.634506
Epoch 8.114: Loss = 0.502319
Epoch 8.115: Loss = 0.590485
Epoch 8.116: Loss = 0.75174
Epoch 8.117: Loss = 0.311264
Epoch 8.118: Loss = 0.273132
Epoch 8.119: Loss = 0.534698
Epoch 8.120: Loss = 0.620621
TRAIN LOSS = 0.690033
TRAIN ACC = 84.7137 % (50830/60000)
Loss = 0.754318
Loss = 0.855743
Loss = 1.09279
Loss = 1.03981
Loss = 1.0753
Loss = 0.769562
Loss = 0.764542
Loss = 1.10892
Loss = 1.07603
Loss = 0.914139
Loss = 0.327072
Loss = 0.598297
Loss = 0.601196
Loss = 0.677994
Loss = 0.435074
Loss = 0.494461
Loss = 0.445923
Loss = 0.0952759
Loss = 0.415955
Loss = 0.954376
TEST LOSS = 0.724839
TEST ACC = 508.299 % (8474/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.658813
Epoch 9.2: Loss = 0.802261
Epoch 9.3: Loss = 0.839798
Epoch 9.4: Loss = 0.582825
Epoch 9.5: Loss = 0.551025
Epoch 9.6: Loss = 0.721893
Epoch 9.7: Loss = 0.633148
Epoch 9.8: Loss = 0.676636
Epoch 9.9: Loss = 0.616653
Epoch 9.10: Loss = 0.66452
Epoch 9.11: Loss = 0.683975
Epoch 9.12: Loss = 0.682663
Epoch 9.13: Loss = 0.677094
Epoch 9.14: Loss = 0.659393
Epoch 9.15: Loss = 0.751953
Epoch 9.16: Loss = 0.739319
Epoch 9.17: Loss = 0.735947
Epoch 9.18: Loss = 1.02428
Epoch 9.19: Loss = 0.734756
Epoch 9.20: Loss = 0.554367
Epoch 9.21: Loss = 0.643112
Epoch 9.22: Loss = 0.580963
Epoch 9.23: Loss = 0.612793
Epoch 9.24: Loss = 1.03911
Epoch 9.25: Loss = 0.70816
Epoch 9.26: Loss = 0.846848
Epoch 9.27: Loss = 0.738159
Epoch 9.28: Loss = 0.730591
Epoch 9.29: Loss = 0.904556
Epoch 9.30: Loss = 0.97023
Epoch 9.31: Loss = 0.608841
Epoch 9.32: Loss = 0.791473
Epoch 9.33: Loss = 0.657623
Epoch 9.34: Loss = 0.895615
Epoch 9.35: Loss = 0.665314
Epoch 9.36: Loss = 0.867325
Epoch 9.37: Loss = 0.584747
Epoch 9.38: Loss = 0.584335
Epoch 9.39: Loss = 0.636459
Epoch 9.40: Loss = 0.734558
Epoch 9.41: Loss = 0.738083
Epoch 9.42: Loss = 0.973846
Epoch 9.43: Loss = 0.624512
Epoch 9.44: Loss = 0.557327
Epoch 9.45: Loss = 0.634888
Epoch 9.46: Loss = 0.859985
Epoch 9.47: Loss = 0.830032
Epoch 9.48: Loss = 0.782516
Epoch 9.49: Loss = 0.738632
Epoch 9.50: Loss = 0.857254
Epoch 9.51: Loss = 0.642944
Epoch 9.52: Loss = 0.689682
Epoch 9.53: Loss = 0.692215
Epoch 9.54: Loss = 0.854401
Epoch 9.55: Loss = 0.772827
Epoch 9.56: Loss = 0.74942
Epoch 9.57: Loss = 0.717056
Epoch 9.58: Loss = 0.699631
Epoch 9.59: Loss = 0.89859
Epoch 9.60: Loss = 1.04501
Epoch 9.61: Loss = 0.923691
Epoch 9.62: Loss = 0.892548
Epoch 9.63: Loss = 1.13126
Epoch 9.64: Loss = 0.930588
Epoch 9.65: Loss = 1.0275
Epoch 9.66: Loss = 0.691818
Epoch 9.67: Loss = 0.790543
Epoch 9.68: Loss = 0.452972
Epoch 9.69: Loss = 0.652695
Epoch 9.70: Loss = 0.941986
Epoch 9.71: Loss = 0.695633
Epoch 9.72: Loss = 0.628174
Epoch 9.73: Loss = 0.725418
Epoch 9.74: Loss = 0.551743
Epoch 9.75: Loss = 1.00929
Epoch 9.76: Loss = 0.702057
Epoch 9.77: Loss = 0.656998
Epoch 9.78: Loss = 0.717117
Epoch 9.79: Loss = 0.873398
Epoch 9.80: Loss = 1.08859
Epoch 9.81: Loss = 0.693192
Epoch 9.82: Loss = 0.670456
Epoch 9.83: Loss = 0.817383
Epoch 9.84: Loss = 0.831787
Epoch 9.85: Loss = 1.0943
Epoch 9.86: Loss = 0.858215
Epoch 9.87: Loss = 0.649124
Epoch 9.88: Loss = 0.777573
Epoch 9.89: Loss = 0.890762
Epoch 9.90: Loss = 0.680359
Epoch 9.91: Loss = 0.892532
Epoch 9.92: Loss = 0.874741
Epoch 9.93: Loss = 0.944199
Epoch 9.94: Loss = 0.641815
Epoch 9.95: Loss = 0.763245
Epoch 9.96: Loss = 1.01271
Epoch 9.97: Loss = 0.76619
Epoch 9.98: Loss = 0.947998
Epoch 9.99: Loss = 0.917847
Epoch 9.100: Loss = 1.22234
Epoch 9.101: Loss = 0.950043
Epoch 9.102: Loss = 0.823059
Epoch 9.103: Loss = 0.740158
Epoch 9.104: Loss = 0.619904
Epoch 9.105: Loss = 1.00174
Epoch 9.106: Loss = 1.01619
Epoch 9.107: Loss = 0.651276
Epoch 9.108: Loss = 0.746689
Epoch 9.109: Loss = 0.687042
Epoch 9.110: Loss = 0.798706
Epoch 9.111: Loss = 0.617569
Epoch 9.112: Loss = 0.712158
Epoch 9.113: Loss = 0.679916
Epoch 9.114: Loss = 0.592667
Epoch 9.115: Loss = 0.639877
Epoch 9.116: Loss = 0.742844
Epoch 9.117: Loss = 0.448837
Epoch 9.118: Loss = 0.33252
Epoch 9.119: Loss = 0.626633
Epoch 9.120: Loss = 0.554184
TRAIN LOSS = 0.759705
TRAIN ACC = 84.4849 % (50693/60000)
Loss = 0.797104
Loss = 0.942398
Loss = 1.14893
Loss = 1.02132
Loss = 1.16319
Loss = 0.780167
Loss = 0.728165
Loss = 1.12361
Loss = 0.970718
Loss = 0.911163
Loss = 0.404877
Loss = 0.735748
Loss = 0.590805
Loss = 0.717728
Loss = 0.425858
Loss = 0.497955
Loss = 0.469925
Loss = 0.115051
Loss = 0.457214
Loss = 0.946564
TEST LOSS = 0.747424
TEST ACC = 506.929 % (8513/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.725754
Epoch 10.2: Loss = 0.833115
Epoch 10.3: Loss = 0.859695
Epoch 10.4: Loss = 0.641876
Epoch 10.5: Loss = 0.588943
Epoch 10.6: Loss = 0.738159
Epoch 10.7: Loss = 0.590363
Epoch 10.8: Loss = 0.778976
Epoch 10.9: Loss = 0.774536
Epoch 10.10: Loss = 0.944016
Epoch 10.11: Loss = 0.865921
Epoch 10.12: Loss = 0.733841
Epoch 10.13: Loss = 0.772552
Epoch 10.14: Loss = 0.740204
Epoch 10.15: Loss = 5111.29
Epoch 10.16: Loss = -1190.92
Epoch 10.17: Loss = 1004.27
Epoch 10.18: Loss = -643.144
Epoch 10.19: Loss = -2164.01
Epoch 10.20: Loss = 1058.22
Epoch 10.21: Loss = -744.437
Epoch 10.22: Loss = -1947.89
Epoch 10.23: Loss = -1122.6
Epoch 10.24: Loss = 415.13
Epoch 10.25: Loss = 240.515
Epoch 10.26: Loss = 1232.66
Epoch 10.27: Loss = 594.065
Epoch 10.28: Loss = 529.076
Epoch 10.29: Loss = -1266.04
Epoch 10.30: Loss = 82.5119
Epoch 10.31: Loss = 713.982
Epoch 10.32: Loss = 983.316
Epoch 10.33: Loss = -883.549
Epoch 10.34: Loss = 1738.44
Epoch 10.35: Loss = 36.4851
Epoch 10.36: Loss = 765.275
Epoch 10.37: Loss = 655.507
Epoch 10.38: Loss = 436.755
Epoch 10.39: Loss = -190.786
Epoch 10.40: Loss = 414.058
Epoch 10.41: Loss = 1569.97
Epoch 10.42: Loss = 3022.12
Epoch 10.43: Loss = 450.851
Epoch 10.44: Loss = 1148.15
Epoch 10.45: Loss = -65.3294
Epoch 10.46: Loss = 379.378
Epoch 10.47: Loss = 1194.89
Epoch 10.48: Loss = -344.171
Epoch 10.49: Loss = -732.402
Epoch 10.50: Loss = 916.027
Epoch 10.51: Loss = 480.78
Epoch 10.52: Loss = 2184.27
Epoch 10.53: Loss = -209.141
Epoch 10.54: Loss = -895.307
Epoch 10.55: Loss = 1203.47
Epoch 10.56: Loss = -1536.97
Epoch 10.57: Loss = 46.182
Epoch 10.58: Loss = -226.467
Epoch 10.59: Loss = 17.3285
Epoch 10.60: Loss = -1688.77
Epoch 10.61: Loss = 362.998
Epoch 10.62: Loss = -14.1583
Epoch 10.63: Loss = 1922.87
Epoch 10.64: Loss = -133.01
Epoch 10.65: Loss = 67.4753
Epoch 10.66: Loss = 292.656
Epoch 10.67: Loss = 1189.66
Epoch 10.68: Loss = -255.28
Epoch 10.69: Loss = 1153.72
Epoch 10.70: Loss = 229.012
Epoch 10.71: Loss = -1065.46
Epoch 10.72: Loss = -1031.36
Epoch 10.73: Loss = -377.611
Epoch 10.74: Loss = -1277.43
Epoch 10.75: Loss = -990.9
Epoch 10.76: Loss = -1158.86
Epoch 10.77: Loss = -904.587
Epoch 10.78: Loss = -310.987
Epoch 10.79: Loss = 644.052
Epoch 10.80: Loss = -68.8788
Epoch 10.81: Loss = -355.952
Epoch 10.82: Loss = -231.7
Epoch 10.83: Loss = 1622.57
Epoch 10.84: Loss = -438.378
Epoch 10.85: Loss = 376.341
Epoch 10.86: Loss = 1061.32
Epoch 10.87: Loss = -1956.45
Epoch 10.88: Loss = -2143.52
Epoch 10.89: Loss = 595.988
Epoch 10.90: Loss = 1066.6
Epoch 10.91: Loss = 478.58
Epoch 10.92: Loss = 2529.58
Epoch 10.93: Loss = -564.032
Epoch 10.94: Loss = -71.0437
Epoch 10.95: Loss = -142.572
Epoch 10.96: Loss = 566.949
Epoch 10.97: Loss = -454.75
Epoch 10.98: Loss = 879.854
Epoch 10.99: Loss = -545.275
Epoch 10.100: Loss = 649.34
Epoch 10.101: Loss = -332.533
Epoch 10.102: Loss = -316.017
Epoch 10.103: Loss = 2835.98
Epoch 10.104: Loss = -935.751
Epoch 10.105: Loss = 844.854
Epoch 10.106: Loss = 473.126
Epoch 10.107: Loss = -724.845
Epoch 10.108: Loss = 1408.97
Epoch 10.109: Loss = -407.655
Epoch 10.110: Loss = 1868.11
Epoch 10.111: Loss = 3083.21
Epoch 10.112: Loss = -250.784
Epoch 10.113: Loss = -2458.11
Epoch 10.114: Loss = -770.49
Epoch 10.115: Loss = -996.356
Epoch 10.116: Loss = -1214.9
Epoch 10.117: Loss = -1425.86
Epoch 10.118: Loss = -1984.75
Epoch 10.119: Loss = -1056.54
Epoch 10.120: Loss = 671.821
TRAIN LOSS = 102.439
TRAIN ACC = 18.602 % (11162/60000)
Loss = 46.4689
Loss = -1544.93
Loss = -394.704
Loss = -1162.8
Loss = 2289.15
Loss = -1065.22
Loss = -1312.44
Loss = -992.722
Loss = 997.259
Loss = 53.2802
Loss = 200.518
Loss = 134.427
Loss = 645.999
Loss = 642.82
Loss = 479.84
Loss = -923.152
Loss = -1006.12
Loss = 1662.09
Loss = -388.224
Loss = -282.562
TEST LOSS = -0.206263
TEST ACC = 111.62 % (1065/10000)
