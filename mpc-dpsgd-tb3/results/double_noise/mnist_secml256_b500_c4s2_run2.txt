Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.27672
Epoch 1.2: Loss = 2.25497
Epoch 1.3: Loss = 2.21581
Epoch 1.4: Loss = 2.17935
Epoch 1.5: Loss = 2.15292
Epoch 1.6: Loss = 2.12346
Epoch 1.7: Loss = 2.11124
Epoch 1.8: Loss = 2.08044
Epoch 1.9: Loss = 2.05817
Epoch 1.10: Loss = 2.02135
Epoch 1.11: Loss = 1.96872
Epoch 1.12: Loss = 1.94159
Epoch 1.13: Loss = 1.88712
Epoch 1.14: Loss = 1.89404
Epoch 1.15: Loss = 1.9462
Epoch 1.16: Loss = 1.83621
Epoch 1.17: Loss = 1.79959
Epoch 1.18: Loss = 1.77927
Epoch 1.19: Loss = 1.74431
Epoch 1.20: Loss = 1.69817
Epoch 1.21: Loss = 1.63315
Epoch 1.22: Loss = 1.64473
Epoch 1.23: Loss = 1.55713
Epoch 1.24: Loss = 1.65274
Epoch 1.25: Loss = 1.5631
Epoch 1.26: Loss = 1.57407
Epoch 1.27: Loss = 1.50519
Epoch 1.28: Loss = 1.50746
Epoch 1.29: Loss = 1.48076
Epoch 1.30: Loss = 1.54698
Epoch 1.31: Loss = 1.41815
Epoch 1.32: Loss = 1.43289
Epoch 1.33: Loss = 1.3566
Epoch 1.34: Loss = 1.38216
Epoch 1.35: Loss = 1.28146
Epoch 1.36: Loss = 1.41608
Epoch 1.37: Loss = 1.26485
Epoch 1.38: Loss = 1.2348
Epoch 1.39: Loss = 1.19835
Epoch 1.40: Loss = 1.14545
Epoch 1.41: Loss = 1.18591
Epoch 1.42: Loss = 1.15305
Epoch 1.43: Loss = 1.14484
Epoch 1.44: Loss = 1.02277
Epoch 1.45: Loss = 1.16873
Epoch 1.46: Loss = 1.11739
Epoch 1.47: Loss = 1.02789
Epoch 1.48: Loss = 1.07104
Epoch 1.49: Loss = 1.0347
Epoch 1.50: Loss = 1.09822
Epoch 1.51: Loss = 0.930115
Epoch 1.52: Loss = 0.947327
Epoch 1.53: Loss = 0.985657
Epoch 1.54: Loss = 1.00015
Epoch 1.55: Loss = 0.976013
Epoch 1.56: Loss = 0.900101
Epoch 1.57: Loss = 0.84906
Epoch 1.58: Loss = 0.880432
Epoch 1.59: Loss = 0.893661
Epoch 1.60: Loss = 1.00319
Epoch 1.61: Loss = 0.92308
Epoch 1.62: Loss = 0.943039
Epoch 1.63: Loss = 0.969727
Epoch 1.64: Loss = 0.916275
Epoch 1.65: Loss = 0.964661
Epoch 1.66: Loss = 0.82991
Epoch 1.67: Loss = 0.829041
Epoch 1.68: Loss = 0.708817
Epoch 1.69: Loss = 0.787125
Epoch 1.70: Loss = 0.827194
Epoch 1.71: Loss = 0.766846
Epoch 1.72: Loss = 0.77533
Epoch 1.73: Loss = 0.813034
Epoch 1.74: Loss = 0.673264
Epoch 1.75: Loss = 0.791656
Epoch 1.76: Loss = 0.764587
Epoch 1.77: Loss = 0.745132
Epoch 1.78: Loss = 0.710632
Epoch 1.79: Loss = 0.718353
Epoch 1.80: Loss = 0.81192
Epoch 1.81: Loss = 0.640686
Epoch 1.82: Loss = 0.645264
Epoch 1.83: Loss = 0.791473
Epoch 1.84: Loss = 0.717056
Epoch 1.85: Loss = 0.757858
Epoch 1.86: Loss = 0.692719
Epoch 1.87: Loss = 0.620499
Epoch 1.88: Loss = 0.66188
Epoch 1.89: Loss = 0.718384
Epoch 1.90: Loss = 0.639725
Epoch 1.91: Loss = 0.696426
Epoch 1.92: Loss = 0.660324
Epoch 1.93: Loss = 0.703827
Epoch 1.94: Loss = 0.556274
Epoch 1.95: Loss = 0.665146
Epoch 1.96: Loss = 0.661911
Epoch 1.97: Loss = 0.519485
Epoch 1.98: Loss = 0.611862
Epoch 1.99: Loss = 0.683685
Epoch 1.100: Loss = 0.815475
Epoch 1.101: Loss = 0.719604
Epoch 1.102: Loss = 0.600418
Epoch 1.103: Loss = 0.563812
Epoch 1.104: Loss = 0.543762
Epoch 1.105: Loss = 0.670761
Epoch 1.106: Loss = 0.664215
Epoch 1.107: Loss = 0.553696
Epoch 1.108: Loss = 0.588501
Epoch 1.109: Loss = 0.550751
Epoch 1.110: Loss = 0.586746
Epoch 1.111: Loss = 0.497757
Epoch 1.112: Loss = 0.473328
Epoch 1.113: Loss = 0.554367
Epoch 1.114: Loss = 0.486618
Epoch 1.115: Loss = 0.559509
Epoch 1.116: Loss = 0.538971
Epoch 1.117: Loss = 0.45813
Epoch 1.118: Loss = 0.40361
Epoch 1.119: Loss = 0.412201
Epoch 1.120: Loss = 0.434204
TRAIN LOSS = 1.08124
TRAIN ACC = 70.9641 % (42580/60000)
Loss = 0.575226
Loss = 0.596359
Loss = 0.711807
Loss = 0.670929
Loss = 0.7099
Loss = 0.590454
Loss = 0.552643
Loss = 0.724564
Loss = 0.686172
Loss = 0.630981
Loss = 0.327148
Loss = 0.443161
Loss = 0.368195
Loss = 0.545654
Loss = 0.407669
Loss = 0.436691
Loss = 0.382172
Loss = 0.227814
Loss = 0.38707
Loss = 0.662216
TEST LOSS = 0.531841
TEST ACC = 425.8 % (8367/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.517197
Epoch 2.2: Loss = 0.639084
Epoch 2.3: Loss = 0.615417
Epoch 2.4: Loss = 0.49173
Epoch 2.5: Loss = 0.496475
Epoch 2.6: Loss = 0.497833
Epoch 2.7: Loss = 0.539536
Epoch 2.8: Loss = 0.498718
Epoch 2.9: Loss = 0.515701
Epoch 2.10: Loss = 0.522049
Epoch 2.11: Loss = 0.515076
Epoch 2.12: Loss = 0.501266
Epoch 2.13: Loss = 0.448608
Epoch 2.14: Loss = 0.47168
Epoch 2.15: Loss = 0.601563
Epoch 2.16: Loss = 0.574951
Epoch 2.17: Loss = 0.566101
Epoch 2.18: Loss = 0.629776
Epoch 2.19: Loss = 0.499847
Epoch 2.20: Loss = 0.483994
Epoch 2.21: Loss = 0.411835
Epoch 2.22: Loss = 0.455109
Epoch 2.23: Loss = 0.42778
Epoch 2.24: Loss = 0.673996
Epoch 2.25: Loss = 0.534409
Epoch 2.26: Loss = 0.636902
Epoch 2.27: Loss = 0.571793
Epoch 2.28: Loss = 0.562805
Epoch 2.29: Loss = 0.620865
Epoch 2.30: Loss = 0.669052
Epoch 2.31: Loss = 0.469421
Epoch 2.32: Loss = 0.598526
Epoch 2.33: Loss = 0.488754
Epoch 2.34: Loss = 0.561676
Epoch 2.35: Loss = 0.506866
Epoch 2.36: Loss = 0.612808
Epoch 2.37: Loss = 0.405838
Epoch 2.38: Loss = 0.442902
Epoch 2.39: Loss = 0.467438
Epoch 2.40: Loss = 0.44635
Epoch 2.41: Loss = 0.503464
Epoch 2.42: Loss = 0.577225
Epoch 2.43: Loss = 0.45813
Epoch 2.44: Loss = 0.363632
Epoch 2.45: Loss = 0.51741
Epoch 2.46: Loss = 0.539719
Epoch 2.47: Loss = 0.437866
Epoch 2.48: Loss = 0.510361
Epoch 2.49: Loss = 0.49762
Epoch 2.50: Loss = 0.560516
Epoch 2.51: Loss = 0.411926
Epoch 2.52: Loss = 0.406052
Epoch 2.53: Loss = 0.475586
Epoch 2.54: Loss = 0.564804
Epoch 2.55: Loss = 0.484131
Epoch 2.56: Loss = 0.425354
Epoch 2.57: Loss = 0.422028
Epoch 2.58: Loss = 0.48793
Epoch 2.59: Loss = 0.528961
Epoch 2.60: Loss = 0.57962
Epoch 2.61: Loss = 0.544174
Epoch 2.62: Loss = 0.578369
Epoch 2.63: Loss = 0.614975
Epoch 2.64: Loss = 0.545227
Epoch 2.65: Loss = 0.643997
Epoch 2.66: Loss = 0.509735
Epoch 2.67: Loss = 0.527832
Epoch 2.68: Loss = 0.329941
Epoch 2.69: Loss = 0.425217
Epoch 2.70: Loss = 0.585541
Epoch 2.71: Loss = 0.418747
Epoch 2.72: Loss = 0.437759
Epoch 2.73: Loss = 0.45401
Epoch 2.74: Loss = 0.345261
Epoch 2.75: Loss = 0.590134
Epoch 2.76: Loss = 0.493958
Epoch 2.77: Loss = 0.45311
Epoch 2.78: Loss = 0.445343
Epoch 2.79: Loss = 0.527725
Epoch 2.80: Loss = 0.539536
Epoch 2.81: Loss = 0.404495
Epoch 2.82: Loss = 0.387268
Epoch 2.83: Loss = 0.558121
Epoch 2.84: Loss = 0.460007
Epoch 2.85: Loss = 0.596054
Epoch 2.86: Loss = 0.499008
Epoch 2.87: Loss = 0.379471
Epoch 2.88: Loss = 0.44278
Epoch 2.89: Loss = 0.528015
Epoch 2.90: Loss = 0.410782
Epoch 2.91: Loss = 0.51091
Epoch 2.92: Loss = 0.469498
Epoch 2.93: Loss = 0.555984
Epoch 2.94: Loss = 0.367249
Epoch 2.95: Loss = 0.467133
Epoch 2.96: Loss = 0.507965
Epoch 2.97: Loss = 0.369019
Epoch 2.98: Loss = 0.411972
Epoch 2.99: Loss = 0.537567
Epoch 2.100: Loss = 0.636627
Epoch 2.101: Loss = 0.576385
Epoch 2.102: Loss = 0.45166
Epoch 2.103: Loss = 0.437469
Epoch 2.104: Loss = 0.389999
Epoch 2.105: Loss = 0.554474
Epoch 2.106: Loss = 0.53508
Epoch 2.107: Loss = 0.389923
Epoch 2.108: Loss = 0.449158
Epoch 2.109: Loss = 0.401672
Epoch 2.110: Loss = 0.4767
Epoch 2.111: Loss = 0.354797
Epoch 2.112: Loss = 0.3508
Epoch 2.113: Loss = 0.408005
Epoch 2.114: Loss = 0.358047
Epoch 2.115: Loss = 0.385223
Epoch 2.116: Loss = 0.388046
Epoch 2.117: Loss = 0.294037
Epoch 2.118: Loss = 0.248245
Epoch 2.119: Loss = 0.31636
Epoch 2.120: Loss = 0.31955
TRAIN LOSS = 0.487839
TRAIN ACC = 85.1334 % (51082/60000)
Loss = 0.419846
Loss = 0.486954
Loss = 0.581467
Loss = 0.564987
Loss = 0.599396
Loss = 0.446945
Loss = 0.41301
Loss = 0.606903
Loss = 0.55188
Loss = 0.521057
Loss = 0.219315
Loss = 0.334457
Loss = 0.284378
Loss = 0.446411
Loss = 0.27388
Loss = 0.336929
Loss = 0.27034
Loss = 0.108856
Loss = 0.278442
Loss = 0.552124
TEST LOSS = 0.414879
TEST ACC = 510.818 % (8742/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.418365
Epoch 3.2: Loss = 0.543823
Epoch 3.3: Loss = 0.512039
Epoch 3.4: Loss = 0.366257
Epoch 3.5: Loss = 0.407379
Epoch 3.6: Loss = 0.391693
Epoch 3.7: Loss = 0.383209
Epoch 3.8: Loss = 0.382095
Epoch 3.9: Loss = 0.398941
Epoch 3.10: Loss = 0.418839
Epoch 3.11: Loss = 0.421738
Epoch 3.12: Loss = 0.408508
Epoch 3.13: Loss = 0.348267
Epoch 3.14: Loss = 0.378342
Epoch 3.15: Loss = 0.479126
Epoch 3.16: Loss = 0.490417
Epoch 3.17: Loss = 0.488434
Epoch 3.18: Loss = 0.585587
Epoch 3.19: Loss = 0.422089
Epoch 3.20: Loss = 0.403732
Epoch 3.21: Loss = 0.338318
Epoch 3.22: Loss = 0.35289
Epoch 3.23: Loss = 0.354965
Epoch 3.24: Loss = 0.594208
Epoch 3.25: Loss = 0.441101
Epoch 3.26: Loss = 0.572281
Epoch 3.27: Loss = 0.505051
Epoch 3.28: Loss = 0.486954
Epoch 3.29: Loss = 0.522888
Epoch 3.30: Loss = 0.577072
Epoch 3.31: Loss = 0.386246
Epoch 3.32: Loss = 0.529602
Epoch 3.33: Loss = 0.393936
Epoch 3.34: Loss = 0.496735
Epoch 3.35: Loss = 0.442505
Epoch 3.36: Loss = 0.524277
Epoch 3.37: Loss = 0.315948
Epoch 3.38: Loss = 0.368301
Epoch 3.39: Loss = 0.383865
Epoch 3.40: Loss = 0.368652
Epoch 3.41: Loss = 0.420761
Epoch 3.42: Loss = 0.541321
Epoch 3.43: Loss = 0.372375
Epoch 3.44: Loss = 0.311844
Epoch 3.45: Loss = 0.449432
Epoch 3.46: Loss = 0.464737
Epoch 3.47: Loss = 0.393982
Epoch 3.48: Loss = 0.428696
Epoch 3.49: Loss = 0.44426
Epoch 3.50: Loss = 0.491043
Epoch 3.51: Loss = 0.340256
Epoch 3.52: Loss = 0.334
Epoch 3.53: Loss = 0.402084
Epoch 3.54: Loss = 0.510498
Epoch 3.55: Loss = 0.42157
Epoch 3.56: Loss = 0.373749
Epoch 3.57: Loss = 0.370865
Epoch 3.58: Loss = 0.430756
Epoch 3.59: Loss = 0.486526
Epoch 3.60: Loss = 0.503036
Epoch 3.61: Loss = 0.474869
Epoch 3.62: Loss = 0.516891
Epoch 3.63: Loss = 0.568542
Epoch 3.64: Loss = 0.496811
Epoch 3.65: Loss = 0.618423
Epoch 3.66: Loss = 0.428024
Epoch 3.67: Loss = 0.459518
Epoch 3.68: Loss = 0.263641
Epoch 3.69: Loss = 0.347076
Epoch 3.70: Loss = 0.536148
Epoch 3.71: Loss = 0.368607
Epoch 3.72: Loss = 0.372391
Epoch 3.73: Loss = 0.406219
Epoch 3.74: Loss = 0.316605
Epoch 3.75: Loss = 0.579727
Epoch 3.76: Loss = 0.435852
Epoch 3.77: Loss = 0.371552
Epoch 3.78: Loss = 0.424652
Epoch 3.79: Loss = 0.484436
Epoch 3.80: Loss = 0.467682
Epoch 3.81: Loss = 0.361893
Epoch 3.82: Loss = 0.325058
Epoch 3.83: Loss = 0.480453
Epoch 3.84: Loss = 0.40976
Epoch 3.85: Loss = 0.582703
Epoch 3.86: Loss = 0.483795
Epoch 3.87: Loss = 0.334305
Epoch 3.88: Loss = 0.398346
Epoch 3.89: Loss = 0.489609
Epoch 3.90: Loss = 0.358215
Epoch 3.91: Loss = 0.475189
Epoch 3.92: Loss = 0.448288
Epoch 3.93: Loss = 0.522339
Epoch 3.94: Loss = 0.3302
Epoch 3.95: Loss = 0.423767
Epoch 3.96: Loss = 0.479355
Epoch 3.97: Loss = 0.338531
Epoch 3.98: Loss = 0.354187
Epoch 3.99: Loss = 0.507233
Epoch 3.100: Loss = 0.585907
Epoch 3.101: Loss = 0.566498
Epoch 3.102: Loss = 0.407898
Epoch 3.103: Loss = 0.3862
Epoch 3.104: Loss = 0.366669
Epoch 3.105: Loss = 0.52977
Epoch 3.106: Loss = 0.508347
Epoch 3.107: Loss = 0.335571
Epoch 3.108: Loss = 0.422897
Epoch 3.109: Loss = 0.36795
Epoch 3.110: Loss = 0.457443
Epoch 3.111: Loss = 0.329483
Epoch 3.112: Loss = 0.31958
Epoch 3.113: Loss = 0.382645
Epoch 3.114: Loss = 0.293198
Epoch 3.115: Loss = 0.342484
Epoch 3.116: Loss = 0.377014
Epoch 3.117: Loss = 0.265076
Epoch 3.118: Loss = 0.213791
Epoch 3.119: Loss = 0.301712
Epoch 3.120: Loss = 0.306824
TRAIN LOSS = 0.426712
TRAIN ACC = 87.3184 % (52393/60000)
Loss = 0.387619
Loss = 0.471115
Loss = 0.539841
Loss = 0.547073
Loss = 0.562607
Loss = 0.414017
Loss = 0.372833
Loss = 0.599335
Loss = 0.529129
Loss = 0.492874
Loss = 0.187057
Loss = 0.306656
Loss = 0.289078
Loss = 0.414459
Loss = 0.223434
Loss = 0.32753
Loss = 0.229111
Loss = 0.0809021
Loss = 0.252136
Loss = 0.543793
TEST LOSS = 0.38853
TEST ACC = 523.929 % (8892/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.380295
Epoch 4.2: Loss = 0.507004
Epoch 4.3: Loss = 0.482376
Epoch 4.4: Loss = 0.347717
Epoch 4.5: Loss = 0.372055
Epoch 4.6: Loss = 0.372665
Epoch 4.7: Loss = 0.336853
Epoch 4.8: Loss = 0.351456
Epoch 4.9: Loss = 0.354996
Epoch 4.10: Loss = 0.391022
Epoch 4.11: Loss = 0.413849
Epoch 4.12: Loss = 0.379715
Epoch 4.13: Loss = 0.307861
Epoch 4.14: Loss = 0.344849
Epoch 4.15: Loss = 0.428268
Epoch 4.16: Loss = 0.456314
Epoch 4.17: Loss = 0.471832
Epoch 4.18: Loss = 0.55864
Epoch 4.19: Loss = 0.409988
Epoch 4.20: Loss = 0.367706
Epoch 4.21: Loss = 0.316452
Epoch 4.22: Loss = 0.325317
Epoch 4.23: Loss = 0.34845
Epoch 4.24: Loss = 0.556396
Epoch 4.25: Loss = 0.410492
Epoch 4.26: Loss = 0.540985
Epoch 4.27: Loss = 0.494171
Epoch 4.28: Loss = 0.467346
Epoch 4.29: Loss = 0.494019
Epoch 4.30: Loss = 0.552444
Epoch 4.31: Loss = 0.372635
Epoch 4.32: Loss = 0.497498
Epoch 4.33: Loss = 0.353546
Epoch 4.34: Loss = 0.466553
Epoch 4.35: Loss = 0.419098
Epoch 4.36: Loss = 0.493362
Epoch 4.37: Loss = 0.297867
Epoch 4.38: Loss = 0.338821
Epoch 4.39: Loss = 0.364105
Epoch 4.40: Loss = 0.340103
Epoch 4.41: Loss = 0.380325
Epoch 4.42: Loss = 0.566574
Epoch 4.43: Loss = 0.350861
Epoch 4.44: Loss = 0.300491
Epoch 4.45: Loss = 0.406448
Epoch 4.46: Loss = 0.45195
Epoch 4.47: Loss = 0.361374
Epoch 4.48: Loss = 0.427933
Epoch 4.49: Loss = 0.404236
Epoch 4.50: Loss = 0.504852
Epoch 4.51: Loss = 0.318375
Epoch 4.52: Loss = 0.330551
Epoch 4.53: Loss = 0.380554
Epoch 4.54: Loss = 0.500885
Epoch 4.55: Loss = 0.387634
Epoch 4.56: Loss = 0.362411
Epoch 4.57: Loss = 0.362869
Epoch 4.58: Loss = 0.416534
Epoch 4.59: Loss = 0.479477
Epoch 4.60: Loss = 0.488937
Epoch 4.61: Loss = 0.431244
Epoch 4.62: Loss = 0.498352
Epoch 4.63: Loss = 0.561157
Epoch 4.64: Loss = 0.487
Epoch 4.65: Loss = 0.618423
Epoch 4.66: Loss = 0.404129
Epoch 4.67: Loss = 0.433136
Epoch 4.68: Loss = 0.235687
Epoch 4.69: Loss = 0.327942
Epoch 4.70: Loss = 0.533295
Epoch 4.71: Loss = 0.349991
Epoch 4.72: Loss = 0.331863
Epoch 4.73: Loss = 0.37085
Epoch 4.74: Loss = 0.294418
Epoch 4.75: Loss = 0.595383
Epoch 4.76: Loss = 0.425934
Epoch 4.77: Loss = 0.350128
Epoch 4.78: Loss = 0.411575
Epoch 4.79: Loss = 0.484238
Epoch 4.80: Loss = 0.444351
Epoch 4.81: Loss = 0.335114
Epoch 4.82: Loss = 0.310974
Epoch 4.83: Loss = 0.461502
Epoch 4.84: Loss = 0.396255
Epoch 4.85: Loss = 0.565109
Epoch 4.86: Loss = 0.483246
Epoch 4.87: Loss = 0.315201
Epoch 4.88: Loss = 0.389954
Epoch 4.89: Loss = 0.473007
Epoch 4.90: Loss = 0.331619
Epoch 4.91: Loss = 0.469635
Epoch 4.92: Loss = 0.437347
Epoch 4.93: Loss = 0.537903
Epoch 4.94: Loss = 0.312088
Epoch 4.95: Loss = 0.39122
Epoch 4.96: Loss = 0.462067
Epoch 4.97: Loss = 0.314026
Epoch 4.98: Loss = 0.351761
Epoch 4.99: Loss = 0.492737
Epoch 4.100: Loss = 0.56662
Epoch 4.101: Loss = 0.551285
Epoch 4.102: Loss = 0.401001
Epoch 4.103: Loss = 0.361984
Epoch 4.104: Loss = 0.349899
Epoch 4.105: Loss = 0.508987
Epoch 4.106: Loss = 0.524887
Epoch 4.107: Loss = 0.331573
Epoch 4.108: Loss = 0.417755
Epoch 4.109: Loss = 0.352829
Epoch 4.110: Loss = 0.437851
Epoch 4.111: Loss = 0.313522
Epoch 4.112: Loss = 0.321884
Epoch 4.113: Loss = 0.368469
Epoch 4.114: Loss = 0.280746
Epoch 4.115: Loss = 0.302032
Epoch 4.116: Loss = 0.368225
Epoch 4.117: Loss = 0.245438
Epoch 4.118: Loss = 0.201828
Epoch 4.119: Loss = 0.293427
Epoch 4.120: Loss = 0.309479
TRAIN LOSS = 0.407486
TRAIN ACC = 88.3606 % (53018/60000)
Loss = 0.36731
Loss = 0.466599
Loss = 0.528442
Loss = 0.551239
Loss = 0.568268
Loss = 0.400513
Loss = 0.338394
Loss = 0.611084
Loss = 0.530701
Loss = 0.468704
Loss = 0.17659
Loss = 0.276733
Loss = 0.312439
Loss = 0.396851
Loss = 0.212097
Loss = 0.30957
Loss = 0.218262
Loss = 0.0654144
Loss = 0.23349
Loss = 0.555786
TEST LOSS = 0.379424
TEST ACC = 530.179 % (8941/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.367142
Epoch 5.2: Loss = 0.485931
Epoch 5.3: Loss = 0.463364
Epoch 5.4: Loss = 0.317612
Epoch 5.5: Loss = 0.349091
Epoch 5.6: Loss = 0.344345
Epoch 5.7: Loss = 0.307877
Epoch 5.8: Loss = 0.33992
Epoch 5.9: Loss = 0.356827
Epoch 5.10: Loss = 0.37674
Epoch 5.11: Loss = 0.403412
Epoch 5.12: Loss = 0.382553
Epoch 5.13: Loss = 0.281769
Epoch 5.14: Loss = 0.3414
Epoch 5.15: Loss = 0.413239
Epoch 5.16: Loss = 0.464737
Epoch 5.17: Loss = 0.488876
Epoch 5.18: Loss = 0.54422
Epoch 5.19: Loss = 0.435501
Epoch 5.20: Loss = 0.343201
Epoch 5.21: Loss = 0.317902
Epoch 5.22: Loss = 0.292892
Epoch 5.23: Loss = 0.345001
Epoch 5.24: Loss = 0.54924
Epoch 5.25: Loss = 0.384964
Epoch 5.26: Loss = 0.534363
Epoch 5.27: Loss = 0.498337
Epoch 5.28: Loss = 0.451553
Epoch 5.29: Loss = 0.477219
Epoch 5.30: Loss = 0.539932
Epoch 5.31: Loss = 0.367874
Epoch 5.32: Loss = 0.480652
Epoch 5.33: Loss = 0.344482
Epoch 5.34: Loss = 0.44783
Epoch 5.35: Loss = 0.388031
Epoch 5.36: Loss = 0.494217
Epoch 5.37: Loss = 0.270645
Epoch 5.38: Loss = 0.352859
Epoch 5.39: Loss = 0.350998
Epoch 5.40: Loss = 0.338303
Epoch 5.41: Loss = 0.363815
Epoch 5.42: Loss = 0.576645
Epoch 5.43: Loss = 0.326691
Epoch 5.44: Loss = 0.291748
Epoch 5.45: Loss = 0.388947
Epoch 5.46: Loss = 0.444656
Epoch 5.47: Loss = 0.369949
Epoch 5.48: Loss = 0.415573
Epoch 5.49: Loss = 0.412201
Epoch 5.50: Loss = 0.500565
Epoch 5.51: Loss = 0.321152
Epoch 5.52: Loss = 0.298508
Epoch 5.53: Loss = 0.364059
Epoch 5.54: Loss = 0.507751
Epoch 5.55: Loss = 0.391434
Epoch 5.56: Loss = 0.345444
Epoch 5.57: Loss = 0.356689
Epoch 5.58: Loss = 0.428223
Epoch 5.59: Loss = 0.47522
Epoch 5.60: Loss = 0.488922
Epoch 5.61: Loss = 0.424667
Epoch 5.62: Loss = 0.486176
Epoch 5.63: Loss = 0.553299
Epoch 5.64: Loss = 0.484451
Epoch 5.65: Loss = 0.611877
Epoch 5.66: Loss = 0.400864
Epoch 5.67: Loss = 0.401993
Epoch 5.68: Loss = 0.22406
Epoch 5.69: Loss = 0.315094
Epoch 5.70: Loss = 0.526413
Epoch 5.71: Loss = 0.335678
Epoch 5.72: Loss = 0.319
Epoch 5.73: Loss = 0.368256
Epoch 5.74: Loss = 0.299149
Epoch 5.75: Loss = 0.603256
Epoch 5.76: Loss = 0.429321
Epoch 5.77: Loss = 0.339859
Epoch 5.78: Loss = 0.397705
Epoch 5.79: Loss = 0.476166
Epoch 5.80: Loss = 0.451492
Epoch 5.81: Loss = 0.340912
Epoch 5.82: Loss = 0.286728
Epoch 5.83: Loss = 0.459137
Epoch 5.84: Loss = 0.378677
Epoch 5.85: Loss = 0.549454
Epoch 5.86: Loss = 0.486938
Epoch 5.87: Loss = 0.297989
Epoch 5.88: Loss = 0.366455
Epoch 5.89: Loss = 0.453537
Epoch 5.90: Loss = 0.299713
Epoch 5.91: Loss = 0.4711
Epoch 5.92: Loss = 0.440002
Epoch 5.93: Loss = 0.528946
Epoch 5.94: Loss = 0.290695
Epoch 5.95: Loss = 0.392624
Epoch 5.96: Loss = 0.462112
Epoch 5.97: Loss = 0.316071
Epoch 5.98: Loss = 0.335587
Epoch 5.99: Loss = 0.472855
Epoch 5.100: Loss = 0.559097
Epoch 5.101: Loss = 0.561295
Epoch 5.102: Loss = 0.360046
Epoch 5.103: Loss = 0.349136
Epoch 5.104: Loss = 0.351837
Epoch 5.105: Loss = 0.528091
Epoch 5.106: Loss = 0.525284
Epoch 5.107: Loss = 0.30867
Epoch 5.108: Loss = 0.407333
Epoch 5.109: Loss = 0.345932
Epoch 5.110: Loss = 0.442886
Epoch 5.111: Loss = 0.33284
Epoch 5.112: Loss = 0.307251
Epoch 5.113: Loss = 0.352081
Epoch 5.114: Loss = 0.282211
Epoch 5.115: Loss = 0.285385
Epoch 5.116: Loss = 0.360718
Epoch 5.117: Loss = 0.237488
Epoch 5.118: Loss = 0.185867
Epoch 5.119: Loss = 0.301529
Epoch 5.120: Loss = 0.320557
TRAIN LOSS = 0.399048
TRAIN ACC = 88.9908 % (53396/60000)
Loss = 0.340073
Loss = 0.462524
Loss = 0.518509
Loss = 0.552856
Loss = 0.557831
Loss = 0.375916
Loss = 0.315994
Loss = 0.593521
Loss = 0.51297
Loss = 0.442123
Loss = 0.181168
Loss = 0.257339
Loss = 0.310623
Loss = 0.375412
Loss = 0.1716
Loss = 0.312469
Loss = 0.210632
Loss = 0.062027
Loss = 0.236832
Loss = 0.526764
TEST LOSS = 0.365859
TEST ACC = 533.958 % (9015/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.338791
Epoch 6.2: Loss = 0.479187
Epoch 6.3: Loss = 0.466522
Epoch 6.4: Loss = 0.309769
Epoch 6.5: Loss = 0.331131
Epoch 6.6: Loss = 0.360626
Epoch 6.7: Loss = 0.305923
Epoch 6.8: Loss = 0.329834
Epoch 6.9: Loss = 0.343658
Epoch 6.10: Loss = 0.356674
Epoch 6.11: Loss = 0.403458
Epoch 6.12: Loss = 0.365097
Epoch 6.13: Loss = 0.281265
Epoch 6.14: Loss = 0.322937
Epoch 6.15: Loss = 0.405884
Epoch 6.16: Loss = 0.454636
Epoch 6.17: Loss = 0.466843
Epoch 6.18: Loss = 0.543701
Epoch 6.19: Loss = 0.425949
Epoch 6.20: Loss = 0.339142
Epoch 6.21: Loss = 0.315399
Epoch 6.22: Loss = 0.27803
Epoch 6.23: Loss = 0.348297
Epoch 6.24: Loss = 0.582382
Epoch 6.25: Loss = 0.368973
Epoch 6.26: Loss = 0.512619
Epoch 6.27: Loss = 0.477066
Epoch 6.28: Loss = 0.440247
Epoch 6.29: Loss = 0.486969
Epoch 6.30: Loss = 0.521393
Epoch 6.31: Loss = 0.340195
Epoch 6.32: Loss = 0.435776
Epoch 6.33: Loss = 0.317657
Epoch 6.34: Loss = 0.435364
Epoch 6.35: Loss = 0.373459
Epoch 6.36: Loss = 0.47023
Epoch 6.37: Loss = 0.255081
Epoch 6.38: Loss = 0.343658
Epoch 6.39: Loss = 0.341751
Epoch 6.40: Loss = 0.343369
Epoch 6.41: Loss = 0.343369
Epoch 6.42: Loss = 0.576996
Epoch 6.43: Loss = 0.333221
Epoch 6.44: Loss = 0.264633
Epoch 6.45: Loss = 0.345032
Epoch 6.46: Loss = 0.444275
Epoch 6.47: Loss = 0.361237
Epoch 6.48: Loss = 0.413177
Epoch 6.49: Loss = 0.392365
Epoch 6.50: Loss = 0.495483
Epoch 6.51: Loss = 0.308136
Epoch 6.52: Loss = 0.290848
Epoch 6.53: Loss = 0.348892
Epoch 6.54: Loss = 0.523346
Epoch 6.55: Loss = 0.399734
Epoch 6.56: Loss = 0.361847
Epoch 6.57: Loss = 0.368332
Epoch 6.58: Loss = 0.422302
Epoch 6.59: Loss = 0.472855
Epoch 6.60: Loss = 0.482132
Epoch 6.61: Loss = 0.420975
Epoch 6.62: Loss = 0.491486
Epoch 6.63: Loss = 0.581787
Epoch 6.64: Loss = 0.480042
Epoch 6.65: Loss = 0.62529
Epoch 6.66: Loss = 0.396439
Epoch 6.67: Loss = 0.394974
Epoch 6.68: Loss = 0.21788
Epoch 6.69: Loss = 0.322281
Epoch 6.70: Loss = 0.49379
Epoch 6.71: Loss = 0.317368
Epoch 6.72: Loss = 0.301605
Epoch 6.73: Loss = 0.364227
Epoch 6.74: Loss = 0.295471
Epoch 6.75: Loss = 0.626022
Epoch 6.76: Loss = 0.421921
Epoch 6.77: Loss = 0.328018
Epoch 6.78: Loss = 0.402008
Epoch 6.79: Loss = 0.456802
Epoch 6.80: Loss = 0.433365
Epoch 6.81: Loss = 0.340942
Epoch 6.82: Loss = 0.286285
Epoch 6.83: Loss = 0.463867
Epoch 6.84: Loss = 0.394379
Epoch 6.85: Loss = 0.540466
Epoch 6.86: Loss = 0.504517
Epoch 6.87: Loss = 0.303604
Epoch 6.88: Loss = 0.392578
Epoch 6.89: Loss = 0.444153
Epoch 6.90: Loss = 0.314606
Epoch 6.91: Loss = 0.46698
Epoch 6.92: Loss = 0.465469
Epoch 6.93: Loss = 0.521469
Epoch 6.94: Loss = 0.284027
Epoch 6.95: Loss = 0.400375
Epoch 6.96: Loss = 0.436646
Epoch 6.97: Loss = 0.294678
Epoch 6.98: Loss = 0.329742
Epoch 6.99: Loss = 0.467285
Epoch 6.100: Loss = 0.578659
Epoch 6.101: Loss = 0.58609
Epoch 6.102: Loss = 0.362381
Epoch 6.103: Loss = 0.344391
Epoch 6.104: Loss = 0.352509
Epoch 6.105: Loss = 0.507721
Epoch 6.106: Loss = 0.535645
Epoch 6.107: Loss = 0.316086
Epoch 6.108: Loss = 0.422119
Epoch 6.109: Loss = 0.352829
Epoch 6.110: Loss = 0.441162
Epoch 6.111: Loss = 0.327972
Epoch 6.112: Loss = 0.313019
Epoch 6.113: Loss = 0.365448
Epoch 6.114: Loss = 0.278351
Epoch 6.115: Loss = 0.283356
Epoch 6.116: Loss = 0.361786
Epoch 6.117: Loss = 0.228516
Epoch 6.118: Loss = 0.191498
Epoch 6.119: Loss = 0.264481
Epoch 6.120: Loss = 0.345322
TRAIN LOSS = 0.394577
TRAIN ACC = 89.4958 % (53700/60000)
Loss = 0.354843
Loss = 0.473312
Loss = 0.535095
Loss = 0.532013
Loss = 0.561752
Loss = 0.372726
Loss = 0.320404
Loss = 0.601501
Loss = 0.526871
Loss = 0.437744
Loss = 0.175339
Loss = 0.277023
Loss = 0.303848
Loss = 0.364243
Loss = 0.18541
Loss = 0.29567
Loss = 0.19722
Loss = 0.0578461
Loss = 0.233994
Loss = 0.528046
TEST LOSS = 0.366745
TEST ACC = 537 % (9043/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.336853
Epoch 7.2: Loss = 0.479828
Epoch 7.3: Loss = 0.48735
Epoch 7.4: Loss = 0.308945
Epoch 7.5: Loss = 0.328629
Epoch 7.6: Loss = 0.360107
Epoch 7.7: Loss = 0.300827
Epoch 7.8: Loss = 0.323364
Epoch 7.9: Loss = 0.326263
Epoch 7.10: Loss = 0.341537
Epoch 7.11: Loss = 0.408554
Epoch 7.12: Loss = 0.356842
Epoch 7.13: Loss = 0.282349
Epoch 7.14: Loss = 0.314651
Epoch 7.15: Loss = 0.400421
Epoch 7.16: Loss = 0.445786
Epoch 7.17: Loss = 0.443863
Epoch 7.18: Loss = 0.581467
Epoch 7.19: Loss = 0.434601
Epoch 7.20: Loss = 0.312164
Epoch 7.21: Loss = 0.327652
Epoch 7.22: Loss = 0.268066
Epoch 7.23: Loss = 0.319427
Epoch 7.24: Loss = 0.58786
Epoch 7.25: Loss = 0.389557
Epoch 7.26: Loss = 0.547318
Epoch 7.27: Loss = 0.483337
Epoch 7.28: Loss = 0.455353
Epoch 7.29: Loss = 0.505966
Epoch 7.30: Loss = 0.520401
Epoch 7.31: Loss = 0.354385
Epoch 7.32: Loss = 0.418259
Epoch 7.33: Loss = 0.318619
Epoch 7.34: Loss = 0.448944
Epoch 7.35: Loss = 0.372131
Epoch 7.36: Loss = 0.437729
Epoch 7.37: Loss = 0.258133
Epoch 7.38: Loss = 0.335831
Epoch 7.39: Loss = 0.332062
Epoch 7.40: Loss = 0.33551
Epoch 7.41: Loss = 0.360001
Epoch 7.42: Loss = 0.596481
Epoch 7.43: Loss = 0.347046
Epoch 7.44: Loss = 0.271713
Epoch 7.45: Loss = 0.366501
Epoch 7.46: Loss = 0.42012
Epoch 7.47: Loss = 0.380859
Epoch 7.48: Loss = 0.403976
Epoch 7.49: Loss = 0.396698
Epoch 7.50: Loss = 0.519089
Epoch 7.51: Loss = 0.302719
Epoch 7.52: Loss = 0.298889
Epoch 7.53: Loss = 0.362122
Epoch 7.54: Loss = 0.52597
Epoch 7.55: Loss = 0.394424
Epoch 7.56: Loss = 0.359222
Epoch 7.57: Loss = 0.366974
Epoch 7.58: Loss = 0.397919
Epoch 7.59: Loss = 0.458237
Epoch 7.60: Loss = 0.499374
Epoch 7.61: Loss = 0.393906
Epoch 7.62: Loss = 0.480423
Epoch 7.63: Loss = 0.577301
Epoch 7.64: Loss = 0.480927
Epoch 7.65: Loss = 0.62149
Epoch 7.66: Loss = 0.39061
Epoch 7.67: Loss = 0.376617
Epoch 7.68: Loss = 0.222443
Epoch 7.69: Loss = 0.311539
Epoch 7.70: Loss = 0.499802
Epoch 7.71: Loss = 0.331635
Epoch 7.72: Loss = 0.300446
Epoch 7.73: Loss = 0.350449
Epoch 7.74: Loss = 0.297653
Epoch 7.75: Loss = 0.64473
Epoch 7.76: Loss = 0.416031
Epoch 7.77: Loss = 0.304306
Epoch 7.78: Loss = 0.387009
Epoch 7.79: Loss = 0.475723
Epoch 7.80: Loss = 0.416809
Epoch 7.81: Loss = 0.322739
Epoch 7.82: Loss = 0.297867
Epoch 7.83: Loss = 0.437119
Epoch 7.84: Loss = 0.376434
Epoch 7.85: Loss = 0.544388
Epoch 7.86: Loss = 0.514893
Epoch 7.87: Loss = 0.287521
Epoch 7.88: Loss = 0.386902
Epoch 7.89: Loss = 0.448959
Epoch 7.90: Loss = 0.299896
Epoch 7.91: Loss = 0.452744
Epoch 7.92: Loss = 0.460968
Epoch 7.93: Loss = 0.528381
Epoch 7.94: Loss = 0.266846
Epoch 7.95: Loss = 0.38916
Epoch 7.96: Loss = 0.444656
Epoch 7.97: Loss = 0.325073
Epoch 7.98: Loss = 0.340622
Epoch 7.99: Loss = 0.460541
Epoch 7.100: Loss = 0.591766
Epoch 7.101: Loss = 0.591339
Epoch 7.102: Loss = 0.367279
Epoch 7.103: Loss = 0.370026
Epoch 7.104: Loss = 0.355286
Epoch 7.105: Loss = 0.487106
Epoch 7.106: Loss = 0.521683
Epoch 7.107: Loss = 0.312042
Epoch 7.108: Loss = 0.413055
Epoch 7.109: Loss = 0.371277
Epoch 7.110: Loss = 0.433075
Epoch 7.111: Loss = 0.326096
Epoch 7.112: Loss = 0.294693
Epoch 7.113: Loss = 0.357773
Epoch 7.114: Loss = 0.266388
Epoch 7.115: Loss = 0.259949
Epoch 7.116: Loss = 0.355484
Epoch 7.117: Loss = 0.205444
Epoch 7.118: Loss = 0.183975
Epoch 7.119: Loss = 0.266998
Epoch 7.120: Loss = 0.338409
TRAIN LOSS = 0.392944
TRAIN ACC = 89.7964 % (53881/60000)
Loss = 0.351349
Loss = 0.469055
Loss = 0.544205
Loss = 0.546402
Loss = 0.553619
Loss = 0.386673
Loss = 0.335129
Loss = 0.613708
Loss = 0.539185
Loss = 0.445862
Loss = 0.17836
Loss = 0.286209
Loss = 0.321732
Loss = 0.378876
Loss = 0.190247
Loss = 0.288162
Loss = 0.212524
Loss = 0.063797
Loss = 0.243805
Loss = 0.508728
TEST LOSS = 0.372881
TEST ACC = 538.809 % (9042/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.331253
Epoch 8.2: Loss = 0.48082
Epoch 8.3: Loss = 0.505264
Epoch 8.4: Loss = 0.323166
Epoch 8.5: Loss = 0.345886
Epoch 8.6: Loss = 0.358688
Epoch 8.7: Loss = 0.305817
Epoch 8.8: Loss = 0.318756
Epoch 8.9: Loss = 0.347366
Epoch 8.10: Loss = 0.338913
Epoch 8.11: Loss = 0.390884
Epoch 8.12: Loss = 0.35054
Epoch 8.13: Loss = 0.26799
Epoch 8.14: Loss = 0.322708
Epoch 8.15: Loss = 0.406738
Epoch 8.16: Loss = 0.437698
Epoch 8.17: Loss = 0.470596
Epoch 8.18: Loss = 0.610886
Epoch 8.19: Loss = 0.443268
Epoch 8.20: Loss = 0.328644
Epoch 8.21: Loss = 0.32045
Epoch 8.22: Loss = 0.287415
Epoch 8.23: Loss = 0.317734
Epoch 8.24: Loss = 0.613083
Epoch 8.25: Loss = 0.399841
Epoch 8.26: Loss = 0.509872
Epoch 8.27: Loss = 0.474762
Epoch 8.28: Loss = 0.464294
Epoch 8.29: Loss = 0.507904
Epoch 8.30: Loss = 0.526764
Epoch 8.31: Loss = 0.354858
Epoch 8.32: Loss = 0.409973
Epoch 8.33: Loss = 0.307144
Epoch 8.34: Loss = 0.44725
Epoch 8.35: Loss = 0.378067
Epoch 8.36: Loss = 0.434311
Epoch 8.37: Loss = 0.273544
Epoch 8.38: Loss = 0.346222
Epoch 8.39: Loss = 0.350098
Epoch 8.40: Loss = 0.35527
Epoch 8.41: Loss = 0.361542
Epoch 8.42: Loss = 0.591873
Epoch 8.43: Loss = 0.348053
Epoch 8.44: Loss = 0.286743
Epoch 8.45: Loss = 0.358322
Epoch 8.46: Loss = 0.420105
Epoch 8.47: Loss = 0.37326
Epoch 8.48: Loss = 0.414886
Epoch 8.49: Loss = 0.397751
Epoch 8.50: Loss = 0.527008
Epoch 8.51: Loss = 0.315948
Epoch 8.52: Loss = 0.300247
Epoch 8.53: Loss = 0.35257
Epoch 8.54: Loss = 0.527145
Epoch 8.55: Loss = 0.389221
Epoch 8.56: Loss = 0.35112
Epoch 8.57: Loss = 0.364273
Epoch 8.58: Loss = 0.369446
Epoch 8.59: Loss = 0.444885
Epoch 8.60: Loss = 0.514069
Epoch 8.61: Loss = 0.394226
Epoch 8.62: Loss = 0.472748
Epoch 8.63: Loss = 0.574646
Epoch 8.64: Loss = 0.479416
Epoch 8.65: Loss = 0.614792
Epoch 8.66: Loss = 0.371994
Epoch 8.67: Loss = 0.386566
Epoch 8.68: Loss = 0.219269
Epoch 8.69: Loss = 0.321793
Epoch 8.70: Loss = 0.514938
Epoch 8.71: Loss = 0.327148
Epoch 8.72: Loss = 0.276672
Epoch 8.73: Loss = 0.380447
Epoch 8.74: Loss = 0.305145
Epoch 8.75: Loss = 0.669342
Epoch 8.76: Loss = 0.424606
Epoch 8.77: Loss = 0.300415
Epoch 8.78: Loss = 0.389679
Epoch 8.79: Loss = 0.484039
Epoch 8.80: Loss = 0.438843
Epoch 8.81: Loss = 0.342361
Epoch 8.82: Loss = 0.29982
Epoch 8.83: Loss = 0.458328
Epoch 8.84: Loss = 0.395309
Epoch 8.85: Loss = 0.533783
Epoch 8.86: Loss = 0.509567
Epoch 8.87: Loss = 0.288818
Epoch 8.88: Loss = 0.390732
Epoch 8.89: Loss = 0.462845
Epoch 8.90: Loss = 0.303223
Epoch 8.91: Loss = 0.446548
Epoch 8.92: Loss = 0.465027
Epoch 8.93: Loss = 0.526779
Epoch 8.94: Loss = 0.24852
Epoch 8.95: Loss = 0.394302
Epoch 8.96: Loss = 0.451385
Epoch 8.97: Loss = 0.314514
Epoch 8.98: Loss = 0.359268
Epoch 8.99: Loss = 0.472549
Epoch 8.100: Loss = 0.610428
Epoch 8.101: Loss = 0.59523
Epoch 8.102: Loss = 0.361374
Epoch 8.103: Loss = 0.333481
Epoch 8.104: Loss = 0.350708
Epoch 8.105: Loss = 0.496155
Epoch 8.106: Loss = 0.544296
Epoch 8.107: Loss = 0.315018
Epoch 8.108: Loss = 0.417862
Epoch 8.109: Loss = 0.370087
Epoch 8.110: Loss = 0.421936
Epoch 8.111: Loss = 0.334213
Epoch 8.112: Loss = 0.31041
Epoch 8.113: Loss = 0.355637
Epoch 8.114: Loss = 0.272919
Epoch 8.115: Loss = 0.269714
Epoch 8.116: Loss = 0.336578
Epoch 8.117: Loss = 0.208099
Epoch 8.118: Loss = 0.178055
Epoch 8.119: Loss = 0.267761
Epoch 8.120: Loss = 0.361603
TRAIN LOSS = 0.395782
TRAIN ACC = 89.8346 % (53904/60000)
Loss = 0.362518
Loss = 0.449524
Loss = 0.523636
Loss = 0.562698
Loss = 0.580841
Loss = 0.376373
Loss = 0.324554
Loss = 0.622513
Loss = 0.558044
Loss = 0.462677
Loss = 0.194778
Loss = 0.26825
Loss = 0.331757
Loss = 0.392105
Loss = 0.168411
Loss = 0.282959
Loss = 0.227768
Loss = 0.0633087
Loss = 0.242004
Loss = 0.545914
TEST LOSS = 0.377032
TEST ACC = 539.04 % (9052/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.328949
Epoch 9.2: Loss = 0.490097
Epoch 9.3: Loss = 0.491882
Epoch 9.4: Loss = 0.328262
Epoch 9.5: Loss = 0.329666
Epoch 9.6: Loss = 0.359528
Epoch 9.7: Loss = 0.287872
Epoch 9.8: Loss = 0.314468
Epoch 9.9: Loss = 0.35289
Epoch 9.10: Loss = 0.350418
Epoch 9.11: Loss = 0.401215
Epoch 9.12: Loss = 0.351532
Epoch 9.13: Loss = 0.263901
Epoch 9.14: Loss = 0.321701
Epoch 9.15: Loss = 0.411011
Epoch 9.16: Loss = 0.467346
Epoch 9.17: Loss = 0.456314
Epoch 9.18: Loss = 0.603867
Epoch 9.19: Loss = 0.428604
Epoch 9.20: Loss = 0.304291
Epoch 9.21: Loss = 0.315735
Epoch 9.22: Loss = 0.294327
Epoch 9.23: Loss = 0.291931
Epoch 9.24: Loss = 0.603912
Epoch 9.25: Loss = 0.411835
Epoch 9.26: Loss = 0.525848
Epoch 9.27: Loss = 0.455658
Epoch 9.28: Loss = 0.442291
Epoch 9.29: Loss = 0.508163
Epoch 9.30: Loss = 0.552582
Epoch 9.31: Loss = 0.367462
Epoch 9.32: Loss = 0.408707
Epoch 9.33: Loss = 0.304932
Epoch 9.34: Loss = 0.460526
Epoch 9.35: Loss = 0.361771
Epoch 9.36: Loss = 0.411667
Epoch 9.37: Loss = 0.26207
Epoch 9.38: Loss = 0.34996
Epoch 9.39: Loss = 0.327362
Epoch 9.40: Loss = 0.34491
Epoch 9.41: Loss = 0.358719
Epoch 9.42: Loss = 0.618546
Epoch 9.43: Loss = 0.336166
Epoch 9.44: Loss = 0.296127
Epoch 9.45: Loss = 0.3582
Epoch 9.46: Loss = 0.439209
Epoch 9.47: Loss = 0.365189
Epoch 9.48: Loss = 0.398804
Epoch 9.49: Loss = 0.414459
Epoch 9.50: Loss = 0.572357
Epoch 9.51: Loss = 0.332443
Epoch 9.52: Loss = 0.291946
Epoch 9.53: Loss = 0.363113
Epoch 9.54: Loss = 0.50975
Epoch 9.55: Loss = 0.375793
Epoch 9.56: Loss = 0.333115
Epoch 9.57: Loss = 0.360199
Epoch 9.58: Loss = 0.385666
Epoch 9.59: Loss = 0.421204
Epoch 9.60: Loss = 0.504227
Epoch 9.61: Loss = 0.412796
Epoch 9.62: Loss = 0.481827
Epoch 9.63: Loss = 0.582718
Epoch 9.64: Loss = 0.471268
Epoch 9.65: Loss = 0.625229
Epoch 9.66: Loss = 0.356628
Epoch 9.67: Loss = 0.385117
Epoch 9.68: Loss = 0.220291
Epoch 9.69: Loss = 0.320709
Epoch 9.70: Loss = 0.49791
Epoch 9.71: Loss = 0.328568
Epoch 9.72: Loss = 0.281891
Epoch 9.73: Loss = 0.426529
Epoch 9.74: Loss = 0.305603
Epoch 9.75: Loss = 0.628448
Epoch 9.76: Loss = 0.416656
Epoch 9.77: Loss = 0.309006
Epoch 9.78: Loss = 0.400314
Epoch 9.79: Loss = 0.489532
Epoch 9.80: Loss = 0.438019
Epoch 9.81: Loss = 0.319107
Epoch 9.82: Loss = 0.31076
Epoch 9.83: Loss = 0.467102
Epoch 9.84: Loss = 0.424057
Epoch 9.85: Loss = 0.522095
Epoch 9.86: Loss = 0.534256
Epoch 9.87: Loss = 0.306549
Epoch 9.88: Loss = 0.374603
Epoch 9.89: Loss = 0.480927
Epoch 9.90: Loss = 0.291367
Epoch 9.91: Loss = 0.451813
Epoch 9.92: Loss = 0.470642
Epoch 9.93: Loss = 0.536133
Epoch 9.94: Loss = 0.265793
Epoch 9.95: Loss = 0.420349
Epoch 9.96: Loss = 0.484772
Epoch 9.97: Loss = 0.333054
Epoch 9.98: Loss = 0.373032
Epoch 9.99: Loss = 0.47052
Epoch 9.100: Loss = 0.62648
Epoch 9.101: Loss = 0.606735
Epoch 9.102: Loss = 0.369919
Epoch 9.103: Loss = 0.357773
Epoch 9.104: Loss = 0.344635
Epoch 9.105: Loss = 0.514511
Epoch 9.106: Loss = 0.558456
Epoch 9.107: Loss = 0.306335
Epoch 9.108: Loss = 0.456253
Epoch 9.109: Loss = 0.369888
Epoch 9.110: Loss = 0.459427
Epoch 9.111: Loss = 0.320129
Epoch 9.112: Loss = 0.310654
Epoch 9.113: Loss = 0.370819
Epoch 9.114: Loss = 0.295898
Epoch 9.115: Loss = 0.279861
Epoch 9.116: Loss = 0.348679
Epoch 9.117: Loss = 0.224808
Epoch 9.118: Loss = 0.160538
Epoch 9.119: Loss = 0.279984
Epoch 9.120: Loss = 0.35289
TRAIN LOSS = 0.398422
TRAIN ACC = 90.0558 % (54036/60000)
Loss = 0.372131
Loss = 0.432129
Loss = 0.567657
Loss = 0.5569
Loss = 0.592102
Loss = 0.381256
Loss = 0.322876
Loss = 0.628143
Loss = 0.54776
Loss = 0.467087
Loss = 0.182144
Loss = 0.301575
Loss = 0.341095
Loss = 0.409103
Loss = 0.164795
Loss = 0.298782
Loss = 0.212784
Loss = 0.0648804
Loss = 0.254776
Loss = 0.552185
TEST LOSS = 0.382508
TEST ACC = 540.359 % (9042/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.345764
Epoch 10.2: Loss = 0.497711
Epoch 10.3: Loss = 0.526672
Epoch 10.4: Loss = 0.313873
Epoch 10.5: Loss = 0.354126
Epoch 10.6: Loss = 0.369278
Epoch 10.7: Loss = 0.295334
Epoch 10.8: Loss = 0.314713
Epoch 10.9: Loss = 0.353928
Epoch 10.10: Loss = 0.369934
Epoch 10.11: Loss = 0.391006
Epoch 10.12: Loss = 0.343079
Epoch 10.13: Loss = 0.285767
Epoch 10.14: Loss = 0.348816
Epoch 10.15: Loss = 0.398575
Epoch 10.16: Loss = 0.448654
Epoch 10.17: Loss = 0.469543
Epoch 10.18: Loss = 0.62677
Epoch 10.19: Loss = 0.430817
Epoch 10.20: Loss = 0.317291
Epoch 10.21: Loss = 0.326813
Epoch 10.22: Loss = 0.311905
Epoch 10.23: Loss = 0.316574
Epoch 10.24: Loss = 0.633392
Epoch 10.25: Loss = 0.408859
Epoch 10.26: Loss = 0.523132
Epoch 10.27: Loss = 0.479218
Epoch 10.28: Loss = 0.451416
Epoch 10.29: Loss = 0.541245
Epoch 10.30: Loss = 0.548691
Epoch 10.31: Loss = 0.364487
Epoch 10.32: Loss = 0.392151
Epoch 10.33: Loss = 0.30957
Epoch 10.34: Loss = 0.474823
Epoch 10.35: Loss = 0.374573
Epoch 10.36: Loss = 0.408112
Epoch 10.37: Loss = 0.273071
Epoch 10.38: Loss = 0.363159
Epoch 10.39: Loss = 0.321732
Epoch 10.40: Loss = 0.333099
Epoch 10.41: Loss = 0.324905
Epoch 10.42: Loss = 0.607437
Epoch 10.43: Loss = 0.34166
Epoch 10.44: Loss = 0.282394
Epoch 10.45: Loss = 0.362625
Epoch 10.46: Loss = 0.466187
Epoch 10.47: Loss = 0.376907
Epoch 10.48: Loss = 0.396576
Epoch 10.49: Loss = 0.38739
Epoch 10.50: Loss = 0.545883
Epoch 10.51: Loss = 0.315826
Epoch 10.52: Loss = 0.323181
Epoch 10.53: Loss = 0.353485
Epoch 10.54: Loss = 0.550552
Epoch 10.55: Loss = 0.391342
Epoch 10.56: Loss = 0.337708
Epoch 10.57: Loss = 0.379776
Epoch 10.58: Loss = 0.362
Epoch 10.59: Loss = 0.444626
Epoch 10.60: Loss = 0.498978
Epoch 10.61: Loss = 0.396072
Epoch 10.62: Loss = 0.486084
Epoch 10.63: Loss = 0.588425
Epoch 10.64: Loss = 0.483459
Epoch 10.65: Loss = 0.664261
Epoch 10.66: Loss = 0.393738
Epoch 10.67: Loss = 0.376968
Epoch 10.68: Loss = 0.215134
Epoch 10.69: Loss = 0.317841
Epoch 10.70: Loss = 0.472244
Epoch 10.71: Loss = 0.334854
Epoch 10.72: Loss = 0.313232
Epoch 10.73: Loss = 0.404922
Epoch 10.74: Loss = 0.315613
Epoch 10.75: Loss = 0.617889
Epoch 10.76: Loss = 0.433807
Epoch 10.77: Loss = 0.287384
Epoch 10.78: Loss = 0.386505
Epoch 10.79: Loss = 0.511795
Epoch 10.80: Loss = 0.415131
Epoch 10.81: Loss = 0.319977
Epoch 10.82: Loss = 0.291092
Epoch 10.83: Loss = 0.484909
Epoch 10.84: Loss = 0.416473
Epoch 10.85: Loss = 0.514679
Epoch 10.86: Loss = 0.560272
Epoch 10.87: Loss = 0.302643
Epoch 10.88: Loss = 0.388458
Epoch 10.89: Loss = 0.482193
Epoch 10.90: Loss = 0.300049
Epoch 10.91: Loss = 0.444016
Epoch 10.92: Loss = 0.472992
Epoch 10.93: Loss = 0.514328
Epoch 10.94: Loss = 0.271439
Epoch 10.95: Loss = 0.382736
Epoch 10.96: Loss = 0.482269
Epoch 10.97: Loss = 0.321442
Epoch 10.98: Loss = 0.376999
Epoch 10.99: Loss = 0.472458
Epoch 10.100: Loss = 0.596069
Epoch 10.101: Loss = 0.622147
Epoch 10.102: Loss = 0.396347
Epoch 10.103: Loss = 0.372787
Epoch 10.104: Loss = 0.349258
Epoch 10.105: Loss = 0.499756
Epoch 10.106: Loss = 0.499573
Epoch 10.107: Loss = 0.300903
Epoch 10.108: Loss = 0.448196
Epoch 10.109: Loss = 0.366409
Epoch 10.110: Loss = 0.456146
Epoch 10.111: Loss = 0.321472
Epoch 10.112: Loss = 0.317795
Epoch 10.113: Loss = 0.38118
Epoch 10.114: Loss = 0.288727
Epoch 10.115: Loss = 0.253387
Epoch 10.116: Loss = 0.366562
Epoch 10.117: Loss = 0.209991
Epoch 10.118: Loss = 0.171814
Epoch 10.119: Loss = 0.267395
Epoch 10.120: Loss = 0.361221
TRAIN LOSS = 0.400574
TRAIN ACC = 90.3366 % (54204/60000)
Loss = 0.364151
Loss = 0.430649
Loss = 0.558624
Loss = 0.569519
Loss = 0.559753
Loss = 0.376694
Loss = 0.332779
Loss = 0.600983
Loss = 0.561462
Loss = 0.441193
Loss = 0.181824
Loss = 0.295227
Loss = 0.338989
Loss = 0.387802
Loss = 0.170853
Loss = 0.278732
Loss = 0.208878
Loss = 0.0497131
Loss = 0.240341
Loss = 0.55542
TEST LOSS = 0.375179
TEST ACC = 542.039 % (9046/10000)
