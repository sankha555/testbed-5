Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 2000
Num Epochs: 10
Learning Rate: 0.4 to 0.4 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.51427
Epoch 1.2: Loss = 2.11388
Epoch 1.3: Loss = 1.84399
Epoch 1.4: Loss = 1.6626
Epoch 1.5: Loss = 1.51419
Epoch 1.6: Loss = 1.38022
Epoch 1.7: Loss = 1.28664
Epoch 1.8: Loss = 1.19649
Epoch 1.9: Loss = 1.16882
Epoch 1.10: Loss = 1.05455
Epoch 1.11: Loss = 1.09465
Epoch 1.12: Loss = 0.999268
Epoch 1.13: Loss = 0.976059
Epoch 1.14: Loss = 0.947098
Epoch 1.15: Loss = 0.908981
Epoch 1.16: Loss = 0.89801
Epoch 1.17: Loss = 0.864883
Epoch 1.18: Loss = 0.869843
Epoch 1.19: Loss = 0.857162
Epoch 1.20: Loss = 0.804367
Epoch 1.21: Loss = 0.804657
Epoch 1.22: Loss = 0.795334
Epoch 1.23: Loss = 0.808136
Epoch 1.24: Loss = 0.794876
Epoch 1.25: Loss = 0.777344
Epoch 1.26: Loss = 0.750977
Epoch 1.27: Loss = 0.808807
Epoch 1.28: Loss = 0.793457
Epoch 1.29: Loss = 0.751221
Epoch 1.30: Loss = 0.819397
TRAIN LOSS = 1.09537
TRAIN ACC = 64.4775 % (38688/60000)
Loss = 0.70549
Loss = 0.76709
Loss = 0.72258
Loss = 0.744186
Loss = 0.718048
TEST LOSS = 0.731479
TEST ACC = 386.879 % (7514/10000)
Reducing learning rate to 0.399994
Epoch 2.1: Loss = 0.721603
Epoch 2.2: Loss = 0.683243
Epoch 2.3: Loss = 0.702408
Epoch 2.4: Loss = 0.731735
Epoch 2.5: Loss = 0.703583
Epoch 2.6: Loss = 0.686234
Epoch 2.7: Loss = 0.708344
Epoch 2.8: Loss = 0.667389
Epoch 2.9: Loss = 0.675201
Epoch 2.10: Loss = 0.654694
Epoch 2.11: Loss = 0.701675
Epoch 2.12: Loss = 0.678223
Epoch 2.13: Loss = 0.685867
Epoch 2.14: Loss = 0.701614
Epoch 2.15: Loss = 0.655243
Epoch 2.16: Loss = 0.661499
Epoch 2.17: Loss = 0.682327
Epoch 2.18: Loss = 0.697617
Epoch 2.19: Loss = 0.664734
Epoch 2.20: Loss = 0.70668
Epoch 2.21: Loss = 0.643951
Epoch 2.22: Loss = 0.641586
Epoch 2.23: Loss = 0.706863
Epoch 2.24: Loss = 0.682877
Epoch 2.25: Loss = 0.651947
Epoch 2.26: Loss = 0.673676
Epoch 2.27: Loss = 0.639191
Epoch 2.28: Loss = 0.705887
Epoch 2.29: Loss = 0.606628
Epoch 2.30: Loss = 0.698013
TRAIN LOSS = 0.680695
TRAIN ACC = 76.8478 % (46111/60000)
Loss = 0.63176
Loss = 0.71022
Loss = 0.671799
Loss = 0.671982
Loss = 0.662369
TEST LOSS = 0.669626
TEST ACC = 461.11 % (7702/10000)
Reducing learning rate to 0.399994
Epoch 3.1: Loss = 0.686646
Epoch 3.2: Loss = 0.747131
Epoch 3.3: Loss = 0.610657
Epoch 3.4: Loss = 0.73082
Epoch 3.5: Loss = 0.61058
Epoch 3.6: Loss = 0.661118
Epoch 3.7: Loss = 0.608963
Epoch 3.8: Loss = 0.636063
Epoch 3.9: Loss = 0.631638
Epoch 3.10: Loss = 0.659348
Epoch 3.11: Loss = 0.641449
Epoch 3.12: Loss = 0.656464
Epoch 3.13: Loss = 0.61673
Epoch 3.14: Loss = 0.665146
Epoch 3.15: Loss = 0.631119
Epoch 3.16: Loss = 0.636612
Epoch 3.17: Loss = 0.65892
Epoch 3.18: Loss = 0.630615
Epoch 3.19: Loss = 0.608948
Epoch 3.20: Loss = 0.645599
Epoch 3.21: Loss = 0.646683
Epoch 3.22: Loss = 0.598755
Epoch 3.23: Loss = 0.603058
Epoch 3.24: Loss = 0.628494
Epoch 3.25: Loss = 0.617142
Epoch 3.26: Loss = 0.706238
Epoch 3.27: Loss = 0.636444
Epoch 3.28: Loss = 0.625229
Epoch 3.29: Loss = 0.612473
Epoch 3.30: Loss = 0.69664
TRAIN LOSS = 0.644867
TRAIN ACC = 78.833 % (47302/60000)
Loss = 0.621338
Loss = 0.696915
Loss = 0.657288
Loss = 0.653183
Loss = 0.64682
TEST LOSS = 0.655108
TEST ACC = 473.019 % (7775/10000)
Reducing learning rate to 0.399994
Epoch 4.1: Loss = 0.6091
Epoch 4.2: Loss = 0.722519
Epoch 4.3: Loss = 0.615952
Epoch 4.4: Loss = 0.653595
Epoch 4.5: Loss = 0.614807
Epoch 4.6: Loss = 0.612564
Epoch 4.7: Loss = 0.613907
Epoch 4.8: Loss = 0.659256
Epoch 4.9: Loss = 0.650543
Epoch 4.10: Loss = 0.665161
Epoch 4.11: Loss = 0.61676
Epoch 4.12: Loss = 0.698822
Epoch 4.13: Loss = 0.614532
Epoch 4.14: Loss = 0.70993
Epoch 4.15: Loss = 0.598679
Epoch 4.16: Loss = 0.722626
Epoch 4.17: Loss = 0.659698
Epoch 4.18: Loss = 0.717041
Epoch 4.19: Loss = 0.634918
Epoch 4.20: Loss = 0.667084
Epoch 4.21: Loss = 0.63475
Epoch 4.22: Loss = 0.687302
Epoch 4.23: Loss = 0.638428
Epoch 4.24: Loss = 0.701828
Epoch 4.25: Loss = 0.6577
Epoch 4.26: Loss = 0.608322
Epoch 4.27: Loss = 0.639542
Epoch 4.28: Loss = 0.63562
Epoch 4.29: Loss = 0.560684
Epoch 4.30: Loss = 0.630676
TRAIN LOSS = 0.648422
TRAIN ACC = 79.4678 % (47683/60000)
Loss = 0.588425
Loss = 0.67514
Loss = 0.644196
Loss = 0.640747
Loss = 0.628754
TEST LOSS = 0.635452
TEST ACC = 476.83 % (7903/10000)
Reducing learning rate to 0.399994
Epoch 5.1: Loss = 0.565002
Epoch 5.2: Loss = 0.575363
Epoch 5.3: Loss = 0.621353
Epoch 5.4: Loss = 0.668564
Epoch 5.5: Loss = 0.628616
Epoch 5.6: Loss = 0.648666
Epoch 5.7: Loss = 0.592163
Epoch 5.8: Loss = 0.69516
Epoch 5.9: Loss = 0.641571
Epoch 5.10: Loss = 0.656158
Epoch 5.11: Loss = 0.632706
Epoch 5.12: Loss = 0.597519
Epoch 5.13: Loss = 0.598495
Epoch 5.14: Loss = 0.641815
Epoch 5.15: Loss = 0.60762
Epoch 5.16: Loss = 0.724564
Epoch 5.17: Loss = 0.616394
Epoch 5.18: Loss = 0.629059
Epoch 5.19: Loss = 0.654083
Epoch 5.20: Loss = 0.672501
Epoch 5.21: Loss = 0.718155
Epoch 5.22: Loss = 0.694763
Epoch 5.23: Loss = 0.65181
Epoch 5.24: Loss = 0.637314
Epoch 5.25: Loss = 0.66011
Epoch 5.26: Loss = 0.651016
Epoch 5.27: Loss = 0.639877
Epoch 5.28: Loss = 0.681091
Epoch 5.29: Loss = 0.644928
Epoch 5.30: Loss = 0.557068
TRAIN LOSS = 0.640121
TRAIN ACC = 80.011 % (48009/60000)
Loss = 0.591003
Loss = 0.678146
Loss = 0.664749
Loss = 0.637558
Loss = 0.633408
TEST LOSS = 0.640973
TEST ACC = 480.089 % (7969/10000)
Reducing learning rate to 0.399994
Epoch 6.1: Loss = 0.648834
Epoch 6.2: Loss = 0.559433
Epoch 6.3: Loss = 0.640045
Epoch 6.4: Loss = 0.609909
Epoch 6.5: Loss = 0.613831
Epoch 6.6: Loss = 0.651093
Epoch 6.7: Loss = 0.630188
Epoch 6.8: Loss = 0.655319
Epoch 6.9: Loss = 0.641006
Epoch 6.10: Loss = 0.710587
Epoch 6.11: Loss = 0.70314
Epoch 6.12: Loss = 0.719879
Epoch 6.13: Loss = 0.629547
Epoch 6.14: Loss = 0.686096
Epoch 6.15: Loss = 0.660141
Epoch 6.16: Loss = 0.586853
Epoch 6.17: Loss = 0.585251
Epoch 6.18: Loss = 0.639679
Epoch 6.19: Loss = 0.587601
Epoch 6.20: Loss = 0.656952
Epoch 6.21: Loss = 0.583435
Epoch 6.22: Loss = 0.606934
Epoch 6.23: Loss = 0.545151
Epoch 6.24: Loss = 0.633423
Epoch 6.25: Loss = 0.563644
Epoch 6.26: Loss = 0.643784
Epoch 6.27: Loss = 0.61116
Epoch 6.28: Loss = 0.649231
Epoch 6.29: Loss = 0.642502
Epoch 6.30: Loss = 0.659225
TRAIN LOSS = 0.631805
TRAIN ACC = 80.6702 % (48404/60000)
Loss = 0.582596
Loss = 0.668869
Loss = 0.648056
Loss = 0.636276
Loss = 0.612259
TEST LOSS = 0.629611
TEST ACC = 484.039 % (8011/10000)
Reducing learning rate to 0.399994
Epoch 7.1: Loss = 0.593094
Epoch 7.2: Loss = 0.647079
Epoch 7.3: Loss = 0.609482
Epoch 7.4: Loss = 0.612045
Epoch 7.5: Loss = 0.613205
Epoch 7.6: Loss = 0.558685
Epoch 7.7: Loss = 0.608307
Epoch 7.8: Loss = 0.561615
Epoch 7.9: Loss = 0.586212
Epoch 7.10: Loss = 0.750397
Epoch 7.11: Loss = 0.623306
Epoch 7.12: Loss = 0.667053
Epoch 7.13: Loss = 0.547745
Epoch 7.14: Loss = 0.585815
Epoch 7.15: Loss = 0.587646
Epoch 7.16: Loss = 0.710922
Epoch 7.17: Loss = 0.58371
Epoch 7.18: Loss = 0.646423
Epoch 7.19: Loss = 0.559891
Epoch 7.20: Loss = 0.613159
Epoch 7.21: Loss = 0.665421
Epoch 7.22: Loss = 0.604919
Epoch 7.23: Loss = 0.582047
Epoch 7.24: Loss = 0.634598
Epoch 7.25: Loss = 0.633118
Epoch 7.26: Loss = 0.604263
Epoch 7.27: Loss = 0.642776
Epoch 7.28: Loss = 0.658768
Epoch 7.29: Loss = 0.575241
Epoch 7.30: Loss = 0.625809
TRAIN LOSS = 0.61644
TRAIN ACC = 81.0822 % (48651/60000)
Loss = 0.565948
Loss = 0.643402
Loss = 0.636597
Loss = 0.614304
Loss = 0.600403
TEST LOSS = 0.612131
TEST ACC = 486.51 % (8066/10000)
Reducing learning rate to 0.399994
Epoch 8.1: Loss = 0.533066
Epoch 8.2: Loss = 0.60466
Epoch 8.3: Loss = 0.62114
Epoch 8.4: Loss = 0.635223
Epoch 8.5: Loss = 0.606537
Epoch 8.6: Loss = 0.556473
Epoch 8.7: Loss = 0.593887
Epoch 8.8: Loss = 0.639389
Epoch 8.9: Loss = 0.585968
Epoch 8.10: Loss = 0.596313
Epoch 8.11: Loss = 0.563293
Epoch 8.12: Loss = 0.666229
Epoch 8.13: Loss = 0.560562
Epoch 8.14: Loss = 0.61232
Epoch 8.15: Loss = 0.619675
Epoch 8.16: Loss = 0.594345
Epoch 8.17: Loss = 0.578461
Epoch 8.18: Loss = 0.634888
Epoch 8.19: Loss = 0.555298
Epoch 8.20: Loss = 0.658798
Epoch 8.21: Loss = 0.624405
Epoch 8.22: Loss = 0.644119
Epoch 8.23: Loss = 0.575043
Epoch 8.24: Loss = 0.679321
Epoch 8.25: Loss = 0.617188
Epoch 8.26: Loss = 0.604385
Epoch 8.27: Loss = 0.58992
Epoch 8.28: Loss = 0.583832
Epoch 8.29: Loss = 0.627289
Epoch 8.30: Loss = 0.595932
TRAIN LOSS = 0.605286
TRAIN ACC = 81.604 % (48965/60000)
Loss = 0.641235
Loss = 0.755707
Loss = 0.72876
Loss = 0.731567
Loss = 0.709305
TEST LOSS = 0.713315
TEST ACC = 489.648 % (8032/10000)
Reducing learning rate to 0.399994
Epoch 9.1: Loss = 0.692291
Epoch 9.2: Loss = 0.650055
Epoch 9.3: Loss = 0.640198
Epoch 9.4: Loss = 0.668976
Epoch 9.5: Loss = 0.665848
Epoch 9.6: Loss = 0.55304
Epoch 9.7: Loss = 0.555862
Epoch 9.8: Loss = 0.586777
Epoch 9.9: Loss = 0.588486
Epoch 9.10: Loss = 0.580505
Epoch 9.11: Loss = 0.590607
Epoch 9.12: Loss = 0.539337
Epoch 9.13: Loss = 0.610199
Epoch 9.14: Loss = 0.581375
Epoch 9.15: Loss = 0.677795
Epoch 9.16: Loss = 0.583755
Epoch 9.17: Loss = 0.632813
Epoch 9.18: Loss = 0.574646
Epoch 9.19: Loss = 0.637787
Epoch 9.20: Loss = 0.599014
Epoch 9.21: Loss = 0.644516
Epoch 9.22: Loss = 0.572037
Epoch 9.23: Loss = 0.614639
Epoch 9.24: Loss = 0.596863
Epoch 9.25: Loss = 0.530212
Epoch 9.26: Loss = 0.629852
Epoch 9.27: Loss = 0.616943
Epoch 9.28: Loss = 0.596619
Epoch 9.29: Loss = 0.681076
Epoch 9.30: Loss = 0.67868
TRAIN LOSS = 0.612381
TRAIN ACC = 81.958 % (49177/60000)
Loss = 0.578583
Loss = 0.667114
Loss = 0.654358
Loss = 0.63829
Loss = 0.621841
TEST LOSS = 0.632037
TEST ACC = 491.769 % (8105/10000)
Reducing learning rate to 0.399994
Epoch 10.1: Loss = 0.62207
Epoch 10.2: Loss = 0.623428
Epoch 10.3: Loss = 0.617996
Epoch 10.4: Loss = 0.668976
Epoch 10.5: Loss = 0.578522
Epoch 10.6: Loss = 0.566345
Epoch 10.7: Loss = 0.588745
Epoch 10.8: Loss = 0.681564
Epoch 10.9: Loss = 0.579224
Epoch 10.10: Loss = 0.642563
Epoch 10.11: Loss = 0.586578
Epoch 10.12: Loss = 0.634033
Epoch 10.13: Loss = 0.572525
Epoch 10.14: Loss = 0.707062
Epoch 10.15: Loss = 0.571152
Epoch 10.16: Loss = 0.7155
Epoch 10.17: Loss = 0.607956
Epoch 10.18: Loss = 0.603516
Epoch 10.19: Loss = 0.623627
Epoch 10.20: Loss = 0.613266
Epoch 10.21: Loss = 0.628494
Epoch 10.22: Loss = 0.635193
Epoch 10.23: Loss = 0.630844
Epoch 10.24: Loss = 0.661179
Epoch 10.25: Loss = 0.58316
Epoch 10.26: Loss = 0.598312
Epoch 10.27: Loss = 0.639206
Epoch 10.28: Loss = 0.6642
Epoch 10.29: Loss = 0.632538
Epoch 10.30: Loss = 0.607727
TRAIN LOSS = 0.622864
TRAIN ACC = 81.8604 % (49119/60000)
Loss = 0.576309
Loss = 0.668976
Loss = 0.661194
Loss = 0.635635
Loss = 0.627716
TEST LOSS = 0.633966
TEST ACC = 491.19 % (8123/10000)
