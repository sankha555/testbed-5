Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.32462
Epoch 1.2: Loss = 2.28415
Epoch 1.3: Loss = 2.24513
Epoch 1.4: Loss = 2.15213
Epoch 1.5: Loss = 2.12917
Epoch 1.6: Loss = 2.08583
Epoch 1.7: Loss = 2.02495
Epoch 1.8: Loss = 2.00134
Epoch 1.9: Loss = 1.96851
Epoch 1.10: Loss = 1.91814
Epoch 1.11: Loss = 1.86835
Epoch 1.12: Loss = 1.85214
Epoch 1.13: Loss = 1.76093
Epoch 1.14: Loss = 1.75584
Epoch 1.15: Loss = 1.79234
Epoch 1.16: Loss = 1.71347
Epoch 1.17: Loss = 1.6718
Epoch 1.18: Loss = 1.61507
Epoch 1.19: Loss = 1.57274
Epoch 1.20: Loss = 1.56505
Epoch 1.21: Loss = 1.47057
Epoch 1.22: Loss = 1.47871
Epoch 1.23: Loss = 1.40987
Epoch 1.24: Loss = 1.50919
Epoch 1.25: Loss = 1.41983
Epoch 1.26: Loss = 1.43938
Epoch 1.27: Loss = 1.39821
Epoch 1.28: Loss = 1.40051
Epoch 1.29: Loss = 1.37418
Epoch 1.30: Loss = 1.44173
Epoch 1.31: Loss = 1.27956
Epoch 1.32: Loss = 1.33998
Epoch 1.33: Loss = 1.24785
Epoch 1.34: Loss = 1.27994
Epoch 1.35: Loss = 1.19664
Epoch 1.36: Loss = 1.33517
Epoch 1.37: Loss = 1.16263
Epoch 1.38: Loss = 1.11179
Epoch 1.39: Loss = 1.1225
Epoch 1.40: Loss = 1.07675
Epoch 1.41: Loss = 1.1199
Epoch 1.42: Loss = 1.12132
Epoch 1.43: Loss = 1.0296
Epoch 1.44: Loss = 0.94986
Epoch 1.45: Loss = 1.13501
Epoch 1.46: Loss = 1.03708
Epoch 1.47: Loss = 1.02234
Epoch 1.48: Loss = 1.03069
Epoch 1.49: Loss = 0.989227
Epoch 1.50: Loss = 1.07115
Epoch 1.51: Loss = 0.892197
Epoch 1.52: Loss = 0.938049
Epoch 1.53: Loss = 0.972488
Epoch 1.54: Loss = 0.969315
Epoch 1.55: Loss = 0.999634
Epoch 1.56: Loss = 0.894974
Epoch 1.57: Loss = 0.81459
Epoch 1.58: Loss = 0.843414
Epoch 1.59: Loss = 0.902115
Epoch 1.60: Loss = 1.00502
Epoch 1.61: Loss = 0.926315
Epoch 1.62: Loss = 0.963379
Epoch 1.63: Loss = 0.959579
Epoch 1.64: Loss = 0.956848
Epoch 1.65: Loss = 0.973282
Epoch 1.66: Loss = 0.849304
Epoch 1.67: Loss = 0.868484
Epoch 1.68: Loss = 0.709839
Epoch 1.69: Loss = 0.775925
Epoch 1.70: Loss = 0.877075
Epoch 1.71: Loss = 0.795181
Epoch 1.72: Loss = 0.799301
Epoch 1.73: Loss = 0.794861
Epoch 1.74: Loss = 0.695557
Epoch 1.75: Loss = 0.790649
Epoch 1.76: Loss = 0.784164
Epoch 1.77: Loss = 0.716568
Epoch 1.78: Loss = 0.726074
Epoch 1.79: Loss = 0.750839
Epoch 1.80: Loss = 0.857773
Epoch 1.81: Loss = 0.728668
Epoch 1.82: Loss = 0.676849
Epoch 1.83: Loss = 0.836594
Epoch 1.84: Loss = 0.767715
Epoch 1.85: Loss = 0.834274
Epoch 1.86: Loss = 0.758331
Epoch 1.87: Loss = 0.701538
Epoch 1.88: Loss = 0.716629
Epoch 1.89: Loss = 0.77887
Epoch 1.90: Loss = 0.662918
Epoch 1.91: Loss = 0.815826
Epoch 1.92: Loss = 0.749069
Epoch 1.93: Loss = 0.753433
Epoch 1.94: Loss = 0.62674
Epoch 1.95: Loss = 0.734085
Epoch 1.96: Loss = 0.69577
Epoch 1.97: Loss = 0.555405
Epoch 1.98: Loss = 0.677795
Epoch 1.99: Loss = 0.760818
Epoch 1.100: Loss = 0.84317
Epoch 1.101: Loss = 0.742722
Epoch 1.102: Loss = 0.662888
Epoch 1.103: Loss = 0.603363
Epoch 1.104: Loss = 0.57605
Epoch 1.105: Loss = 0.714508
Epoch 1.106: Loss = 0.713287
Epoch 1.107: Loss = 0.580368
Epoch 1.108: Loss = 0.690826
Epoch 1.109: Loss = 0.657211
Epoch 1.110: Loss = 0.640381
Epoch 1.111: Loss = 0.544846
Epoch 1.112: Loss = 0.521484
Epoch 1.113: Loss = 0.632202
Epoch 1.114: Loss = 0.544189
Epoch 1.115: Loss = 0.625244
Epoch 1.116: Loss = 0.596649
Epoch 1.117: Loss = 0.48175
Epoch 1.118: Loss = 0.43663
Epoch 1.119: Loss = 0.459656
Epoch 1.120: Loss = 0.46904
TRAIN LOSS = 1.06396
TRAIN ACC = 70.6696 % (42404/60000)
Loss = 0.656891
Loss = 0.69165
Loss = 0.796906
Loss = 0.734528
Loss = 0.791412
Loss = 0.664047
Loss = 0.656448
Loss = 0.783005
Loss = 0.762817
Loss = 0.703171
Loss = 0.342575
Loss = 0.54155
Loss = 0.339661
Loss = 0.542374
Loss = 0.469025
Loss = 0.477234
Loss = 0.409561
Loss = 0.202423
Loss = 0.4142
Loss = 0.720428
TEST LOSS = 0.584995
TEST ACC = 424.039 % (8215/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.565704
Epoch 2.2: Loss = 0.705994
Epoch 2.3: Loss = 0.634369
Epoch 2.4: Loss = 0.523254
Epoch 2.5: Loss = 0.541534
Epoch 2.6: Loss = 0.543655
Epoch 2.7: Loss = 0.618332
Epoch 2.8: Loss = 0.577682
Epoch 2.9: Loss = 0.536133
Epoch 2.10: Loss = 0.561218
Epoch 2.11: Loss = 0.582077
Epoch 2.12: Loss = 0.543243
Epoch 2.13: Loss = 0.454117
Epoch 2.14: Loss = 0.514847
Epoch 2.15: Loss = 0.669785
Epoch 2.16: Loss = 0.619064
Epoch 2.17: Loss = 0.646393
Epoch 2.18: Loss = 0.704178
Epoch 2.19: Loss = 0.547318
Epoch 2.20: Loss = 0.503967
Epoch 2.21: Loss = 0.463333
Epoch 2.22: Loss = 0.468704
Epoch 2.23: Loss = 0.522232
Epoch 2.24: Loss = 0.672852
Epoch 2.25: Loss = 0.599533
Epoch 2.26: Loss = 0.656418
Epoch 2.27: Loss = 0.618393
Epoch 2.28: Loss = 0.609573
Epoch 2.29: Loss = 0.659409
Epoch 2.30: Loss = 0.735962
Epoch 2.31: Loss = 0.470566
Epoch 2.32: Loss = 0.680847
Epoch 2.33: Loss = 0.543518
Epoch 2.34: Loss = 0.576614
Epoch 2.35: Loss = 0.565018
Epoch 2.36: Loss = 0.688522
Epoch 2.37: Loss = 0.48053
Epoch 2.38: Loss = 0.462799
Epoch 2.39: Loss = 0.526352
Epoch 2.40: Loss = 0.519211
Epoch 2.41: Loss = 0.578354
Epoch 2.42: Loss = 0.62439
Epoch 2.43: Loss = 0.502258
Epoch 2.44: Loss = 0.422348
Epoch 2.45: Loss = 0.641235
Epoch 2.46: Loss = 0.541107
Epoch 2.47: Loss = 0.499359
Epoch 2.48: Loss = 0.553299
Epoch 2.49: Loss = 0.530563
Epoch 2.50: Loss = 0.610565
Epoch 2.51: Loss = 0.477798
Epoch 2.52: Loss = 0.488113
Epoch 2.53: Loss = 0.514648
Epoch 2.54: Loss = 0.598618
Epoch 2.55: Loss = 0.572723
Epoch 2.56: Loss = 0.507996
Epoch 2.57: Loss = 0.448776
Epoch 2.58: Loss = 0.492859
Epoch 2.59: Loss = 0.612411
Epoch 2.60: Loss = 0.633057
Epoch 2.61: Loss = 0.57605
Epoch 2.62: Loss = 0.626694
Epoch 2.63: Loss = 0.674011
Epoch 2.64: Loss = 0.615128
Epoch 2.65: Loss = 0.723923
Epoch 2.66: Loss = 0.548599
Epoch 2.67: Loss = 0.600464
Epoch 2.68: Loss = 0.393387
Epoch 2.69: Loss = 0.47023
Epoch 2.70: Loss = 0.616882
Epoch 2.71: Loss = 0.477386
Epoch 2.72: Loss = 0.481934
Epoch 2.73: Loss = 0.542847
Epoch 2.74: Loss = 0.431305
Epoch 2.75: Loss = 0.61731
Epoch 2.76: Loss = 0.575073
Epoch 2.77: Loss = 0.488586
Epoch 2.78: Loss = 0.516403
Epoch 2.79: Loss = 0.533096
Epoch 2.80: Loss = 0.588455
Epoch 2.81: Loss = 0.456223
Epoch 2.82: Loss = 0.431458
Epoch 2.83: Loss = 0.614944
Epoch 2.84: Loss = 0.560532
Epoch 2.85: Loss = 0.698364
Epoch 2.86: Loss = 0.560867
Epoch 2.87: Loss = 0.454956
Epoch 2.88: Loss = 0.52977
Epoch 2.89: Loss = 0.614258
Epoch 2.90: Loss = 0.427185
Epoch 2.91: Loss = 0.61673
Epoch 2.92: Loss = 0.567169
Epoch 2.93: Loss = 0.579758
Epoch 2.94: Loss = 0.442215
Epoch 2.95: Loss = 0.528839
Epoch 2.96: Loss = 0.526306
Epoch 2.97: Loss = 0.390259
Epoch 2.98: Loss = 0.46994
Epoch 2.99: Loss = 0.60231
Epoch 2.100: Loss = 0.686935
Epoch 2.101: Loss = 0.626877
Epoch 2.102: Loss = 0.519348
Epoch 2.103: Loss = 0.431412
Epoch 2.104: Loss = 0.429871
Epoch 2.105: Loss = 0.599716
Epoch 2.106: Loss = 0.598694
Epoch 2.107: Loss = 0.404449
Epoch 2.108: Loss = 0.522491
Epoch 2.109: Loss = 0.474167
Epoch 2.110: Loss = 0.544815
Epoch 2.111: Loss = 0.422714
Epoch 2.112: Loss = 0.412552
Epoch 2.113: Loss = 0.486786
Epoch 2.114: Loss = 0.426514
Epoch 2.115: Loss = 0.448227
Epoch 2.116: Loss = 0.477875
Epoch 2.117: Loss = 0.33725
Epoch 2.118: Loss = 0.305328
Epoch 2.119: Loss = 0.359833
Epoch 2.120: Loss = 0.330948
TRAIN LOSS = 0.541519
TRAIN ACC = 83.3984 % (50041/60000)
Loss = 0.520889
Loss = 0.610275
Loss = 0.676544
Loss = 0.625687
Loss = 0.696442
Loss = 0.549118
Loss = 0.528015
Loss = 0.675812
Loss = 0.65123
Loss = 0.632614
Loss = 0.291611
Loss = 0.40744
Loss = 0.324173
Loss = 0.481735
Loss = 0.312378
Loss = 0.400925
Loss = 0.29126
Loss = 0.108322
Loss = 0.341507
Loss = 0.657242
TEST LOSS = 0.489161
TEST ACC = 500.409 % (8513/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.449051
Epoch 3.2: Loss = 0.6241
Epoch 3.3: Loss = 0.574387
Epoch 3.4: Loss = 0.417023
Epoch 3.5: Loss = 0.455978
Epoch 3.6: Loss = 0.481125
Epoch 3.7: Loss = 0.483521
Epoch 3.8: Loss = 0.478058
Epoch 3.9: Loss = 0.455002
Epoch 3.10: Loss = 0.480789
Epoch 3.11: Loss = 0.526749
Epoch 3.12: Loss = 0.466049
Epoch 3.13: Loss = 0.40567
Epoch 3.14: Loss = 0.426971
Epoch 3.15: Loss = 0.513351
Epoch 3.16: Loss = 0.523926
Epoch 3.17: Loss = 0.562531
Epoch 3.18: Loss = 0.631561
Epoch 3.19: Loss = 0.490616
Epoch 3.20: Loss = 0.447495
Epoch 3.21: Loss = 0.40271
Epoch 3.22: Loss = 0.424088
Epoch 3.23: Loss = 0.455795
Epoch 3.24: Loss = 0.654327
Epoch 3.25: Loss = 0.584381
Epoch 3.26: Loss = 0.710373
Epoch 3.27: Loss = 0.591309
Epoch 3.28: Loss = 0.569595
Epoch 3.29: Loss = 0.613235
Epoch 3.30: Loss = 0.675323
Epoch 3.31: Loss = 0.416031
Epoch 3.32: Loss = 0.624359
Epoch 3.33: Loss = 0.487137
Epoch 3.34: Loss = 0.541
Epoch 3.35: Loss = 0.53006
Epoch 3.36: Loss = 0.629944
Epoch 3.37: Loss = 0.421066
Epoch 3.38: Loss = 0.401871
Epoch 3.39: Loss = 0.483322
Epoch 3.40: Loss = 0.473282
Epoch 3.41: Loss = 0.492844
Epoch 3.42: Loss = 0.649307
Epoch 3.43: Loss = 0.429459
Epoch 3.44: Loss = 0.367981
Epoch 3.45: Loss = 0.588562
Epoch 3.46: Loss = 0.580704
Epoch 3.47: Loss = 0.509247
Epoch 3.48: Loss = 0.538681
Epoch 3.49: Loss = 0.52681
Epoch 3.50: Loss = 0.555008
Epoch 3.51: Loss = 0.441116
Epoch 3.52: Loss = 0.46167
Epoch 3.53: Loss = 0.493332
Epoch 3.54: Loss = 0.648987
Epoch 3.55: Loss = 0.578705
Epoch 3.56: Loss = 0.487091
Epoch 3.57: Loss = 0.44281
Epoch 3.58: Loss = 0.466446
Epoch 3.59: Loss = 0.587051
Epoch 3.60: Loss = 0.609711
Epoch 3.61: Loss = 0.59903
Epoch 3.62: Loss = 0.617813
Epoch 3.63: Loss = 0.709198
Epoch 3.64: Loss = 0.642593
Epoch 3.65: Loss = 0.708588
Epoch 3.66: Loss = 0.529236
Epoch 3.67: Loss = 0.586502
Epoch 3.68: Loss = 0.358032
Epoch 3.69: Loss = 0.446915
Epoch 3.70: Loss = 0.589722
Epoch 3.71: Loss = 0.471161
Epoch 3.72: Loss = 0.451508
Epoch 3.73: Loss = 0.509277
Epoch 3.74: Loss = 0.417969
Epoch 3.75: Loss = 0.65979
Epoch 3.76: Loss = 0.560867
Epoch 3.77: Loss = 0.428253
Epoch 3.78: Loss = 0.522308
Epoch 3.79: Loss = 0.535416
Epoch 3.80: Loss = 0.569595
Epoch 3.81: Loss = 0.466614
Epoch 3.82: Loss = 0.443939
Epoch 3.83: Loss = 0.594345
Epoch 3.84: Loss = 0.55307
Epoch 3.85: Loss = 0.760498
Epoch 3.86: Loss = 0.56369
Epoch 3.87: Loss = 0.435501
Epoch 3.88: Loss = 0.523987
Epoch 3.89: Loss = 0.604721
Epoch 3.90: Loss = 0.439133
Epoch 3.91: Loss = 0.590195
Epoch 3.92: Loss = 0.591721
Epoch 3.93: Loss = 0.613098
Epoch 3.94: Loss = 0.431763
Epoch 3.95: Loss = 0.527267
Epoch 3.96: Loss = 0.524963
Epoch 3.97: Loss = 0.39679
Epoch 3.98: Loss = 0.469894
Epoch 3.99: Loss = 0.612808
Epoch 3.100: Loss = 0.678284
Epoch 3.101: Loss = 0.621216
Epoch 3.102: Loss = 0.507553
Epoch 3.103: Loss = 0.396698
Epoch 3.104: Loss = 0.424347
Epoch 3.105: Loss = 0.615097
Epoch 3.106: Loss = 0.645996
Epoch 3.107: Loss = 0.414566
Epoch 3.108: Loss = 0.56781
Epoch 3.109: Loss = 0.487549
Epoch 3.110: Loss = 0.563263
Epoch 3.111: Loss = 0.423477
Epoch 3.112: Loss = 0.381027
Epoch 3.113: Loss = 0.471039
Epoch 3.114: Loss = 0.40863
Epoch 3.115: Loss = 0.449066
Epoch 3.116: Loss = 0.495895
Epoch 3.117: Loss = 0.315781
Epoch 3.118: Loss = 0.297287
Epoch 3.119: Loss = 0.369812
Epoch 3.120: Loss = 0.39444
TRAIN LOSS = 0.516891
TRAIN ACC = 84.4101 % (50649/60000)
Loss = 0.519287
Loss = 0.618179
Loss = 0.699005
Loss = 0.613495
Loss = 0.691177
Loss = 0.539871
Loss = 0.495544
Loss = 0.641769
Loss = 0.618027
Loss = 0.58815
Loss = 0.278854
Loss = 0.430908
Loss = 0.351288
Loss = 0.472931
Loss = 0.295197
Loss = 0.392136
Loss = 0.32428
Loss = 0.104614
Loss = 0.360779
Loss = 0.720398
TEST LOSS = 0.487794
TEST ACC = 506.49 % (8579/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.46489
Epoch 4.2: Loss = 0.636642
Epoch 4.3: Loss = 0.547867
Epoch 4.4: Loss = 0.42717
Epoch 4.5: Loss = 0.438232
Epoch 4.6: Loss = 0.511475
Epoch 4.7: Loss = 0.520676
Epoch 4.8: Loss = 0.473083
Epoch 4.9: Loss = 0.462997
Epoch 4.10: Loss = 0.494064
Epoch 4.11: Loss = 0.54039
Epoch 4.12: Loss = 0.467056
Epoch 4.13: Loss = 0.424103
Epoch 4.14: Loss = 0.471863
Epoch 4.15: Loss = 0.555237
Epoch 4.16: Loss = 0.500702
Epoch 4.17: Loss = 0.548477
Epoch 4.18: Loss = 0.735794
Epoch 4.19: Loss = 0.528397
Epoch 4.20: Loss = 0.473373
Epoch 4.21: Loss = 0.407898
Epoch 4.22: Loss = 0.412262
Epoch 4.23: Loss = 0.429047
Epoch 4.24: Loss = 0.597977
Epoch 4.25: Loss = 0.639023
Epoch 4.26: Loss = 0.733643
Epoch 4.27: Loss = 0.576431
Epoch 4.28: Loss = 0.583771
Epoch 4.29: Loss = 0.66272
Epoch 4.30: Loss = 0.67804
Epoch 4.31: Loss = 0.392456
Epoch 4.32: Loss = 0.616043
Epoch 4.33: Loss = 0.479294
Epoch 4.34: Loss = 0.546555
Epoch 4.35: Loss = 0.499435
Epoch 4.36: Loss = 0.588837
Epoch 4.37: Loss = 0.435989
Epoch 4.38: Loss = 0.41423
Epoch 4.39: Loss = 0.49559
Epoch 4.40: Loss = 0.490189
Epoch 4.41: Loss = 0.465073
Epoch 4.42: Loss = 0.640015
Epoch 4.43: Loss = 0.403122
Epoch 4.44: Loss = 0.360748
Epoch 4.45: Loss = 0.566925
Epoch 4.46: Loss = 0.565018
Epoch 4.47: Loss = 0.450256
Epoch 4.48: Loss = 0.541092
Epoch 4.49: Loss = 0.512405
Epoch 4.50: Loss = 0.552109
Epoch 4.51: Loss = 0.416046
Epoch 4.52: Loss = 0.464264
Epoch 4.53: Loss = 0.490448
Epoch 4.54: Loss = 0.666794
Epoch 4.55: Loss = 0.547531
Epoch 4.56: Loss = 0.515289
Epoch 4.57: Loss = 0.468979
Epoch 4.58: Loss = 0.429962
Epoch 4.59: Loss = 0.572159
Epoch 4.60: Loss = 0.578018
Epoch 4.61: Loss = 0.580383
Epoch 4.62: Loss = 0.650009
Epoch 4.63: Loss = 0.698746
Epoch 4.64: Loss = 0.634964
Epoch 4.65: Loss = 0.73967
Epoch 4.66: Loss = 0.501007
Epoch 4.67: Loss = 0.53772
Epoch 4.68: Loss = 0.352509
Epoch 4.69: Loss = 0.416992
Epoch 4.70: Loss = 0.609726
Epoch 4.71: Loss = 0.493118
Epoch 4.72: Loss = 0.453796
Epoch 4.73: Loss = 0.524536
Epoch 4.74: Loss = 0.415176
Epoch 4.75: Loss = 0.697433
Epoch 4.76: Loss = 0.5616
Epoch 4.77: Loss = 0.442825
Epoch 4.78: Loss = 0.546768
Epoch 4.79: Loss = 0.580109
Epoch 4.80: Loss = 0.568054
Epoch 4.81: Loss = 0.480347
Epoch 4.82: Loss = 0.444366
Epoch 4.83: Loss = 0.622177
Epoch 4.84: Loss = 0.583923
Epoch 4.85: Loss = 0.755325
Epoch 4.86: Loss = 0.589783
Epoch 4.87: Loss = 0.43663
Epoch 4.88: Loss = 0.543716
Epoch 4.89: Loss = 0.585388
Epoch 4.90: Loss = 0.46521
Epoch 4.91: Loss = 0.632172
Epoch 4.92: Loss = 0.588348
Epoch 4.93: Loss = 0.624496
Epoch 4.94: Loss = 0.431946
Epoch 4.95: Loss = 0.542084
Epoch 4.96: Loss = 0.515717
Epoch 4.97: Loss = 0.442413
Epoch 4.98: Loss = 0.461899
Epoch 4.99: Loss = 0.619522
Epoch 4.100: Loss = 0.704376
Epoch 4.101: Loss = 0.687302
Epoch 4.102: Loss = 0.52269
Epoch 4.103: Loss = 0.404388
Epoch 4.104: Loss = 0.428223
Epoch 4.105: Loss = 0.627747
Epoch 4.106: Loss = 0.692719
Epoch 4.107: Loss = 0.431747
Epoch 4.108: Loss = 0.623154
Epoch 4.109: Loss = 0.459137
Epoch 4.110: Loss = 0.595932
Epoch 4.111: Loss = 0.416412
Epoch 4.112: Loss = 0.404205
Epoch 4.113: Loss = 0.463013
Epoch 4.114: Loss = 0.397217
Epoch 4.115: Loss = 0.410767
Epoch 4.116: Loss = 0.485657
Epoch 4.117: Loss = 0.355225
Epoch 4.118: Loss = 0.297119
Epoch 4.119: Loss = 0.396561
Epoch 4.120: Loss = 0.409653
TRAIN LOSS = 0.522446
TRAIN ACC = 85.0739 % (51047/60000)
Loss = 0.561264
Loss = 0.646576
Loss = 0.703461
Loss = 0.701813
Loss = 0.790421
Loss = 0.574417
Loss = 0.520172
Loss = 0.729019
Loss = 0.662979
Loss = 0.626648
Loss = 0.302216
Loss = 0.367981
Loss = 0.341507
Loss = 0.477737
Loss = 0.332779
Loss = 0.472092
Loss = 0.353104
Loss = 0.0982208
Loss = 0.357971
Loss = 0.817444
TEST LOSS = 0.521891
TEST ACC = 510.469 % (8564/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.478699
Epoch 5.2: Loss = 0.662781
Epoch 5.3: Loss = 0.535172
Epoch 5.4: Loss = 0.384521
Epoch 5.5: Loss = 0.453461
Epoch 5.6: Loss = 0.51474
Epoch 5.7: Loss = 0.543915
Epoch 5.8: Loss = 0.54747
Epoch 5.9: Loss = 0.443756
Epoch 5.10: Loss = 0.505524
Epoch 5.11: Loss = 0.567856
Epoch 5.12: Loss = 0.514343
Epoch 5.13: Loss = 0.46344
Epoch 5.14: Loss = 0.45903
Epoch 5.15: Loss = 0.554245
Epoch 5.16: Loss = 0.559479
Epoch 5.17: Loss = 0.635239
Epoch 5.18: Loss = 0.807556
Epoch 5.19: Loss = 0.559845
Epoch 5.20: Loss = 0.504349
Epoch 5.21: Loss = 0.440506
Epoch 5.22: Loss = 0.426041
Epoch 5.23: Loss = 0.435883
Epoch 5.24: Loss = 0.657745
Epoch 5.25: Loss = 0.698776
Epoch 5.26: Loss = 0.79924
Epoch 5.27: Loss = 0.64595
Epoch 5.28: Loss = 0.575073
Epoch 5.29: Loss = 0.701035
Epoch 5.30: Loss = 0.69313
Epoch 5.31: Loss = 0.437775
Epoch 5.32: Loss = 0.621124
Epoch 5.33: Loss = 0.486435
Epoch 5.34: Loss = 0.570221
Epoch 5.35: Loss = 0.579254
Epoch 5.36: Loss = 0.59346
Epoch 5.37: Loss = 0.441757
Epoch 5.38: Loss = 0.47226
Epoch 5.39: Loss = 0.4814
Epoch 5.40: Loss = 0.501816
Epoch 5.41: Loss = 0.473724
Epoch 5.42: Loss = 0.639694
Epoch 5.43: Loss = 0.414093
Epoch 5.44: Loss = 0.420532
Epoch 5.45: Loss = 0.635651
Epoch 5.46: Loss = 0.654846
Epoch 5.47: Loss = 0.517242
Epoch 5.48: Loss = 0.545883
Epoch 5.49: Loss = 0.539307
Epoch 5.50: Loss = 0.612335
Epoch 5.51: Loss = 0.45517
Epoch 5.52: Loss = 0.457352
Epoch 5.53: Loss = 0.585052
Epoch 5.54: Loss = 0.715958
Epoch 5.55: Loss = 0.541016
Epoch 5.56: Loss = 0.592743
Epoch 5.57: Loss = 0.498978
Epoch 5.58: Loss = 0.485046
Epoch 5.59: Loss = 0.596558
Epoch 5.60: Loss = 0.623444
Epoch 5.61: Loss = 0.610733
Epoch 5.62: Loss = 0.690125
Epoch 5.63: Loss = 0.734482
Epoch 5.64: Loss = 0.717422
Epoch 5.65: Loss = 0.736313
Epoch 5.66: Loss = 0.515717
Epoch 5.67: Loss = 0.614532
Epoch 5.68: Loss = 0.384476
Epoch 5.69: Loss = 0.445572
Epoch 5.70: Loss = 0.632736
Epoch 5.71: Loss = 0.516861
Epoch 5.72: Loss = 0.501358
Epoch 5.73: Loss = 0.615616
Epoch 5.74: Loss = 0.467911
Epoch 5.75: Loss = 0.71434
Epoch 5.76: Loss = 0.582687
Epoch 5.77: Loss = 0.433975
Epoch 5.78: Loss = 0.527252
Epoch 5.79: Loss = 0.6689
Epoch 5.80: Loss = 0.62674
Epoch 5.81: Loss = 0.517258
Epoch 5.82: Loss = 0.454056
Epoch 5.83: Loss = 0.633072
Epoch 5.84: Loss = 0.601166
Epoch 5.85: Loss = 0.747437
Epoch 5.86: Loss = 0.626129
Epoch 5.87: Loss = 0.421219
Epoch 5.88: Loss = 0.598709
Epoch 5.89: Loss = 0.656647
Epoch 5.90: Loss = 0.455795
Epoch 5.91: Loss = 0.662781
Epoch 5.92: Loss = 0.595337
Epoch 5.93: Loss = 0.664856
Epoch 5.94: Loss = 0.424179
Epoch 5.95: Loss = 0.58371
Epoch 5.96: Loss = 0.556686
Epoch 5.97: Loss = 0.427109
Epoch 5.98: Loss = 0.483246
Epoch 5.99: Loss = 0.692642
Epoch 5.100: Loss = 0.736847
Epoch 5.101: Loss = 0.665863
Epoch 5.102: Loss = 0.582993
Epoch 5.103: Loss = 0.480759
Epoch 5.104: Loss = 0.431107
Epoch 5.105: Loss = 0.655655
Epoch 5.106: Loss = 0.687637
Epoch 5.107: Loss = 0.445084
Epoch 5.108: Loss = 0.590912
Epoch 5.109: Loss = 0.460693
Epoch 5.110: Loss = 0.607391
Epoch 5.111: Loss = 0.405563
Epoch 5.112: Loss = 0.435471
Epoch 5.113: Loss = 0.471756
Epoch 5.114: Loss = 0.43811
Epoch 5.115: Loss = 0.439087
Epoch 5.116: Loss = 0.522583
Epoch 5.117: Loss = 0.376709
Epoch 5.118: Loss = 0.307693
Epoch 5.119: Loss = 0.412231
Epoch 5.120: Loss = 0.476563
TRAIN LOSS = 0.551102
TRAIN ACC = 84.9762 % (50988/60000)
Loss = 0.558578
Loss = 0.620453
Loss = 0.73053
Loss = 0.764435
Loss = 0.744797
Loss = 0.504807
Loss = 0.528305
Loss = 0.735199
Loss = 0.685165
Loss = 0.580338
Loss = 0.327423
Loss = 0.446808
Loss = 0.344376
Loss = 0.529495
Loss = 0.333359
Loss = 0.504776
Loss = 0.373734
Loss = 0.0803528
Loss = 0.417816
Loss = 0.767029
TEST LOSS = 0.528889
TEST ACC = 509.879 % (8585/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.570999
Epoch 6.2: Loss = 0.636169
Epoch 6.3: Loss = 0.622177
Epoch 6.4: Loss = 0.426987
Epoch 6.5: Loss = 0.436935
Epoch 6.6: Loss = 0.518677
Epoch 6.7: Loss = 0.506271
Epoch 6.8: Loss = 0.506119
Epoch 6.9: Loss = 0.433228
Epoch 6.10: Loss = 0.519241
Epoch 6.11: Loss = 0.522552
Epoch 6.12: Loss = 0.48262
Epoch 6.13: Loss = 0.471771
Epoch 6.14: Loss = 0.486633
Epoch 6.15: Loss = 0.602722
Epoch 6.16: Loss = 0.592377
Epoch 6.17: Loss = 0.620728
Epoch 6.18: Loss = 0.83873
Epoch 6.19: Loss = 0.556061
Epoch 6.20: Loss = 0.475143
Epoch 6.21: Loss = 0.462265
Epoch 6.22: Loss = 0.435028
Epoch 6.23: Loss = 0.440445
Epoch 6.24: Loss = 0.72525
Epoch 6.25: Loss = 0.704605
Epoch 6.26: Loss = 0.728134
Epoch 6.27: Loss = 0.674194
Epoch 6.28: Loss = 0.573441
Epoch 6.29: Loss = 0.71196
Epoch 6.30: Loss = 0.746597
Epoch 6.31: Loss = 0.433029
Epoch 6.32: Loss = 0.700592
Epoch 6.33: Loss = 0.521774
Epoch 6.34: Loss = 0.600098
Epoch 6.35: Loss = 0.602829
Epoch 6.36: Loss = 0.669937
Epoch 6.37: Loss = 0.413101
Epoch 6.38: Loss = 0.507629
Epoch 6.39: Loss = 0.426331
Epoch 6.40: Loss = 0.500717
Epoch 6.41: Loss = 0.486023
Epoch 6.42: Loss = 0.695068
Epoch 6.43: Loss = 0.460419
Epoch 6.44: Loss = 0.407928
Epoch 6.45: Loss = 0.672714
Epoch 6.46: Loss = 0.700623
Epoch 6.47: Loss = 0.530899
Epoch 6.48: Loss = 0.585342
Epoch 6.49: Loss = 0.563919
Epoch 6.50: Loss = 0.677689
Epoch 6.51: Loss = 0.491547
Epoch 6.52: Loss = 0.410553
Epoch 6.53: Loss = 0.558319
Epoch 6.54: Loss = 0.744751
Epoch 6.55: Loss = 0.542831
Epoch 6.56: Loss = 0.602448
Epoch 6.57: Loss = 0.49379
Epoch 6.58: Loss = 0.522964
Epoch 6.59: Loss = 0.624969
Epoch 6.60: Loss = 0.667419
Epoch 6.61: Loss = 0.616547
Epoch 6.62: Loss = 0.703323
Epoch 6.63: Loss = 0.746536
Epoch 6.64: Loss = 0.767441
Epoch 6.65: Loss = 0.755508
Epoch 6.66: Loss = 0.533234
Epoch 6.67: Loss = 0.619431
Epoch 6.68: Loss = 0.381653
Epoch 6.69: Loss = 0.470032
Epoch 6.70: Loss = 0.721222
Epoch 6.71: Loss = 0.524063
Epoch 6.72: Loss = 0.492477
Epoch 6.73: Loss = 0.59523
Epoch 6.74: Loss = 0.481781
Epoch 6.75: Loss = 0.766235
Epoch 6.76: Loss = 0.567368
Epoch 6.77: Loss = 0.440308
Epoch 6.78: Loss = 0.548325
Epoch 6.79: Loss = 0.706711
Epoch 6.80: Loss = 0.675247
Epoch 6.81: Loss = 0.519089
Epoch 6.82: Loss = 0.462936
Epoch 6.83: Loss = 0.648544
Epoch 6.84: Loss = 0.617752
Epoch 6.85: Loss = 0.767639
Epoch 6.86: Loss = 0.602478
Epoch 6.87: Loss = 0.426895
Epoch 6.88: Loss = 0.586304
Epoch 6.89: Loss = 0.682587
Epoch 6.90: Loss = 0.518066
Epoch 6.91: Loss = 0.699844
Epoch 6.92: Loss = 0.670929
Epoch 6.93: Loss = 0.677765
Epoch 6.94: Loss = 0.402695
Epoch 6.95: Loss = 0.575546
Epoch 6.96: Loss = 0.627502
Epoch 6.97: Loss = 0.459259
Epoch 6.98: Loss = 0.480057
Epoch 6.99: Loss = 0.693024
Epoch 6.100: Loss = 0.751541
Epoch 6.101: Loss = 0.698502
Epoch 6.102: Loss = 0.539886
Epoch 6.103: Loss = 0.561615
Epoch 6.104: Loss = 0.507431
Epoch 6.105: Loss = 0.746231
Epoch 6.106: Loss = 0.750336
Epoch 6.107: Loss = 0.483551
Epoch 6.108: Loss = 0.562363
Epoch 6.109: Loss = 0.505386
Epoch 6.110: Loss = 0.586029
Epoch 6.111: Loss = 0.504623
Epoch 6.112: Loss = 0.480713
Epoch 6.113: Loss = 0.459045
Epoch 6.114: Loss = 0.46907
Epoch 6.115: Loss = 0.469574
Epoch 6.116: Loss = 0.548813
Epoch 6.117: Loss = 0.341812
Epoch 6.118: Loss = 0.348785
Epoch 6.119: Loss = 0.481674
Epoch 6.120: Loss = 0.500153
TRAIN LOSS = 0.569794
TRAIN ACC = 84.9747 % (50987/60000)
Loss = 0.551605
Loss = 0.663116
Loss = 0.745895
Loss = 0.827698
Loss = 0.786118
Loss = 0.619247
Loss = 0.616852
Loss = 0.843155
Loss = 0.74971
Loss = 0.649689
Loss = 0.330276
Loss = 0.439087
Loss = 0.390015
Loss = 0.60173
Loss = 0.305939
Loss = 0.496277
Loss = 0.358765
Loss = 0.076416
Loss = 0.402725
Loss = 0.745636
TEST LOSS = 0.559997
TEST ACC = 509.869 % (8532/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.604492
Epoch 7.2: Loss = 0.674652
Epoch 7.3: Loss = 0.619141
Epoch 7.4: Loss = 0.444992
Epoch 7.5: Loss = 0.45575
Epoch 7.6: Loss = 0.586853
Epoch 7.7: Loss = 0.497406
Epoch 7.8: Loss = 0.518967
Epoch 7.9: Loss = 0.47023
Epoch 7.10: Loss = 0.607925
Epoch 7.11: Loss = 0.598099
Epoch 7.12: Loss = 0.548309
Epoch 7.13: Loss = 0.463654
Epoch 7.14: Loss = 0.496353
Epoch 7.15: Loss = 0.618927
Epoch 7.16: Loss = 0.597229
Epoch 7.17: Loss = 0.640244
Epoch 7.18: Loss = 0.86908
Epoch 7.19: Loss = 0.567078
Epoch 7.20: Loss = 0.477905
Epoch 7.21: Loss = 0.479889
Epoch 7.22: Loss = 0.490555
Epoch 7.23: Loss = 0.466202
Epoch 7.24: Loss = 0.688461
Epoch 7.25: Loss = 0.721848
Epoch 7.26: Loss = 0.753067
Epoch 7.27: Loss = 0.719269
Epoch 7.28: Loss = 0.63945
Epoch 7.29: Loss = 0.77536
Epoch 7.30: Loss = 0.764343
Epoch 7.31: Loss = 0.466309
Epoch 7.32: Loss = 0.672562
Epoch 7.33: Loss = 0.559616
Epoch 7.34: Loss = 0.72049
Epoch 7.35: Loss = 0.599197
Epoch 7.36: Loss = 0.771072
Epoch 7.37: Loss = 0.453568
Epoch 7.38: Loss = 0.538574
Epoch 7.39: Loss = 0.519089
Epoch 7.40: Loss = 0.63446
Epoch 7.41: Loss = 0.536392
Epoch 7.42: Loss = 0.695435
Epoch 7.43: Loss = 0.453003
Epoch 7.44: Loss = 0.495422
Epoch 7.45: Loss = 0.689163
Epoch 7.46: Loss = 0.672699
Epoch 7.47: Loss = 0.588699
Epoch 7.48: Loss = 0.637772
Epoch 7.49: Loss = 0.548157
Epoch 7.50: Loss = 0.693665
Epoch 7.51: Loss = 0.489517
Epoch 7.52: Loss = 0.434723
Epoch 7.53: Loss = 0.612625
Epoch 7.54: Loss = 0.831619
Epoch 7.55: Loss = 0.611694
Epoch 7.56: Loss = 0.599365
Epoch 7.57: Loss = 0.492905
Epoch 7.58: Loss = 0.550705
Epoch 7.59: Loss = 0.658371
Epoch 7.60: Loss = 0.726944
Epoch 7.61: Loss = 0.650909
Epoch 7.62: Loss = 0.723724
Epoch 7.63: Loss = 0.818192
Epoch 7.64: Loss = 0.790878
Epoch 7.65: Loss = 0.802689
Epoch 7.66: Loss = 0.556076
Epoch 7.67: Loss = 0.675476
Epoch 7.68: Loss = 0.433838
Epoch 7.69: Loss = 0.520416
Epoch 7.70: Loss = 0.809067
Epoch 7.71: Loss = 0.542007
Epoch 7.72: Loss = 0.498566
Epoch 7.73: Loss = 0.589783
Epoch 7.74: Loss = 0.518982
Epoch 7.75: Loss = 0.820038
Epoch 7.76: Loss = 0.629776
Epoch 7.77: Loss = 0.490433
Epoch 7.78: Loss = 0.577225
Epoch 7.79: Loss = 0.772385
Epoch 7.80: Loss = 0.674713
Epoch 7.81: Loss = 0.514542
Epoch 7.82: Loss = 0.470215
Epoch 7.83: Loss = 0.748062
Epoch 7.84: Loss = 0.634659
Epoch 7.85: Loss = 0.836288
Epoch 7.86: Loss = 0.673874
Epoch 7.87: Loss = 0.419571
Epoch 7.88: Loss = 0.661469
Epoch 7.89: Loss = 0.715347
Epoch 7.90: Loss = 0.555267
Epoch 7.91: Loss = 0.704575
Epoch 7.92: Loss = 0.745789
Epoch 7.93: Loss = 0.708221
Epoch 7.94: Loss = 0.422058
Epoch 7.95: Loss = 0.636078
Epoch 7.96: Loss = 0.652954
Epoch 7.97: Loss = 0.43663
Epoch 7.98: Loss = 0.501709
Epoch 7.99: Loss = 0.7435
Epoch 7.100: Loss = 0.720093
Epoch 7.101: Loss = 0.79747
Epoch 7.102: Loss = 0.605606
Epoch 7.103: Loss = 0.574127
Epoch 7.104: Loss = 0.485825
Epoch 7.105: Loss = 0.747375
Epoch 7.106: Loss = 0.844162
Epoch 7.107: Loss = 0.52475
Epoch 7.108: Loss = 0.593079
Epoch 7.109: Loss = 0.524673
Epoch 7.110: Loss = 0.613007
Epoch 7.111: Loss = 0.485565
Epoch 7.112: Loss = 0.53476
Epoch 7.113: Loss = 0.522568
Epoch 7.114: Loss = 0.498688
Epoch 7.115: Loss = 0.493286
Epoch 7.116: Loss = 0.550964
Epoch 7.117: Loss = 0.345535
Epoch 7.118: Loss = 0.300293
Epoch 7.119: Loss = 0.511383
Epoch 7.120: Loss = 0.475037
TRAIN LOSS = 0.601974
TRAIN ACC = 84.8923 % (50938/60000)
Loss = 0.559921
Loss = 0.687607
Loss = 0.814224
Loss = 0.817581
Loss = 0.863174
Loss = 0.623779
Loss = 0.584702
Loss = 0.875961
Loss = 0.753067
Loss = 0.709229
Loss = 0.284683
Loss = 0.423981
Loss = 0.345703
Loss = 0.569962
Loss = 0.288895
Loss = 0.521591
Loss = 0.364929
Loss = 0.0818329
Loss = 0.394287
Loss = 0.790527
TEST LOSS = 0.567782
TEST ACC = 509.38 % (8582/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.590225
Epoch 8.2: Loss = 0.720657
Epoch 8.3: Loss = 0.733597
Epoch 8.4: Loss = 0.474258
Epoch 8.5: Loss = 0.450317
Epoch 8.6: Loss = 0.520065
Epoch 8.7: Loss = 0.509827
Epoch 8.8: Loss = 0.599045
Epoch 8.9: Loss = 0.477081
Epoch 8.10: Loss = 0.575562
Epoch 8.11: Loss = 0.562347
Epoch 8.12: Loss = 0.550369
Epoch 8.13: Loss = 0.496994
Epoch 8.14: Loss = 0.478912
Epoch 8.15: Loss = 0.639557
Epoch 8.16: Loss = 0.588211
Epoch 8.17: Loss = 0.635391
Epoch 8.18: Loss = 0.904358
Epoch 8.19: Loss = 0.58046
Epoch 8.20: Loss = 0.525452
Epoch 8.21: Loss = 0.497696
Epoch 8.22: Loss = 0.475433
Epoch 8.23: Loss = 0.497742
Epoch 8.24: Loss = 0.844925
Epoch 8.25: Loss = 0.742249
Epoch 8.26: Loss = 0.759445
Epoch 8.27: Loss = 0.720093
Epoch 8.28: Loss = 0.655777
Epoch 8.29: Loss = 0.841141
Epoch 8.30: Loss = 0.779373
Epoch 8.31: Loss = 0.485077
Epoch 8.32: Loss = 0.640381
Epoch 8.33: Loss = 0.553558
Epoch 8.34: Loss = 0.752319
Epoch 8.35: Loss = 0.659576
Epoch 8.36: Loss = 0.766586
Epoch 8.37: Loss = 0.482178
Epoch 8.38: Loss = 0.538223
Epoch 8.39: Loss = 0.587708
Epoch 8.40: Loss = 0.633118
Epoch 8.41: Loss = 0.556091
Epoch 8.42: Loss = 0.80098
Epoch 8.43: Loss = 0.425934
Epoch 8.44: Loss = 0.467972
Epoch 8.45: Loss = 0.664917
Epoch 8.46: Loss = 0.702484
Epoch 8.47: Loss = 0.630569
Epoch 8.48: Loss = 0.657578
Epoch 8.49: Loss = 0.602509
Epoch 8.50: Loss = 0.749847
Epoch 8.51: Loss = 0.441635
Epoch 8.52: Loss = 0.510834
Epoch 8.53: Loss = 0.629776
Epoch 8.54: Loss = 0.778625
Epoch 8.55: Loss = 0.715515
Epoch 8.56: Loss = 0.554871
Epoch 8.57: Loss = 0.55986
Epoch 8.58: Loss = 0.616135
Epoch 8.59: Loss = 0.683289
Epoch 8.60: Loss = 0.679077
Epoch 8.61: Loss = 0.64502
Epoch 8.62: Loss = 0.734848
Epoch 8.63: Loss = 0.845901
Epoch 8.64: Loss = 0.748367
Epoch 8.65: Loss = 0.827896
Epoch 8.66: Loss = 0.586731
Epoch 8.67: Loss = 0.723358
Epoch 8.68: Loss = 0.406036
Epoch 8.69: Loss = 0.57196
Epoch 8.70: Loss = 0.817459
Epoch 8.71: Loss = 0.537827
Epoch 8.72: Loss = 0.583801
Epoch 8.73: Loss = 0.690613
Epoch 8.74: Loss = 0.541046
Epoch 8.75: Loss = 0.859818
Epoch 8.76: Loss = 0.646835
Epoch 8.77: Loss = 0.505905
Epoch 8.78: Loss = 0.613632
Epoch 8.79: Loss = 0.845398
Epoch 8.80: Loss = 0.74704
Epoch 8.81: Loss = 0.592773
Epoch 8.82: Loss = 0.50148
Epoch 8.83: Loss = 0.798599
Epoch 8.84: Loss = 0.667694
Epoch 8.85: Loss = 0.789749
Epoch 8.86: Loss = 0.688751
Epoch 8.87: Loss = 0.520477
Epoch 8.88: Loss = 0.668289
Epoch 8.89: Loss = 0.763687
Epoch 8.90: Loss = 0.574493
Epoch 8.91: Loss = 0.747238
Epoch 8.92: Loss = 0.681549
Epoch 8.93: Loss = 0.814285
Epoch 8.94: Loss = 0.500793
Epoch 8.95: Loss = 0.663498
Epoch 8.96: Loss = 0.709503
Epoch 8.97: Loss = 0.418411
Epoch 8.98: Loss = 0.594742
Epoch 8.99: Loss = 0.81546
Epoch 8.100: Loss = 0.812912
Epoch 8.101: Loss = 0.897461
Epoch 8.102: Loss = 0.718079
Epoch 8.103: Loss = 0.63887
Epoch 8.104: Loss = 0.507584
Epoch 8.105: Loss = 0.799789
Epoch 8.106: Loss = 0.827866
Epoch 8.107: Loss = 0.564514
Epoch 8.108: Loss = 0.646088
Epoch 8.109: Loss = 0.484802
Epoch 8.110: Loss = 0.661972
Epoch 8.111: Loss = 0.498138
Epoch 8.112: Loss = 0.463715
Epoch 8.113: Loss = 0.5896
Epoch 8.114: Loss = 0.495605
Epoch 8.115: Loss = 0.512009
Epoch 8.116: Loss = 0.582993
Epoch 8.117: Loss = 0.35495
Epoch 8.118: Loss = 0.404861
Epoch 8.119: Loss = 0.497177
Epoch 8.120: Loss = 0.567337
TRAIN LOSS = 0.628937
TRAIN ACC = 85.0464 % (51030/60000)
Loss = 0.569077
Loss = 0.699432
Loss = 0.970306
Loss = 0.891083
Loss = 0.930161
Loss = 0.610123
Loss = 0.585709
Loss = 0.849503
Loss = 0.79567
Loss = 0.697937
Loss = 0.310364
Loss = 0.48941
Loss = 0.413315
Loss = 0.664642
Loss = 0.375473
Loss = 0.492752
Loss = 0.424881
Loss = 0.0737457
Loss = 0.507324
Loss = 0.885315
TEST LOSS = 0.611811
TEST ACC = 510.3 % (8586/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.610825
Epoch 9.2: Loss = 0.724594
Epoch 9.3: Loss = 0.726181
Epoch 9.4: Loss = 0.465942
Epoch 9.5: Loss = 0.490982
Epoch 9.6: Loss = 0.512802
Epoch 9.7: Loss = 0.584305
Epoch 9.8: Loss = 0.587494
Epoch 9.9: Loss = 0.567291
Epoch 9.10: Loss = 0.627182
Epoch 9.11: Loss = 0.606415
Epoch 9.12: Loss = 0.546265
Epoch 9.13: Loss = 0.552734
Epoch 9.14: Loss = 0.577408
Epoch 9.15: Loss = 0.676224
Epoch 9.16: Loss = 0.603348
Epoch 9.17: Loss = 0.685135
Epoch 9.18: Loss = 0.99173
Epoch 9.19: Loss = 0.625427
Epoch 9.20: Loss = 0.528122
Epoch 9.21: Loss = 0.544434
Epoch 9.22: Loss = 0.488525
Epoch 9.23: Loss = 0.533859
Epoch 9.24: Loss = 0.852142
Epoch 9.25: Loss = 0.826248
Epoch 9.26: Loss = 0.836548
Epoch 9.27: Loss = 0.775864
Epoch 9.28: Loss = 0.781723
Epoch 9.29: Loss = 0.94281
Epoch 9.30: Loss = 0.932281
Epoch 9.31: Loss = 0.544968
Epoch 9.32: Loss = 0.769424
Epoch 9.33: Loss = 0.603256
Epoch 9.34: Loss = 0.793442
Epoch 9.35: Loss = 0.691071
Epoch 9.36: Loss = 0.882751
Epoch 9.37: Loss = 0.527725
Epoch 9.38: Loss = 0.549225
Epoch 9.39: Loss = 0.654861
Epoch 9.40: Loss = 0.726334
Epoch 9.41: Loss = 0.631561
Epoch 9.42: Loss = 0.964355
Epoch 9.43: Loss = 0.477341
Epoch 9.44: Loss = 0.50708
Epoch 9.45: Loss = 0.710114
Epoch 9.46: Loss = 0.730972
Epoch 9.47: Loss = 0.578537
Epoch 9.48: Loss = 0.732544
Epoch 9.49: Loss = 0.637909
Epoch 9.50: Loss = 0.8862
Epoch 9.51: Loss = 0.477966
Epoch 9.52: Loss = 0.483002
Epoch 9.53: Loss = 0.646576
Epoch 9.54: Loss = 0.822601
Epoch 9.55: Loss = 0.780228
Epoch 9.56: Loss = 0.594727
Epoch 9.57: Loss = 0.605698
Epoch 9.58: Loss = 0.714096
Epoch 9.59: Loss = 0.751862
Epoch 9.60: Loss = 0.827713
Epoch 9.61: Loss = 0.759338
Epoch 9.62: Loss = 0.83754
Epoch 9.63: Loss = 0.926575
Epoch 9.64: Loss = 0.872192
Epoch 9.65: Loss = 1.00945
Epoch 9.66: Loss = 0.669769
Epoch 9.67: Loss = 0.782074
Epoch 9.68: Loss = 0.453827
Epoch 9.69: Loss = 0.566467
Epoch 9.70: Loss = 0.89122
Epoch 9.71: Loss = 0.600555
Epoch 9.72: Loss = 0.7173
Epoch 9.73: Loss = 0.70134
Epoch 9.74: Loss = 0.602692
Epoch 9.75: Loss = 1.05029
Epoch 9.76: Loss = 0.653564
Epoch 9.77: Loss = 0.60112
Epoch 9.78: Loss = 0.682175
Epoch 9.79: Loss = 0.822754
Epoch 9.80: Loss = 0.839951
Epoch 9.81: Loss = 0.685059
Epoch 9.82: Loss = 0.499466
Epoch 9.83: Loss = 0.932556
Epoch 9.84: Loss = 0.686539
Epoch 9.85: Loss = 1.00244
Epoch 9.86: Loss = 0.789917
Epoch 9.87: Loss = 0.509903
Epoch 9.88: Loss = 0.72403
Epoch 9.89: Loss = 0.862762
Epoch 9.90: Loss = 0.568466
Epoch 9.91: Loss = 0.817932
Epoch 9.92: Loss = 0.758896
Epoch 9.93: Loss = 0.922501
Epoch 9.94: Loss = 0.493423
Epoch 9.95: Loss = 0.639069
Epoch 9.96: Loss = 0.790833
Epoch 9.97: Loss = 0.483368
Epoch 9.98: Loss = 0.662186
Epoch 9.99: Loss = 0.799515
Epoch 9.100: Loss = 0.899857
Epoch 9.101: Loss = 0.956329
Epoch 9.102: Loss = 0.779465
Epoch 9.103: Loss = 0.66011
Epoch 9.104: Loss = 0.564301
Epoch 9.105: Loss = 0.897308
Epoch 9.106: Loss = 0.959457
Epoch 9.107: Loss = 0.553238
Epoch 9.108: Loss = 0.654861
Epoch 9.109: Loss = 0.584183
Epoch 9.110: Loss = 0.721115
Epoch 9.111: Loss = 0.56456
Epoch 9.112: Loss = 0.545807
Epoch 9.113: Loss = 0.615555
Epoch 9.114: Loss = 0.526077
Epoch 9.115: Loss = 0.53157
Epoch 9.116: Loss = 0.578156
Epoch 9.117: Loss = 0.38121
Epoch 9.118: Loss = 0.352722
Epoch 9.119: Loss = 0.487732
Epoch 9.120: Loss = 0.58548
TRAIN LOSS = 0.68483
TRAIN ACC = 84.8358 % (50904/60000)
Loss = 0.639435
Loss = 0.796234
Loss = 0.947891
Loss = 0.922531
Loss = 0.965195
Loss = 0.626358
Loss = 0.655258
Loss = 1.01184
Loss = 0.905365
Loss = 0.776489
Loss = 0.342438
Loss = 0.461182
Loss = 0.453354
Loss = 0.713882
Loss = 0.399231
Loss = 0.598602
Loss = 0.512009
Loss = 0.08992
Loss = 0.546799
Loss = 0.986053
TEST LOSS = 0.667503
TEST ACC = 509.039 % (8522/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.596222
Epoch 10.2: Loss = 0.841324
Epoch 10.3: Loss = 0.80722
Epoch 10.4: Loss = 0.49585
Epoch 10.5: Loss = 0.491882
Epoch 10.6: Loss = 0.579376
Epoch 10.7: Loss = 0.55835
Epoch 10.8: Loss = 0.600098
Epoch 10.9: Loss = 0.616852
Epoch 10.10: Loss = 0.642807
Epoch 10.11: Loss = 0.628159
Epoch 10.12: Loss = 0.59404
Epoch 10.13: Loss = 0.588959
Epoch 10.14: Loss = 0.6306
Epoch 10.15: Loss = 0.742218
Epoch 10.16: Loss = 0.671097
Epoch 10.17: Loss = 0.664795
Epoch 10.18: Loss = 0.992126
Epoch 10.19: Loss = 0.698761
Epoch 10.20: Loss = 0.635468
Epoch 10.21: Loss = 0.589264
Epoch 10.22: Loss = 0.571472
Epoch 10.23: Loss = 0.618576
Epoch 10.24: Loss = 0.890396
Epoch 10.25: Loss = 0.922012
Epoch 10.26: Loss = 0.93866
Epoch 10.27: Loss = 0.82132
Epoch 10.28: Loss = 0.79747
Epoch 10.29: Loss = 1.00172
Epoch 10.30: Loss = 0.874893
Epoch 10.31: Loss = 0.635818
Epoch 10.32: Loss = 0.770874
Epoch 10.33: Loss = 0.648773
Epoch 10.34: Loss = 0.859222
Epoch 10.35: Loss = 0.683777
Epoch 10.36: Loss = 0.87738
Epoch 10.37: Loss = 0.549255
Epoch 10.38: Loss = 0.613068
Epoch 10.39: Loss = 0.658997
Epoch 10.40: Loss = 0.683228
Epoch 10.41: Loss = 0.653732
Epoch 10.42: Loss = 0.974579
Epoch 10.43: Loss = 0.510925
Epoch 10.44: Loss = 0.534714
Epoch 10.45: Loss = 0.73468
Epoch 10.46: Loss = 0.730911
Epoch 10.47: Loss = 0.622269
Epoch 10.48: Loss = 0.729095
Epoch 10.49: Loss = 0.658447
Epoch 10.50: Loss = 0.97583
Epoch 10.51: Loss = 0.539795
Epoch 10.52: Loss = 0.486252
Epoch 10.53: Loss = 0.620636
Epoch 10.54: Loss = 0.802261
Epoch 10.55: Loss = 0.837997
Epoch 10.56: Loss = 0.653015
Epoch 10.57: Loss = 0.603455
Epoch 10.58: Loss = 0.674988
Epoch 10.59: Loss = 0.724411
Epoch 10.60: Loss = 0.869202
Epoch 10.61: Loss = 0.767548
Epoch 10.62: Loss = 0.834946
Epoch 10.63: Loss = 0.996338
Epoch 10.64: Loss = 0.954742
Epoch 10.65: Loss = 1.03432
Epoch 10.66: Loss = 0.619888
Epoch 10.67: Loss = 0.762146
Epoch 10.68: Loss = 0.530823
Epoch 10.69: Loss = 0.570724
Epoch 10.70: Loss = 0.87439
Epoch 10.71: Loss = 0.641891
Epoch 10.72: Loss = 0.69165
Epoch 10.73: Loss = 0.682693
Epoch 10.74: Loss = 0.636337
Epoch 10.75: Loss = 1.0578
Epoch 10.76: Loss = 0.64119
Epoch 10.77: Loss = 0.542587
Epoch 10.78: Loss = 0.654968
Epoch 10.79: Loss = 0.874634
Epoch 10.80: Loss = 0.835968
Epoch 10.81: Loss = 0.630508
Epoch 10.82: Loss = 0.528671
Epoch 10.83: Loss = 0.946564
Epoch 10.84: Loss = 0.802505
Epoch 10.85: Loss = 0.926483
Epoch 10.86: Loss = 0.764709
Epoch 10.87: Loss = 0.488083
Epoch 10.88: Loss = 0.661545
Epoch 10.89: Loss = 0.817444
Epoch 10.90: Loss = 0.518494
Epoch 10.91: Loss = 0.776688
Epoch 10.92: Loss = 0.80368
Epoch 10.93: Loss = 0.883087
Epoch 10.94: Loss = 0.474243
Epoch 10.95: Loss = 0.712616
Epoch 10.96: Loss = 0.869888
Epoch 10.97: Loss = 0.515381
Epoch 10.98: Loss = 0.651932
Epoch 10.99: Loss = 0.899277
Epoch 10.100: Loss = 0.955765
Epoch 10.101: Loss = 0.968811
Epoch 10.102: Loss = 0.824219
Epoch 10.103: Loss = 0.686234
Epoch 10.104: Loss = 0.570145
Epoch 10.105: Loss = 0.917648
Epoch 10.106: Loss = 0.889008
Epoch 10.107: Loss = 0.600616
Epoch 10.108: Loss = 0.734863
Epoch 10.109: Loss = 0.593781
Epoch 10.110: Loss = 0.786697
Epoch 10.111: Loss = 0.575272
Epoch 10.112: Loss = 0.600021
Epoch 10.113: Loss = 0.641937
Epoch 10.114: Loss = 0.518661
Epoch 10.115: Loss = 0.548767
Epoch 10.116: Loss = 0.689041
Epoch 10.117: Loss = 0.446594
Epoch 10.118: Loss = 0.381592
Epoch 10.119: Loss = 0.484879
Epoch 10.120: Loss = 0.61879
TRAIN LOSS = 0.707764
TRAIN ACC = 84.5764 % (50748/60000)
Loss = 0.633362
Loss = 0.844086
Loss = 1.03833
Loss = 0.927185
Loss = 1.05426
Loss = 0.701645
Loss = 0.683105
Loss = 0.93132
Loss = 0.894043
Loss = 0.819382
Loss = 0.381516
Loss = 0.533676
Loss = 0.436676
Loss = 0.667587
Loss = 0.384506
Loss = 0.697296
Loss = 0.519104
Loss = 0.120316
Loss = 0.488388
Loss = 1.07556
TEST LOSS = 0.691567
TEST ACC = 507.48 % (8531/10000)
