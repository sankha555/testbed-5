Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.27737
Epoch 1.2: Loss = 2.18512
Epoch 1.3: Loss = 2.1873
Epoch 1.4: Loss = 2.16254
Epoch 1.5: Loss = 2.08928
Epoch 1.6: Loss = 2.04779
Epoch 1.7: Loss = 2.00487
Epoch 1.8: Loss = 1.96634
Epoch 1.9: Loss = 1.91586
Epoch 1.10: Loss = 1.91524
Epoch 1.11: Loss = 1.79811
Epoch 1.12: Loss = 1.77881
Epoch 1.13: Loss = 1.72003
Epoch 1.14: Loss = 1.73845
Epoch 1.15: Loss = 1.78831
Epoch 1.16: Loss = 1.66153
Epoch 1.17: Loss = 1.62059
Epoch 1.18: Loss = 1.62381
Epoch 1.19: Loss = 1.5627
Epoch 1.20: Loss = 1.53716
Epoch 1.21: Loss = 1.44685
Epoch 1.22: Loss = 1.46082
Epoch 1.23: Loss = 1.39468
Epoch 1.24: Loss = 1.48172
Epoch 1.25: Loss = 1.39452
Epoch 1.26: Loss = 1.44334
Epoch 1.27: Loss = 1.3512
Epoch 1.28: Loss = 1.34053
Epoch 1.29: Loss = 1.36398
Epoch 1.30: Loss = 1.42551
Epoch 1.31: Loss = 1.26677
Epoch 1.32: Loss = 1.30072
Epoch 1.33: Loss = 1.24725
Epoch 1.34: Loss = 1.26735
Epoch 1.35: Loss = 1.20038
Epoch 1.36: Loss = 1.31833
Epoch 1.37: Loss = 1.19621
Epoch 1.38: Loss = 1.1369
Epoch 1.39: Loss = 1.12505
Epoch 1.40: Loss = 1.06647
Epoch 1.41: Loss = 1.13115
Epoch 1.42: Loss = 1.09975
Epoch 1.43: Loss = 1.05873
Epoch 1.44: Loss = 0.95285
Epoch 1.45: Loss = 1.09505
Epoch 1.46: Loss = 1.0287
Epoch 1.47: Loss = 0.997986
Epoch 1.48: Loss = 1.01422
Epoch 1.49: Loss = 0.975815
Epoch 1.50: Loss = 1.05127
Epoch 1.51: Loss = 0.884216
Epoch 1.52: Loss = 0.913498
Epoch 1.53: Loss = 0.937485
Epoch 1.54: Loss = 0.963226
Epoch 1.55: Loss = 0.930817
Epoch 1.56: Loss = 0.84758
Epoch 1.57: Loss = 0.782806
Epoch 1.58: Loss = 0.860611
Epoch 1.59: Loss = 0.877335
Epoch 1.60: Loss = 0.965851
Epoch 1.61: Loss = 0.9207
Epoch 1.62: Loss = 0.943344
Epoch 1.63: Loss = 0.971191
Epoch 1.64: Loss = 0.900955
Epoch 1.65: Loss = 0.96756
Epoch 1.66: Loss = 0.834808
Epoch 1.67: Loss = 0.849655
Epoch 1.68: Loss = 0.693832
Epoch 1.69: Loss = 0.772415
Epoch 1.70: Loss = 0.882874
Epoch 1.71: Loss = 0.763321
Epoch 1.72: Loss = 0.792557
Epoch 1.73: Loss = 0.789185
Epoch 1.74: Loss = 0.705475
Epoch 1.75: Loss = 0.796753
Epoch 1.76: Loss = 0.782196
Epoch 1.77: Loss = 0.764374
Epoch 1.78: Loss = 0.731628
Epoch 1.79: Loss = 0.747849
Epoch 1.80: Loss = 0.807388
Epoch 1.81: Loss = 0.689636
Epoch 1.82: Loss = 0.681427
Epoch 1.83: Loss = 0.862854
Epoch 1.84: Loss = 0.75412
Epoch 1.85: Loss = 0.855988
Epoch 1.86: Loss = 0.739273
Epoch 1.87: Loss = 0.633743
Epoch 1.88: Loss = 0.691132
Epoch 1.89: Loss = 0.771988
Epoch 1.90: Loss = 0.63208
Epoch 1.91: Loss = 0.738083
Epoch 1.92: Loss = 0.701797
Epoch 1.93: Loss = 0.747772
Epoch 1.94: Loss = 0.583603
Epoch 1.95: Loss = 0.692078
Epoch 1.96: Loss = 0.681366
Epoch 1.97: Loss = 0.537552
Epoch 1.98: Loss = 0.62587
Epoch 1.99: Loss = 0.70903
Epoch 1.100: Loss = 0.796097
Epoch 1.101: Loss = 0.68721
Epoch 1.102: Loss = 0.688492
Epoch 1.103: Loss = 0.598679
Epoch 1.104: Loss = 0.548782
Epoch 1.105: Loss = 0.682327
Epoch 1.106: Loss = 0.658905
Epoch 1.107: Loss = 0.564804
Epoch 1.108: Loss = 0.63829
Epoch 1.109: Loss = 0.582947
Epoch 1.110: Loss = 0.617111
Epoch 1.111: Loss = 0.527878
Epoch 1.112: Loss = 0.526184
Epoch 1.113: Loss = 0.582733
Epoch 1.114: Loss = 0.507492
Epoch 1.115: Loss = 0.607986
Epoch 1.116: Loss = 0.570786
Epoch 1.117: Loss = 0.470413
Epoch 1.118: Loss = 0.396072
Epoch 1.119: Loss = 0.442673
Epoch 1.120: Loss = 0.460892
TRAIN LOSS = 1.04237
TRAIN ACC = 70.9061 % (42546/60000)
Loss = 0.611404
Loss = 0.663574
Loss = 0.733292
Loss = 0.699142
Loss = 0.709763
Loss = 0.623627
Loss = 0.586884
Loss = 0.72583
Loss = 0.693024
Loss = 0.655533
Loss = 0.344177
Loss = 0.492889
Loss = 0.381241
Loss = 0.541565
Loss = 0.456085
Loss = 0.486282
Loss = 0.414154
Loss = 0.254288
Loss = 0.411469
Loss = 0.713547
TEST LOSS = 0.559888
TEST ACC = 425.459 % (8335/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.57045
Epoch 2.2: Loss = 0.6772
Epoch 2.3: Loss = 0.667389
Epoch 2.4: Loss = 0.514679
Epoch 2.5: Loss = 0.509308
Epoch 2.6: Loss = 0.512451
Epoch 2.7: Loss = 0.560913
Epoch 2.8: Loss = 0.569656
Epoch 2.9: Loss = 0.55777
Epoch 2.10: Loss = 0.551849
Epoch 2.11: Loss = 0.554474
Epoch 2.12: Loss = 0.540329
Epoch 2.13: Loss = 0.452316
Epoch 2.14: Loss = 0.544952
Epoch 2.15: Loss = 0.634293
Epoch 2.16: Loss = 0.602341
Epoch 2.17: Loss = 0.61441
Epoch 2.18: Loss = 0.691071
Epoch 2.19: Loss = 0.49707
Epoch 2.20: Loss = 0.469849
Epoch 2.21: Loss = 0.462967
Epoch 2.22: Loss = 0.465134
Epoch 2.23: Loss = 0.465988
Epoch 2.24: Loss = 0.653564
Epoch 2.25: Loss = 0.579361
Epoch 2.26: Loss = 0.665466
Epoch 2.27: Loss = 0.621719
Epoch 2.28: Loss = 0.621811
Epoch 2.29: Loss = 0.693008
Epoch 2.30: Loss = 0.710373
Epoch 2.31: Loss = 0.501694
Epoch 2.32: Loss = 0.634949
Epoch 2.33: Loss = 0.524673
Epoch 2.34: Loss = 0.603729
Epoch 2.35: Loss = 0.578339
Epoch 2.36: Loss = 0.65802
Epoch 2.37: Loss = 0.47525
Epoch 2.38: Loss = 0.463852
Epoch 2.39: Loss = 0.530487
Epoch 2.40: Loss = 0.496185
Epoch 2.41: Loss = 0.539017
Epoch 2.42: Loss = 0.571548
Epoch 2.43: Loss = 0.489807
Epoch 2.44: Loss = 0.399887
Epoch 2.45: Loss = 0.538986
Epoch 2.46: Loss = 0.581512
Epoch 2.47: Loss = 0.463821
Epoch 2.48: Loss = 0.532013
Epoch 2.49: Loss = 0.553787
Epoch 2.50: Loss = 0.595108
Epoch 2.51: Loss = 0.461639
Epoch 2.52: Loss = 0.454727
Epoch 2.53: Loss = 0.494751
Epoch 2.54: Loss = 0.59903
Epoch 2.55: Loss = 0.541382
Epoch 2.56: Loss = 0.466278
Epoch 2.57: Loss = 0.454147
Epoch 2.58: Loss = 0.496887
Epoch 2.59: Loss = 0.571701
Epoch 2.60: Loss = 0.623718
Epoch 2.61: Loss = 0.596039
Epoch 2.62: Loss = 0.611832
Epoch 2.63: Loss = 0.684387
Epoch 2.64: Loss = 0.629852
Epoch 2.65: Loss = 0.663925
Epoch 2.66: Loss = 0.531982
Epoch 2.67: Loss = 0.560638
Epoch 2.68: Loss = 0.371399
Epoch 2.69: Loss = 0.468109
Epoch 2.70: Loss = 0.604736
Epoch 2.71: Loss = 0.473129
Epoch 2.72: Loss = 0.500671
Epoch 2.73: Loss = 0.517944
Epoch 2.74: Loss = 0.405823
Epoch 2.75: Loss = 0.597092
Epoch 2.76: Loss = 0.535339
Epoch 2.77: Loss = 0.450851
Epoch 2.78: Loss = 0.495682
Epoch 2.79: Loss = 0.534882
Epoch 2.80: Loss = 0.546494
Epoch 2.81: Loss = 0.425476
Epoch 2.82: Loss = 0.461868
Epoch 2.83: Loss = 0.612518
Epoch 2.84: Loss = 0.496887
Epoch 2.85: Loss = 0.637512
Epoch 2.86: Loss = 0.547821
Epoch 2.87: Loss = 0.392303
Epoch 2.88: Loss = 0.465988
Epoch 2.89: Loss = 0.576279
Epoch 2.90: Loss = 0.432831
Epoch 2.91: Loss = 0.560074
Epoch 2.92: Loss = 0.548737
Epoch 2.93: Loss = 0.596634
Epoch 2.94: Loss = 0.420197
Epoch 2.95: Loss = 0.533936
Epoch 2.96: Loss = 0.535858
Epoch 2.97: Loss = 0.379425
Epoch 2.98: Loss = 0.471756
Epoch 2.99: Loss = 0.58812
Epoch 2.100: Loss = 0.655457
Epoch 2.101: Loss = 0.561996
Epoch 2.102: Loss = 0.537613
Epoch 2.103: Loss = 0.449097
Epoch 2.104: Loss = 0.443283
Epoch 2.105: Loss = 0.635849
Epoch 2.106: Loss = 0.557785
Epoch 2.107: Loss = 0.423264
Epoch 2.108: Loss = 0.497589
Epoch 2.109: Loss = 0.445175
Epoch 2.110: Loss = 0.496536
Epoch 2.111: Loss = 0.403046
Epoch 2.112: Loss = 0.418839
Epoch 2.113: Loss = 0.434769
Epoch 2.114: Loss = 0.397736
Epoch 2.115: Loss = 0.443802
Epoch 2.116: Loss = 0.438873
Epoch 2.117: Loss = 0.326859
Epoch 2.118: Loss = 0.272995
Epoch 2.119: Loss = 0.325455
Epoch 2.120: Loss = 0.361298
TRAIN LOSS = 0.526016
TRAIN ACC = 83.9737 % (50387/60000)
Loss = 0.486969
Loss = 0.531815
Loss = 0.609756
Loss = 0.642197
Loss = 0.621307
Loss = 0.478592
Loss = 0.482193
Loss = 0.647079
Loss = 0.596832
Loss = 0.59111
Loss = 0.228027
Loss = 0.362503
Loss = 0.300583
Loss = 0.434387
Loss = 0.277359
Loss = 0.350128
Loss = 0.315933
Loss = 0.110397
Loss = 0.286896
Loss = 0.590881
TEST LOSS = 0.447247
TEST ACC = 503.87 % (8616/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.44104
Epoch 3.2: Loss = 0.553711
Epoch 3.3: Loss = 0.565338
Epoch 3.4: Loss = 0.405914
Epoch 3.5: Loss = 0.373306
Epoch 3.6: Loss = 0.423325
Epoch 3.7: Loss = 0.424118
Epoch 3.8: Loss = 0.450409
Epoch 3.9: Loss = 0.445435
Epoch 3.10: Loss = 0.451691
Epoch 3.11: Loss = 0.491959
Epoch 3.12: Loss = 0.436661
Epoch 3.13: Loss = 0.398087
Epoch 3.14: Loss = 0.47023
Epoch 3.15: Loss = 0.535065
Epoch 3.16: Loss = 0.496704
Epoch 3.17: Loss = 0.523163
Epoch 3.18: Loss = 0.648285
Epoch 3.19: Loss = 0.439636
Epoch 3.20: Loss = 0.387344
Epoch 3.21: Loss = 0.390839
Epoch 3.22: Loss = 0.371735
Epoch 3.23: Loss = 0.407913
Epoch 3.24: Loss = 0.53537
Epoch 3.25: Loss = 0.511856
Epoch 3.26: Loss = 0.611206
Epoch 3.27: Loss = 0.561859
Epoch 3.28: Loss = 0.57193
Epoch 3.29: Loss = 0.592331
Epoch 3.30: Loss = 0.689285
Epoch 3.31: Loss = 0.433258
Epoch 3.32: Loss = 0.564941
Epoch 3.33: Loss = 0.424881
Epoch 3.34: Loss = 0.506683
Epoch 3.35: Loss = 0.484116
Epoch 3.36: Loss = 0.572723
Epoch 3.37: Loss = 0.386856
Epoch 3.38: Loss = 0.375809
Epoch 3.39: Loss = 0.457062
Epoch 3.40: Loss = 0.430908
Epoch 3.41: Loss = 0.426941
Epoch 3.42: Loss = 0.554962
Epoch 3.43: Loss = 0.463608
Epoch 3.44: Loss = 0.35701
Epoch 3.45: Loss = 0.492477
Epoch 3.46: Loss = 0.533569
Epoch 3.47: Loss = 0.419571
Epoch 3.48: Loss = 0.503738
Epoch 3.49: Loss = 0.49295
Epoch 3.50: Loss = 0.525497
Epoch 3.51: Loss = 0.419479
Epoch 3.52: Loss = 0.382385
Epoch 3.53: Loss = 0.413757
Epoch 3.54: Loss = 0.56778
Epoch 3.55: Loss = 0.492569
Epoch 3.56: Loss = 0.414093
Epoch 3.57: Loss = 0.414688
Epoch 3.58: Loss = 0.436295
Epoch 3.59: Loss = 0.517776
Epoch 3.60: Loss = 0.564758
Epoch 3.61: Loss = 0.524445
Epoch 3.62: Loss = 0.560287
Epoch 3.63: Loss = 0.636368
Epoch 3.64: Loss = 0.597321
Epoch 3.65: Loss = 0.57666
Epoch 3.66: Loss = 0.484314
Epoch 3.67: Loss = 0.463974
Epoch 3.68: Loss = 0.326569
Epoch 3.69: Loss = 0.36821
Epoch 3.70: Loss = 0.560867
Epoch 3.71: Loss = 0.408035
Epoch 3.72: Loss = 0.418686
Epoch 3.73: Loss = 0.471069
Epoch 3.74: Loss = 0.363251
Epoch 3.75: Loss = 0.615845
Epoch 3.76: Loss = 0.463516
Epoch 3.77: Loss = 0.384323
Epoch 3.78: Loss = 0.453598
Epoch 3.79: Loss = 0.521027
Epoch 3.80: Loss = 0.51561
Epoch 3.81: Loss = 0.393768
Epoch 3.82: Loss = 0.407578
Epoch 3.83: Loss = 0.563782
Epoch 3.84: Loss = 0.459778
Epoch 3.85: Loss = 0.593918
Epoch 3.86: Loss = 0.532333
Epoch 3.87: Loss = 0.36467
Epoch 3.88: Loss = 0.45311
Epoch 3.89: Loss = 0.498978
Epoch 3.90: Loss = 0.414169
Epoch 3.91: Loss = 0.541565
Epoch 3.92: Loss = 0.501785
Epoch 3.93: Loss = 0.582977
Epoch 3.94: Loss = 0.378082
Epoch 3.95: Loss = 0.492203
Epoch 3.96: Loss = 0.498413
Epoch 3.97: Loss = 0.334885
Epoch 3.98: Loss = 0.434021
Epoch 3.99: Loss = 0.564194
Epoch 3.100: Loss = 0.664749
Epoch 3.101: Loss = 0.565933
Epoch 3.102: Loss = 0.446777
Epoch 3.103: Loss = 0.43045
Epoch 3.104: Loss = 0.41394
Epoch 3.105: Loss = 0.575836
Epoch 3.106: Loss = 0.564575
Epoch 3.107: Loss = 0.368469
Epoch 3.108: Loss = 0.516769
Epoch 3.109: Loss = 0.432922
Epoch 3.110: Loss = 0.478897
Epoch 3.111: Loss = 0.365067
Epoch 3.112: Loss = 0.357132
Epoch 3.113: Loss = 0.41098
Epoch 3.114: Loss = 0.371017
Epoch 3.115: Loss = 0.411972
Epoch 3.116: Loss = 0.421097
Epoch 3.117: Loss = 0.328323
Epoch 3.118: Loss = 0.250381
Epoch 3.119: Loss = 0.321213
Epoch 3.120: Loss = 0.327286
TRAIN LOSS = 0.469879
TRAIN ACC = 85.7376 % (51445/60000)
Loss = 0.459351
Loss = 0.526031
Loss = 0.597748
Loss = 0.647858
Loss = 0.617706
Loss = 0.475616
Loss = 0.477249
Loss = 0.6763
Loss = 0.595261
Loss = 0.577881
Loss = 0.19931
Loss = 0.359924
Loss = 0.280777
Loss = 0.383698
Loss = 0.271088
Loss = 0.275284
Loss = 0.289398
Loss = 0.100143
Loss = 0.259933
Loss = 0.593124
TEST LOSS = 0.433184
TEST ACC = 514.449 % (8736/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.417923
Epoch 4.2: Loss = 0.552139
Epoch 4.3: Loss = 0.570511
Epoch 4.4: Loss = 0.35144
Epoch 4.5: Loss = 0.387756
Epoch 4.6: Loss = 0.453491
Epoch 4.7: Loss = 0.403595
Epoch 4.8: Loss = 0.45134
Epoch 4.9: Loss = 0.427673
Epoch 4.10: Loss = 0.453491
Epoch 4.11: Loss = 0.447586
Epoch 4.12: Loss = 0.4142
Epoch 4.13: Loss = 0.356491
Epoch 4.14: Loss = 0.445374
Epoch 4.15: Loss = 0.499664
Epoch 4.16: Loss = 0.461151
Epoch 4.17: Loss = 0.498428
Epoch 4.18: Loss = 0.672012
Epoch 4.19: Loss = 0.464554
Epoch 4.20: Loss = 0.389801
Epoch 4.21: Loss = 0.366699
Epoch 4.22: Loss = 0.353745
Epoch 4.23: Loss = 0.410263
Epoch 4.24: Loss = 0.541183
Epoch 4.25: Loss = 0.51709
Epoch 4.26: Loss = 0.617325
Epoch 4.27: Loss = 0.51561
Epoch 4.28: Loss = 0.533463
Epoch 4.29: Loss = 0.616104
Epoch 4.30: Loss = 0.728699
Epoch 4.31: Loss = 0.408798
Epoch 4.32: Loss = 0.51828
Epoch 4.33: Loss = 0.42305
Epoch 4.34: Loss = 0.490036
Epoch 4.35: Loss = 0.448578
Epoch 4.36: Loss = 0.55777
Epoch 4.37: Loss = 0.372162
Epoch 4.38: Loss = 0.382965
Epoch 4.39: Loss = 0.486206
Epoch 4.40: Loss = 0.425095
Epoch 4.41: Loss = 0.45787
Epoch 4.42: Loss = 0.579422
Epoch 4.43: Loss = 0.417023
Epoch 4.44: Loss = 0.343048
Epoch 4.45: Loss = 0.519196
Epoch 4.46: Loss = 0.507477
Epoch 4.47: Loss = 0.415878
Epoch 4.48: Loss = 0.509079
Epoch 4.49: Loss = 0.50264
Epoch 4.50: Loss = 0.54216
Epoch 4.51: Loss = 0.425491
Epoch 4.52: Loss = 0.362473
Epoch 4.53: Loss = 0.424805
Epoch 4.54: Loss = 0.598984
Epoch 4.55: Loss = 0.478363
Epoch 4.56: Loss = 0.443649
Epoch 4.57: Loss = 0.3983
Epoch 4.58: Loss = 0.432144
Epoch 4.59: Loss = 0.53038
Epoch 4.60: Loss = 0.554565
Epoch 4.61: Loss = 0.542633
Epoch 4.62: Loss = 0.542984
Epoch 4.63: Loss = 0.675079
Epoch 4.64: Loss = 0.60994
Epoch 4.65: Loss = 0.634644
Epoch 4.66: Loss = 0.466827
Epoch 4.67: Loss = 0.481232
Epoch 4.68: Loss = 0.298508
Epoch 4.69: Loss = 0.36731
Epoch 4.70: Loss = 0.566315
Epoch 4.71: Loss = 0.418259
Epoch 4.72: Loss = 0.430542
Epoch 4.73: Loss = 0.435104
Epoch 4.74: Loss = 0.362122
Epoch 4.75: Loss = 0.623291
Epoch 4.76: Loss = 0.471878
Epoch 4.77: Loss = 0.382355
Epoch 4.78: Loss = 0.476044
Epoch 4.79: Loss = 0.530212
Epoch 4.80: Loss = 0.501007
Epoch 4.81: Loss = 0.39624
Epoch 4.82: Loss = 0.403961
Epoch 4.83: Loss = 0.57515
Epoch 4.84: Loss = 0.464966
Epoch 4.85: Loss = 0.646652
Epoch 4.86: Loss = 0.525009
Epoch 4.87: Loss = 0.36235
Epoch 4.88: Loss = 0.442139
Epoch 4.89: Loss = 0.528824
Epoch 4.90: Loss = 0.433762
Epoch 4.91: Loss = 0.560913
Epoch 4.92: Loss = 0.483124
Epoch 4.93: Loss = 0.653473
Epoch 4.94: Loss = 0.369888
Epoch 4.95: Loss = 0.46669
Epoch 4.96: Loss = 0.525986
Epoch 4.97: Loss = 0.377716
Epoch 4.98: Loss = 0.455933
Epoch 4.99: Loss = 0.594559
Epoch 4.100: Loss = 0.618668
Epoch 4.101: Loss = 0.600327
Epoch 4.102: Loss = 0.459671
Epoch 4.103: Loss = 0.435913
Epoch 4.104: Loss = 0.413162
Epoch 4.105: Loss = 0.579071
Epoch 4.106: Loss = 0.558105
Epoch 4.107: Loss = 0.361984
Epoch 4.108: Loss = 0.523392
Epoch 4.109: Loss = 0.425995
Epoch 4.110: Loss = 0.466812
Epoch 4.111: Loss = 0.343414
Epoch 4.112: Loss = 0.356125
Epoch 4.113: Loss = 0.387665
Epoch 4.114: Loss = 0.354965
Epoch 4.115: Loss = 0.407669
Epoch 4.116: Loss = 0.398376
Epoch 4.117: Loss = 0.286118
Epoch 4.118: Loss = 0.225113
Epoch 4.119: Loss = 0.32576
Epoch 4.120: Loss = 0.332642
TRAIN LOSS = 0.468491
TRAIN ACC = 86.3235 % (51797/60000)
Loss = 0.452698
Loss = 0.524292
Loss = 0.575287
Loss = 0.65683
Loss = 0.617477
Loss = 0.464432
Loss = 0.466354
Loss = 0.678696
Loss = 0.63588
Loss = 0.561722
Loss = 0.189941
Loss = 0.340942
Loss = 0.285995
Loss = 0.375488
Loss = 0.255478
Loss = 0.293243
Loss = 0.294937
Loss = 0.0865784
Loss = 0.250381
Loss = 0.567291
TEST LOSS = 0.428697
TEST ACC = 517.969 % (8755/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.420013
Epoch 5.2: Loss = 0.547165
Epoch 5.3: Loss = 0.571671
Epoch 5.4: Loss = 0.338242
Epoch 5.5: Loss = 0.398636
Epoch 5.6: Loss = 0.454544
Epoch 5.7: Loss = 0.40834
Epoch 5.8: Loss = 0.426102
Epoch 5.9: Loss = 0.440247
Epoch 5.10: Loss = 0.450043
Epoch 5.11: Loss = 0.447647
Epoch 5.12: Loss = 0.399017
Epoch 5.13: Loss = 0.318451
Epoch 5.14: Loss = 0.432495
Epoch 5.15: Loss = 0.494171
Epoch 5.16: Loss = 0.524979
Epoch 5.17: Loss = 0.517044
Epoch 5.18: Loss = 0.700607
Epoch 5.19: Loss = 0.421158
Epoch 5.20: Loss = 0.408539
Epoch 5.21: Loss = 0.360046
Epoch 5.22: Loss = 0.356613
Epoch 5.23: Loss = 0.372452
Epoch 5.24: Loss = 0.599182
Epoch 5.25: Loss = 0.54451
Epoch 5.26: Loss = 0.663681
Epoch 5.27: Loss = 0.551956
Epoch 5.28: Loss = 0.529907
Epoch 5.29: Loss = 0.598602
Epoch 5.30: Loss = 0.704987
Epoch 5.31: Loss = 0.399094
Epoch 5.32: Loss = 0.553024
Epoch 5.33: Loss = 0.4189
Epoch 5.34: Loss = 0.493134
Epoch 5.35: Loss = 0.441772
Epoch 5.36: Loss = 0.577652
Epoch 5.37: Loss = 0.363251
Epoch 5.38: Loss = 0.34346
Epoch 5.39: Loss = 0.483353
Epoch 5.40: Loss = 0.416336
Epoch 5.41: Loss = 0.453903
Epoch 5.42: Loss = 0.575836
Epoch 5.43: Loss = 0.450226
Epoch 5.44: Loss = 0.366791
Epoch 5.45: Loss = 0.513397
Epoch 5.46: Loss = 0.530701
Epoch 5.47: Loss = 0.438004
Epoch 5.48: Loss = 0.507431
Epoch 5.49: Loss = 0.500183
Epoch 5.50: Loss = 0.540695
Epoch 5.51: Loss = 0.400436
Epoch 5.52: Loss = 0.384171
Epoch 5.53: Loss = 0.483978
Epoch 5.54: Loss = 0.560684
Epoch 5.55: Loss = 0.558929
Epoch 5.56: Loss = 0.382233
Epoch 5.57: Loss = 0.403702
Epoch 5.58: Loss = 0.470016
Epoch 5.59: Loss = 0.571289
Epoch 5.60: Loss = 0.546371
Epoch 5.61: Loss = 0.497772
Epoch 5.62: Loss = 0.560913
Epoch 5.63: Loss = 0.69957
Epoch 5.64: Loss = 0.562927
Epoch 5.65: Loss = 0.655518
Epoch 5.66: Loss = 0.445587
Epoch 5.67: Loss = 0.51857
Epoch 5.68: Loss = 0.289932
Epoch 5.69: Loss = 0.38208
Epoch 5.70: Loss = 0.611267
Epoch 5.71: Loss = 0.411179
Epoch 5.72: Loss = 0.439423
Epoch 5.73: Loss = 0.453262
Epoch 5.74: Loss = 0.37709
Epoch 5.75: Loss = 0.704178
Epoch 5.76: Loss = 0.472473
Epoch 5.77: Loss = 0.387436
Epoch 5.78: Loss = 0.436935
Epoch 5.79: Loss = 0.593643
Epoch 5.80: Loss = 0.530212
Epoch 5.81: Loss = 0.406982
Epoch 5.82: Loss = 0.400406
Epoch 5.83: Loss = 0.603958
Epoch 5.84: Loss = 0.467529
Epoch 5.85: Loss = 0.710464
Epoch 5.86: Loss = 0.562149
Epoch 5.87: Loss = 0.392227
Epoch 5.88: Loss = 0.436218
Epoch 5.89: Loss = 0.492645
Epoch 5.90: Loss = 0.445984
Epoch 5.91: Loss = 0.54129
Epoch 5.92: Loss = 0.535645
Epoch 5.93: Loss = 0.653748
Epoch 5.94: Loss = 0.393707
Epoch 5.95: Loss = 0.507065
Epoch 5.96: Loss = 0.576843
Epoch 5.97: Loss = 0.384857
Epoch 5.98: Loss = 0.480576
Epoch 5.99: Loss = 0.573624
Epoch 5.100: Loss = 0.641724
Epoch 5.101: Loss = 0.608673
Epoch 5.102: Loss = 0.45636
Epoch 5.103: Loss = 0.426865
Epoch 5.104: Loss = 0.387512
Epoch 5.105: Loss = 0.564285
Epoch 5.106: Loss = 0.572296
Epoch 5.107: Loss = 0.336517
Epoch 5.108: Loss = 0.524826
Epoch 5.109: Loss = 0.421661
Epoch 5.110: Loss = 0.485352
Epoch 5.111: Loss = 0.38414
Epoch 5.112: Loss = 0.397156
Epoch 5.113: Loss = 0.42244
Epoch 5.114: Loss = 0.35379
Epoch 5.115: Loss = 0.398071
Epoch 5.116: Loss = 0.402267
Epoch 5.117: Loss = 0.248154
Epoch 5.118: Loss = 0.230576
Epoch 5.119: Loss = 0.309555
Epoch 5.120: Loss = 0.375351
TRAIN LOSS = 0.475616
TRAIN ACC = 86.5738 % (51947/60000)
Loss = 0.468491
Loss = 0.529144
Loss = 0.60936
Loss = 0.678085
Loss = 0.634201
Loss = 0.492035
Loss = 0.465317
Loss = 0.68187
Loss = 0.633835
Loss = 0.618469
Loss = 0.199615
Loss = 0.346588
Loss = 0.286514
Loss = 0.439728
Loss = 0.250687
Loss = 0.357819
Loss = 0.284317
Loss = 0.0867462
Loss = 0.222656
Loss = 0.597168
TEST LOSS = 0.444132
TEST ACC = 519.469 % (8741/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.427597
Epoch 6.2: Loss = 0.540878
Epoch 6.3: Loss = 0.572632
Epoch 6.4: Loss = 0.339035
Epoch 6.5: Loss = 0.450058
Epoch 6.6: Loss = 0.468597
Epoch 6.7: Loss = 0.413071
Epoch 6.8: Loss = 0.436539
Epoch 6.9: Loss = 0.445831
Epoch 6.10: Loss = 0.463242
Epoch 6.11: Loss = 0.475967
Epoch 6.12: Loss = 0.392715
Epoch 6.13: Loss = 0.341553
Epoch 6.14: Loss = 0.454483
Epoch 6.15: Loss = 0.531616
Epoch 6.16: Loss = 0.50386
Epoch 6.17: Loss = 0.548416
Epoch 6.18: Loss = 0.71524
Epoch 6.19: Loss = 0.438293
Epoch 6.20: Loss = 0.405746
Epoch 6.21: Loss = 0.375732
Epoch 6.22: Loss = 0.378754
Epoch 6.23: Loss = 0.387848
Epoch 6.24: Loss = 0.623047
Epoch 6.25: Loss = 0.592911
Epoch 6.26: Loss = 0.663055
Epoch 6.27: Loss = 0.555267
Epoch 6.28: Loss = 0.556885
Epoch 6.29: Loss = 0.63533
Epoch 6.30: Loss = 0.772827
Epoch 6.31: Loss = 0.394913
Epoch 6.32: Loss = 0.583237
Epoch 6.33: Loss = 0.45665
Epoch 6.34: Loss = 0.536804
Epoch 6.35: Loss = 0.467041
Epoch 6.36: Loss = 0.608398
Epoch 6.37: Loss = 0.39003
Epoch 6.38: Loss = 0.364746
Epoch 6.39: Loss = 0.507294
Epoch 6.40: Loss = 0.475601
Epoch 6.41: Loss = 0.476486
Epoch 6.42: Loss = 0.654099
Epoch 6.43: Loss = 0.492569
Epoch 6.44: Loss = 0.360275
Epoch 6.45: Loss = 0.47728
Epoch 6.46: Loss = 0.556992
Epoch 6.47: Loss = 0.427994
Epoch 6.48: Loss = 0.500488
Epoch 6.49: Loss = 0.512466
Epoch 6.50: Loss = 0.539398
Epoch 6.51: Loss = 0.414307
Epoch 6.52: Loss = 0.400223
Epoch 6.53: Loss = 0.475281
Epoch 6.54: Loss = 0.58606
Epoch 6.55: Loss = 0.504837
Epoch 6.56: Loss = 0.422684
Epoch 6.57: Loss = 0.445709
Epoch 6.58: Loss = 0.455917
Epoch 6.59: Loss = 0.556244
Epoch 6.60: Loss = 0.564102
Epoch 6.61: Loss = 0.46759
Epoch 6.62: Loss = 0.54631
Epoch 6.63: Loss = 0.702866
Epoch 6.64: Loss = 0.526932
Epoch 6.65: Loss = 0.655121
Epoch 6.66: Loss = 0.451065
Epoch 6.67: Loss = 0.477005
Epoch 6.68: Loss = 0.307434
Epoch 6.69: Loss = 0.369186
Epoch 6.70: Loss = 0.567764
Epoch 6.71: Loss = 0.415314
Epoch 6.72: Loss = 0.420349
Epoch 6.73: Loss = 0.475754
Epoch 6.74: Loss = 0.361862
Epoch 6.75: Loss = 0.790634
Epoch 6.76: Loss = 0.441238
Epoch 6.77: Loss = 0.400986
Epoch 6.78: Loss = 0.478043
Epoch 6.79: Loss = 0.533585
Epoch 6.80: Loss = 0.546814
Epoch 6.81: Loss = 0.420746
Epoch 6.82: Loss = 0.409363
Epoch 6.83: Loss = 0.567245
Epoch 6.84: Loss = 0.469635
Epoch 6.85: Loss = 0.675949
Epoch 6.86: Loss = 0.581482
Epoch 6.87: Loss = 0.414948
Epoch 6.88: Loss = 0.445251
Epoch 6.89: Loss = 0.472137
Epoch 6.90: Loss = 0.463959
Epoch 6.91: Loss = 0.56926
Epoch 6.92: Loss = 0.567444
Epoch 6.93: Loss = 0.687195
Epoch 6.94: Loss = 0.389969
Epoch 6.95: Loss = 0.482956
Epoch 6.96: Loss = 0.563599
Epoch 6.97: Loss = 0.36908
Epoch 6.98: Loss = 0.47197
Epoch 6.99: Loss = 0.574234
Epoch 6.100: Loss = 0.680573
Epoch 6.101: Loss = 0.619507
Epoch 6.102: Loss = 0.464081
Epoch 6.103: Loss = 0.420975
Epoch 6.104: Loss = 0.408707
Epoch 6.105: Loss = 0.555725
Epoch 6.106: Loss = 0.612427
Epoch 6.107: Loss = 0.344162
Epoch 6.108: Loss = 0.501587
Epoch 6.109: Loss = 0.407211
Epoch 6.110: Loss = 0.47081
Epoch 6.111: Loss = 0.394287
Epoch 6.112: Loss = 0.383713
Epoch 6.113: Loss = 0.430908
Epoch 6.114: Loss = 0.407272
Epoch 6.115: Loss = 0.406464
Epoch 6.116: Loss = 0.42308
Epoch 6.117: Loss = 0.260025
Epoch 6.118: Loss = 0.257324
Epoch 6.119: Loss = 0.344971
Epoch 6.120: Loss = 0.402451
TRAIN LOSS = 0.485367
TRAIN ACC = 86.6394 % (51986/60000)
Loss = 0.436707
Loss = 0.476181
Loss = 0.612808
Loss = 0.67186
Loss = 0.636078
Loss = 0.466492
Loss = 0.437408
Loss = 0.66272
Loss = 0.636383
Loss = 0.584824
Loss = 0.216187
Loss = 0.385254
Loss = 0.322159
Loss = 0.440979
Loss = 0.263977
Loss = 0.373901
Loss = 0.320007
Loss = 0.0965424
Loss = 0.223862
Loss = 0.606354
TEST LOSS = 0.443534
TEST ACC = 519.859 % (8752/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.44812
Epoch 7.2: Loss = 0.532837
Epoch 7.3: Loss = 0.582001
Epoch 7.4: Loss = 0.356461
Epoch 7.5: Loss = 0.410904
Epoch 7.6: Loss = 0.519516
Epoch 7.7: Loss = 0.388672
Epoch 7.8: Loss = 0.431076
Epoch 7.9: Loss = 0.420303
Epoch 7.10: Loss = 0.470352
Epoch 7.11: Loss = 0.48851
Epoch 7.12: Loss = 0.403091
Epoch 7.13: Loss = 0.311096
Epoch 7.14: Loss = 0.495453
Epoch 7.15: Loss = 0.539963
Epoch 7.16: Loss = 0.51828
Epoch 7.17: Loss = 0.552902
Epoch 7.18: Loss = 0.73262
Epoch 7.19: Loss = 0.471725
Epoch 7.20: Loss = 0.476563
Epoch 7.21: Loss = 0.443542
Epoch 7.22: Loss = 0.418671
Epoch 7.23: Loss = 0.387451
Epoch 7.24: Loss = 0.696701
Epoch 7.25: Loss = 0.553223
Epoch 7.26: Loss = 0.605728
Epoch 7.27: Loss = 0.556076
Epoch 7.28: Loss = 0.605438
Epoch 7.29: Loss = 0.628479
Epoch 7.30: Loss = 0.76535
Epoch 7.31: Loss = 0.420517
Epoch 7.32: Loss = 0.617676
Epoch 7.33: Loss = 0.496857
Epoch 7.34: Loss = 0.540909
Epoch 7.35: Loss = 0.501282
Epoch 7.36: Loss = 0.662888
Epoch 7.37: Loss = 0.388519
Epoch 7.38: Loss = 0.366974
Epoch 7.39: Loss = 0.499573
Epoch 7.40: Loss = 0.496475
Epoch 7.41: Loss = 0.408981
Epoch 7.42: Loss = 0.710129
Epoch 7.43: Loss = 0.530167
Epoch 7.44: Loss = 0.36705
Epoch 7.45: Loss = 0.485321
Epoch 7.46: Loss = 0.579849
Epoch 7.47: Loss = 0.433167
Epoch 7.48: Loss = 0.541611
Epoch 7.49: Loss = 0.510239
Epoch 7.50: Loss = 0.575912
Epoch 7.51: Loss = 0.445618
Epoch 7.52: Loss = 0.388626
Epoch 7.53: Loss = 0.485016
Epoch 7.54: Loss = 0.613739
Epoch 7.55: Loss = 0.572021
Epoch 7.56: Loss = 0.38562
Epoch 7.57: Loss = 0.450745
Epoch 7.58: Loss = 0.48114
Epoch 7.59: Loss = 0.602768
Epoch 7.60: Loss = 0.605698
Epoch 7.61: Loss = 0.436264
Epoch 7.62: Loss = 0.621429
Epoch 7.63: Loss = 0.740906
Epoch 7.64: Loss = 0.522415
Epoch 7.65: Loss = 0.659485
Epoch 7.66: Loss = 0.439011
Epoch 7.67: Loss = 0.461212
Epoch 7.68: Loss = 0.343582
Epoch 7.69: Loss = 0.390137
Epoch 7.70: Loss = 0.60556
Epoch 7.71: Loss = 0.434082
Epoch 7.72: Loss = 0.393509
Epoch 7.73: Loss = 0.479294
Epoch 7.74: Loss = 0.413864
Epoch 7.75: Loss = 0.745224
Epoch 7.76: Loss = 0.522827
Epoch 7.77: Loss = 0.413132
Epoch 7.78: Loss = 0.559052
Epoch 7.79: Loss = 0.548477
Epoch 7.80: Loss = 0.621445
Epoch 7.81: Loss = 0.442352
Epoch 7.82: Loss = 0.4422
Epoch 7.83: Loss = 0.586029
Epoch 7.84: Loss = 0.468689
Epoch 7.85: Loss = 0.637695
Epoch 7.86: Loss = 0.57634
Epoch 7.87: Loss = 0.450851
Epoch 7.88: Loss = 0.513718
Epoch 7.89: Loss = 0.513123
Epoch 7.90: Loss = 0.458908
Epoch 7.91: Loss = 0.594131
Epoch 7.92: Loss = 0.61673
Epoch 7.93: Loss = 0.70575
Epoch 7.94: Loss = 0.383041
Epoch 7.95: Loss = 0.512772
Epoch 7.96: Loss = 0.624786
Epoch 7.97: Loss = 0.386658
Epoch 7.98: Loss = 0.531631
Epoch 7.99: Loss = 0.583908
Epoch 7.100: Loss = 0.665207
Epoch 7.101: Loss = 0.663071
Epoch 7.102: Loss = 0.513123
Epoch 7.103: Loss = 0.456345
Epoch 7.104: Loss = 0.447891
Epoch 7.105: Loss = 0.598282
Epoch 7.106: Loss = 0.689178
Epoch 7.107: Loss = 0.383514
Epoch 7.108: Loss = 0.539063
Epoch 7.109: Loss = 0.418259
Epoch 7.110: Loss = 0.48288
Epoch 7.111: Loss = 0.428604
Epoch 7.112: Loss = 0.392685
Epoch 7.113: Loss = 0.445297
Epoch 7.114: Loss = 0.433624
Epoch 7.115: Loss = 0.417862
Epoch 7.116: Loss = 0.474197
Epoch 7.117: Loss = 0.258133
Epoch 7.118: Loss = 0.289719
Epoch 7.119: Loss = 0.363663
Epoch 7.120: Loss = 0.454895
TRAIN LOSS = 0.504776
TRAIN ACC = 86.5555 % (51936/60000)
Loss = 0.486816
Loss = 0.552872
Loss = 0.645111
Loss = 0.733658
Loss = 0.649811
Loss = 0.496658
Loss = 0.455963
Loss = 0.684006
Loss = 0.691818
Loss = 0.586914
Loss = 0.204361
Loss = 0.386856
Loss = 0.345337
Loss = 0.474945
Loss = 0.265381
Loss = 0.331726
Loss = 0.335602
Loss = 0.0942993
Loss = 0.269394
Loss = 0.730911
TEST LOSS = 0.471122
TEST ACC = 519.359 % (8745/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.498367
Epoch 8.2: Loss = 0.664398
Epoch 8.3: Loss = 0.648987
Epoch 8.4: Loss = 0.385056
Epoch 8.5: Loss = 0.423965
Epoch 8.6: Loss = 0.495056
Epoch 8.7: Loss = 0.443832
Epoch 8.8: Loss = 0.432556
Epoch 8.9: Loss = 0.45433
Epoch 8.10: Loss = 0.501221
Epoch 8.11: Loss = 0.523819
Epoch 8.12: Loss = 0.455124
Epoch 8.13: Loss = 0.381195
Epoch 8.14: Loss = 0.540848
Epoch 8.15: Loss = 0.551071
Epoch 8.16: Loss = 0.571609
Epoch 8.17: Loss = 0.622803
Epoch 8.18: Loss = 0.690887
Epoch 8.19: Loss = 0.483566
Epoch 8.20: Loss = 0.471924
Epoch 8.21: Loss = 0.458694
Epoch 8.22: Loss = 0.440399
Epoch 8.23: Loss = 0.363205
Epoch 8.24: Loss = 0.658615
Epoch 8.25: Loss = 0.568329
Epoch 8.26: Loss = 0.649155
Epoch 8.27: Loss = 0.569275
Epoch 8.28: Loss = 0.624115
Epoch 8.29: Loss = 0.677505
Epoch 8.30: Loss = 0.743637
Epoch 8.31: Loss = 0.473434
Epoch 8.32: Loss = 0.63858
Epoch 8.33: Loss = 0.500061
Epoch 8.34: Loss = 0.573181
Epoch 8.35: Loss = 0.546692
Epoch 8.36: Loss = 0.668991
Epoch 8.37: Loss = 0.41748
Epoch 8.38: Loss = 0.398621
Epoch 8.39: Loss = 0.485718
Epoch 8.40: Loss = 0.481522
Epoch 8.41: Loss = 0.455704
Epoch 8.42: Loss = 0.770386
Epoch 8.43: Loss = 0.515472
Epoch 8.44: Loss = 0.3936
Epoch 8.45: Loss = 0.532532
Epoch 8.46: Loss = 0.614548
Epoch 8.47: Loss = 0.463806
Epoch 8.48: Loss = 0.542938
Epoch 8.49: Loss = 0.571136
Epoch 8.50: Loss = 0.560852
Epoch 8.51: Loss = 0.463242
Epoch 8.52: Loss = 0.418106
Epoch 8.53: Loss = 0.498108
Epoch 8.54: Loss = 0.630783
Epoch 8.55: Loss = 0.554611
Epoch 8.56: Loss = 0.414627
Epoch 8.57: Loss = 0.451187
Epoch 8.58: Loss = 0.501343
Epoch 8.59: Loss = 0.646774
Epoch 8.60: Loss = 0.549408
Epoch 8.61: Loss = 0.549469
Epoch 8.62: Loss = 0.724182
Epoch 8.63: Loss = 0.718414
Epoch 8.64: Loss = 0.618027
Epoch 8.65: Loss = 0.687317
Epoch 8.66: Loss = 0.480179
Epoch 8.67: Loss = 0.480286
Epoch 8.68: Loss = 0.331894
Epoch 8.69: Loss = 0.436096
Epoch 8.70: Loss = 0.65097
Epoch 8.71: Loss = 0.457275
Epoch 8.72: Loss = 0.344681
Epoch 8.73: Loss = 0.48735
Epoch 8.74: Loss = 0.407349
Epoch 8.75: Loss = 0.802231
Epoch 8.76: Loss = 0.544891
Epoch 8.77: Loss = 0.444366
Epoch 8.78: Loss = 0.520218
Epoch 8.79: Loss = 0.581131
Epoch 8.80: Loss = 0.612183
Epoch 8.81: Loss = 0.440048
Epoch 8.82: Loss = 0.430069
Epoch 8.83: Loss = 0.661026
Epoch 8.84: Loss = 0.49054
Epoch 8.85: Loss = 0.684769
Epoch 8.86: Loss = 0.599075
Epoch 8.87: Loss = 0.411011
Epoch 8.88: Loss = 0.550537
Epoch 8.89: Loss = 0.59494
Epoch 8.90: Loss = 0.463882
Epoch 8.91: Loss = 0.619492
Epoch 8.92: Loss = 0.652969
Epoch 8.93: Loss = 0.70546
Epoch 8.94: Loss = 0.382889
Epoch 8.95: Loss = 0.513794
Epoch 8.96: Loss = 0.616043
Epoch 8.97: Loss = 0.400757
Epoch 8.98: Loss = 0.519592
Epoch 8.99: Loss = 0.572556
Epoch 8.100: Loss = 0.671936
Epoch 8.101: Loss = 0.673431
Epoch 8.102: Loss = 0.529785
Epoch 8.103: Loss = 0.455765
Epoch 8.104: Loss = 0.450623
Epoch 8.105: Loss = 0.61232
Epoch 8.106: Loss = 0.672791
Epoch 8.107: Loss = 0.39122
Epoch 8.108: Loss = 0.544632
Epoch 8.109: Loss = 0.4431
Epoch 8.110: Loss = 0.488144
Epoch 8.111: Loss = 0.42453
Epoch 8.112: Loss = 0.419632
Epoch 8.113: Loss = 0.482391
Epoch 8.114: Loss = 0.452835
Epoch 8.115: Loss = 0.479782
Epoch 8.116: Loss = 0.480988
Epoch 8.117: Loss = 0.362442
Epoch 8.118: Loss = 0.267868
Epoch 8.119: Loss = 0.319702
Epoch 8.120: Loss = 0.483536
TRAIN LOSS = 0.525192
TRAIN ACC = 86.499 % (51902/60000)
Loss = 0.521912
Loss = 0.600571
Loss = 0.693878
Loss = 0.83754
Loss = 0.708572
Loss = 0.531433
Loss = 0.506973
Loss = 0.748505
Loss = 0.731201
Loss = 0.628799
Loss = 0.171021
Loss = 0.46286
Loss = 0.389282
Loss = 0.474823
Loss = 0.285507
Loss = 0.418503
Loss = 0.301971
Loss = 0.101669
Loss = 0.313232
Loss = 0.761017
TEST LOSS = 0.509463
TEST ACC = 519.019 % (8702/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.504013
Epoch 9.2: Loss = 0.61528
Epoch 9.3: Loss = 0.666687
Epoch 9.4: Loss = 0.412857
Epoch 9.5: Loss = 0.427933
Epoch 9.6: Loss = 0.521667
Epoch 9.7: Loss = 0.434998
Epoch 9.8: Loss = 0.490005
Epoch 9.9: Loss = 0.50058
Epoch 9.10: Loss = 0.502686
Epoch 9.11: Loss = 0.631729
Epoch 9.12: Loss = 0.497696
Epoch 9.13: Loss = 0.383286
Epoch 9.14: Loss = 0.552353
Epoch 9.15: Loss = 0.614334
Epoch 9.16: Loss = 0.597382
Epoch 9.17: Loss = 0.596771
Epoch 9.18: Loss = 0.808716
Epoch 9.19: Loss = 0.546463
Epoch 9.20: Loss = 0.464172
Epoch 9.21: Loss = 0.505722
Epoch 9.22: Loss = 0.422974
Epoch 9.23: Loss = 0.40715
Epoch 9.24: Loss = 0.619293
Epoch 9.25: Loss = 0.629074
Epoch 9.26: Loss = 0.763718
Epoch 9.27: Loss = 0.590881
Epoch 9.28: Loss = 0.59079
Epoch 9.29: Loss = 0.749557
Epoch 9.30: Loss = 0.792953
Epoch 9.31: Loss = 0.493973
Epoch 9.32: Loss = 0.596558
Epoch 9.33: Loss = 0.473587
Epoch 9.34: Loss = 0.597641
Epoch 9.35: Loss = 0.569733
Epoch 9.36: Loss = 0.678268
Epoch 9.37: Loss = 0.458466
Epoch 9.38: Loss = 0.455917
Epoch 9.39: Loss = 0.494049
Epoch 9.40: Loss = 0.480621
Epoch 9.41: Loss = 0.468292
Epoch 9.42: Loss = 0.792801
Epoch 9.43: Loss = 0.498718
Epoch 9.44: Loss = 0.380341
Epoch 9.45: Loss = 0.492371
Epoch 9.46: Loss = 0.73761
Epoch 9.47: Loss = 0.520523
Epoch 9.48: Loss = 0.569031
Epoch 9.49: Loss = 0.557388
Epoch 9.50: Loss = 0.650146
Epoch 9.51: Loss = 0.464066
Epoch 9.52: Loss = 0.426636
Epoch 9.53: Loss = 0.495575
Epoch 9.54: Loss = 0.732727
Epoch 9.55: Loss = 0.550522
Epoch 9.56: Loss = 0.458633
Epoch 9.57: Loss = 0.490417
Epoch 9.58: Loss = 0.554153
Epoch 9.59: Loss = 0.713287
Epoch 9.60: Loss = 0.614456
Epoch 9.61: Loss = 0.592606
Epoch 9.62: Loss = 0.718811
Epoch 9.63: Loss = 0.819275
Epoch 9.64: Loss = 0.662659
Epoch 9.65: Loss = 0.733917
Epoch 9.66: Loss = 0.508972
Epoch 9.67: Loss = 0.509308
Epoch 9.68: Loss = 0.358368
Epoch 9.69: Loss = 0.451797
Epoch 9.70: Loss = 0.704056
Epoch 9.71: Loss = 0.494263
Epoch 9.72: Loss = 0.393158
Epoch 9.73: Loss = 0.543228
Epoch 9.74: Loss = 0.415314
Epoch 9.75: Loss = 0.850861
Epoch 9.76: Loss = 0.56601
Epoch 9.77: Loss = 0.482819
Epoch 9.78: Loss = 0.554535
Epoch 9.79: Loss = 0.545685
Epoch 9.80: Loss = 0.629898
Epoch 9.81: Loss = 0.449493
Epoch 9.82: Loss = 0.46579
Epoch 9.83: Loss = 0.679062
Epoch 9.84: Loss = 0.508438
Epoch 9.85: Loss = 0.838425
Epoch 9.86: Loss = 0.628738
Epoch 9.87: Loss = 0.459793
Epoch 9.88: Loss = 0.536057
Epoch 9.89: Loss = 0.632339
Epoch 9.90: Loss = 0.444611
Epoch 9.91: Loss = 0.582748
Epoch 9.92: Loss = 0.668869
Epoch 9.93: Loss = 0.756699
Epoch 9.94: Loss = 0.422791
Epoch 9.95: Loss = 0.496613
Epoch 9.96: Loss = 0.6203
Epoch 9.97: Loss = 0.434906
Epoch 9.98: Loss = 0.531555
Epoch 9.99: Loss = 0.62381
Epoch 9.100: Loss = 0.740799
Epoch 9.101: Loss = 0.698029
Epoch 9.102: Loss = 0.53241
Epoch 9.103: Loss = 0.492355
Epoch 9.104: Loss = 0.508865
Epoch 9.105: Loss = 0.621796
Epoch 9.106: Loss = 0.689178
Epoch 9.107: Loss = 0.391586
Epoch 9.108: Loss = 0.591415
Epoch 9.109: Loss = 0.513123
Epoch 9.110: Loss = 0.524475
Epoch 9.111: Loss = 0.447433
Epoch 9.112: Loss = 0.471695
Epoch 9.113: Loss = 0.493713
Epoch 9.114: Loss = 0.471313
Epoch 9.115: Loss = 0.481369
Epoch 9.116: Loss = 0.546066
Epoch 9.117: Loss = 0.355804
Epoch 9.118: Loss = 0.248276
Epoch 9.119: Loss = 0.310776
Epoch 9.120: Loss = 0.531128
TRAIN LOSS = 0.552399
TRAIN ACC = 86.6165 % (51973/60000)
Loss = 0.519318
Loss = 0.642029
Loss = 0.77269
Loss = 0.877762
Loss = 0.738861
Loss = 0.558853
Loss = 0.548172
Loss = 0.863205
Loss = 0.803268
Loss = 0.673187
Loss = 0.220276
Loss = 0.4328
Loss = 0.328644
Loss = 0.483932
Loss = 0.262634
Loss = 0.399155
Loss = 0.328049
Loss = 0.0544891
Loss = 0.292191
Loss = 0.658768
TEST LOSS = 0.522914
TEST ACC = 519.73 % (8711/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.513123
Epoch 10.2: Loss = 0.623856
Epoch 10.3: Loss = 0.676743
Epoch 10.4: Loss = 0.454971
Epoch 10.5: Loss = 0.452835
Epoch 10.6: Loss = 0.550217
Epoch 10.7: Loss = 0.521545
Epoch 10.8: Loss = 0.5522
Epoch 10.9: Loss = 0.519135
Epoch 10.10: Loss = 0.538986
Epoch 10.11: Loss = 0.669586
Epoch 10.12: Loss = 0.535187
Epoch 10.13: Loss = 0.484528
Epoch 10.14: Loss = 0.568726
Epoch 10.15: Loss = 0.617706
Epoch 10.16: Loss = 0.621735
Epoch 10.17: Loss = 0.641922
Epoch 10.18: Loss = 0.805603
Epoch 10.19: Loss = 0.578064
Epoch 10.20: Loss = 0.484055
Epoch 10.21: Loss = 0.566254
Epoch 10.22: Loss = 0.438461
Epoch 10.23: Loss = 0.416946
Epoch 10.24: Loss = 0.618515
Epoch 10.25: Loss = 0.606934
Epoch 10.26: Loss = 0.758102
Epoch 10.27: Loss = 0.648773
Epoch 10.28: Loss = 0.611877
Epoch 10.29: Loss = 0.736954
Epoch 10.30: Loss = 0.806
Epoch 10.31: Loss = 0.531525
Epoch 10.32: Loss = 0.628082
Epoch 10.33: Loss = 0.521194
Epoch 10.34: Loss = 0.586044
Epoch 10.35: Loss = 0.604034
Epoch 10.36: Loss = 0.758759
Epoch 10.37: Loss = 0.496063
Epoch 10.38: Loss = 0.493439
Epoch 10.39: Loss = 0.545868
Epoch 10.40: Loss = 0.538788
Epoch 10.41: Loss = 0.465637
Epoch 10.42: Loss = 0.886215
Epoch 10.43: Loss = 0.485046
Epoch 10.44: Loss = 0.385696
Epoch 10.45: Loss = 0.525787
Epoch 10.46: Loss = 0.761017
Epoch 10.47: Loss = 0.560043
Epoch 10.48: Loss = 0.6138
Epoch 10.49: Loss = 0.628815
Epoch 10.50: Loss = 0.702347
Epoch 10.51: Loss = 0.474808
Epoch 10.52: Loss = 0.465134
Epoch 10.53: Loss = 0.525223
Epoch 10.54: Loss = 0.694366
Epoch 10.55: Loss = 0.553864
Epoch 10.56: Loss = 0.455322
Epoch 10.57: Loss = 0.489456
Epoch 10.58: Loss = 0.555695
Epoch 10.59: Loss = 0.754547
Epoch 10.60: Loss = 0.697769
Epoch 10.61: Loss = 0.603531
Epoch 10.62: Loss = 0.840881
Epoch 10.63: Loss = 0.82666
Epoch 10.64: Loss = 0.675049
Epoch 10.65: Loss = 0.765884
Epoch 10.66: Loss = 0.522522
Epoch 10.67: Loss = 0.510483
Epoch 10.68: Loss = 0.396973
Epoch 10.69: Loss = 0.455292
Epoch 10.70: Loss = 0.748581
Epoch 10.71: Loss = 0.516296
Epoch 10.72: Loss = 0.422119
Epoch 10.73: Loss = 0.621017
Epoch 10.74: Loss = 0.432007
Epoch 10.75: Loss = 0.879578
Epoch 10.76: Loss = 0.622604
Epoch 10.77: Loss = 0.500412
Epoch 10.78: Loss = 0.568085
Epoch 10.79: Loss = 0.590958
Epoch 10.80: Loss = 0.639664
Epoch 10.81: Loss = 0.446762
Epoch 10.82: Loss = 0.395081
Epoch 10.83: Loss = 0.690094
Epoch 10.84: Loss = 0.500534
Epoch 10.85: Loss = 0.802216
Epoch 10.86: Loss = 0.627762
Epoch 10.87: Loss = 0.502487
Epoch 10.88: Loss = 0.5569
Epoch 10.89: Loss = 0.619812
Epoch 10.90: Loss = 0.43132
Epoch 10.91: Loss = 0.632156
Epoch 10.92: Loss = 0.684677
Epoch 10.93: Loss = 0.768646
Epoch 10.94: Loss = 0.408966
Epoch 10.95: Loss = 0.51413
Epoch 10.96: Loss = 0.656754
Epoch 10.97: Loss = 0.431
Epoch 10.98: Loss = 0.577545
Epoch 10.99: Loss = 0.665741
Epoch 10.100: Loss = 0.686493
Epoch 10.101: Loss = 0.693634
Epoch 10.102: Loss = 0.511063
Epoch 10.103: Loss = 0.448669
Epoch 10.104: Loss = 0.50589
Epoch 10.105: Loss = 0.684692
Epoch 10.106: Loss = 0.754364
Epoch 10.107: Loss = 0.431519
Epoch 10.108: Loss = 0.621918
Epoch 10.109: Loss = 0.477585
Epoch 10.110: Loss = 0.553452
Epoch 10.111: Loss = 0.469482
Epoch 10.112: Loss = 0.446381
Epoch 10.113: Loss = 0.521545
Epoch 10.114: Loss = 0.464355
Epoch 10.115: Loss = 0.468506
Epoch 10.116: Loss = 0.57959
Epoch 10.117: Loss = 0.418762
Epoch 10.118: Loss = 0.268036
Epoch 10.119: Loss = 0.373718
Epoch 10.120: Loss = 0.537766
TRAIN LOSS = 0.574783
TRAIN ACC = 86.4273 % (51858/60000)
Loss = 0.593369
Loss = 0.688156
Loss = 0.800064
Loss = 0.860764
Loss = 0.81459
Loss = 0.515549
Loss = 0.585129
Loss = 0.9133
Loss = 0.777084
Loss = 0.680954
Loss = 0.256256
Loss = 0.42984
Loss = 0.402893
Loss = 0.51033
Loss = 0.240967
Loss = 0.332275
Loss = 0.338669
Loss = 0.0506134
Loss = 0.336243
Loss = 0.737411
TEST LOSS = 0.543223
TEST ACC = 518.579 % (8706/10000)
