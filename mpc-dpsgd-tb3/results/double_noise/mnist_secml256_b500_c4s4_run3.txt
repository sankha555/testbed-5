Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.37587
Epoch 1.2: Loss = 2.3511
Epoch 1.3: Loss = 2.31636
Epoch 1.4: Loss = 2.27293
Epoch 1.5: Loss = 2.23022
Epoch 1.6: Loss = 2.20546
Epoch 1.7: Loss = 2.20091
Epoch 1.8: Loss = 2.14436
Epoch 1.9: Loss = 2.12819
Epoch 1.10: Loss = 2.08949
Epoch 1.11: Loss = 2.02734
Epoch 1.12: Loss = 2.0087
Epoch 1.13: Loss = 1.96407
Epoch 1.14: Loss = 1.96527
Epoch 1.15: Loss = 1.99409
Epoch 1.16: Loss = 1.90988
Epoch 1.17: Loss = 1.87241
Epoch 1.18: Loss = 1.85631
Epoch 1.19: Loss = 1.83142
Epoch 1.20: Loss = 1.80121
Epoch 1.21: Loss = 1.70729
Epoch 1.22: Loss = 1.70238
Epoch 1.23: Loss = 1.66553
Epoch 1.24: Loss = 1.75005
Epoch 1.25: Loss = 1.63515
Epoch 1.26: Loss = 1.64656
Epoch 1.27: Loss = 1.59033
Epoch 1.28: Loss = 1.58432
Epoch 1.29: Loss = 1.5889
Epoch 1.30: Loss = 1.63065
Epoch 1.31: Loss = 1.48163
Epoch 1.32: Loss = 1.50891
Epoch 1.33: Loss = 1.42673
Epoch 1.34: Loss = 1.45735
Epoch 1.35: Loss = 1.39153
Epoch 1.36: Loss = 1.51518
Epoch 1.37: Loss = 1.366
Epoch 1.38: Loss = 1.29704
Epoch 1.39: Loss = 1.27499
Epoch 1.40: Loss = 1.18736
Epoch 1.41: Loss = 1.24977
Epoch 1.42: Loss = 1.22989
Epoch 1.43: Loss = 1.17282
Epoch 1.44: Loss = 1.09776
Epoch 1.45: Loss = 1.24269
Epoch 1.46: Loss = 1.17369
Epoch 1.47: Loss = 1.11743
Epoch 1.48: Loss = 1.15298
Epoch 1.49: Loss = 1.08064
Epoch 1.50: Loss = 1.13712
Epoch 1.51: Loss = 0.988174
Epoch 1.52: Loss = 1.00821
Epoch 1.53: Loss = 1.05708
Epoch 1.54: Loss = 1.05235
Epoch 1.55: Loss = 1.06139
Epoch 1.56: Loss = 0.959106
Epoch 1.57: Loss = 0.885742
Epoch 1.58: Loss = 0.920731
Epoch 1.59: Loss = 0.916809
Epoch 1.60: Loss = 1.05141
Epoch 1.61: Loss = 0.955093
Epoch 1.62: Loss = 0.991989
Epoch 1.63: Loss = 1.02782
Epoch 1.64: Loss = 0.968933
Epoch 1.65: Loss = 0.992035
Epoch 1.66: Loss = 0.865692
Epoch 1.67: Loss = 0.886978
Epoch 1.68: Loss = 0.720123
Epoch 1.69: Loss = 0.796127
Epoch 1.70: Loss = 0.886185
Epoch 1.71: Loss = 0.802444
Epoch 1.72: Loss = 0.826385
Epoch 1.73: Loss = 0.826065
Epoch 1.74: Loss = 0.702774
Epoch 1.75: Loss = 0.8517
Epoch 1.76: Loss = 0.800949
Epoch 1.77: Loss = 0.784943
Epoch 1.78: Loss = 0.77002
Epoch 1.79: Loss = 0.751678
Epoch 1.80: Loss = 0.879547
Epoch 1.81: Loss = 0.716278
Epoch 1.82: Loss = 0.668152
Epoch 1.83: Loss = 0.852264
Epoch 1.84: Loss = 0.787445
Epoch 1.85: Loss = 0.839371
Epoch 1.86: Loss = 0.755753
Epoch 1.87: Loss = 0.697601
Epoch 1.88: Loss = 0.727844
Epoch 1.89: Loss = 0.78038
Epoch 1.90: Loss = 0.647049
Epoch 1.91: Loss = 0.757355
Epoch 1.92: Loss = 0.702942
Epoch 1.93: Loss = 0.76033
Epoch 1.94: Loss = 0.580902
Epoch 1.95: Loss = 0.728577
Epoch 1.96: Loss = 0.707077
Epoch 1.97: Loss = 0.535431
Epoch 1.98: Loss = 0.662949
Epoch 1.99: Loss = 0.768448
Epoch 1.100: Loss = 0.844696
Epoch 1.101: Loss = 0.735321
Epoch 1.102: Loss = 0.64032
Epoch 1.103: Loss = 0.603271
Epoch 1.104: Loss = 0.588318
Epoch 1.105: Loss = 0.67363
Epoch 1.106: Loss = 0.708069
Epoch 1.107: Loss = 0.614151
Epoch 1.108: Loss = 0.636032
Epoch 1.109: Loss = 0.600723
Epoch 1.110: Loss = 0.631638
Epoch 1.111: Loss = 0.550049
Epoch 1.112: Loss = 0.514587
Epoch 1.113: Loss = 0.598419
Epoch 1.114: Loss = 0.540436
Epoch 1.115: Loss = 0.59668
Epoch 1.116: Loss = 0.584808
Epoch 1.117: Loss = 0.503006
Epoch 1.118: Loss = 0.438126
Epoch 1.119: Loss = 0.466019
Epoch 1.120: Loss = 0.454529
TRAIN LOSS = 1.13997
TRAIN ACC = 67.8406 % (40706/60000)
Loss = 0.638138
Loss = 0.660492
Loss = 0.723007
Loss = 0.708557
Loss = 0.757309
Loss = 0.651001
Loss = 0.610214
Loss = 0.796906
Loss = 0.732483
Loss = 0.706711
Loss = 0.334
Loss = 0.50119
Loss = 0.380936
Loss = 0.560593
Loss = 0.449524
Loss = 0.509415
Loss = 0.393661
Loss = 0.237686
Loss = 0.390488
Loss = 0.742844
TEST LOSS = 0.574257
TEST ACC = 407.059 % (8223/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.542267
Epoch 2.2: Loss = 0.727661
Epoch 2.3: Loss = 0.669708
Epoch 2.4: Loss = 0.545364
Epoch 2.5: Loss = 0.54483
Epoch 2.6: Loss = 0.537689
Epoch 2.7: Loss = 0.62706
Epoch 2.8: Loss = 0.583649
Epoch 2.9: Loss = 0.534851
Epoch 2.10: Loss = 0.56311
Epoch 2.11: Loss = 0.580017
Epoch 2.12: Loss = 0.562759
Epoch 2.13: Loss = 0.476227
Epoch 2.14: Loss = 0.532898
Epoch 2.15: Loss = 0.661667
Epoch 2.16: Loss = 0.624756
Epoch 2.17: Loss = 0.620377
Epoch 2.18: Loss = 0.710022
Epoch 2.19: Loss = 0.544769
Epoch 2.20: Loss = 0.469772
Epoch 2.21: Loss = 0.473999
Epoch 2.22: Loss = 0.46463
Epoch 2.23: Loss = 0.475281
Epoch 2.24: Loss = 0.72998
Epoch 2.25: Loss = 0.604584
Epoch 2.26: Loss = 0.659302
Epoch 2.27: Loss = 0.617584
Epoch 2.28: Loss = 0.572601
Epoch 2.29: Loss = 0.673904
Epoch 2.30: Loss = 0.764252
Epoch 2.31: Loss = 0.518127
Epoch 2.32: Loss = 0.645065
Epoch 2.33: Loss = 0.554031
Epoch 2.34: Loss = 0.590363
Epoch 2.35: Loss = 0.558182
Epoch 2.36: Loss = 0.648239
Epoch 2.37: Loss = 0.50798
Epoch 2.38: Loss = 0.466049
Epoch 2.39: Loss = 0.544846
Epoch 2.40: Loss = 0.495667
Epoch 2.41: Loss = 0.572418
Epoch 2.42: Loss = 0.629608
Epoch 2.43: Loss = 0.494492
Epoch 2.44: Loss = 0.398392
Epoch 2.45: Loss = 0.593903
Epoch 2.46: Loss = 0.624664
Epoch 2.47: Loss = 0.463425
Epoch 2.48: Loss = 0.561127
Epoch 2.49: Loss = 0.538147
Epoch 2.50: Loss = 0.620163
Epoch 2.51: Loss = 0.489853
Epoch 2.52: Loss = 0.433014
Epoch 2.53: Loss = 0.499374
Epoch 2.54: Loss = 0.620956
Epoch 2.55: Loss = 0.54631
Epoch 2.56: Loss = 0.463837
Epoch 2.57: Loss = 0.466736
Epoch 2.58: Loss = 0.514709
Epoch 2.59: Loss = 0.545898
Epoch 2.60: Loss = 0.638535
Epoch 2.61: Loss = 0.633331
Epoch 2.62: Loss = 0.598602
Epoch 2.63: Loss = 0.682709
Epoch 2.64: Loss = 0.598358
Epoch 2.65: Loss = 0.734207
Epoch 2.66: Loss = 0.531006
Epoch 2.67: Loss = 0.578918
Epoch 2.68: Loss = 0.39212
Epoch 2.69: Loss = 0.441788
Epoch 2.70: Loss = 0.649277
Epoch 2.71: Loss = 0.459274
Epoch 2.72: Loss = 0.487518
Epoch 2.73: Loss = 0.582153
Epoch 2.74: Loss = 0.410919
Epoch 2.75: Loss = 0.70433
Epoch 2.76: Loss = 0.543991
Epoch 2.77: Loss = 0.465286
Epoch 2.78: Loss = 0.502243
Epoch 2.79: Loss = 0.598145
Epoch 2.80: Loss = 0.644348
Epoch 2.81: Loss = 0.505661
Epoch 2.82: Loss = 0.409332
Epoch 2.83: Loss = 0.66571
Epoch 2.84: Loss = 0.520493
Epoch 2.85: Loss = 0.65155
Epoch 2.86: Loss = 0.558563
Epoch 2.87: Loss = 0.433563
Epoch 2.88: Loss = 0.483658
Epoch 2.89: Loss = 0.63588
Epoch 2.90: Loss = 0.430832
Epoch 2.91: Loss = 0.538345
Epoch 2.92: Loss = 0.551239
Epoch 2.93: Loss = 0.601364
Epoch 2.94: Loss = 0.389709
Epoch 2.95: Loss = 0.549759
Epoch 2.96: Loss = 0.554031
Epoch 2.97: Loss = 0.39389
Epoch 2.98: Loss = 0.468658
Epoch 2.99: Loss = 0.64682
Epoch 2.100: Loss = 0.693237
Epoch 2.101: Loss = 0.638245
Epoch 2.102: Loss = 0.505264
Epoch 2.103: Loss = 0.481232
Epoch 2.104: Loss = 0.443924
Epoch 2.105: Loss = 0.587631
Epoch 2.106: Loss = 0.560364
Epoch 2.107: Loss = 0.454193
Epoch 2.108: Loss = 0.538834
Epoch 2.109: Loss = 0.459412
Epoch 2.110: Loss = 0.515427
Epoch 2.111: Loss = 0.404175
Epoch 2.112: Loss = 0.365036
Epoch 2.113: Loss = 0.486816
Epoch 2.114: Loss = 0.417633
Epoch 2.115: Loss = 0.398941
Epoch 2.116: Loss = 0.44577
Epoch 2.117: Loss = 0.372757
Epoch 2.118: Loss = 0.279022
Epoch 2.119: Loss = 0.3573
Epoch 2.120: Loss = 0.332748
TRAIN LOSS = 0.540894
TRAIN ACC = 83.1223 % (49876/60000)
Loss = 0.500107
Loss = 0.584717
Loss = 0.651688
Loss = 0.614563
Loss = 0.64006
Loss = 0.511368
Loss = 0.471954
Loss = 0.708054
Loss = 0.628479
Loss = 0.575119
Loss = 0.251495
Loss = 0.401688
Loss = 0.313446
Loss = 0.471954
Loss = 0.29425
Loss = 0.453949
Loss = 0.303329
Loss = 0.155396
Loss = 0.301559
Loss = 0.685089
TEST LOSS = 0.475913
TEST ACC = 498.759 % (8565/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.439423
Epoch 3.2: Loss = 0.60968
Epoch 3.3: Loss = 0.554779
Epoch 3.4: Loss = 0.413666
Epoch 3.5: Loss = 0.424301
Epoch 3.6: Loss = 0.43071
Epoch 3.7: Loss = 0.476501
Epoch 3.8: Loss = 0.491989
Epoch 3.9: Loss = 0.428787
Epoch 3.10: Loss = 0.473206
Epoch 3.11: Loss = 0.508957
Epoch 3.12: Loss = 0.463974
Epoch 3.13: Loss = 0.393372
Epoch 3.14: Loss = 0.466202
Epoch 3.15: Loss = 0.545258
Epoch 3.16: Loss = 0.570572
Epoch 3.17: Loss = 0.594467
Epoch 3.18: Loss = 0.693954
Epoch 3.19: Loss = 0.444916
Epoch 3.20: Loss = 0.430954
Epoch 3.21: Loss = 0.414795
Epoch 3.22: Loss = 0.353775
Epoch 3.23: Loss = 0.407013
Epoch 3.24: Loss = 0.669327
Epoch 3.25: Loss = 0.604111
Epoch 3.26: Loss = 0.623688
Epoch 3.27: Loss = 0.581436
Epoch 3.28: Loss = 0.509918
Epoch 3.29: Loss = 0.607208
Epoch 3.30: Loss = 0.644302
Epoch 3.31: Loss = 0.407928
Epoch 3.32: Loss = 0.61264
Epoch 3.33: Loss = 0.459061
Epoch 3.34: Loss = 0.51178
Epoch 3.35: Loss = 0.507187
Epoch 3.36: Loss = 0.566422
Epoch 3.37: Loss = 0.423019
Epoch 3.38: Loss = 0.403641
Epoch 3.39: Loss = 0.472153
Epoch 3.40: Loss = 0.408768
Epoch 3.41: Loss = 0.488266
Epoch 3.42: Loss = 0.625626
Epoch 3.43: Loss = 0.456039
Epoch 3.44: Loss = 0.342926
Epoch 3.45: Loss = 0.52388
Epoch 3.46: Loss = 0.599747
Epoch 3.47: Loss = 0.45076
Epoch 3.48: Loss = 0.458755
Epoch 3.49: Loss = 0.523819
Epoch 3.50: Loss = 0.56134
Epoch 3.51: Loss = 0.427719
Epoch 3.52: Loss = 0.408722
Epoch 3.53: Loss = 0.462067
Epoch 3.54: Loss = 0.593948
Epoch 3.55: Loss = 0.577484
Epoch 3.56: Loss = 0.464325
Epoch 3.57: Loss = 0.444809
Epoch 3.58: Loss = 0.502335
Epoch 3.59: Loss = 0.574585
Epoch 3.60: Loss = 0.607239
Epoch 3.61: Loss = 0.581146
Epoch 3.62: Loss = 0.58992
Epoch 3.63: Loss = 0.712479
Epoch 3.64: Loss = 0.540543
Epoch 3.65: Loss = 0.65213
Epoch 3.66: Loss = 0.462341
Epoch 3.67: Loss = 0.575333
Epoch 3.68: Loss = 0.330338
Epoch 3.69: Loss = 0.425644
Epoch 3.70: Loss = 0.632874
Epoch 3.71: Loss = 0.409119
Epoch 3.72: Loss = 0.42485
Epoch 3.73: Loss = 0.532532
Epoch 3.74: Loss = 0.356598
Epoch 3.75: Loss = 0.72403
Epoch 3.76: Loss = 0.509735
Epoch 3.77: Loss = 0.454315
Epoch 3.78: Loss = 0.505219
Epoch 3.79: Loss = 0.603638
Epoch 3.80: Loss = 0.64653
Epoch 3.81: Loss = 0.453827
Epoch 3.82: Loss = 0.397461
Epoch 3.83: Loss = 0.634949
Epoch 3.84: Loss = 0.514877
Epoch 3.85: Loss = 0.604324
Epoch 3.86: Loss = 0.524902
Epoch 3.87: Loss = 0.407654
Epoch 3.88: Loss = 0.463531
Epoch 3.89: Loss = 0.631714
Epoch 3.90: Loss = 0.378189
Epoch 3.91: Loss = 0.570343
Epoch 3.92: Loss = 0.538986
Epoch 3.93: Loss = 0.63002
Epoch 3.94: Loss = 0.375854
Epoch 3.95: Loss = 0.551376
Epoch 3.96: Loss = 0.62146
Epoch 3.97: Loss = 0.371124
Epoch 3.98: Loss = 0.464844
Epoch 3.99: Loss = 0.662918
Epoch 3.100: Loss = 0.721161
Epoch 3.101: Loss = 0.609314
Epoch 3.102: Loss = 0.504837
Epoch 3.103: Loss = 0.47612
Epoch 3.104: Loss = 0.443741
Epoch 3.105: Loss = 0.573303
Epoch 3.106: Loss = 0.612823
Epoch 3.107: Loss = 0.434952
Epoch 3.108: Loss = 0.568802
Epoch 3.109: Loss = 0.439972
Epoch 3.110: Loss = 0.555725
Epoch 3.111: Loss = 0.371613
Epoch 3.112: Loss = 0.377213
Epoch 3.113: Loss = 0.444229
Epoch 3.114: Loss = 0.403564
Epoch 3.115: Loss = 0.36763
Epoch 3.116: Loss = 0.463333
Epoch 3.117: Loss = 0.327133
Epoch 3.118: Loss = 0.263901
Epoch 3.119: Loss = 0.329559
Epoch 3.120: Loss = 0.328018
TRAIN LOSS = 0.501877
TRAIN ACC = 85.2493 % (51152/60000)
Loss = 0.502502
Loss = 0.589905
Loss = 0.680359
Loss = 0.60524
Loss = 0.679199
Loss = 0.509277
Loss = 0.402451
Loss = 0.730942
Loss = 0.630768
Loss = 0.592255
Loss = 0.241135
Loss = 0.35611
Loss = 0.298309
Loss = 0.414642
Loss = 0.266968
Loss = 0.436615
Loss = 0.266098
Loss = 0.128036
Loss = 0.28743
Loss = 0.71106
TEST LOSS = 0.466465
TEST ACC = 511.519 % (8658/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.474533
Epoch 4.2: Loss = 0.586395
Epoch 4.3: Loss = 0.54715
Epoch 4.4: Loss = 0.426697
Epoch 4.5: Loss = 0.397186
Epoch 4.6: Loss = 0.446503
Epoch 4.7: Loss = 0.475769
Epoch 4.8: Loss = 0.471344
Epoch 4.9: Loss = 0.369873
Epoch 4.10: Loss = 0.455887
Epoch 4.11: Loss = 0.494614
Epoch 4.12: Loss = 0.497986
Epoch 4.13: Loss = 0.410965
Epoch 4.14: Loss = 0.482651
Epoch 4.15: Loss = 0.559036
Epoch 4.16: Loss = 0.581879
Epoch 4.17: Loss = 0.605148
Epoch 4.18: Loss = 0.720413
Epoch 4.19: Loss = 0.509018
Epoch 4.20: Loss = 0.468506
Epoch 4.21: Loss = 0.427628
Epoch 4.22: Loss = 0.349335
Epoch 4.23: Loss = 0.386078
Epoch 4.24: Loss = 0.691238
Epoch 4.25: Loss = 0.522278
Epoch 4.26: Loss = 0.615723
Epoch 4.27: Loss = 0.589844
Epoch 4.28: Loss = 0.562286
Epoch 4.29: Loss = 0.645462
Epoch 4.30: Loss = 0.680099
Epoch 4.31: Loss = 0.459366
Epoch 4.32: Loss = 0.596786
Epoch 4.33: Loss = 0.424637
Epoch 4.34: Loss = 0.532364
Epoch 4.35: Loss = 0.500626
Epoch 4.36: Loss = 0.53949
Epoch 4.37: Loss = 0.446671
Epoch 4.38: Loss = 0.421799
Epoch 4.39: Loss = 0.430405
Epoch 4.40: Loss = 0.425049
Epoch 4.41: Loss = 0.478317
Epoch 4.42: Loss = 0.642883
Epoch 4.43: Loss = 0.448196
Epoch 4.44: Loss = 0.349426
Epoch 4.45: Loss = 0.540024
Epoch 4.46: Loss = 0.614639
Epoch 4.47: Loss = 0.433578
Epoch 4.48: Loss = 0.460114
Epoch 4.49: Loss = 0.489838
Epoch 4.50: Loss = 0.547836
Epoch 4.51: Loss = 0.428864
Epoch 4.52: Loss = 0.413177
Epoch 4.53: Loss = 0.461304
Epoch 4.54: Loss = 0.582184
Epoch 4.55: Loss = 0.531174
Epoch 4.56: Loss = 0.497467
Epoch 4.57: Loss = 0.488724
Epoch 4.58: Loss = 0.506714
Epoch 4.59: Loss = 0.555923
Epoch 4.60: Loss = 0.613998
Epoch 4.61: Loss = 0.578903
Epoch 4.62: Loss = 0.604324
Epoch 4.63: Loss = 0.759842
Epoch 4.64: Loss = 0.582687
Epoch 4.65: Loss = 0.641281
Epoch 4.66: Loss = 0.513657
Epoch 4.67: Loss = 0.551758
Epoch 4.68: Loss = 0.340332
Epoch 4.69: Loss = 0.419449
Epoch 4.70: Loss = 0.631912
Epoch 4.71: Loss = 0.439133
Epoch 4.72: Loss = 0.375137
Epoch 4.73: Loss = 0.50943
Epoch 4.74: Loss = 0.386307
Epoch 4.75: Loss = 0.704346
Epoch 4.76: Loss = 0.482666
Epoch 4.77: Loss = 0.487778
Epoch 4.78: Loss = 0.541229
Epoch 4.79: Loss = 0.619934
Epoch 4.80: Loss = 0.60968
Epoch 4.81: Loss = 0.453125
Epoch 4.82: Loss = 0.374695
Epoch 4.83: Loss = 0.634964
Epoch 4.84: Loss = 0.549774
Epoch 4.85: Loss = 0.651138
Epoch 4.86: Loss = 0.604874
Epoch 4.87: Loss = 0.439026
Epoch 4.88: Loss = 0.524094
Epoch 4.89: Loss = 0.640259
Epoch 4.90: Loss = 0.432495
Epoch 4.91: Loss = 0.582993
Epoch 4.92: Loss = 0.652557
Epoch 4.93: Loss = 0.657898
Epoch 4.94: Loss = 0.410721
Epoch 4.95: Loss = 0.536499
Epoch 4.96: Loss = 0.57843
Epoch 4.97: Loss = 0.364563
Epoch 4.98: Loss = 0.471115
Epoch 4.99: Loss = 0.650574
Epoch 4.100: Loss = 0.703156
Epoch 4.101: Loss = 0.705399
Epoch 4.102: Loss = 0.551788
Epoch 4.103: Loss = 0.492218
Epoch 4.104: Loss = 0.481628
Epoch 4.105: Loss = 0.584824
Epoch 4.106: Loss = 0.638046
Epoch 4.107: Loss = 0.428757
Epoch 4.108: Loss = 0.565826
Epoch 4.109: Loss = 0.469574
Epoch 4.110: Loss = 0.593994
Epoch 4.111: Loss = 0.431198
Epoch 4.112: Loss = 0.403763
Epoch 4.113: Loss = 0.441132
Epoch 4.114: Loss = 0.394775
Epoch 4.115: Loss = 0.404755
Epoch 4.116: Loss = 0.492203
Epoch 4.117: Loss = 0.292587
Epoch 4.118: Loss = 0.278427
Epoch 4.119: Loss = 0.353882
Epoch 4.120: Loss = 0.392136
TRAIN LOSS = 0.511703
TRAIN ACC = 85.5331 % (51323/60000)
Loss = 0.526611
Loss = 0.655258
Loss = 0.764084
Loss = 0.65451
Loss = 0.739288
Loss = 0.527206
Loss = 0.464783
Loss = 0.816956
Loss = 0.694504
Loss = 0.642181
Loss = 0.237625
Loss = 0.529449
Loss = 0.286041
Loss = 0.437088
Loss = 0.268112
Loss = 0.388016
Loss = 0.281006
Loss = 0.117096
Loss = 0.316879
Loss = 0.688995
TEST LOSS = 0.501784
TEST ACC = 513.229 % (8603/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.504654
Epoch 5.2: Loss = 0.616898
Epoch 5.3: Loss = 0.61026
Epoch 5.4: Loss = 0.445526
Epoch 5.5: Loss = 0.371155
Epoch 5.6: Loss = 0.45816
Epoch 5.7: Loss = 0.44632
Epoch 5.8: Loss = 0.536346
Epoch 5.9: Loss = 0.430191
Epoch 5.10: Loss = 0.544281
Epoch 5.11: Loss = 0.52832
Epoch 5.12: Loss = 0.509567
Epoch 5.13: Loss = 0.437027
Epoch 5.14: Loss = 0.535553
Epoch 5.15: Loss = 0.583023
Epoch 5.16: Loss = 0.59024
Epoch 5.17: Loss = 0.586472
Epoch 5.18: Loss = 0.758377
Epoch 5.19: Loss = 0.594635
Epoch 5.20: Loss = 0.41774
Epoch 5.21: Loss = 0.44603
Epoch 5.22: Loss = 0.319275
Epoch 5.23: Loss = 0.361145
Epoch 5.24: Loss = 0.655441
Epoch 5.25: Loss = 0.552567
Epoch 5.26: Loss = 0.690338
Epoch 5.27: Loss = 0.56575
Epoch 5.28: Loss = 0.656891
Epoch 5.29: Loss = 0.678162
Epoch 5.30: Loss = 0.707397
Epoch 5.31: Loss = 0.485474
Epoch 5.32: Loss = 0.630814
Epoch 5.33: Loss = 0.471237
Epoch 5.34: Loss = 0.626266
Epoch 5.35: Loss = 0.563797
Epoch 5.36: Loss = 0.597275
Epoch 5.37: Loss = 0.388214
Epoch 5.38: Loss = 0.432205
Epoch 5.39: Loss = 0.45134
Epoch 5.40: Loss = 0.472076
Epoch 5.41: Loss = 0.493073
Epoch 5.42: Loss = 0.674515
Epoch 5.43: Loss = 0.452118
Epoch 5.44: Loss = 0.354431
Epoch 5.45: Loss = 0.598221
Epoch 5.46: Loss = 0.615021
Epoch 5.47: Loss = 0.460861
Epoch 5.48: Loss = 0.559937
Epoch 5.49: Loss = 0.523544
Epoch 5.50: Loss = 0.628403
Epoch 5.51: Loss = 0.462463
Epoch 5.52: Loss = 0.42395
Epoch 5.53: Loss = 0.434082
Epoch 5.54: Loss = 0.696259
Epoch 5.55: Loss = 0.54187
Epoch 5.56: Loss = 0.522156
Epoch 5.57: Loss = 0.450134
Epoch 5.58: Loss = 0.521347
Epoch 5.59: Loss = 0.632919
Epoch 5.60: Loss = 0.658279
Epoch 5.61: Loss = 0.551407
Epoch 5.62: Loss = 0.705322
Epoch 5.63: Loss = 0.792725
Epoch 5.64: Loss = 0.677841
Epoch 5.65: Loss = 0.750458
Epoch 5.66: Loss = 0.522461
Epoch 5.67: Loss = 0.68956
Epoch 5.68: Loss = 0.361542
Epoch 5.69: Loss = 0.450287
Epoch 5.70: Loss = 0.673065
Epoch 5.71: Loss = 0.504532
Epoch 5.72: Loss = 0.39502
Epoch 5.73: Loss = 0.572403
Epoch 5.74: Loss = 0.400085
Epoch 5.75: Loss = 0.906876
Epoch 5.76: Loss = 0.546341
Epoch 5.77: Loss = 0.551605
Epoch 5.78: Loss = 0.58252
Epoch 5.79: Loss = 0.664627
Epoch 5.80: Loss = 0.658356
Epoch 5.81: Loss = 0.432968
Epoch 5.82: Loss = 0.423462
Epoch 5.83: Loss = 0.736435
Epoch 5.84: Loss = 0.559692
Epoch 5.85: Loss = 0.741974
Epoch 5.86: Loss = 0.651947
Epoch 5.87: Loss = 0.455353
Epoch 5.88: Loss = 0.523575
Epoch 5.89: Loss = 0.763412
Epoch 5.90: Loss = 0.449738
Epoch 5.91: Loss = 0.638275
Epoch 5.92: Loss = 0.691589
Epoch 5.93: Loss = 0.750977
Epoch 5.94: Loss = 0.458038
Epoch 5.95: Loss = 0.578369
Epoch 5.96: Loss = 0.634506
Epoch 5.97: Loss = 0.436539
Epoch 5.98: Loss = 0.520889
Epoch 5.99: Loss = 0.725174
Epoch 5.100: Loss = 0.79776
Epoch 5.101: Loss = 0.7397
Epoch 5.102: Loss = 0.530029
Epoch 5.103: Loss = 0.538406
Epoch 5.104: Loss = 0.523407
Epoch 5.105: Loss = 0.656189
Epoch 5.106: Loss = 0.683823
Epoch 5.107: Loss = 0.460739
Epoch 5.108: Loss = 0.613693
Epoch 5.109: Loss = 0.508286
Epoch 5.110: Loss = 0.689468
Epoch 5.111: Loss = 0.453354
Epoch 5.112: Loss = 0.486069
Epoch 5.113: Loss = 0.469681
Epoch 5.114: Loss = 0.445526
Epoch 5.115: Loss = 0.459152
Epoch 5.116: Loss = 0.522491
Epoch 5.117: Loss = 0.32959
Epoch 5.118: Loss = 0.284882
Epoch 5.119: Loss = 0.375839
Epoch 5.120: Loss = 0.46936
TRAIN LOSS = 0.551086
TRAIN ACC = 85.4874 % (51295/60000)
Loss = 0.56636
Loss = 0.673996
Loss = 0.824326
Loss = 0.73317
Loss = 0.799576
Loss = 0.531601
Loss = 0.491547
Loss = 0.879608
Loss = 0.803925
Loss = 0.684326
Loss = 0.225571
Loss = 0.532822
Loss = 0.388641
Loss = 0.498459
Loss = 0.281845
Loss = 0.37915
Loss = 0.41333
Loss = 0.114517
Loss = 0.33165
Loss = 0.762146
TEST LOSS = 0.545828
TEST ACC = 512.949 % (8608/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.553436
Epoch 6.2: Loss = 0.715805
Epoch 6.3: Loss = 0.610092
Epoch 6.4: Loss = 0.496735
Epoch 6.5: Loss = 0.441635
Epoch 6.6: Loss = 0.475815
Epoch 6.7: Loss = 0.499741
Epoch 6.8: Loss = 0.574951
Epoch 6.9: Loss = 0.43042
Epoch 6.10: Loss = 0.521133
Epoch 6.11: Loss = 0.598801
Epoch 6.12: Loss = 0.58548
Epoch 6.13: Loss = 0.454163
Epoch 6.14: Loss = 0.474731
Epoch 6.15: Loss = 0.609497
Epoch 6.16: Loss = 0.576141
Epoch 6.17: Loss = 0.602631
Epoch 6.18: Loss = 0.820496
Epoch 6.19: Loss = 0.562515
Epoch 6.20: Loss = 0.499573
Epoch 6.21: Loss = 0.516785
Epoch 6.22: Loss = 0.345581
Epoch 6.23: Loss = 0.454041
Epoch 6.24: Loss = 0.675613
Epoch 6.25: Loss = 0.607315
Epoch 6.26: Loss = 0.709747
Epoch 6.27: Loss = 0.659714
Epoch 6.28: Loss = 0.680359
Epoch 6.29: Loss = 0.764572
Epoch 6.30: Loss = 0.744339
Epoch 6.31: Loss = 0.477325
Epoch 6.32: Loss = 0.648438
Epoch 6.33: Loss = 0.511658
Epoch 6.34: Loss = 0.657806
Epoch 6.35: Loss = 0.663635
Epoch 6.36: Loss = 0.646423
Epoch 6.37: Loss = 0.436874
Epoch 6.38: Loss = 0.521652
Epoch 6.39: Loss = 0.50174
Epoch 6.40: Loss = 0.585022
Epoch 6.41: Loss = 0.528336
Epoch 6.42: Loss = 0.752319
Epoch 6.43: Loss = 0.444031
Epoch 6.44: Loss = 0.384613
Epoch 6.45: Loss = 0.565567
Epoch 6.46: Loss = 0.641953
Epoch 6.47: Loss = 0.543396
Epoch 6.48: Loss = 0.636322
Epoch 6.49: Loss = 0.550232
Epoch 6.50: Loss = 0.626404
Epoch 6.51: Loss = 0.468964
Epoch 6.52: Loss = 0.437515
Epoch 6.53: Loss = 0.53299
Epoch 6.54: Loss = 0.733398
Epoch 6.55: Loss = 0.574356
Epoch 6.56: Loss = 0.516785
Epoch 6.57: Loss = 0.482391
Epoch 6.58: Loss = 0.602005
Epoch 6.59: Loss = 0.617828
Epoch 6.60: Loss = 0.721878
Epoch 6.61: Loss = 0.65535
Epoch 6.62: Loss = 0.760208
Epoch 6.63: Loss = 0.794159
Epoch 6.64: Loss = 0.750427
Epoch 6.65: Loss = 0.768936
Epoch 6.66: Loss = 0.506607
Epoch 6.67: Loss = 0.602386
Epoch 6.68: Loss = 0.405609
Epoch 6.69: Loss = 0.463272
Epoch 6.70: Loss = 0.675552
Epoch 6.71: Loss = 0.502533
Epoch 6.72: Loss = 0.423492
Epoch 6.73: Loss = 0.642044
Epoch 6.74: Loss = 0.409576
Epoch 6.75: Loss = 0.948776
Epoch 6.76: Loss = 0.52681
Epoch 6.77: Loss = 0.548111
Epoch 6.78: Loss = 0.613464
Epoch 6.79: Loss = 0.613632
Epoch 6.80: Loss = 0.701279
Epoch 6.81: Loss = 0.447937
Epoch 6.82: Loss = 0.415985
Epoch 6.83: Loss = 0.718231
Epoch 6.84: Loss = 0.498093
Epoch 6.85: Loss = 0.724625
Epoch 6.86: Loss = 0.739655
Epoch 6.87: Loss = 0.399216
Epoch 6.88: Loss = 0.532883
Epoch 6.89: Loss = 0.752975
Epoch 6.90: Loss = 0.565323
Epoch 6.91: Loss = 0.569885
Epoch 6.92: Loss = 0.69899
Epoch 6.93: Loss = 0.823914
Epoch 6.94: Loss = 0.488342
Epoch 6.95: Loss = 0.620163
Epoch 6.96: Loss = 0.584656
Epoch 6.97: Loss = 0.364197
Epoch 6.98: Loss = 0.603409
Epoch 6.99: Loss = 0.677017
Epoch 6.100: Loss = 0.760147
Epoch 6.101: Loss = 0.805832
Epoch 6.102: Loss = 0.525742
Epoch 6.103: Loss = 0.498199
Epoch 6.104: Loss = 0.519821
Epoch 6.105: Loss = 0.626907
Epoch 6.106: Loss = 0.665283
Epoch 6.107: Loss = 0.512955
Epoch 6.108: Loss = 0.706131
Epoch 6.109: Loss = 0.494232
Epoch 6.110: Loss = 0.677643
Epoch 6.111: Loss = 0.45752
Epoch 6.112: Loss = 0.46492
Epoch 6.113: Loss = 0.490204
Epoch 6.114: Loss = 0.417801
Epoch 6.115: Loss = 0.431564
Epoch 6.116: Loss = 0.52829
Epoch 6.117: Loss = 0.287018
Epoch 6.118: Loss = 0.292862
Epoch 6.119: Loss = 0.400452
Epoch 6.120: Loss = 0.466522
TRAIN LOSS = 0.573685
TRAIN ACC = 85.6781 % (51409/60000)
Loss = 0.514816
Loss = 0.62117
Loss = 0.776169
Loss = 0.692673
Loss = 0.831299
Loss = 0.536499
Loss = 0.483292
Loss = 0.891632
Loss = 0.780548
Loss = 0.617706
Loss = 0.242523
Loss = 0.458801
Loss = 0.393982
Loss = 0.485764
Loss = 0.281982
Loss = 0.489899
Loss = 0.391998
Loss = 0.113647
Loss = 0.283661
Loss = 0.731445
TEST LOSS = 0.530975
TEST ACC = 514.088 % (8671/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.532501
Epoch 7.2: Loss = 0.762573
Epoch 7.3: Loss = 0.70224
Epoch 7.4: Loss = 0.431564
Epoch 7.5: Loss = 0.43692
Epoch 7.6: Loss = 0.50058
Epoch 7.7: Loss = 0.450943
Epoch 7.8: Loss = 0.594711
Epoch 7.9: Loss = 0.472595
Epoch 7.10: Loss = 0.578751
Epoch 7.11: Loss = 0.668625
Epoch 7.12: Loss = 0.575424
Epoch 7.13: Loss = 0.492493
Epoch 7.14: Loss = 0.518661
Epoch 7.15: Loss = 0.599915
Epoch 7.16: Loss = 0.55098
Epoch 7.17: Loss = 0.561279
Epoch 7.18: Loss = 0.789352
Epoch 7.19: Loss = 0.596924
Epoch 7.20: Loss = 0.528122
Epoch 7.21: Loss = 0.511108
Epoch 7.22: Loss = 0.348068
Epoch 7.23: Loss = 0.404694
Epoch 7.24: Loss = 0.71463
Epoch 7.25: Loss = 0.601486
Epoch 7.26: Loss = 0.656891
Epoch 7.27: Loss = 0.718216
Epoch 7.28: Loss = 0.714401
Epoch 7.29: Loss = 0.791672
Epoch 7.30: Loss = 0.842316
Epoch 7.31: Loss = 0.530807
Epoch 7.32: Loss = 0.744034
Epoch 7.33: Loss = 0.539734
Epoch 7.34: Loss = 0.744873
Epoch 7.35: Loss = 0.603867
Epoch 7.36: Loss = 0.703934
Epoch 7.37: Loss = 0.373398
Epoch 7.38: Loss = 0.550644
Epoch 7.39: Loss = 0.527496
Epoch 7.40: Loss = 0.606796
Epoch 7.41: Loss = 0.518143
Epoch 7.42: Loss = 0.820587
Epoch 7.43: Loss = 0.508759
Epoch 7.44: Loss = 0.403244
Epoch 7.45: Loss = 0.558228
Epoch 7.46: Loss = 0.737289
Epoch 7.47: Loss = 0.505341
Epoch 7.48: Loss = 0.605453
Epoch 7.49: Loss = 0.563858
Epoch 7.50: Loss = 0.700851
Epoch 7.51: Loss = 0.520828
Epoch 7.52: Loss = 0.524719
Epoch 7.53: Loss = 0.531128
Epoch 7.54: Loss = 0.719543
Epoch 7.55: Loss = 0.623734
Epoch 7.56: Loss = 0.534332
Epoch 7.57: Loss = 0.58255
Epoch 7.58: Loss = 0.634079
Epoch 7.59: Loss = 0.615356
Epoch 7.60: Loss = 0.703644
Epoch 7.61: Loss = 0.688293
Epoch 7.62: Loss = 0.77681
Epoch 7.63: Loss = 0.854141
Epoch 7.64: Loss = 0.733978
Epoch 7.65: Loss = 0.882858
Epoch 7.66: Loss = 0.538757
Epoch 7.67: Loss = 0.58432
Epoch 7.68: Loss = 0.416107
Epoch 7.69: Loss = 0.528564
Epoch 7.70: Loss = 0.70079
Epoch 7.71: Loss = 0.514297
Epoch 7.72: Loss = 0.465851
Epoch 7.73: Loss = 0.629547
Epoch 7.74: Loss = 0.406097
Epoch 7.75: Loss = 0.917099
Epoch 7.76: Loss = 0.577911
Epoch 7.77: Loss = 0.511581
Epoch 7.78: Loss = 0.636307
Epoch 7.79: Loss = 0.678848
Epoch 7.80: Loss = 0.769302
Epoch 7.81: Loss = 0.461594
Epoch 7.82: Loss = 0.402054
Epoch 7.83: Loss = 0.67865
Epoch 7.84: Loss = 0.547302
Epoch 7.85: Loss = 0.832977
Epoch 7.86: Loss = 0.792709
Epoch 7.87: Loss = 0.435776
Epoch 7.88: Loss = 0.595016
Epoch 7.89: Loss = 0.779022
Epoch 7.90: Loss = 0.525146
Epoch 7.91: Loss = 0.623795
Epoch 7.92: Loss = 0.740906
Epoch 7.93: Loss = 0.83165
Epoch 7.94: Loss = 0.559677
Epoch 7.95: Loss = 0.744293
Epoch 7.96: Loss = 0.694839
Epoch 7.97: Loss = 0.402908
Epoch 7.98: Loss = 0.609116
Epoch 7.99: Loss = 0.78096
Epoch 7.100: Loss = 0.85524
Epoch 7.101: Loss = 0.843384
Epoch 7.102: Loss = 0.589218
Epoch 7.103: Loss = 0.515335
Epoch 7.104: Loss = 0.583023
Epoch 7.105: Loss = 0.683228
Epoch 7.106: Loss = 0.794342
Epoch 7.107: Loss = 0.508423
Epoch 7.108: Loss = 0.77655
Epoch 7.109: Loss = 0.577316
Epoch 7.110: Loss = 0.705841
Epoch 7.111: Loss = 0.508804
Epoch 7.112: Loss = 0.50972
Epoch 7.113: Loss = 0.602875
Epoch 7.114: Loss = 0.521301
Epoch 7.115: Loss = 0.554718
Epoch 7.116: Loss = 0.575104
Epoch 7.117: Loss = 0.332306
Epoch 7.118: Loss = 0.37442
Epoch 7.119: Loss = 0.471802
Epoch 7.120: Loss = 0.469528
TRAIN LOSS = 0.605759
TRAIN ACC = 85.7956 % (51479/60000)
Loss = 0.577087
Loss = 0.741364
Loss = 0.945816
Loss = 0.795227
Loss = 0.896667
Loss = 0.577896
Loss = 0.549957
Loss = 1.04784
Loss = 0.844208
Loss = 0.7556
Loss = 0.262283
Loss = 0.57695
Loss = 0.528839
Loss = 0.591141
Loss = 0.306366
Loss = 0.387726
Loss = 0.518753
Loss = 0.12999
Loss = 0.286469
Loss = 0.852142
TEST LOSS = 0.608616
TEST ACC = 514.789 % (8629/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.614426
Epoch 8.2: Loss = 0.944672
Epoch 8.3: Loss = 0.853806
Epoch 8.4: Loss = 0.507492
Epoch 8.5: Loss = 0.460892
Epoch 8.6: Loss = 0.577057
Epoch 8.7: Loss = 0.471451
Epoch 8.8: Loss = 0.656586
Epoch 8.9: Loss = 0.540131
Epoch 8.10: Loss = 0.567261
Epoch 8.11: Loss = 0.643082
Epoch 8.12: Loss = 0.688568
Epoch 8.13: Loss = 0.482162
Epoch 8.14: Loss = 0.49675
Epoch 8.15: Loss = 0.675354
Epoch 8.16: Loss = 0.625488
Epoch 8.17: Loss = 0.658936
Epoch 8.18: Loss = 0.908463
Epoch 8.19: Loss = 0.679459
Epoch 8.20: Loss = 0.560776
Epoch 8.21: Loss = 0.472794
Epoch 8.22: Loss = 0.400955
Epoch 8.23: Loss = 0.450195
Epoch 8.24: Loss = 0.820053
Epoch 8.25: Loss = 0.805496
Epoch 8.26: Loss = 0.767838
Epoch 8.27: Loss = 0.74263
Epoch 8.28: Loss = 0.773941
Epoch 8.29: Loss = 0.921967
Epoch 8.30: Loss = 0.929153
Epoch 8.31: Loss = 0.532303
Epoch 8.32: Loss = 0.772293
Epoch 8.33: Loss = 0.579102
Epoch 8.34: Loss = 0.84993
Epoch 8.35: Loss = 0.709259
Epoch 8.36: Loss = 0.721344
Epoch 8.37: Loss = 0.437332
Epoch 8.38: Loss = 0.579559
Epoch 8.39: Loss = 0.636368
Epoch 8.40: Loss = 0.605789
Epoch 8.41: Loss = 0.670029
Epoch 8.42: Loss = 0.873062
Epoch 8.43: Loss = 0.548065
Epoch 8.44: Loss = 0.448959
Epoch 8.45: Loss = 0.621811
Epoch 8.46: Loss = 0.695053
Epoch 8.47: Loss = 0.54216
Epoch 8.48: Loss = 0.629395
Epoch 8.49: Loss = 0.618439
Epoch 8.50: Loss = 0.768936
Epoch 8.51: Loss = 0.559128
Epoch 8.52: Loss = 0.473236
Epoch 8.53: Loss = 0.582535
Epoch 8.54: Loss = 0.769363
Epoch 8.55: Loss = 0.719193
Epoch 8.56: Loss = 0.638321
Epoch 8.57: Loss = 0.616104
Epoch 8.58: Loss = 0.64476
Epoch 8.59: Loss = 0.644699
Epoch 8.60: Loss = 0.731552
Epoch 8.61: Loss = 0.728104
Epoch 8.62: Loss = 0.794952
Epoch 8.63: Loss = 0.8815
Epoch 8.64: Loss = 0.790268
Epoch 8.65: Loss = 0.893906
Epoch 8.66: Loss = 0.5466
Epoch 8.67: Loss = 0.710037
Epoch 8.68: Loss = 0.432922
Epoch 8.69: Loss = 0.641678
Epoch 8.70: Loss = 0.745316
Epoch 8.71: Loss = 0.547409
Epoch 8.72: Loss = 0.502197
Epoch 8.73: Loss = 0.681229
Epoch 8.74: Loss = 0.41391
Epoch 8.75: Loss = 1.01396
Epoch 8.76: Loss = 0.716827
Epoch 8.77: Loss = 0.640488
Epoch 8.78: Loss = 0.711411
Epoch 8.79: Loss = 0.790054
Epoch 8.80: Loss = 0.926834
Epoch 8.81: Loss = 0.505005
Epoch 8.82: Loss = 0.476898
Epoch 8.83: Loss = 0.809128
Epoch 8.84: Loss = 0.581268
Epoch 8.85: Loss = 0.929184
Epoch 8.86: Loss = 0.878769
Epoch 8.87: Loss = 0.504333
Epoch 8.88: Loss = 0.615036
Epoch 8.89: Loss = 0.839523
Epoch 8.90: Loss = 0.585587
Epoch 8.91: Loss = 0.703934
Epoch 8.92: Loss = 0.832703
Epoch 8.93: Loss = 1.07097
Epoch 8.94: Loss = 0.6716
Epoch 8.95: Loss = 0.921036
Epoch 8.96: Loss = 0.77504
Epoch 8.97: Loss = 0.441162
Epoch 8.98: Loss = 0.713257
Epoch 8.99: Loss = 0.929535
Epoch 8.100: Loss = 0.999237
Epoch 8.101: Loss = 0.970215
Epoch 8.102: Loss = 0.57666
Epoch 8.103: Loss = 0.561966
Epoch 8.104: Loss = 0.55658
Epoch 8.105: Loss = 0.773071
Epoch 8.106: Loss = 0.910004
Epoch 8.107: Loss = 0.614029
Epoch 8.108: Loss = 0.877914
Epoch 8.109: Loss = 0.65155
Epoch 8.110: Loss = 0.711136
Epoch 8.111: Loss = 0.575287
Epoch 8.112: Loss = 0.513519
Epoch 8.113: Loss = 0.622482
Epoch 8.114: Loss = 0.52092
Epoch 8.115: Loss = 0.559128
Epoch 8.116: Loss = 0.649536
Epoch 8.117: Loss = 0.365173
Epoch 8.118: Loss = 0.384033
Epoch 8.119: Loss = 0.497986
Epoch 8.120: Loss = 0.578094
TRAIN LOSS = 0.669006
TRAIN ACC = 85.4507 % (51273/60000)
Loss = 0.671585
Loss = 0.900452
Loss = 0.974075
Loss = 0.98024
Loss = 0.988113
Loss = 0.64183
Loss = 0.659637
Loss = 1.13419
Loss = 0.974304
Loss = 0.913254
Loss = 0.253021
Loss = 0.611694
Loss = 0.546341
Loss = 0.656326
Loss = 0.338898
Loss = 0.476501
Loss = 0.596069
Loss = 0.136627
Loss = 0.359314
Loss = 0.987656
TEST LOSS = 0.690006
TEST ACC = 512.729 % (8566/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.653061
Epoch 9.2: Loss = 0.945908
Epoch 9.3: Loss = 1.01289
Epoch 9.4: Loss = 0.586182
Epoch 9.5: Loss = 0.629593
Epoch 9.6: Loss = 0.613251
Epoch 9.7: Loss = 0.614532
Epoch 9.8: Loss = 0.600418
Epoch 9.9: Loss = 0.589157
Epoch 9.10: Loss = 0.584183
Epoch 9.11: Loss = 0.636215
Epoch 9.12: Loss = 0.710739
Epoch 9.13: Loss = 0.622162
Epoch 9.14: Loss = 0.68837
Epoch 9.15: Loss = 0.705994
Epoch 9.16: Loss = 0.65892
Epoch 9.17: Loss = 0.865143
Epoch 9.18: Loss = 1.01012
Epoch 9.19: Loss = 0.769775
Epoch 9.20: Loss = 0.572372
Epoch 9.21: Loss = 0.542618
Epoch 9.22: Loss = 0.493027
Epoch 9.23: Loss = 0.4711
Epoch 9.24: Loss = 0.864151
Epoch 9.25: Loss = 0.951904
Epoch 9.26: Loss = 0.875443
Epoch 9.27: Loss = 0.955444
Epoch 9.28: Loss = 0.830673
Epoch 9.29: Loss = 1.02463
Epoch 9.30: Loss = 1.03238
Epoch 9.31: Loss = 0.626801
Epoch 9.32: Loss = 0.837997
Epoch 9.33: Loss = 0.61412
Epoch 9.34: Loss = 0.934723
Epoch 9.35: Loss = 0.682755
Epoch 9.36: Loss = 0.835678
Epoch 9.37: Loss = 0.518539
Epoch 9.38: Loss = 0.528748
Epoch 9.39: Loss = 0.686218
Epoch 9.40: Loss = 0.73819
Epoch 9.41: Loss = 0.748077
Epoch 9.42: Loss = 0.878738
Epoch 9.43: Loss = 0.607391
Epoch 9.44: Loss = 0.464294
Epoch 9.45: Loss = 0.669388
Epoch 9.46: Loss = 0.803223
Epoch 9.47: Loss = 0.578629
Epoch 9.48: Loss = 0.704712
Epoch 9.49: Loss = 0.734009
Epoch 9.50: Loss = 0.755966
Epoch 9.51: Loss = 0.539185
Epoch 9.52: Loss = 0.490891
Epoch 9.53: Loss = 0.650177
Epoch 9.54: Loss = 0.862595
Epoch 9.55: Loss = 0.828888
Epoch 9.56: Loss = 0.686508
Epoch 9.57: Loss = 0.700836
Epoch 9.58: Loss = 0.684311
Epoch 9.59: Loss = 0.683014
Epoch 9.60: Loss = 0.905334
Epoch 9.61: Loss = 0.856552
Epoch 9.62: Loss = 0.922989
Epoch 9.63: Loss = 1.01253
Epoch 9.64: Loss = 0.944794
Epoch 9.65: Loss = 1.04614
Epoch 9.66: Loss = 0.640121
Epoch 9.67: Loss = 0.762451
Epoch 9.68: Loss = 0.544968
Epoch 9.69: Loss = 0.654846
Epoch 9.70: Loss = 0.801193
Epoch 9.71: Loss = 0.624939
Epoch 9.72: Loss = 0.501144
Epoch 9.73: Loss = 0.718277
Epoch 9.74: Loss = 0.505051
Epoch 9.75: Loss = 1.03349
Epoch 9.76: Loss = 0.784164
Epoch 9.77: Loss = 0.691879
Epoch 9.78: Loss = 0.750824
Epoch 9.79: Loss = 0.868484
Epoch 9.80: Loss = 0.974258
Epoch 9.81: Loss = 0.441513
Epoch 9.82: Loss = 0.490356
Epoch 9.83: Loss = 0.950195
Epoch 9.84: Loss = 0.637833
Epoch 9.85: Loss = 1.00058
Epoch 9.86: Loss = 1.02708
Epoch 9.87: Loss = 0.546097
Epoch 9.88: Loss = 0.710999
Epoch 9.89: Loss = 0.845749
Epoch 9.90: Loss = 0.666382
Epoch 9.91: Loss = 0.845856
Epoch 9.92: Loss = 0.859558
Epoch 9.93: Loss = 1.04214
Epoch 9.94: Loss = 0.672638
Epoch 9.95: Loss = 0.997528
Epoch 9.96: Loss = 0.826859
Epoch 9.97: Loss = 0.514908
Epoch 9.98: Loss = 0.743317
Epoch 9.99: Loss = 1.11452
Epoch 9.100: Loss = 1.07889
Epoch 9.101: Loss = 1.08414
Epoch 9.102: Loss = 0.657776
Epoch 9.103: Loss = 0.639099
Epoch 9.104: Loss = 0.657333
Epoch 9.105: Loss = 0.840546
Epoch 9.106: Loss = 0.853088
Epoch 9.107: Loss = 0.590027
Epoch 9.108: Loss = 0.94545
Epoch 9.109: Loss = 0.721863
Epoch 9.110: Loss = 0.83638
Epoch 9.111: Loss = 0.61145
Epoch 9.112: Loss = 0.678772
Epoch 9.113: Loss = 0.691422
Epoch 9.114: Loss = 0.521347
Epoch 9.115: Loss = 0.61554
Epoch 9.116: Loss = 0.759979
Epoch 9.117: Loss = 0.411591
Epoch 9.118: Loss = 0.356735
Epoch 9.119: Loss = 0.38829
Epoch 9.120: Loss = 0.616013
TRAIN LOSS = 0.735184
TRAIN ACC = 85.408 % (51247/60000)
Loss = 0.742386
Loss = 0.891022
Loss = 1.02182
Loss = 1.10155
Loss = 1.14859
Loss = 0.68309
Loss = 0.726578
Loss = 1.17432
Loss = 1.01247
Loss = 0.965408
Loss = 0.240326
Loss = 0.549393
Loss = 0.604004
Loss = 0.670105
Loss = 0.361755
Loss = 0.618622
Loss = 0.491699
Loss = 0.0998535
Loss = 0.359833
Loss = 1.05414
TEST LOSS = 0.725847
TEST ACC = 512.469 % (8631/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.772095
Epoch 10.2: Loss = 1.02208
Epoch 10.3: Loss = 0.987946
Epoch 10.4: Loss = 0.561234
Epoch 10.5: Loss = 0.659103
Epoch 10.6: Loss = 0.603134
Epoch 10.7: Loss = 0.635696
Epoch 10.8: Loss = 0.658737
Epoch 10.9: Loss = 0.638275
Epoch 10.10: Loss = 0.631439
Epoch 10.11: Loss = 0.73201
Epoch 10.12: Loss = 0.694
Epoch 10.13: Loss = 0.517502
Epoch 10.14: Loss = 0.769333
Epoch 10.15: Loss = 0.786484
Epoch 10.16: Loss = 0.728439
Epoch 10.17: Loss = 0.875031
Epoch 10.18: Loss = 1.1337
Epoch 10.19: Loss = 0.876205
Epoch 10.20: Loss = 0.569458
Epoch 10.21: Loss = 0.629196
Epoch 10.22: Loss = 0.532654
Epoch 10.23: Loss = 0.557129
Epoch 10.24: Loss = 0.995041
Epoch 10.25: Loss = 0.947617
Epoch 10.26: Loss = 0.852509
Epoch 10.27: Loss = 1.119
Epoch 10.28: Loss = 0.959381
Epoch 10.29: Loss = 1.14268
Epoch 10.30: Loss = 1.13835
Epoch 10.31: Loss = 0.609848
Epoch 10.32: Loss = 0.911041
Epoch 10.33: Loss = 0.674271
Epoch 10.34: Loss = 0.921402
Epoch 10.35: Loss = 0.756668
Epoch 10.36: Loss = 0.866089
Epoch 10.37: Loss = 0.58313
Epoch 10.38: Loss = 0.632492
Epoch 10.39: Loss = 0.621643
Epoch 10.40: Loss = 0.83551
Epoch 10.41: Loss = 0.746429
Epoch 10.42: Loss = 0.918472
Epoch 10.43: Loss = 0.683075
Epoch 10.44: Loss = 0.529572
Epoch 10.45: Loss = 0.837952
Epoch 10.46: Loss = 0.762802
Epoch 10.47: Loss = 0.719238
Epoch 10.48: Loss = 0.77449
Epoch 10.49: Loss = 0.847961
Epoch 10.50: Loss = 0.813171
Epoch 10.51: Loss = 0.584015
Epoch 10.52: Loss = 0.606506
Epoch 10.53: Loss = 0.766266
Epoch 10.54: Loss = 0.96347
Epoch 10.55: Loss = 0.881317
Epoch 10.56: Loss = 0.698563
Epoch 10.57: Loss = 0.741776
Epoch 10.58: Loss = 0.784973
Epoch 10.59: Loss = 0.804474
Epoch 10.60: Loss = 0.943619
Epoch 10.61: Loss = 0.954178
Epoch 10.62: Loss = 1.00273
Epoch 10.63: Loss = 1.1132
Epoch 10.64: Loss = 1.01485
Epoch 10.65: Loss = 1.23924
Epoch 10.66: Loss = 0.74205
Epoch 10.67: Loss = 0.924393
Epoch 10.68: Loss = 0.582458
Epoch 10.69: Loss = 0.738586
Epoch 10.70: Loss = 0.964066
Epoch 10.71: Loss = 0.667633
Epoch 10.72: Loss = 0.615692
Epoch 10.73: Loss = 0.774109
Epoch 10.74: Loss = 0.481583
Epoch 10.75: Loss = 1.18773
Epoch 10.76: Loss = 0.847473
Epoch 10.77: Loss = 0.786896
Epoch 10.78: Loss = 0.715027
Epoch 10.79: Loss = 0.981796
Epoch 10.80: Loss = 1.03485
Epoch 10.81: Loss = 0.570572
Epoch 10.82: Loss = 0.511856
Epoch 10.83: Loss = 1.06006
Epoch 10.84: Loss = 0.665024
Epoch 10.85: Loss = 1.13559
Epoch 10.86: Loss = 1.16206
Epoch 10.87: Loss = 0.530991
Epoch 10.88: Loss = 0.668579
Epoch 10.89: Loss = 0.936035
Epoch 10.90: Loss = 0.75592
Epoch 10.91: Loss = 0.967834
Epoch 10.92: Loss = 0.83783
Epoch 10.93: Loss = 1.14612
Epoch 10.94: Loss = 0.60321
Epoch 10.95: Loss = 0.988235
Epoch 10.96: Loss = 0.944962
Epoch 10.97: Loss = 0.646637
Epoch 10.98: Loss = 0.846924
Epoch 10.99: Loss = 1.22183
Epoch 10.100: Loss = 1.22348
Epoch 10.101: Loss = 1.08774
Epoch 10.102: Loss = 0.67276
Epoch 10.103: Loss = 0.749664
Epoch 10.104: Loss = 0.741165
Epoch 10.105: Loss = 0.911255
Epoch 10.106: Loss = 0.885284
Epoch 10.107: Loss = 0.63707
Epoch 10.108: Loss = 0.976349
Epoch 10.109: Loss = 0.732941
Epoch 10.110: Loss = 0.85585
Epoch 10.111: Loss = 0.63092
Epoch 10.112: Loss = 0.606644
Epoch 10.113: Loss = 0.831619
Epoch 10.114: Loss = 0.56459
Epoch 10.115: Loss = 0.712646
Epoch 10.116: Loss = 0.885834
Epoch 10.117: Loss = 0.418137
Epoch 10.118: Loss = 0.376129
Epoch 10.119: Loss = 0.427902
Epoch 10.120: Loss = 0.660278
TRAIN LOSS = 0.797546
TRAIN ACC = 85.0327 % (51022/60000)
Loss = 0.845657
Loss = 0.899155
Loss = 1.16502
Loss = 1.09695
Loss = 1.13528
Loss = 0.680283
Loss = 0.747208
Loss = 1.16006
Loss = 1.06317
Loss = 0.916122
Loss = 0.281052
Loss = 0.6017
Loss = 0.632614
Loss = 0.616821
Loss = 0.437256
Loss = 0.52121
Loss = 0.549774
Loss = 0.0835571
Loss = 0.398071
Loss = 1.0562
TEST LOSS = 0.744359
TEST ACC = 510.219 % (8625/10000)
