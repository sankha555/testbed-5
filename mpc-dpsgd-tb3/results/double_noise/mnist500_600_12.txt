Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 10
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 3
***********************************************************
Epoch 1.1: Loss = 2.37079
Epoch 1.2: Loss = 2.36279
Epoch 1.3: Loss = 2.2901
Epoch 1.4: Loss = 2.23199
Epoch 1.5: Loss = 2.18813
Epoch 1.6: Loss = 2.14073
Epoch 1.7: Loss = 2.08321
Epoch 1.8: Loss = 2.07166
Epoch 1.9: Loss = 2.00357
Epoch 1.10: Loss = 1.95938
Epoch 1.11: Loss = 1.9326
Epoch 1.12: Loss = 1.89885
Epoch 1.13: Loss = 1.84305
Epoch 1.14: Loss = 1.82353
Epoch 1.15: Loss = 1.78462
Epoch 1.16: Loss = 1.75626
Epoch 1.17: Loss = 1.73204
Epoch 1.18: Loss = 1.70683
Epoch 1.19: Loss = 1.6595
Epoch 1.20: Loss = 1.59474
Epoch 1.21: Loss = 1.59334
Epoch 1.22: Loss = 1.57356
Epoch 1.23: Loss = 1.54807
Epoch 1.24: Loss = 1.48979
Epoch 1.25: Loss = 1.44464
Epoch 1.26: Loss = 1.43053
Epoch 1.27: Loss = 1.42661
Epoch 1.28: Loss = 1.39494
Epoch 1.29: Loss = 1.35158
Epoch 1.30: Loss = 1.33348
Epoch 1.31: Loss = 1.31734
Epoch 1.32: Loss = 1.28859
Epoch 1.33: Loss = 1.24573
Epoch 1.34: Loss = 1.22998
Epoch 1.35: Loss = 1.20483
Epoch 1.36: Loss = 1.20435
Epoch 1.37: Loss = 1.20624
Epoch 1.38: Loss = 1.13248
Epoch 1.39: Loss = 1.16461
Epoch 1.40: Loss = 1.09718
Epoch 1.41: Loss = 1.12628
Epoch 1.42: Loss = 1.13205
Epoch 1.43: Loss = 1.08308
Epoch 1.44: Loss = 1.09061
Epoch 1.45: Loss = 1.03319
Epoch 1.46: Loss = 1.03384
Epoch 1.47: Loss = 1.00871
Epoch 1.48: Loss = 1.04678
Epoch 1.49: Loss = 1.01588
Epoch 1.50: Loss = 1.05659
Epoch 1.51: Loss = 1.07484
Epoch 1.52: Loss = 0.92392
Epoch 1.53: Loss = 0.918137
Epoch 1.54: Loss = 0.922028
Epoch 1.55: Loss = 0.915924
Epoch 1.56: Loss = 0.858719
Epoch 1.57: Loss = 0.935425
Epoch 1.58: Loss = 0.919235
Epoch 1.59: Loss = 0.898819
Epoch 1.60: Loss = 0.882797
Epoch 1.61: Loss = 0.862396
Epoch 1.62: Loss = 0.814743
Epoch 1.63: Loss = 0.810776
Epoch 1.64: Loss = 0.860291
Epoch 1.65: Loss = 0.81015
Epoch 1.66: Loss = 0.746796
Epoch 1.67: Loss = 0.835983
Epoch 1.68: Loss = 0.729996
Epoch 1.69: Loss = 0.77652
Epoch 1.70: Loss = 0.801041
Epoch 1.71: Loss = 0.756287
Epoch 1.72: Loss = 0.79129
Epoch 1.73: Loss = 0.757751
Epoch 1.74: Loss = 0.70282
Epoch 1.75: Loss = 0.78685
Epoch 1.76: Loss = 0.780411
Epoch 1.77: Loss = 0.741547
Epoch 1.78: Loss = 0.699951
Epoch 1.79: Loss = 0.680374
Epoch 1.80: Loss = 0.711334
Epoch 1.81: Loss = 0.689957
Epoch 1.82: Loss = 0.784134
Epoch 1.83: Loss = 0.726395
Epoch 1.84: Loss = 0.717453
Epoch 1.85: Loss = 0.742279
Epoch 1.86: Loss = 0.71521
Epoch 1.87: Loss = 0.65007
Epoch 1.88: Loss = 0.622223
Epoch 1.89: Loss = 0.680222
Epoch 1.90: Loss = 0.663391
Epoch 1.91: Loss = 0.615692
Epoch 1.92: Loss = 0.667709
Epoch 1.93: Loss = 0.646866
Epoch 1.94: Loss = 0.681778
Epoch 1.95: Loss = 0.673813
Epoch 1.96: Loss = 0.599213
Epoch 1.97: Loss = 0.635468
Epoch 1.98: Loss = 0.636475
Epoch 1.99: Loss = 0.585953
Epoch 1.100: Loss = 0.639801
TRAIN LOSS = 1.14821
TRAIN ACC = 69.5007 % (41703/60000)
Loss = 0.651886
Loss = 0.663239
Loss = 0.792313
Loss = 0.72403
Loss = 0.650696
Loss = 0.644211
Loss = 0.741653
Loss = 0.706635
Loss = 0.530304
Loss = 0.47258
Loss = 0.426956
Loss = 0.522247
Loss = 0.485214
Loss = 0.483795
Loss = 0.28212
Loss = 0.450577
Loss = 0.739487
TEST LOSS = 0.583287
TEST ACC = 417.029 % (8370/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.626297
Epoch 2.2: Loss = 0.589874
Epoch 2.3: Loss = 0.593994
Epoch 2.4: Loss = 0.621552
Epoch 2.5: Loss = 0.650299
Epoch 2.6: Loss = 0.599716
Epoch 2.7: Loss = 0.597214
Epoch 2.8: Loss = 0.590454
Epoch 2.9: Loss = 0.611191
Epoch 2.10: Loss = 0.563705
Epoch 2.11: Loss = 0.58139
Epoch 2.12: Loss = 0.638077
Epoch 2.13: Loss = 0.548065
Epoch 2.14: Loss = 0.589676
Epoch 2.15: Loss = 0.54039
Epoch 2.16: Loss = 0.546387
Epoch 2.17: Loss = 0.585602
Epoch 2.18: Loss = 0.552444
Epoch 2.19: Loss = 0.560394
Epoch 2.20: Loss = 0.539658
Epoch 2.21: Loss = 0.545212
Epoch 2.22: Loss = 0.562714
Epoch 2.23: Loss = 0.56105
Epoch 2.24: Loss = 0.531525
Epoch 2.25: Loss = 0.543045
Epoch 2.26: Loss = 0.554535
Epoch 2.27: Loss = 0.54924
Epoch 2.28: Loss = 0.594238
Epoch 2.29: Loss = 0.505203
Epoch 2.30: Loss = 0.501328
Epoch 2.31: Loss = 0.545792
Epoch 2.32: Loss = 0.511932
Epoch 2.33: Loss = 0.50531
Epoch 2.34: Loss = 0.480515
Epoch 2.35: Loss = 0.525299
Epoch 2.36: Loss = 0.551788
Epoch 2.37: Loss = 0.553238
Epoch 2.38: Loss = 0.496948
Epoch 2.39: Loss = 0.536667
Epoch 2.40: Loss = 0.50824
Epoch 2.41: Loss = 0.522171
Epoch 2.42: Loss = 0.5681
Epoch 2.43: Loss = 0.519409
Epoch 2.44: Loss = 0.436478
Epoch 2.45: Loss = 0.481552
Epoch 2.46: Loss = 0.541153
Epoch 2.47: Loss = 0.482986
Epoch 2.48: Loss = 0.505524
Epoch 2.49: Loss = 0.499573
Epoch 2.50: Loss = 0.505981
Epoch 2.51: Loss = 0.521439
Epoch 2.52: Loss = 0.531708
Epoch 2.53: Loss = 0.537933
Epoch 2.54: Loss = 0.494125
Epoch 2.55: Loss = 0.501663
Epoch 2.56: Loss = 0.521896
Epoch 2.57: Loss = 0.491272
Epoch 2.58: Loss = 0.515518
Epoch 2.59: Loss = 0.528976
Epoch 2.60: Loss = 0.520615
Epoch 2.61: Loss = 0.522598
Epoch 2.62: Loss = 0.510101
Epoch 2.63: Loss = 0.48085
Epoch 2.64: Loss = 0.481079
Epoch 2.65: Loss = 0.546783
Epoch 2.66: Loss = 0.455933
Epoch 2.67: Loss = 0.492859
Epoch 2.68: Loss = 0.483459
Epoch 2.69: Loss = 0.450882
Epoch 2.70: Loss = 0.516403
Epoch 2.71: Loss = 0.48584
Epoch 2.72: Loss = 0.472977
Epoch 2.73: Loss = 0.545319
Epoch 2.74: Loss = 0.487167
Epoch 2.75: Loss = 0.48848
Epoch 2.76: Loss = 0.462753
Epoch 2.77: Loss = 0.436951
Epoch 2.78: Loss = 0.541336
Epoch 2.79: Loss = 0.565109
Epoch 2.80: Loss = 0.460495
Epoch 2.81: Loss = 0.50235
Epoch 2.82: Loss = 0.475723
Epoch 2.83: Loss = 0.456467
Epoch 2.84: Loss = 0.425262
Epoch 2.85: Loss = 0.554977
Epoch 2.86: Loss = 0.485641
Epoch 2.87: Loss = 0.437408
Epoch 2.88: Loss = 0.481277
Epoch 2.89: Loss = 0.471863
Epoch 2.90: Loss = 0.469498
Epoch 2.91: Loss = 0.387894
Epoch 2.92: Loss = 0.486099
Epoch 2.93: Loss = 0.42601
Epoch 2.94: Loss = 0.480011
Epoch 2.95: Loss = 0.469711
Epoch 2.96: Loss = 0.508102
Epoch 2.97: Loss = 0.471375
Epoch 2.98: Loss = 0.432999
Epoch 2.99: Loss = 0.490326
Epoch 2.100: Loss = 0.432419
TRAIN LOSS = 0.518494
TRAIN ACC = 85.0754 % (51048/60000)
Loss = 0.481964
Loss = 0.501312
Loss = 0.645218
Loss = 0.589081
Loss = 0.472809
Loss = 0.472244
Loss = 0.604919
Loss = 0.546814
Loss = 0.395767
Loss = 0.323151
Loss = 0.32373
Loss = 0.37178
Loss = 0.306381
Loss = 0.370438
Loss = 0.144684
Loss = 0.296661
Loss = 0.617157
TEST LOSS = 0.435503
TEST ACC = 510.48 % (8729/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.481354
Epoch 3.2: Loss = 0.438354
Epoch 3.3: Loss = 0.46785
Epoch 3.4: Loss = 0.488861
Epoch 3.5: Loss = 0.456085
Epoch 3.6: Loss = 0.377609
Epoch 3.7: Loss = 0.411133
Epoch 3.8: Loss = 0.407425
Epoch 3.9: Loss = 0.454697
Epoch 3.10: Loss = 0.451447
Epoch 3.11: Loss = 0.447006
Epoch 3.12: Loss = 0.502533
Epoch 3.13: Loss = 0.447876
Epoch 3.14: Loss = 0.439163
Epoch 3.15: Loss = 0.424454
Epoch 3.16: Loss = 0.426056
Epoch 3.17: Loss = 0.463242
Epoch 3.18: Loss = 0.426086
Epoch 3.19: Loss = 0.438934
Epoch 3.20: Loss = 0.472122
Epoch 3.21: Loss = 0.444458
Epoch 3.22: Loss = 0.417267
Epoch 3.23: Loss = 0.412842
Epoch 3.24: Loss = 0.449081
Epoch 3.25: Loss = 0.437683
Epoch 3.26: Loss = 0.37677
Epoch 3.27: Loss = 0.423477
Epoch 3.28: Loss = 0.444397
Epoch 3.29: Loss = 0.410416
Epoch 3.30: Loss = 0.310562
Epoch 3.31: Loss = 0.420334
Epoch 3.32: Loss = 0.476334
Epoch 3.33: Loss = 0.436371
Epoch 3.34: Loss = 0.419739
Epoch 3.35: Loss = 0.415894
Epoch 3.36: Loss = 0.371811
Epoch 3.37: Loss = 0.439713
Epoch 3.38: Loss = 0.448288
Epoch 3.39: Loss = 0.443283
Epoch 3.40: Loss = 0.500244
Epoch 3.41: Loss = 0.448822
Epoch 3.42: Loss = 0.427032
Epoch 3.43: Loss = 0.434799
Epoch 3.44: Loss = 0.406174
Epoch 3.45: Loss = 0.422577
Epoch 3.46: Loss = 0.447952
Epoch 3.47: Loss = 0.477631
Epoch 3.48: Loss = 0.467316
Epoch 3.49: Loss = 0.437134
Epoch 3.50: Loss = 0.410141
Epoch 3.51: Loss = 0.39357
Epoch 3.52: Loss = 0.383667
Epoch 3.53: Loss = 0.386169
Epoch 3.54: Loss = 0.496826
Epoch 3.55: Loss = 0.385178
Epoch 3.56: Loss = 0.442734
Epoch 3.57: Loss = 0.469727
Epoch 3.58: Loss = 0.479111
Epoch 3.59: Loss = 0.465317
Epoch 3.60: Loss = 0.355179
Epoch 3.61: Loss = 0.395996
Epoch 3.62: Loss = 0.432129
Epoch 3.63: Loss = 0.441238
Epoch 3.64: Loss = 0.469116
Epoch 3.65: Loss = 0.489929
Epoch 3.66: Loss = 0.441513
Epoch 3.67: Loss = 0.335449
Epoch 3.68: Loss = 0.402023
Epoch 3.69: Loss = 0.473373
Epoch 3.70: Loss = 0.430496
Epoch 3.71: Loss = 0.398636
Epoch 3.72: Loss = 0.439743
Epoch 3.73: Loss = 0.412643
Epoch 3.74: Loss = 0.411102
Epoch 3.75: Loss = 0.410309
Epoch 3.76: Loss = 0.457321
Epoch 3.77: Loss = 0.475143
Epoch 3.78: Loss = 0.438629
Epoch 3.79: Loss = 0.45813
Epoch 3.80: Loss = 0.487732
Epoch 3.81: Loss = 0.505814
Epoch 3.82: Loss = 0.447372
Epoch 3.83: Loss = 0.422607
Epoch 3.84: Loss = 0.417984
Epoch 3.85: Loss = 0.460571
Epoch 3.86: Loss = 0.401566
Epoch 3.87: Loss = 0.469193
Epoch 3.88: Loss = 0.444366
Epoch 3.89: Loss = 0.516312
Epoch 3.90: Loss = 0.43515
Epoch 3.91: Loss = 0.416901
Epoch 3.92: Loss = 0.481766
Epoch 3.93: Loss = 0.434326
Epoch 3.94: Loss = 0.385849
Epoch 3.95: Loss = 0.398102
Epoch 3.96: Loss = 0.395554
Epoch 3.97: Loss = 0.407455
Epoch 3.98: Loss = 0.409988
Epoch 3.99: Loss = 0.409302
Epoch 3.100: Loss = 0.482834
TRAIN LOSS = 0.435028
TRAIN ACC = 87.1994 % (52323/60000)
Loss = 0.426956
Loss = 0.446548
Loss = 0.59729
Loss = 0.54277
Loss = 0.426163
Loss = 0.41391
Loss = 0.572479
Loss = 0.505402
Loss = 0.348206
Loss = 0.291595
Loss = 0.285889
Loss = 0.306229
Loss = 0.249344
Loss = 0.313019
Loss = 0.107132
Loss = 0.239594
Loss = 0.581879
TEST LOSS = 0.387627
TEST ACC = 523.228 % (8854/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.378922
Epoch 4.2: Loss = 0.45488
Epoch 4.3: Loss = 0.458786
Epoch 4.4: Loss = 0.371796
Epoch 4.5: Loss = 0.374664
Epoch 4.6: Loss = 0.433975
Epoch 4.7: Loss = 0.366959
Epoch 4.8: Loss = 0.461334
Epoch 4.9: Loss = 0.429474
Epoch 4.10: Loss = 0.378418
Epoch 4.11: Loss = 0.430954
Epoch 4.12: Loss = 0.424927
Epoch 4.13: Loss = 0.364838
Epoch 4.14: Loss = 0.425171
Epoch 4.15: Loss = 0.37442
Epoch 4.16: Loss = 0.377548
Epoch 4.17: Loss = 0.459106
Epoch 4.18: Loss = 0.358368
Epoch 4.19: Loss = 0.426743
Epoch 4.20: Loss = 0.356033
Epoch 4.21: Loss = 0.451981
Epoch 4.22: Loss = 0.470993
Epoch 4.23: Loss = 0.410416
Epoch 4.24: Loss = 0.426636
Epoch 4.25: Loss = 0.366852
Epoch 4.26: Loss = 0.486282
Epoch 4.27: Loss = 0.455521
Epoch 4.28: Loss = 0.426834
Epoch 4.29: Loss = 0.413681
Epoch 4.30: Loss = 0.375092
Epoch 4.31: Loss = 0.38002
Epoch 4.32: Loss = 0.448654
Epoch 4.33: Loss = 0.44075
Epoch 4.34: Loss = 0.473175
Epoch 4.35: Loss = 0.464859
Epoch 4.36: Loss = 0.411194
Epoch 4.37: Loss = 0.396561
Epoch 4.38: Loss = 0.363205
Epoch 4.39: Loss = 0.458847
Epoch 4.40: Loss = 0.372238
Epoch 4.41: Loss = 0.413864
Epoch 4.42: Loss = 0.421783
Epoch 4.43: Loss = 0.38089
Epoch 4.44: Loss = 0.333389
Epoch 4.45: Loss = 0.422592
Epoch 4.46: Loss = 0.343109
Epoch 4.47: Loss = 0.443405
Epoch 4.48: Loss = 0.389359
Epoch 4.49: Loss = 0.436752
Epoch 4.50: Loss = 0.436844
Epoch 4.51: Loss = 0.349899
Epoch 4.52: Loss = 0.3871
Epoch 4.53: Loss = 0.441071
Epoch 4.54: Loss = 0.405533
Epoch 4.55: Loss = 0.381271
Epoch 4.56: Loss = 0.439087
Epoch 4.57: Loss = 0.369904
Epoch 4.58: Loss = 0.381165
Epoch 4.59: Loss = 0.425827
Epoch 4.60: Loss = 0.394241
Epoch 4.61: Loss = 0.423035
Epoch 4.62: Loss = 0.399796
Epoch 4.63: Loss = 0.379684
Epoch 4.64: Loss = 0.360123
Epoch 4.65: Loss = 0.34848
Epoch 4.66: Loss = 0.477219
Epoch 4.67: Loss = 0.322449
Epoch 4.68: Loss = 0.422775
Epoch 4.69: Loss = 0.388412
Epoch 4.70: Loss = 0.446701
Epoch 4.71: Loss = 0.391434
Epoch 4.72: Loss = 0.398788
Epoch 4.73: Loss = 0.402161
Epoch 4.74: Loss = 0.409225
Epoch 4.75: Loss = 0.349609
Epoch 4.76: Loss = 0.381317
Epoch 4.77: Loss = 0.415726
Epoch 4.78: Loss = 0.403549
Epoch 4.79: Loss = 0.437164
Epoch 4.80: Loss = 0.40834
Epoch 4.81: Loss = 0.439133
Epoch 4.82: Loss = 0.380554
Epoch 4.83: Loss = 0.328812
Epoch 4.84: Loss = 0.395004
Epoch 4.85: Loss = 0.414291
Epoch 4.86: Loss = 0.39003
Epoch 4.87: Loss = 0.364563
Epoch 4.88: Loss = 0.432724
Epoch 4.89: Loss = 0.43779
Epoch 4.90: Loss = 0.386642
Epoch 4.91: Loss = 0.352448
Epoch 4.92: Loss = 0.400528
Epoch 4.93: Loss = 0.399399
Epoch 4.94: Loss = 0.450302
Epoch 4.95: Loss = 0.412323
Epoch 4.96: Loss = 0.363617
Epoch 4.97: Loss = 0.367645
Epoch 4.98: Loss = 0.436874
Epoch 4.99: Loss = 0.400574
Epoch 4.100: Loss = 0.396362
TRAIN LOSS = 0.405151
TRAIN ACC = 88.0203 % (52815/60000)
Loss = 0.404678
Loss = 0.429504
Loss = 0.566696
Loss = 0.53511
Loss = 0.411591
Loss = 0.396622
Loss = 0.560715
Loss = 0.471909
Loss = 0.337875
Loss = 0.267609
Loss = 0.274185
Loss = 0.274948
Loss = 0.216675
Loss = 0.297623
Loss = 0.0909576
Loss = 0.225235
Loss = 0.55246
TEST LOSS = 0.367814
TEST ACC = 528.149 % (8918/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.436798
Epoch 5.2: Loss = 0.381241
Epoch 5.3: Loss = 0.335617
Epoch 5.4: Loss = 0.350098
Epoch 5.5: Loss = 0.380325
Epoch 5.6: Loss = 0.427658
Epoch 5.7: Loss = 0.432129
Epoch 5.8: Loss = 0.396988
Epoch 5.9: Loss = 0.42485
Epoch 5.10: Loss = 0.43985
Epoch 5.11: Loss = 0.404724
Epoch 5.12: Loss = 0.354385
Epoch 5.13: Loss = 0.368835
Epoch 5.14: Loss = 0.382797
Epoch 5.15: Loss = 0.403458
Epoch 5.16: Loss = 0.448532
Epoch 5.17: Loss = 0.371078
Epoch 5.18: Loss = 0.347
Epoch 5.19: Loss = 0.384354
Epoch 5.20: Loss = 0.412231
Epoch 5.21: Loss = 0.346619
Epoch 5.22: Loss = 0.384995
Epoch 5.23: Loss = 0.404922
Epoch 5.24: Loss = 0.389725
Epoch 5.25: Loss = 0.411423
Epoch 5.26: Loss = 0.3573
Epoch 5.27: Loss = 0.413483
Epoch 5.28: Loss = 0.340515
Epoch 5.29: Loss = 0.406128
Epoch 5.30: Loss = 0.40242
Epoch 5.31: Loss = 0.41301
Epoch 5.32: Loss = 0.412186
Epoch 5.33: Loss = 0.410828
Epoch 5.34: Loss = 0.400299
Epoch 5.35: Loss = 0.450439
Epoch 5.36: Loss = 0.343887
Epoch 5.37: Loss = 0.367798
Epoch 5.38: Loss = 0.432465
Epoch 5.39: Loss = 0.421295
Epoch 5.40: Loss = 0.407425
Epoch 5.41: Loss = 0.364944
Epoch 5.42: Loss = 0.465668
Epoch 5.43: Loss = 0.355377
Epoch 5.44: Loss = 0.36409
Epoch 5.45: Loss = 0.346451
Epoch 5.46: Loss = 0.379456
Epoch 5.47: Loss = 0.383804
Epoch 5.48: Loss = 0.412552
Epoch 5.49: Loss = 0.41153
Epoch 5.50: Loss = 0.406693
Epoch 5.51: Loss = 0.392593
Epoch 5.52: Loss = 0.45726
Epoch 5.53: Loss = 0.408081
Epoch 5.54: Loss = 0.390167
Epoch 5.55: Loss = 0.330933
Epoch 5.56: Loss = 0.486176
Epoch 5.57: Loss = 0.391724
Epoch 5.58: Loss = 0.386688
Epoch 5.59: Loss = 0.346924
Epoch 5.60: Loss = 0.399399
Epoch 5.61: Loss = 0.342377
Epoch 5.62: Loss = 0.35405
Epoch 5.63: Loss = 0.391434
Epoch 5.64: Loss = 0.35611
Epoch 5.65: Loss = 0.411118
Epoch 5.66: Loss = 0.400421
Epoch 5.67: Loss = 0.345779
Epoch 5.68: Loss = 0.371323
Epoch 5.69: Loss = 0.427689
Epoch 5.70: Loss = 0.369415
Epoch 5.71: Loss = 0.461578
Epoch 5.72: Loss = 0.337463
Epoch 5.73: Loss = 0.367874
Epoch 5.74: Loss = 0.359695
Epoch 5.75: Loss = 0.373703
Epoch 5.76: Loss = 0.343109
Epoch 5.77: Loss = 0.356766
Epoch 5.78: Loss = 0.326538
Epoch 5.79: Loss = 0.386978
Epoch 5.80: Loss = 0.399658
Epoch 5.81: Loss = 0.390869
Epoch 5.82: Loss = 0.456543
Epoch 5.83: Loss = 0.372238
Epoch 5.84: Loss = 0.421127
Epoch 5.85: Loss = 0.382034
Epoch 5.86: Loss = 0.341248
Epoch 5.87: Loss = 0.358582
Epoch 5.88: Loss = 0.390137
Epoch 5.89: Loss = 0.439545
Epoch 5.90: Loss = 0.424805
Epoch 5.91: Loss = 0.404251
Epoch 5.92: Loss = 0.403992
Epoch 5.93: Loss = 0.378372
Epoch 5.94: Loss = 0.338638
Epoch 5.95: Loss = 0.432129
Epoch 5.96: Loss = 0.412323
Epoch 5.97: Loss = 0.330719
Epoch 5.98: Loss = 0.311691
Epoch 5.99: Loss = 0.346451
Epoch 5.100: Loss = 0.383392
TRAIN LOSS = 0.388824
TRAIN ACC = 88.559 % (53138/60000)
Loss = 0.391937
Loss = 0.424759
Loss = 0.561554
Loss = 0.528122
Loss = 0.405746
Loss = 0.387451
Loss = 0.560791
Loss = 0.468842
Loss = 0.324036
Loss = 0.250229
Loss = 0.281815
Loss = 0.265976
Loss = 0.197739
Loss = 0.267044
Loss = 0.0802917
Loss = 0.220428
Loss = 0.551193
TEST LOSS = 0.359053
TEST ACC = 531.38 % (8940/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.334778
Epoch 6.2: Loss = 0.326675
Epoch 6.3: Loss = 0.347366
Epoch 6.4: Loss = 0.399948
Epoch 6.5: Loss = 0.370926
Epoch 6.6: Loss = 0.386078
Epoch 6.7: Loss = 0.44368
Epoch 6.8: Loss = 0.402054
Epoch 6.9: Loss = 0.319931
Epoch 6.10: Loss = 0.323914
Epoch 6.11: Loss = 0.382767
Epoch 6.12: Loss = 0.442169
Epoch 6.13: Loss = 0.295334
Epoch 6.14: Loss = 0.361694
Epoch 6.15: Loss = 0.440475
Epoch 6.16: Loss = 0.435226
Epoch 6.17: Loss = 0.400818
Epoch 6.18: Loss = 0.380966
Epoch 6.19: Loss = 0.345932
Epoch 6.20: Loss = 0.397293
Epoch 6.21: Loss = 0.375153
Epoch 6.22: Loss = 0.44426
Epoch 6.23: Loss = 0.32724
Epoch 6.24: Loss = 0.429413
Epoch 6.25: Loss = 0.365631
Epoch 6.26: Loss = 0.354767
Epoch 6.27: Loss = 0.383652
Epoch 6.28: Loss = 0.34935
Epoch 6.29: Loss = 0.395981
Epoch 6.30: Loss = 0.428223
Epoch 6.31: Loss = 0.354324
Epoch 6.32: Loss = 0.399643
Epoch 6.33: Loss = 0.353851
Epoch 6.34: Loss = 0.328613
Epoch 6.35: Loss = 0.390701
Epoch 6.36: Loss = 0.360199
Epoch 6.37: Loss = 0.322083
Epoch 6.38: Loss = 0.409897
Epoch 6.39: Loss = 0.302032
Epoch 6.40: Loss = 0.307373
Epoch 6.41: Loss = 0.361145
Epoch 6.42: Loss = 0.41507
Epoch 6.43: Loss = 0.374252
Epoch 6.44: Loss = 0.34906
Epoch 6.45: Loss = 0.373764
Epoch 6.46: Loss = 0.436905
Epoch 6.47: Loss = 0.361557
Epoch 6.48: Loss = 0.383865
Epoch 6.49: Loss = 0.321487
Epoch 6.50: Loss = 0.412796
Epoch 6.51: Loss = 0.376358
Epoch 6.52: Loss = 0.343781
Epoch 6.53: Loss = 0.356857
Epoch 6.54: Loss = 0.41835
Epoch 6.55: Loss = 0.379105
Epoch 6.56: Loss = 0.483704
Epoch 6.57: Loss = 0.421249
Epoch 6.58: Loss = 0.316101
Epoch 6.59: Loss = 0.445297
Epoch 6.60: Loss = 0.337128
Epoch 6.61: Loss = 0.313049
Epoch 6.62: Loss = 0.403564
Epoch 6.63: Loss = 0.319336
Epoch 6.64: Loss = 0.500275
Epoch 6.65: Loss = 0.398972
Epoch 6.66: Loss = 0.440735
Epoch 6.67: Loss = 0.46077
Epoch 6.68: Loss = 0.37471
Epoch 6.69: Loss = 0.384171
Epoch 6.70: Loss = 0.338425
Epoch 6.71: Loss = 0.38765
Epoch 6.72: Loss = 0.33728
Epoch 6.73: Loss = 0.405319
Epoch 6.74: Loss = 0.385391
Epoch 6.75: Loss = 0.340988
Epoch 6.76: Loss = 0.328476
Epoch 6.77: Loss = 0.415604
Epoch 6.78: Loss = 0.30632
Epoch 6.79: Loss = 0.387085
Epoch 6.80: Loss = 0.378601
Epoch 6.81: Loss = 0.400391
Epoch 6.82: Loss = 0.398499
Epoch 6.83: Loss = 0.369141
Epoch 6.84: Loss = 0.339127
Epoch 6.85: Loss = 0.429794
Epoch 6.86: Loss = 0.400543
Epoch 6.87: Loss = 0.363708
Epoch 6.88: Loss = 0.35788
Epoch 6.89: Loss = 0.401077
Epoch 6.90: Loss = 0.419815
Epoch 6.91: Loss = 0.435898
Epoch 6.92: Loss = 0.390472
Epoch 6.93: Loss = 0.335388
Epoch 6.94: Loss = 0.34967
Epoch 6.95: Loss = 0.379715
Epoch 6.96: Loss = 0.368668
Epoch 6.97: Loss = 0.336243
Epoch 6.98: Loss = 0.393524
Epoch 6.99: Loss = 0.40979
Epoch 6.100: Loss = 0.402985
TRAIN LOSS = 0.378784
TRAIN ACC = 88.9465 % (53370/60000)
Loss = 0.372864
Loss = 0.404816
Loss = 0.560623
Loss = 0.514359
Loss = 0.391052
Loss = 0.372253
Loss = 0.546906
Loss = 0.454147
Loss = 0.315933
Loss = 0.240601
Loss = 0.291199
Loss = 0.264801
Loss = 0.196442
Loss = 0.260452
Loss = 0.0751648
Loss = 0.223404
Loss = 0.549591
TEST LOSS = 0.351084
TEST ACC = 533.699 % (8983/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.359375
Epoch 7.2: Loss = 0.328247
Epoch 7.3: Loss = 0.336349
Epoch 7.4: Loss = 0.384369
Epoch 7.5: Loss = 0.345322
Epoch 7.6: Loss = 0.369019
Epoch 7.7: Loss = 0.334335
Epoch 7.8: Loss = 0.430893
Epoch 7.9: Loss = 0.397415
Epoch 7.10: Loss = 0.386841
Epoch 7.11: Loss = 0.431808
Epoch 7.12: Loss = 0.39241
Epoch 7.13: Loss = 0.366501
Epoch 7.14: Loss = 0.323227
Epoch 7.15: Loss = 0.308624
Epoch 7.16: Loss = 0.371124
Epoch 7.17: Loss = 0.321091
Epoch 7.18: Loss = 0.301117
Epoch 7.19: Loss = 0.353485
Epoch 7.20: Loss = 0.408478
Epoch 7.21: Loss = 0.369247
Epoch 7.22: Loss = 0.461197
Epoch 7.23: Loss = 0.341919
Epoch 7.24: Loss = 0.33699
Epoch 7.25: Loss = 0.413361
Epoch 7.26: Loss = 0.357559
Epoch 7.27: Loss = 0.321747
Epoch 7.28: Loss = 0.429657
Epoch 7.29: Loss = 0.324265
Epoch 7.30: Loss = 0.400665
Epoch 7.31: Loss = 0.373062
Epoch 7.32: Loss = 0.282196
Epoch 7.33: Loss = 0.349091
Epoch 7.34: Loss = 0.33786
Epoch 7.35: Loss = 0.399887
Epoch 7.36: Loss = 0.421677
Epoch 7.37: Loss = 0.442871
Epoch 7.38: Loss = 0.35759
Epoch 7.39: Loss = 0.364548
Epoch 7.40: Loss = 0.345245
Epoch 7.41: Loss = 0.455093
Epoch 7.42: Loss = 0.366287
Epoch 7.43: Loss = 0.282837
Epoch 7.44: Loss = 0.340729
Epoch 7.45: Loss = 0.453476
Epoch 7.46: Loss = 0.383682
Epoch 7.47: Loss = 0.400375
Epoch 7.48: Loss = 0.412186
Epoch 7.49: Loss = 0.33667
Epoch 7.50: Loss = 0.450821
Epoch 7.51: Loss = 0.338043
Epoch 7.52: Loss = 0.453079
Epoch 7.53: Loss = 0.279694
Epoch 7.54: Loss = 0.369965
Epoch 7.55: Loss = 0.321198
Epoch 7.56: Loss = 0.377975
Epoch 7.57: Loss = 0.401321
Epoch 7.58: Loss = 0.351578
Epoch 7.59: Loss = 0.380936
Epoch 7.60: Loss = 0.394821
Epoch 7.61: Loss = 0.346359
Epoch 7.62: Loss = 0.321198
Epoch 7.63: Loss = 0.413589
Epoch 7.64: Loss = 0.3517
Epoch 7.65: Loss = 0.378296
Epoch 7.66: Loss = 0.287094
Epoch 7.67: Loss = 0.399139
Epoch 7.68: Loss = 0.293472
Epoch 7.69: Loss = 0.361771
Epoch 7.70: Loss = 0.301849
Epoch 7.71: Loss = 0.377151
Epoch 7.72: Loss = 0.398605
Epoch 7.73: Loss = 0.357712
Epoch 7.74: Loss = 0.362961
Epoch 7.75: Loss = 0.404221
Epoch 7.76: Loss = 0.393982
Epoch 7.77: Loss = 0.367462
Epoch 7.78: Loss = 0.402115
Epoch 7.79: Loss = 0.401505
Epoch 7.80: Loss = 0.416718
Epoch 7.81: Loss = 0.358276
Epoch 7.82: Loss = 0.41684
Epoch 7.83: Loss = 0.365555
Epoch 7.84: Loss = 0.368744
Epoch 7.85: Loss = 0.449738
Epoch 7.86: Loss = 0.336456
Epoch 7.87: Loss = 0.28894
Epoch 7.88: Loss = 0.470963
Epoch 7.89: Loss = 0.369873
Epoch 7.90: Loss = 0.418793
Epoch 7.91: Loss = 0.299103
Epoch 7.92: Loss = 0.409058
Epoch 7.93: Loss = 0.274689
Epoch 7.94: Loss = 0.321716
Epoch 7.95: Loss = 0.329453
Epoch 7.96: Loss = 0.3284
Epoch 7.97: Loss = 0.416977
Epoch 7.98: Loss = 0.342682
Epoch 7.99: Loss = 0.40416
Epoch 7.100: Loss = 0.368607
TRAIN LOSS = 0.369064
TRAIN ACC = 89.357 % (53616/60000)
Loss = 0.365997
Loss = 0.390686
Loss = 0.557861
Loss = 0.506622
Loss = 0.387299
Loss = 0.366226
Loss = 0.542389
Loss = 0.444763
Loss = 0.313675
Loss = 0.237366
Loss = 0.289551
Loss = 0.261536
Loss = 0.191223
Loss = 0.261703
Loss = 0.074173
Loss = 0.214005
Loss = 0.543396
TEST LOSS = 0.34604
TEST ACC = 536.159 % (9000/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.409988
Epoch 8.2: Loss = 0.381424
Epoch 8.3: Loss = 0.368896
Epoch 8.4: Loss = 0.395309
Epoch 8.5: Loss = 0.411331
Epoch 8.6: Loss = 0.39502
Epoch 8.7: Loss = 0.365417
Epoch 8.8: Loss = 0.323029
Epoch 8.9: Loss = 0.398758
Epoch 8.10: Loss = 0.381439
Epoch 8.11: Loss = 0.342728
Epoch 8.12: Loss = 0.329697
Epoch 8.13: Loss = 0.375931
Epoch 8.14: Loss = 0.378311
Epoch 8.15: Loss = 0.396133
Epoch 8.16: Loss = 0.383179
Epoch 8.17: Loss = 0.316727
Epoch 8.18: Loss = 0.368912
Epoch 8.19: Loss = 0.356216
Epoch 8.20: Loss = 0.340942
Epoch 8.21: Loss = 0.309998
Epoch 8.22: Loss = 0.325195
Epoch 8.23: Loss = 0.370743
Epoch 8.24: Loss = 0.363739
Epoch 8.25: Loss = 0.329498
Epoch 8.26: Loss = 0.376755
Epoch 8.27: Loss = 0.32666
Epoch 8.28: Loss = 0.289322
Epoch 8.29: Loss = 0.270782
Epoch 8.30: Loss = 0.353226
Epoch 8.31: Loss = 0.298584
Epoch 8.32: Loss = 0.293762
Epoch 8.33: Loss = 0.391144
Epoch 8.34: Loss = 0.326752
Epoch 8.35: Loss = 0.283188
Epoch 8.36: Loss = 0.329132
Epoch 8.37: Loss = 0.368256
Epoch 8.38: Loss = 0.447601
Epoch 8.39: Loss = 0.318329
Epoch 8.40: Loss = 0.410812
Epoch 8.41: Loss = 0.425644
Epoch 8.42: Loss = 0.314117
Epoch 8.43: Loss = 0.385101
Epoch 8.44: Loss = 0.404877
Epoch 8.45: Loss = 0.365692
Epoch 8.46: Loss = 0.411667
Epoch 8.47: Loss = 0.406326
Epoch 8.48: Loss = 0.305527
Epoch 8.49: Loss = 0.333893
Epoch 8.50: Loss = 0.336624
Epoch 8.51: Loss = 0.292419
Epoch 8.52: Loss = 0.347916
Epoch 8.53: Loss = 0.420975
Epoch 8.54: Loss = 0.365341
Epoch 8.55: Loss = 0.380585
Epoch 8.56: Loss = 0.35881
Epoch 8.57: Loss = 0.419464
Epoch 8.58: Loss = 0.386703
Epoch 8.59: Loss = 0.359818
Epoch 8.60: Loss = 0.362122
Epoch 8.61: Loss = 0.33194
Epoch 8.62: Loss = 0.386398
Epoch 8.63: Loss = 0.312958
Epoch 8.64: Loss = 0.533264
Epoch 8.65: Loss = 0.370285
Epoch 8.66: Loss = 0.43132
Epoch 8.67: Loss = 0.358582
Epoch 8.68: Loss = 0.332565
Epoch 8.69: Loss = 0.354843
Epoch 8.70: Loss = 0.345108
Epoch 8.71: Loss = 0.272018
Epoch 8.72: Loss = 0.359634
Epoch 8.73: Loss = 0.368469
Epoch 8.74: Loss = 0.344254
Epoch 8.75: Loss = 0.331848
Epoch 8.76: Loss = 0.36821
Epoch 8.77: Loss = 0.387146
Epoch 8.78: Loss = 0.329575
Epoch 8.79: Loss = 0.402344
Epoch 8.80: Loss = 0.36969
Epoch 8.81: Loss = 0.429092
Epoch 8.82: Loss = 0.378067
Epoch 8.83: Loss = 0.405304
Epoch 8.84: Loss = 0.352814
Epoch 8.85: Loss = 0.393875
Epoch 8.86: Loss = 0.412277
Epoch 8.87: Loss = 0.368118
Epoch 8.88: Loss = 0.342117
Epoch 8.89: Loss = 0.332809
Epoch 8.90: Loss = 0.406387
Epoch 8.91: Loss = 0.4216
Epoch 8.92: Loss = 0.4133
Epoch 8.93: Loss = 0.412598
Epoch 8.94: Loss = 0.326172
Epoch 8.95: Loss = 0.320862
Epoch 8.96: Loss = 0.333145
Epoch 8.97: Loss = 0.425339
Epoch 8.98: Loss = 0.374908
Epoch 8.99: Loss = 0.374847
Epoch 8.100: Loss = 0.40654
TRAIN LOSS = 0.365082
TRAIN ACC = 89.5813 % (53751/60000)
Loss = 0.358078
Loss = 0.389404
Loss = 0.549927
Loss = 0.511597
Loss = 0.375214
Loss = 0.359192
Loss = 0.548157
Loss = 0.437531
Loss = 0.311447
Loss = 0.227203
Loss = 0.29039
Loss = 0.251572
Loss = 0.182648
Loss = 0.267639
Loss = 0.0726013
Loss = 0.213669
Loss = 0.55011
TEST LOSS = 0.34278
TEST ACC = 537.509 % (9035/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.328644
Epoch 9.2: Loss = 0.404633
Epoch 9.3: Loss = 0.339737
Epoch 9.4: Loss = 0.372253
Epoch 9.5: Loss = 0.351822
Epoch 9.6: Loss = 0.36882
Epoch 9.7: Loss = 0.423553
Epoch 9.8: Loss = 0.373016
Epoch 9.9: Loss = 0.359528
Epoch 9.10: Loss = 0.340927
Epoch 9.11: Loss = 0.328201
Epoch 9.12: Loss = 0.345047
Epoch 9.13: Loss = 0.437195
Epoch 9.14: Loss = 0.346619
Epoch 9.15: Loss = 0.449402
Epoch 9.16: Loss = 0.368027
Epoch 9.17: Loss = 0.374161
Epoch 9.18: Loss = 0.305374
Epoch 9.19: Loss = 0.380768
Epoch 9.20: Loss = 0.297806
Epoch 9.21: Loss = 0.337173
Epoch 9.22: Loss = 0.33107
Epoch 9.23: Loss = 0.377289
Epoch 9.24: Loss = 0.414001
Epoch 9.25: Loss = 0.320282
Epoch 9.26: Loss = 0.288742
Epoch 9.27: Loss = 0.305771
Epoch 9.28: Loss = 0.35582
Epoch 9.29: Loss = 0.350037
Epoch 9.30: Loss = 0.379501
Epoch 9.31: Loss = 0.405624
Epoch 9.32: Loss = 0.381042
Epoch 9.33: Loss = 0.397781
Epoch 9.34: Loss = 0.399704
Epoch 9.35: Loss = 0.392441
Epoch 9.36: Loss = 0.312958
Epoch 9.37: Loss = 0.34433
Epoch 9.38: Loss = 0.36557
Epoch 9.39: Loss = 0.291107
Epoch 9.40: Loss = 0.362946
Epoch 9.41: Loss = 0.381332
Epoch 9.42: Loss = 0.404114
Epoch 9.43: Loss = 0.323883
Epoch 9.44: Loss = 0.281906
Epoch 9.45: Loss = 0.372742
Epoch 9.46: Loss = 0.351151
Epoch 9.47: Loss = 0.422607
Epoch 9.48: Loss = 0.485504
Epoch 9.49: Loss = 0.363998
Epoch 9.50: Loss = 0.297409
Epoch 9.51: Loss = 0.431213
Epoch 9.52: Loss = 0.403793
Epoch 9.53: Loss = 0.384918
Epoch 9.54: Loss = 0.317627
Epoch 9.55: Loss = 0.354218
Epoch 9.56: Loss = 0.385117
Epoch 9.57: Loss = 0.366776
Epoch 9.58: Loss = 0.385468
Epoch 9.59: Loss = 0.30098
Epoch 9.60: Loss = 0.325699
Epoch 9.61: Loss = 0.363937
Epoch 9.62: Loss = 0.348724
Epoch 9.63: Loss = 0.436859
Epoch 9.64: Loss = 0.30954
Epoch 9.65: Loss = 0.382019
Epoch 9.66: Loss = 0.395096
Epoch 9.67: Loss = 0.369797
Epoch 9.68: Loss = 0.469635
Epoch 9.69: Loss = 0.428741
Epoch 9.70: Loss = 0.389084
Epoch 9.71: Loss = 0.384933
Epoch 9.72: Loss = 0.35321
Epoch 9.73: Loss = 0.311676
Epoch 9.74: Loss = 0.384705
Epoch 9.75: Loss = 0.391251
Epoch 9.76: Loss = 0.325668
Epoch 9.77: Loss = 0.370728
Epoch 9.78: Loss = 0.401657
Epoch 9.79: Loss = 0.385864
Epoch 9.80: Loss = 0.324448
Epoch 9.81: Loss = 0.314209
Epoch 9.82: Loss = 0.334915
Epoch 9.83: Loss = 0.34111
Epoch 9.84: Loss = 0.312042
Epoch 9.85: Loss = 0.413513
Epoch 9.86: Loss = 0.396866
Epoch 9.87: Loss = 0.380234
Epoch 9.88: Loss = 0.33432
Epoch 9.89: Loss = 0.349838
Epoch 9.90: Loss = 0.321533
Epoch 9.91: Loss = 0.378159
Epoch 9.92: Loss = 0.374878
Epoch 9.93: Loss = 0.401596
Epoch 9.94: Loss = 0.363602
Epoch 9.95: Loss = 0.347885
Epoch 9.96: Loss = 0.291931
Epoch 9.97: Loss = 0.427124
Epoch 9.98: Loss = 0.422073
Epoch 9.99: Loss = 0.297211
Epoch 9.100: Loss = 0.298965
TRAIN LOSS = 0.363785
TRAIN ACC = 89.7049 % (53825/60000)
Loss = 0.348007
Loss = 0.385757
Loss = 0.551941
Loss = 0.505051
Loss = 0.367325
Loss = 0.358643
Loss = 0.548141
Loss = 0.436523
Loss = 0.299484
Loss = 0.223175
Loss = 0.285507
Loss = 0.241714
Loss = 0.1763
Loss = 0.265915
Loss = 0.0650177
Loss = 0.201004
Loss = 0.545349
TEST LOSS = 0.337384
TEST ACC = 538.249 % (9061/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.387238
Epoch 10.2: Loss = 0.329666
Epoch 10.3: Loss = 0.41481
Epoch 10.4: Loss = 0.405243
Epoch 10.5: Loss = 0.464722
Epoch 10.6: Loss = 0.340729
Epoch 10.7: Loss = 0.327316
Epoch 10.8: Loss = 0.393585
Epoch 10.9: Loss = 0.317749
Epoch 10.10: Loss = 0.474808
Epoch 10.11: Loss = 0.360947
Epoch 10.12: Loss = 0.357254
Epoch 10.13: Loss = 0.306763
Epoch 10.14: Loss = 0.317734
Epoch 10.15: Loss = 0.31044
Epoch 10.16: Loss = 0.417633
Epoch 10.17: Loss = 0.41423
Epoch 10.18: Loss = 0.377548
Epoch 10.19: Loss = 0.357208
Epoch 10.20: Loss = 0.363068
Epoch 10.21: Loss = 0.334152
Epoch 10.22: Loss = 0.388153
Epoch 10.23: Loss = 0.408585
Epoch 10.24: Loss = 0.42514
Epoch 10.25: Loss = 0.375931
Epoch 10.26: Loss = 0.339691
Epoch 10.27: Loss = 0.314377
Epoch 10.28: Loss = 0.429367
Epoch 10.29: Loss = 0.395844
Epoch 10.30: Loss = 0.38327
Epoch 10.31: Loss = 0.412979
Epoch 10.32: Loss = 0.366867
Epoch 10.33: Loss = 0.35437
Epoch 10.34: Loss = 0.411972
Epoch 10.35: Loss = 0.304367
Epoch 10.36: Loss = 0.384872
Epoch 10.37: Loss = 0.360352
Epoch 10.38: Loss = 0.38353
Epoch 10.39: Loss = 0.31134
Epoch 10.40: Loss = 0.268188
Epoch 10.41: Loss = 0.294098
Epoch 10.42: Loss = 0.330322
Epoch 10.43: Loss = 0.436127
Epoch 10.44: Loss = 0.343353
Epoch 10.45: Loss = 0.349228
Epoch 10.46: Loss = 0.34819
Epoch 10.47: Loss = 0.365494
Epoch 10.48: Loss = 0.299271
Epoch 10.49: Loss = 0.342316
Epoch 10.50: Loss = 0.320465
Epoch 10.51: Loss = 0.3452
Epoch 10.52: Loss = 0.369659
Epoch 10.53: Loss = 0.354401
Epoch 10.54: Loss = 0.280838
Epoch 10.55: Loss = 0.419586
Epoch 10.56: Loss = 0.386093
Epoch 10.57: Loss = 0.461304
Epoch 10.58: Loss = 0.274689
Epoch 10.59: Loss = 0.30954
Epoch 10.60: Loss = 0.411011
Epoch 10.61: Loss = 0.340073
Epoch 10.62: Loss = 0.35611
Epoch 10.63: Loss = 0.312912
Epoch 10.64: Loss = 0.318283
Epoch 10.65: Loss = 0.350159
Epoch 10.66: Loss = 0.405289
Epoch 10.67: Loss = 0.332626
Epoch 10.68: Loss = 0.383804
Epoch 10.69: Loss = 0.351822
Epoch 10.70: Loss = 0.333618
Epoch 10.71: Loss = 0.313339
Epoch 10.72: Loss = 0.356461
Epoch 10.73: Loss = 0.378891
Epoch 10.74: Loss = 0.320511
Epoch 10.75: Loss = 0.454651
Epoch 10.76: Loss = 0.447998
Epoch 10.77: Loss = 0.37413
Epoch 10.78: Loss = 0.345078
Epoch 10.79: Loss = 0.252441
Epoch 10.80: Loss = 0.350037
Epoch 10.81: Loss = 0.334122
Epoch 10.82: Loss = 0.350296
Epoch 10.83: Loss = 0.388718
Epoch 10.84: Loss = 0.341904
Epoch 10.85: Loss = 0.325241
Epoch 10.86: Loss = 0.415161
Epoch 10.87: Loss = 0.394684
Epoch 10.88: Loss = 0.30423
Epoch 10.89: Loss = 0.375916
Epoch 10.90: Loss = 0.408844
Epoch 10.91: Loss = 0.340073
Epoch 10.92: Loss = 0.461121
Epoch 10.93: Loss = 0.453476
Epoch 10.94: Loss = 0.356461
Epoch 10.95: Loss = 0.33139
Epoch 10.96: Loss = 0.327103
Epoch 10.97: Loss = 0.323166
Epoch 10.98: Loss = 0.345306
Epoch 10.99: Loss = 0.358978
Epoch 10.100: Loss = 0.369797
TRAIN LOSS = 0.362183
TRAIN ACC = 89.7522 % (53854/60000)
Loss = 0.340485
Loss = 0.384827
Loss = 0.543274
Loss = 0.510834
Loss = 0.36293
Loss = 0.357834
Loss = 0.553757
Loss = 0.430344
Loss = 0.299042
Loss = 0.228271
Loss = 0.280533
Loss = 0.241058
Loss = 0.171066
Loss = 0.275925
Loss = 0.0648193
Loss = 0.196213
Loss = 0.540085
TEST LOSS = 0.336076
TEST ACC = 538.539 % (9059/10000)
The following benchmarks are including preprocessing (offline phase).
Time = 23132.1 seconds 
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
