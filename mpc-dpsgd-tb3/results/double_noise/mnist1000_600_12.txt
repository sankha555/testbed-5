Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 1000]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 20
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 3
***********************************************************
Epoch 1.1: Loss = 2.33217
Epoch 1.2: Loss = 2.26666
Epoch 1.3: Loss = 2.21437
Epoch 1.4: Loss = 2.16962
Epoch 1.5: Loss = 2.1373
Epoch 1.6: Loss = 2.09779
Epoch 1.7: Loss = 2.05237
Epoch 1.8: Loss = 2.02319
Epoch 1.9: Loss = 1.98175
Epoch 1.10: Loss = 1.91975
Epoch 1.11: Loss = 1.90659
Epoch 1.12: Loss = 1.85965
Epoch 1.13: Loss = 1.82254
Epoch 1.14: Loss = 1.76631
Epoch 1.15: Loss = 1.73578
Epoch 1.16: Loss = 1.70724
Epoch 1.17: Loss = 1.69312
Epoch 1.18: Loss = 1.62079
Epoch 1.19: Loss = 1.62074
Epoch 1.20: Loss = 1.58357
Epoch 1.21: Loss = 1.55644
Epoch 1.22: Loss = 1.52803
Epoch 1.23: Loss = 1.51382
Epoch 1.24: Loss = 1.46568
Epoch 1.25: Loss = 1.44472
Epoch 1.26: Loss = 1.36168
Epoch 1.27: Loss = 1.39603
Epoch 1.28: Loss = 1.36934
Epoch 1.29: Loss = 1.32353
Epoch 1.30: Loss = 1.30272
Epoch 1.31: Loss = 1.26143
Epoch 1.32: Loss = 1.25076
Epoch 1.33: Loss = 1.21834
Epoch 1.34: Loss = 1.24931
Epoch 1.35: Loss = 1.23013
Epoch 1.36: Loss = 1.14719
Epoch 1.37: Loss = 1.15648
Epoch 1.38: Loss = 1.1039
Epoch 1.39: Loss = 1.10356
Epoch 1.40: Loss = 1.06961
Epoch 1.41: Loss = 1.09561
Epoch 1.42: Loss = 1.02509
Epoch 1.43: Loss = 1.02205
Epoch 1.44: Loss = 1.01054
Epoch 1.45: Loss = 0.990158
Epoch 1.46: Loss = 1.05472
Epoch 1.47: Loss = 0.942673
Epoch 1.48: Loss = 1.00296
Epoch 1.49: Loss = 0.98468
Epoch 1.50: Loss = 0.934418
Epoch 1.51: Loss = 0.89476
Epoch 1.52: Loss = 0.900101
Epoch 1.53: Loss = 0.957123
Epoch 1.54: Loss = 0.922943
Epoch 1.55: Loss = 0.913757
Epoch 1.56: Loss = 0.915359
Epoch 1.57: Loss = 0.925903
Epoch 1.58: Loss = 0.808517
Epoch 1.59: Loss = 0.84375
Epoch 1.60: Loss = 0.818878
Epoch 1.61: Loss = 0.833603
Epoch 1.62: Loss = 0.818451
Epoch 1.63: Loss = 0.841553
Epoch 1.64: Loss = 0.78598
Epoch 1.65: Loss = 0.753113
Epoch 1.66: Loss = 0.768845
Epoch 1.67: Loss = 0.771652
Epoch 1.68: Loss = 0.800766
Epoch 1.69: Loss = 0.769226
Epoch 1.70: Loss = 0.751205
Epoch 1.71: Loss = 0.808624
Epoch 1.72: Loss = 0.78891
Epoch 1.73: Loss = 0.7108
Epoch 1.74: Loss = 0.701706
Epoch 1.75: Loss = 0.749115
Epoch 1.76: Loss = 0.736359
Epoch 1.77: Loss = 0.647385
Epoch 1.78: Loss = 0.72171
Epoch 1.79: Loss = 0.653656
Epoch 1.80: Loss = 0.64299
Epoch 1.81: Loss = 0.656021
Epoch 1.82: Loss = 0.66832
Epoch 1.83: Loss = 0.673737
Epoch 1.84: Loss = 0.625626
Epoch 1.85: Loss = 0.683502
Epoch 1.86: Loss = 0.671997
Epoch 1.87: Loss = 0.675171
Epoch 1.88: Loss = 0.684296
Epoch 1.89: Loss = 0.646774
Epoch 1.90: Loss = 0.641449
Epoch 1.91: Loss = 0.638794
Epoch 1.92: Loss = 0.65686
Epoch 1.93: Loss = 0.630112
Epoch 1.94: Loss = 0.644516
Epoch 1.95: Loss = 0.662689
Epoch 1.96: Loss = 0.61853
Epoch 1.97: Loss = 0.696457
Epoch 1.98: Loss = 0.594559
Epoch 1.99: Loss = 0.667114
Epoch 1.100: Loss = 0.551437
TRAIN LOSS = 1.11574
TRAIN ACC = 71.0297 % (42620/60000)
Loss = 0.664459
Loss = 0.654633
Loss = 0.792648
Loss = 0.738113
Loss = 0.649933
Loss = 0.65303
Loss = 0.737427
Loss = 0.712799
Loss = 0.52356
Loss = 0.480759
Loss = 0.428116
Loss = 0.505722
Loss = 0.46756
Loss = 0.48027
Loss = 0.288712
Loss = 0.452072
Loss = 0.743896
TEST LOSS = 0.583544
TEST ACC = 426.199 % (8413/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.63295
Epoch 2.2: Loss = 0.558594
Epoch 2.3: Loss = 0.642624
Epoch 2.4: Loss = 0.600296
Epoch 2.5: Loss = 0.57077
Epoch 2.6: Loss = 0.610367
Epoch 2.7: Loss = 0.599472
Epoch 2.8: Loss = 0.56543
Epoch 2.9: Loss = 0.575424
Epoch 2.10: Loss = 0.584335
Epoch 2.11: Loss = 0.555023
Epoch 2.12: Loss = 0.571259
Epoch 2.13: Loss = 0.545059
Epoch 2.14: Loss = 0.599487
Epoch 2.15: Loss = 0.596741
Epoch 2.16: Loss = 0.574097
Epoch 2.17: Loss = 0.550232
Epoch 2.18: Loss = 0.591339
Epoch 2.19: Loss = 0.569855
Epoch 2.20: Loss = 0.571381
Epoch 2.21: Loss = 0.522339
Epoch 2.22: Loss = 0.511108
Epoch 2.23: Loss = 0.553726
Epoch 2.24: Loss = 0.503845
Epoch 2.25: Loss = 0.580856
Epoch 2.26: Loss = 0.565353
Epoch 2.27: Loss = 0.529007
Epoch 2.28: Loss = 0.522842
Epoch 2.29: Loss = 0.519379
Epoch 2.30: Loss = 0.501923
Epoch 2.31: Loss = 0.49202
Epoch 2.32: Loss = 0.551453
Epoch 2.33: Loss = 0.525681
Epoch 2.34: Loss = 0.531204
Epoch 2.35: Loss = 0.531418
Epoch 2.36: Loss = 0.532928
Epoch 2.37: Loss = 0.561249
Epoch 2.38: Loss = 0.516022
Epoch 2.39: Loss = 0.556702
Epoch 2.40: Loss = 0.483627
Epoch 2.41: Loss = 0.495972
Epoch 2.42: Loss = 0.480835
Epoch 2.43: Loss = 0.555939
Epoch 2.44: Loss = 0.544113
Epoch 2.45: Loss = 0.57402
Epoch 2.46: Loss = 0.524338
Epoch 2.47: Loss = 0.502899
Epoch 2.48: Loss = 0.540802
Epoch 2.49: Loss = 0.487305
Epoch 2.50: Loss = 0.43515
Epoch 2.51: Loss = 0.505051
Epoch 2.52: Loss = 0.510925
Epoch 2.53: Loss = 0.465912
Epoch 2.54: Loss = 0.563828
Epoch 2.55: Loss = 0.533386
Epoch 2.56: Loss = 0.530212
Epoch 2.57: Loss = 0.543518
Epoch 2.58: Loss = 0.509949
Epoch 2.59: Loss = 0.467606
Epoch 2.60: Loss = 0.457886
Epoch 2.61: Loss = 0.497604
Epoch 2.62: Loss = 0.46817
Epoch 2.63: Loss = 0.483459
Epoch 2.64: Loss = 0.481155
Epoch 2.65: Loss = 0.455414
Epoch 2.66: Loss = 0.497162
Epoch 2.67: Loss = 0.501572
Epoch 2.68: Loss = 0.425583
Epoch 2.69: Loss = 0.530212
Epoch 2.70: Loss = 0.534225
Epoch 2.71: Loss = 0.472229
Epoch 2.72: Loss = 0.485184
Epoch 2.73: Loss = 0.527039
Epoch 2.74: Loss = 0.534531
Epoch 2.75: Loss = 0.561417
Epoch 2.76: Loss = 0.458084
Epoch 2.77: Loss = 0.501251
Epoch 2.78: Loss = 0.5
Epoch 2.79: Loss = 0.471817
Epoch 2.80: Loss = 0.481247
Epoch 2.81: Loss = 0.531754
Epoch 2.82: Loss = 0.47905
Epoch 2.83: Loss = 0.530518
Epoch 2.84: Loss = 0.439392
Epoch 2.85: Loss = 0.54715
Epoch 2.86: Loss = 0.495468
Epoch 2.87: Loss = 0.497742
Epoch 2.88: Loss = 0.497757
Epoch 2.89: Loss = 0.460144
Epoch 2.90: Loss = 0.463409
Epoch 2.91: Loss = 0.506317
Epoch 2.92: Loss = 0.503647
Epoch 2.93: Loss = 0.441742
Epoch 2.94: Loss = 0.501816
Epoch 2.95: Loss = 0.435944
Epoch 2.96: Loss = 0.472931
Epoch 2.97: Loss = 0.486954
Epoch 2.98: Loss = 0.424179
Epoch 2.99: Loss = 0.541382
Epoch 2.100: Loss = 0.451202
TRAIN LOSS = 0.52063
TRAIN ACC = 84.8969 % (50940/60000)
Loss = 0.485397
Loss = 0.501328
Loss = 0.635574
Loss = 0.601379
Loss = 0.467056
Loss = 0.493179
Loss = 0.605301
Loss = 0.541122
Loss = 0.392487
Loss = 0.339096
Loss = 0.321609
Loss = 0.351593
Loss = 0.291901
Loss = 0.346436
Loss = 0.141495
Loss = 0.297562
Loss = 0.63913
TEST LOSS = 0.434316
TEST ACC = 509.399 % (8720/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.473114
Epoch 3.2: Loss = 0.465836
Epoch 3.3: Loss = 0.438599
Epoch 3.4: Loss = 0.448578
Epoch 3.5: Loss = 0.471039
Epoch 3.6: Loss = 0.477707
Epoch 3.7: Loss = 0.452103
Epoch 3.8: Loss = 0.460388
Epoch 3.9: Loss = 0.472855
Epoch 3.10: Loss = 0.39325
Epoch 3.11: Loss = 0.436722
Epoch 3.12: Loss = 0.431274
Epoch 3.13: Loss = 0.44101
Epoch 3.14: Loss = 0.471451
Epoch 3.15: Loss = 0.403366
Epoch 3.16: Loss = 0.503448
Epoch 3.17: Loss = 0.45253
Epoch 3.18: Loss = 0.48761
Epoch 3.19: Loss = 0.434662
Epoch 3.20: Loss = 0.428314
Epoch 3.21: Loss = 0.481583
Epoch 3.22: Loss = 0.442291
Epoch 3.23: Loss = 0.458679
Epoch 3.24: Loss = 0.453949
Epoch 3.25: Loss = 0.411545
Epoch 3.26: Loss = 0.446014
Epoch 3.27: Loss = 0.467865
Epoch 3.28: Loss = 0.458923
Epoch 3.29: Loss = 0.520737
Epoch 3.30: Loss = 0.426361
Epoch 3.31: Loss = 0.406662
Epoch 3.32: Loss = 0.384171
Epoch 3.33: Loss = 0.44072
Epoch 3.34: Loss = 0.446228
Epoch 3.35: Loss = 0.416229
Epoch 3.36: Loss = 0.454025
Epoch 3.37: Loss = 0.413864
Epoch 3.38: Loss = 0.465057
Epoch 3.39: Loss = 0.415253
Epoch 3.40: Loss = 0.420486
Epoch 3.41: Loss = 0.425186
Epoch 3.42: Loss = 0.442963
Epoch 3.43: Loss = 0.486618
Epoch 3.44: Loss = 0.459457
Epoch 3.45: Loss = 0.407623
Epoch 3.46: Loss = 0.486908
Epoch 3.47: Loss = 0.446075
Epoch 3.48: Loss = 0.451843
Epoch 3.49: Loss = 0.392761
Epoch 3.50: Loss = 0.482407
Epoch 3.51: Loss = 0.482498
Epoch 3.52: Loss = 0.411514
Epoch 3.53: Loss = 0.454803
Epoch 3.54: Loss = 0.445297
Epoch 3.55: Loss = 0.508392
Epoch 3.56: Loss = 0.387436
Epoch 3.57: Loss = 0.386154
Epoch 3.58: Loss = 0.428421
Epoch 3.59: Loss = 0.411484
Epoch 3.60: Loss = 0.450485
Epoch 3.61: Loss = 0.435547
Epoch 3.62: Loss = 0.463272
Epoch 3.63: Loss = 0.410309
Epoch 3.64: Loss = 0.414444
Epoch 3.65: Loss = 0.467468
Epoch 3.66: Loss = 0.391724
Epoch 3.67: Loss = 0.425766
Epoch 3.68: Loss = 0.49614
Epoch 3.69: Loss = 0.461395
Epoch 3.70: Loss = 0.436111
Epoch 3.71: Loss = 0.465317
Epoch 3.72: Loss = 0.413422
Epoch 3.73: Loss = 0.394196
Epoch 3.74: Loss = 0.482971
Epoch 3.75: Loss = 0.464203
Epoch 3.76: Loss = 0.436157
Epoch 3.77: Loss = 0.35289
Epoch 3.78: Loss = 0.469696
Epoch 3.79: Loss = 0.427124
Epoch 3.80: Loss = 0.458008
Epoch 3.81: Loss = 0.359421
Epoch 3.82: Loss = 0.450241
Epoch 3.83: Loss = 0.376053
Epoch 3.84: Loss = 0.378677
Epoch 3.85: Loss = 0.425476
Epoch 3.86: Loss = 0.420624
Epoch 3.87: Loss = 0.38855
Epoch 3.88: Loss = 0.470184
Epoch 3.89: Loss = 0.456665
Epoch 3.90: Loss = 0.45517
Epoch 3.91: Loss = 0.424515
Epoch 3.92: Loss = 0.462265
Epoch 3.93: Loss = 0.398376
Epoch 3.94: Loss = 0.441757
Epoch 3.95: Loss = 0.462296
Epoch 3.96: Loss = 0.465134
Epoch 3.97: Loss = 0.39119
Epoch 3.98: Loss = 0.431
Epoch 3.99: Loss = 0.41066
Epoch 3.100: Loss = 0.430176
TRAIN LOSS = 0.439896
TRAIN ACC = 86.8347 % (52103/60000)
Loss = 0.433105
Loss = 0.464706
Loss = 0.587555
Loss = 0.563477
Loss = 0.40834
Loss = 0.431381
Loss = 0.556656
Loss = 0.491791
Loss = 0.334061
Loss = 0.299667
Loss = 0.296295
Loss = 0.29744
Loss = 0.237701
Loss = 0.323273
Loss = 0.108047
Loss = 0.253845
Loss = 0.633575
TEST LOSS = 0.390583
TEST ACC = 521.03 % (8856/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.433014
Epoch 4.2: Loss = 0.533035
Epoch 4.3: Loss = 0.448425
Epoch 4.4: Loss = 0.344284
Epoch 4.5: Loss = 0.330856
Epoch 4.6: Loss = 0.463303
Epoch 4.7: Loss = 0.407883
Epoch 4.8: Loss = 0.419083
Epoch 4.9: Loss = 0.435684
Epoch 4.10: Loss = 0.378098
Epoch 4.11: Loss = 0.404724
Epoch 4.12: Loss = 0.389359
Epoch 4.13: Loss = 0.433273
Epoch 4.14: Loss = 0.401367
Epoch 4.15: Loss = 0.389801
Epoch 4.16: Loss = 0.421738
Epoch 4.17: Loss = 0.430878
Epoch 4.18: Loss = 0.402283
Epoch 4.19: Loss = 0.385651
Epoch 4.20: Loss = 0.440689
Epoch 4.21: Loss = 0.405579
Epoch 4.22: Loss = 0.3629
Epoch 4.23: Loss = 0.389984
Epoch 4.24: Loss = 0.474274
Epoch 4.25: Loss = 0.404297
Epoch 4.26: Loss = 0.460709
Epoch 4.27: Loss = 0.386108
Epoch 4.28: Loss = 0.38681
Epoch 4.29: Loss = 0.454498
Epoch 4.30: Loss = 0.412415
Epoch 4.31: Loss = 0.406052
Epoch 4.32: Loss = 0.410477
Epoch 4.33: Loss = 0.400223
Epoch 4.34: Loss = 0.431061
Epoch 4.35: Loss = 0.382156
Epoch 4.36: Loss = 0.444183
Epoch 4.37: Loss = 0.379501
Epoch 4.38: Loss = 0.415085
Epoch 4.39: Loss = 0.388306
Epoch 4.40: Loss = 0.453613
Epoch 4.41: Loss = 0.421585
Epoch 4.42: Loss = 0.376129
Epoch 4.43: Loss = 0.437576
Epoch 4.44: Loss = 0.354355
Epoch 4.45: Loss = 0.386063
Epoch 4.46: Loss = 0.36055
Epoch 4.47: Loss = 0.410233
Epoch 4.48: Loss = 0.456772
Epoch 4.49: Loss = 0.327209
Epoch 4.50: Loss = 0.391174
Epoch 4.51: Loss = 0.37738
Epoch 4.52: Loss = 0.481873
Epoch 4.53: Loss = 0.474182
Epoch 4.54: Loss = 0.376419
Epoch 4.55: Loss = 0.39267
Epoch 4.56: Loss = 0.386948
Epoch 4.57: Loss = 0.402985
Epoch 4.58: Loss = 0.408371
Epoch 4.59: Loss = 0.403915
Epoch 4.60: Loss = 0.431488
Epoch 4.61: Loss = 0.375885
Epoch 4.62: Loss = 0.398361
Epoch 4.63: Loss = 0.361099
Epoch 4.64: Loss = 0.417313
Epoch 4.65: Loss = 0.397797
Epoch 4.66: Loss = 0.422226
Epoch 4.67: Loss = 0.363815
Epoch 4.68: Loss = 0.405273
Epoch 4.69: Loss = 0.411789
Epoch 4.70: Loss = 0.376022
Epoch 4.71: Loss = 0.42868
Epoch 4.72: Loss = 0.441513
Epoch 4.73: Loss = 0.433197
Epoch 4.74: Loss = 0.37796
Epoch 4.75: Loss = 0.385956
Epoch 4.76: Loss = 0.40918
Epoch 4.77: Loss = 0.399338
Epoch 4.78: Loss = 0.415924
Epoch 4.79: Loss = 0.407867
Epoch 4.80: Loss = 0.361816
Epoch 4.81: Loss = 0.400391
Epoch 4.82: Loss = 0.366653
Epoch 4.83: Loss = 0.364822
Epoch 4.84: Loss = 0.424408
Epoch 4.85: Loss = 0.479828
Epoch 4.86: Loss = 0.405823
Epoch 4.87: Loss = 0.434341
Epoch 4.88: Loss = 0.386734
Epoch 4.89: Loss = 0.402695
Epoch 4.90: Loss = 0.457047
Epoch 4.91: Loss = 0.408417
Epoch 4.92: Loss = 0.477234
Epoch 4.93: Loss = 0.43663
Epoch 4.94: Loss = 0.411743
Epoch 4.95: Loss = 0.393265
Epoch 4.96: Loss = 0.3405
Epoch 4.97: Loss = 0.392456
Epoch 4.98: Loss = 0.373474
Epoch 4.99: Loss = 0.395737
Epoch 4.100: Loss = 0.385162
TRAIN LOSS = 0.407272
TRAIN ACC = 87.8128 % (52690/60000)
Loss = 0.416061
Loss = 0.448471
Loss = 0.578354
Loss = 0.540329
Loss = 0.381882
Loss = 0.406555
Loss = 0.551132
Loss = 0.479584
Loss = 0.321701
Loss = 0.279205
Loss = 0.301819
Loss = 0.280945
Loss = 0.219742
Loss = 0.324188
Loss = 0.0949707
Loss = 0.232712
Loss = 0.606812
TEST LOSS = 0.375731
TEST ACC = 526.9 % (8886/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.473663
Epoch 5.2: Loss = 0.523087
Epoch 5.3: Loss = 0.407028
Epoch 5.4: Loss = 0.360901
Epoch 5.5: Loss = 0.436493
Epoch 5.6: Loss = 0.433212
Epoch 5.7: Loss = 0.401978
Epoch 5.8: Loss = 0.449524
Epoch 5.9: Loss = 0.419342
Epoch 5.10: Loss = 0.431396
Epoch 5.11: Loss = 0.40155
Epoch 5.12: Loss = 0.414963
Epoch 5.13: Loss = 0.430984
Epoch 5.14: Loss = 0.355682
Epoch 5.15: Loss = 0.347168
Epoch 5.16: Loss = 0.326248
Epoch 5.17: Loss = 0.339874
Epoch 5.18: Loss = 0.368744
Epoch 5.19: Loss = 0.382477
Epoch 5.20: Loss = 0.409897
Epoch 5.21: Loss = 0.349213
Epoch 5.22: Loss = 0.289093
Epoch 5.23: Loss = 0.332581
Epoch 5.24: Loss = 0.442276
Epoch 5.25: Loss = 0.398666
Epoch 5.26: Loss = 0.367554
Epoch 5.27: Loss = 0.427185
Epoch 5.28: Loss = 0.317245
Epoch 5.29: Loss = 0.375381
Epoch 5.30: Loss = 0.295364
Epoch 5.31: Loss = 0.352203
Epoch 5.32: Loss = 0.410385
Epoch 5.33: Loss = 0.475555
Epoch 5.34: Loss = 0.372742
Epoch 5.35: Loss = 0.31517
Epoch 5.36: Loss = 0.441788
Epoch 5.37: Loss = 0.406921
Epoch 5.38: Loss = 0.394592
Epoch 5.39: Loss = 0.352432
Epoch 5.40: Loss = 0.464279
Epoch 5.41: Loss = 0.458633
Epoch 5.42: Loss = 0.31546
Epoch 5.43: Loss = 0.347458
Epoch 5.44: Loss = 0.412872
Epoch 5.45: Loss = 0.384735
Epoch 5.46: Loss = 0.470261
Epoch 5.47: Loss = 0.427979
Epoch 5.48: Loss = 0.335831
Epoch 5.49: Loss = 0.434494
Epoch 5.50: Loss = 0.423492
Epoch 5.51: Loss = 0.420959
Epoch 5.52: Loss = 0.30806
Epoch 5.53: Loss = 0.35556
Epoch 5.54: Loss = 0.394806
Epoch 5.55: Loss = 0.379349
Epoch 5.56: Loss = 0.410294
Epoch 5.57: Loss = 0.439255
Epoch 5.58: Loss = 0.416931
Epoch 5.59: Loss = 0.364288
Epoch 5.60: Loss = 0.42746
Epoch 5.61: Loss = 0.505753
Epoch 5.62: Loss = 0.356567
Epoch 5.63: Loss = 0.326218
Epoch 5.64: Loss = 0.385117
Epoch 5.65: Loss = 0.303101
Epoch 5.66: Loss = 0.382034
Epoch 5.67: Loss = 0.469543
Epoch 5.68: Loss = 0.47084
Epoch 5.69: Loss = 0.393051
Epoch 5.70: Loss = 0.364685
Epoch 5.71: Loss = 0.321838
Epoch 5.72: Loss = 0.403381
Epoch 5.73: Loss = 0.464737
Epoch 5.74: Loss = 0.352783
Epoch 5.75: Loss = 0.421875
Epoch 5.76: Loss = 0.381073
Epoch 5.77: Loss = 0.373077
Epoch 5.78: Loss = 0.341141
Epoch 5.79: Loss = 0.41095
Epoch 5.80: Loss = 0.389999
Epoch 5.81: Loss = 0.361328
Epoch 5.82: Loss = 0.394196
Epoch 5.83: Loss = 0.366608
Epoch 5.84: Loss = 0.456512
Epoch 5.85: Loss = 0.352631
Epoch 5.86: Loss = 0.333374
Epoch 5.87: Loss = 0.357071
Epoch 5.88: Loss = 0.418121
Epoch 5.89: Loss = 0.315887
Epoch 5.90: Loss = 0.428528
Epoch 5.91: Loss = 0.352753
Epoch 5.92: Loss = 0.397476
Epoch 5.93: Loss = 0.386948
Epoch 5.94: Loss = 0.391129
Epoch 5.95: Loss = 0.369598
Epoch 5.96: Loss = 0.412766
Epoch 5.97: Loss = 0.394028
Epoch 5.98: Loss = 0.405914
Epoch 5.99: Loss = 0.432846
Epoch 5.100: Loss = 0.390762
TRAIN LOSS = 0.391586
TRAIN ACC = 88.472 % (53086/60000)
Loss = 0.400024
Loss = 0.436005
Loss = 0.565323
Loss = 0.530304
Loss = 0.36821
Loss = 0.404053
Loss = 0.555817
Loss = 0.472824
Loss = 0.30162
Loss = 0.264267
Loss = 0.315109
Loss = 0.258972
Loss = 0.203308
Loss = 0.305695
Loss = 0.0769958
Loss = 0.222046
Loss = 0.577332
TEST LOSS = 0.363928
TEST ACC = 530.859 % (8964/10000)
Reducing learning rate to 0.0722351
Epoch 6.1: Loss = 0.386002
Epoch 6.2: Loss = 0.304413
Epoch 6.3: Loss = 0.342117
Epoch 6.4: Loss = 0.346451
Epoch 6.5: Loss = 0.313644
Epoch 6.6: Loss = 0.382721
Epoch 6.7: Loss = 0.356247
Epoch 6.8: Loss = 0.398483
Epoch 6.9: Loss = 0.420563
Epoch 6.10: Loss = 0.429764
Epoch 6.11: Loss = 0.314178
Epoch 6.12: Loss = 0.382263
Epoch 6.13: Loss = 0.381699
Epoch 6.14: Loss = 0.402679
Epoch 6.15: Loss = 0.340317
Epoch 6.16: Loss = 0.350372
Epoch 6.17: Loss = 0.390823
Epoch 6.18: Loss = 0.395432
Epoch 6.19: Loss = 0.456573
Epoch 6.20: Loss = 0.391281
Epoch 6.21: Loss = 0.381088
Epoch 6.22: Loss = 0.322983
Epoch 6.23: Loss = 0.384659
Epoch 6.24: Loss = 0.3293
Epoch 6.25: Loss = 0.353119
Epoch 6.26: Loss = 0.309265
Epoch 6.27: Loss = 0.331009
Epoch 6.28: Loss = 0.396667
Epoch 6.29: Loss = 0.396622
Epoch 6.30: Loss = 0.38443
Epoch 6.31: Loss = 0.381012
Epoch 6.32: Loss = 0.42215
Epoch 6.33: Loss = 0.404297
Epoch 6.34: Loss = 0.436951
Epoch 6.35: Loss = 0.368896
Epoch 6.36: Loss = 0.335358
Epoch 6.37: Loss = 0.436005
Epoch 6.38: Loss = 0.371567
Epoch 6.39: Loss = 0.449875
Epoch 6.40: Loss = 0.419479
Epoch 6.41: Loss = 0.446854
Epoch 6.42: Loss = 0.396301
Epoch 6.43: Loss = 0.404358
Epoch 6.44: Loss = 0.360718
Epoch 6.45: Loss = 0.332565
Epoch 6.46: Loss = 0.428177
Epoch 6.47: Loss = 0.420822
Epoch 6.48: Loss = 0.392899
Epoch 6.49: Loss = 0.406189
Epoch 6.50: Loss = 0.398895
Epoch 6.51: Loss = 0.326019
Epoch 6.52: Loss = 0.332657
Epoch 6.53: Loss = 0.400894
Epoch 6.54: Loss = 0.349503
Epoch 6.55: Loss = 0.411087
Epoch 6.56: Loss = 0.336151
Epoch 6.57: Loss = 0.408417
Epoch 6.58: Loss = 0.42157
Epoch 6.59: Loss = 0.374161
Epoch 6.60: Loss = 0.358124
Epoch 6.61: Loss = 0.396667
Epoch 6.62: Loss = 0.372162
Epoch 6.63: Loss = 0.364441
Epoch 6.64: Loss = 0.398285
Epoch 6.65: Loss = 0.364655
Epoch 6.66: Loss = 0.378723
Epoch 6.67: Loss = 0.325577
Epoch 6.68: Loss = 0.346405
Epoch 6.69: Loss = 0.487167
Epoch 6.70: Loss = 0.408173
Epoch 6.71: Loss = 0.434402
Epoch 6.72: Loss = 0.394318
Epoch 6.73: Loss = 0.409241
Epoch 6.74: Loss = 0.501114
Epoch 6.75: Loss = 0.346634
Epoch 6.76: Loss = 0.392334
Epoch 6.77: Loss = 0.358734
Epoch 6.78: Loss = 0.448486
Epoch 6.79: Loss = 0.365967
Epoch 6.80: Loss = 0.264359
Epoch 6.81: Loss = 0.384201
Epoch 6.82: Loss = 0.378403
Epoch 6.83: Loss = 0.413498
Epoch 6.84: Loss = 0.367905
Epoch 6.85: Loss = 0.420456
Epoch 6.86: Loss = 0.324814
Epoch 6.87: Loss = 0.458237
Epoch 6.88: Loss = 0.335129
Epoch 6.89: Loss = 0.421768
Epoch 6.90: Loss = 0.323349
Epoch 6.91: Loss = 0.397552
Epoch 6.92: Loss = 0.402817
Epoch 6.93: Loss = 0.425293
Epoch 6.94: Loss = 0.400803
Epoch 6.95: Loss = 0.332153
Epoch 6.96: Loss = 0.367386
Epoch 6.97: Loss = 0.377914
Epoch 6.98: Loss = 0.35379
Epoch 6.99: Loss = 0.346161
Epoch 6.100: Loss = 0.293152
TRAIN LOSS = 0.380875
TRAIN ACC = 88.8947 % (53338/60000)
Loss = 0.379745
Loss = 0.432144
Loss = 0.551727
Loss = 0.510254
Loss = 0.358658
Loss = 0.38324
Loss = 0.524475
Loss = 0.450272
Loss = 0.29422
Loss = 0.248306
Loss = 0.290253
Loss = 0.248764
Loss = 0.194366
Loss = 0.305466
Loss = 0.0710907
Loss = 0.21019
Loss = 0.578247
TEST LOSS = 0.35032
TEST ACC = 533.379 % (8999/10000)
Reducing learning rate to 0.0666809
Epoch 7.1: Loss = 0.349823
Epoch 7.2: Loss = 0.338135
Epoch 7.3: Loss = 0.323578
Epoch 7.4: Loss = 0.370392
Epoch 7.5: Loss = 0.330872
Epoch 7.6: Loss = 0.361557
Epoch 7.7: Loss = 0.363419
Epoch 7.8: Loss = 0.351639
Epoch 7.9: Loss = 0.437057
Epoch 7.10: Loss = 0.331406
Epoch 7.11: Loss = 0.395554
Epoch 7.12: Loss = 0.359451
Epoch 7.13: Loss = 0.376801
Epoch 7.14: Loss = 0.390884
Epoch 7.15: Loss = 0.384949
Epoch 7.16: Loss = 0.323196
Epoch 7.17: Loss = 0.407898
Epoch 7.18: Loss = 0.335266
Epoch 7.19: Loss = 0.449265
Epoch 7.20: Loss = 0.399292
Epoch 7.21: Loss = 0.347855
Epoch 7.22: Loss = 0.449905
Epoch 7.23: Loss = 0.365936
Epoch 7.24: Loss = 0.377548
Epoch 7.25: Loss = 0.404953
Epoch 7.26: Loss = 0.441147
Epoch 7.27: Loss = 0.328201
Epoch 7.28: Loss = 0.409302
Epoch 7.29: Loss = 0.449036
Epoch 7.30: Loss = 0.353455
Epoch 7.31: Loss = 0.387222
Epoch 7.32: Loss = 0.365356
Epoch 7.33: Loss = 0.508347
Epoch 7.34: Loss = 0.33638
Epoch 7.35: Loss = 0.346298
Epoch 7.36: Loss = 0.384583
Epoch 7.37: Loss = 0.380859
Epoch 7.38: Loss = 0.413528
Epoch 7.39: Loss = 0.273819
Epoch 7.40: Loss = 0.428253
Epoch 7.41: Loss = 0.335373
Epoch 7.42: Loss = 0.355911
Epoch 7.43: Loss = 0.337265
Epoch 7.44: Loss = 0.343475
Epoch 7.45: Loss = 0.34819
Epoch 7.46: Loss = 0.501801
Epoch 7.47: Loss = 0.357361
Epoch 7.48: Loss = 0.323471
Epoch 7.49: Loss = 0.339035
Epoch 7.50: Loss = 0.344788
Epoch 7.51: Loss = 0.387207
Epoch 7.52: Loss = 0.343811
Epoch 7.53: Loss = 0.365982
Epoch 7.54: Loss = 0.402985
Epoch 7.55: Loss = 0.393143
Epoch 7.56: Loss = 0.372284
Epoch 7.57: Loss = 0.311996
Epoch 7.58: Loss = 0.396286
Epoch 7.59: Loss = 0.474838
Epoch 7.60: Loss = 0.367981
Epoch 7.61: Loss = 0.394226
Epoch 7.62: Loss = 0.368652
Epoch 7.63: Loss = 0.361496
Epoch 7.64: Loss = 0.398621
Epoch 7.65: Loss = 0.341309
Epoch 7.66: Loss = 0.340622
Epoch 7.67: Loss = 0.439407
Epoch 7.68: Loss = 0.312561
Epoch 7.69: Loss = 0.384415
Epoch 7.70: Loss = 0.369507
Epoch 7.71: Loss = 0.4095
Epoch 7.72: Loss = 0.334381
Epoch 7.73: Loss = 0.3703
Epoch 7.74: Loss = 0.360519
Epoch 7.75: Loss = 0.318481
Epoch 7.76: Loss = 0.298599
Epoch 7.77: Loss = 0.365021
Epoch 7.78: Loss = 0.390366
Epoch 7.79: Loss = 0.381592
Epoch 7.80: Loss = 0.419662
Epoch 7.81: Loss = 0.325638
Epoch 7.82: Loss = 0.306824
Epoch 7.83: Loss = 0.358322
Epoch 7.84: Loss = 0.380905
Epoch 7.85: Loss = 0.345184
Epoch 7.86: Loss = 0.36145
Epoch 7.87: Loss = 0.362701
Epoch 7.88: Loss = 0.396179
Epoch 7.89: Loss = 0.387756
Epoch 7.90: Loss = 0.353043
Epoch 7.91: Loss = 0.390747
Epoch 7.92: Loss = 0.396088
Epoch 7.93: Loss = 0.380051
Epoch 7.94: Loss = 0.334244
Epoch 7.95: Loss = 0.334396
Epoch 7.96: Loss = 0.339264
Epoch 7.97: Loss = 0.355072
Epoch 7.98: Loss = 0.378983
Epoch 7.99: Loss = 0.478226
Epoch 7.100: Loss = 0.405029
TRAIN LOSS = 0.372665
TRAIN ACC = 89.1571 % (53497/60000)
Loss = 0.373627
Loss = 0.42453
Loss = 0.544174
Loss = 0.515259
Loss = 0.348129
Loss = 0.374741
Loss = 0.528534
Loss = 0.454742
Loss = 0.290497
Loss = 0.234192
Loss = 0.276428
Loss = 0.243881
Loss = 0.18306
Loss = 0.294235
Loss = 0.0690765
Loss = 0.210526
Loss = 0.552429
TEST LOSS = 0.344035
TEST ACC = 534.969 % (9012/10000)
Reducing learning rate to 0.0611267
Epoch 8.1: Loss = 0.378281
Epoch 8.2: Loss = 0.358124
Epoch 8.3: Loss = 0.365265
Epoch 8.4: Loss = 0.369583
Epoch 8.5: Loss = 0.305176
Epoch 8.6: Loss = 0.417908
Epoch 8.7: Loss = 0.353958
Epoch 8.8: Loss = 0.379318
Epoch 8.9: Loss = 0.339783
Epoch 8.10: Loss = 0.432693
Epoch 8.11: Loss = 0.305191
Epoch 8.12: Loss = 0.360626
Epoch 8.13: Loss = 0.312958
Epoch 8.14: Loss = 0.305756
Epoch 8.15: Loss = 0.354889
Epoch 8.16: Loss = 0.3909
Epoch 8.17: Loss = 0.354675
Epoch 8.18: Loss = 0.373047
Epoch 8.19: Loss = 0.382019
Epoch 8.20: Loss = 0.347748
Epoch 8.21: Loss = 0.457352
Epoch 8.22: Loss = 0.401489
Epoch 8.23: Loss = 0.382797
Epoch 8.24: Loss = 0.36763
Epoch 8.25: Loss = 0.330414
Epoch 8.26: Loss = 0.365799
Epoch 8.27: Loss = 0.40538
Epoch 8.28: Loss = 0.362106
Epoch 8.29: Loss = 0.322281
Epoch 8.30: Loss = 0.373825
Epoch 8.31: Loss = 0.394943
Epoch 8.32: Loss = 0.360107
Epoch 8.33: Loss = 0.381393
Epoch 8.34: Loss = 0.363632
Epoch 8.35: Loss = 0.422882
Epoch 8.36: Loss = 0.367966
Epoch 8.37: Loss = 0.300247
Epoch 8.38: Loss = 0.353317
Epoch 8.39: Loss = 0.425934
Epoch 8.40: Loss = 0.331207
Epoch 8.41: Loss = 0.519699
Epoch 8.42: Loss = 0.336899
Epoch 8.43: Loss = 0.333649
Epoch 8.44: Loss = 0.321106
Epoch 8.45: Loss = 0.36702
Epoch 8.46: Loss = 0.325989
Epoch 8.47: Loss = 0.360672
Epoch 8.48: Loss = 0.421463
Epoch 8.49: Loss = 0.358292
Epoch 8.50: Loss = 0.379776
Epoch 8.51: Loss = 0.316025
Epoch 8.52: Loss = 0.432404
Epoch 8.53: Loss = 0.345413
Epoch 8.54: Loss = 0.412933
Epoch 8.55: Loss = 0.340454
Epoch 8.56: Loss = 0.362381
Epoch 8.57: Loss = 0.305634
Epoch 8.58: Loss = 0.36554
Epoch 8.59: Loss = 0.363983
Epoch 8.60: Loss = 0.333832
Epoch 8.61: Loss = 0.388229
Epoch 8.62: Loss = 0.299179
Epoch 8.63: Loss = 0.407745
Epoch 8.64: Loss = 0.395966
Epoch 8.65: Loss = 0.478882
Epoch 8.66: Loss = 0.367477
Epoch 8.67: Loss = 0.340622
Epoch 8.68: Loss = 0.374084
Epoch 8.69: Loss = 0.420837
Epoch 8.70: Loss = 0.357147
Epoch 8.71: Loss = 0.317871
Epoch 8.72: Loss = 0.397095
Epoch 8.73: Loss = 0.327301
Epoch 8.74: Loss = 0.426483
Epoch 8.75: Loss = 0.351578
Epoch 8.76: Loss = 0.320831
Epoch 8.77: Loss = 0.444031
Epoch 8.78: Loss = 0.382889
Epoch 8.79: Loss = 0.318344
Epoch 8.80: Loss = 0.323685
Epoch 8.81: Loss = 0.472733
Epoch 8.82: Loss = 0.363113
Epoch 8.83: Loss = 0.315231
Epoch 8.84: Loss = 0.375244
Epoch 8.85: Loss = 0.379181
Epoch 8.86: Loss = 0.274506
Epoch 8.87: Loss = 0.360916
Epoch 8.88: Loss = 0.325439
Epoch 8.89: Loss = 0.422485
Epoch 8.90: Loss = 0.402756
Epoch 8.91: Loss = 0.318619
Epoch 8.92: Loss = 0.305603
Epoch 8.93: Loss = 0.313019
Epoch 8.94: Loss = 0.380066
Epoch 8.95: Loss = 0.419037
Epoch 8.96: Loss = 0.339783
Epoch 8.97: Loss = 0.351883
Epoch 8.98: Loss = 0.323441
Epoch 8.99: Loss = 0.418991
Epoch 8.100: Loss = 0.332733
TRAIN LOSS = 0.365967
TRAIN ACC = 89.4211 % (53656/60000)
Loss = 0.359802
Loss = 0.416794
Loss = 0.536072
Loss = 0.518814
Loss = 0.347672
Loss = 0.356155
Loss = 0.52829
Loss = 0.43988
Loss = 0.283661
Loss = 0.239105
Loss = 0.27681
Loss = 0.238983
Loss = 0.172089
Loss = 0.290314
Loss = 0.0666656
Loss = 0.212616
Loss = 0.545685
TEST LOSS = 0.338851
TEST ACC = 536.559 % (9044/10000)
Reducing learning rate to 0.0555725
Epoch 9.1: Loss = 0.357498
Epoch 9.2: Loss = 0.269501
Epoch 9.3: Loss = 0.353409
Epoch 9.4: Loss = 0.316635
Epoch 9.5: Loss = 0.278229
Epoch 9.6: Loss = 0.265564
Epoch 9.7: Loss = 0.443207
Epoch 9.8: Loss = 0.354843
Epoch 9.9: Loss = 0.400223
Epoch 9.10: Loss = 0.331192
Epoch 9.11: Loss = 0.370514
Epoch 9.12: Loss = 0.418503
Epoch 9.13: Loss = 0.367584
Epoch 9.14: Loss = 0.34024
Epoch 9.15: Loss = 0.318069
Epoch 9.16: Loss = 0.350388
Epoch 9.17: Loss = 0.347183
Epoch 9.18: Loss = 0.401428
Epoch 9.19: Loss = 0.281876
Epoch 9.20: Loss = 0.326172
Epoch 9.21: Loss = 0.399185
Epoch 9.22: Loss = 0.338226
Epoch 9.23: Loss = 0.413971
Epoch 9.24: Loss = 0.406876
Epoch 9.25: Loss = 0.356232
Epoch 9.26: Loss = 0.334595
Epoch 9.27: Loss = 0.413834
Epoch 9.28: Loss = 0.367355
Epoch 9.29: Loss = 0.355255
Epoch 9.30: Loss = 0.473846
Epoch 9.31: Loss = 0.319168
Epoch 9.32: Loss = 0.374207
Epoch 9.33: Loss = 0.313141
Epoch 9.34: Loss = 0.345215
Epoch 9.35: Loss = 0.350601
Epoch 9.36: Loss = 0.273453
Epoch 9.37: Loss = 0.321854
Epoch 9.38: Loss = 0.382446
Epoch 9.39: Loss = 0.308884
Epoch 9.40: Loss = 0.42511
Epoch 9.41: Loss = 0.558548
Epoch 9.42: Loss = 0.324539
Epoch 9.43: Loss = 0.256363
Epoch 9.44: Loss = 0.343353
Epoch 9.45: Loss = 0.370529
Epoch 9.46: Loss = 0.266006
Epoch 9.47: Loss = 0.372147
Epoch 9.48: Loss = 0.359375
Epoch 9.49: Loss = 0.335846
Epoch 9.50: Loss = 0.393082
Epoch 9.51: Loss = 0.353119
Epoch 9.52: Loss = 0.365662
Epoch 9.53: Loss = 0.347519
Epoch 9.54: Loss = 0.364822
Epoch 9.55: Loss = 0.408539
Epoch 9.56: Loss = 0.385345
Epoch 9.57: Loss = 0.306671
Epoch 9.58: Loss = 0.375305
Epoch 9.59: Loss = 0.405045
Epoch 9.60: Loss = 0.296051
Epoch 9.61: Loss = 0.289993
Epoch 9.62: Loss = 0.399765
Epoch 9.63: Loss = 0.329605
Epoch 9.64: Loss = 0.343018
Epoch 9.65: Loss = 0.382446
Epoch 9.66: Loss = 0.306717
Epoch 9.67: Loss = 0.376755
Epoch 9.68: Loss = 0.380432
Epoch 9.69: Loss = 0.35022
Epoch 9.70: Loss = 0.337875
Epoch 9.71: Loss = 0.424255
Epoch 9.72: Loss = 0.341019
Epoch 9.73: Loss = 0.409927
Epoch 9.74: Loss = 0.407776
Epoch 9.75: Loss = 0.353226
Epoch 9.76: Loss = 0.357681
Epoch 9.77: Loss = 0.448853
Epoch 9.78: Loss = 0.322937
Epoch 9.79: Loss = 0.373505
Epoch 9.80: Loss = 0.37442
Epoch 9.81: Loss = 0.395172
Epoch 9.82: Loss = 0.349854
Epoch 9.83: Loss = 0.338821
Epoch 9.84: Loss = 0.336288
Epoch 9.85: Loss = 0.364563
Epoch 9.86: Loss = 0.347427
Epoch 9.87: Loss = 0.338013
Epoch 9.88: Loss = 0.340454
Epoch 9.89: Loss = 0.462692
Epoch 9.90: Loss = 0.374634
Epoch 9.91: Loss = 0.468155
Epoch 9.92: Loss = 0.421402
Epoch 9.93: Loss = 0.333405
Epoch 9.94: Loss = 0.434097
Epoch 9.95: Loss = 0.329178
Epoch 9.96: Loss = 0.403961
Epoch 9.97: Loss = 0.386276
Epoch 9.98: Loss = 0.359909
Epoch 9.99: Loss = 0.303009
Epoch 9.100: Loss = 0.322968
TRAIN LOSS = 0.360748
TRAIN ACC = 89.6347 % (53783/60000)
Loss = 0.352722
Loss = 0.407593
Loss = 0.524261
Loss = 0.50766
Loss = 0.34436
Loss = 0.341919
Loss = 0.514282
Loss = 0.437241
Loss = 0.277374
Loss = 0.229721
Loss = 0.267792
Loss = 0.242111
Loss = 0.16951
Loss = 0.305466
Loss = 0.0627899
Loss = 0.206787
Loss = 0.554672
TEST LOSS = 0.333682
TEST ACC = 537.83 % (9060/10000)
Reducing learning rate to 0.0500183
Epoch 10.1: Loss = 0.302185
Epoch 10.2: Loss = 0.326065
Epoch 10.3: Loss = 0.329269
Epoch 10.4: Loss = 0.304001
Epoch 10.5: Loss = 0.355545
Epoch 10.6: Loss = 0.375427
Epoch 10.7: Loss = 0.39122
Epoch 10.8: Loss = 0.372055
Epoch 10.9: Loss = 0.355728
Epoch 10.10: Loss = 0.293793
Epoch 10.11: Loss = 0.293884
Epoch 10.12: Loss = 0.358887
Epoch 10.13: Loss = 0.372406
Epoch 10.14: Loss = 0.398758
Epoch 10.15: Loss = 0.388672
Epoch 10.16: Loss = 0.360901
Epoch 10.17: Loss = 0.395081
Epoch 10.18: Loss = 0.353363
Epoch 10.19: Loss = 0.383789
Epoch 10.20: Loss = 0.383545
Epoch 10.21: Loss = 0.329224
Epoch 10.22: Loss = 0.411148
Epoch 10.23: Loss = 0.33075
Epoch 10.24: Loss = 0.327942
Epoch 10.25: Loss = 0.324493
Epoch 10.26: Loss = 0.408844
Epoch 10.27: Loss = 0.310043
Epoch 10.28: Loss = 0.3918
Epoch 10.29: Loss = 0.359268
Epoch 10.30: Loss = 0.384781
Epoch 10.31: Loss = 0.390396
Epoch 10.32: Loss = 0.405426
Epoch 10.33: Loss = 0.323456
Epoch 10.34: Loss = 0.318665
Epoch 10.35: Loss = 0.325851
Epoch 10.36: Loss = 0.491272
Epoch 10.37: Loss = 0.340286
Epoch 10.38: Loss = 0.406067
Epoch 10.39: Loss = 0.367599
Epoch 10.40: Loss = 0.299484
Epoch 10.41: Loss = 0.353043
Epoch 10.42: Loss = 0.321625
Epoch 10.43: Loss = 0.315842
Epoch 10.44: Loss = 0.382828
Epoch 10.45: Loss = 0.350891
Epoch 10.46: Loss = 0.385315
Epoch 10.47: Loss = 0.264862
Epoch 10.48: Loss = 0.341782
Epoch 10.49: Loss = 0.292099
Epoch 10.50: Loss = 0.372665
Epoch 10.51: Loss = 0.3853
Epoch 10.52: Loss = 0.280838
Epoch 10.53: Loss = 0.438293
Epoch 10.54: Loss = 0.277924
Epoch 10.55: Loss = 0.384644
Epoch 10.56: Loss = 0.387985
Epoch 10.57: Loss = 0.259567
Epoch 10.58: Loss = 0.308411
Epoch 10.59: Loss = 0.373322
Epoch 10.60: Loss = 0.384644
Epoch 10.61: Loss = 0.310013
Epoch 10.62: Loss = 0.274979
Epoch 10.63: Loss = 0.412277
Epoch 10.64: Loss = 0.422043
Epoch 10.65: Loss = 0.341476
Epoch 10.66: Loss = 0.368973
Epoch 10.67: Loss = 0.350632
Epoch 10.68: Loss = 0.366577
Epoch 10.69: Loss = 0.403275
Epoch 10.70: Loss = 0.291245
Epoch 10.71: Loss = 0.387909
Epoch 10.72: Loss = 0.407608
Epoch 10.73: Loss = 0.361267
Epoch 10.74: Loss = 0.399246
Epoch 10.75: Loss = 0.366257
Epoch 10.76: Loss = 0.321701
Epoch 10.77: Loss = 0.333221
Epoch 10.78: Loss = 0.356903
Epoch 10.79: Loss = 0.36348
Epoch 10.80: Loss = 0.358109
Epoch 10.81: Loss = 0.369263
Epoch 10.82: Loss = 0.414001
Epoch 10.83: Loss = 0.330017
Epoch 10.84: Loss = 0.318634
Epoch 10.85: Loss = 0.415268
Epoch 10.86: Loss = 0.374008
Epoch 10.87: Loss = 0.35878
Epoch 10.88: Loss = 0.342331
Epoch 10.89: Loss = 0.394196
Epoch 10.90: Loss = 0.411331
Epoch 10.91: Loss = 0.323349
Epoch 10.92: Loss = 0.366211
Epoch 10.93: Loss = 0.435638
Epoch 10.94: Loss = 0.333755
Epoch 10.95: Loss = 0.375671
Epoch 10.96: Loss = 0.343063
Epoch 10.97: Loss = 0.340347
Epoch 10.98: Loss = 0.31958
Epoch 10.99: Loss = 0.316025
Epoch 10.100: Loss = 0.406372
TRAIN LOSS = 0.356873
TRAIN ACC = 89.7369 % (53845/60000)
Loss = 0.339478
Loss = 0.403976
Loss = 0.524704
Loss = 0.490204
Loss = 0.346161
Loss = 0.339355
Loss = 0.50528
Loss = 0.421783
Loss = 0.272675
Loss = 0.232712
Loss = 0.268692
Loss = 0.237549
Loss = 0.160233
Loss = 0.304184
Loss = 0.0613403
Loss = 0.216537
Loss = 0.538177
TEST LOSS = 0.329019
TEST ACC = 538.449 % (9060/10000)
Epoch 11.1: Loss = 0.386658
Epoch 11.2: Loss = 0.396759
Epoch 11.3: Loss = 0.288101
Epoch 11.4: Loss = 0.331635
Epoch 11.5: Loss = 0.381256
Epoch 11.6: Loss = 0.42894
Epoch 11.7: Loss = 0.289032
Epoch 11.8: Loss = 0.377228
Epoch 11.9: Loss = 0.379425
Epoch 11.10: Loss = 0.370163
Epoch 11.11: Loss = 0.417572
Epoch 11.12: Loss = 0.396118
Epoch 11.13: Loss = 0.347382
Epoch 11.14: Loss = 0.324356
Epoch 11.15: Loss = 0.313751
Epoch 11.16: Loss = 0.291397
Epoch 11.17: Loss = 0.362427
Epoch 11.18: Loss = 0.354248
Epoch 11.19: Loss = 0.392365
Epoch 11.20: Loss = 0.307739
Epoch 11.21: Loss = 0.447021
Epoch 11.22: Loss = 0.276276
Epoch 11.23: Loss = 0.370255
Epoch 11.24: Loss = 0.365433
Epoch 11.25: Loss = 0.33432
Epoch 11.26: Loss = 0.378036
Epoch 11.27: Loss = 0.313919
Epoch 11.28: Loss = 0.34053
Epoch 11.29: Loss = 0.312439
Epoch 11.30: Loss = 0.345123
Epoch 11.31: Loss = 0.4216
Epoch 11.32: Loss = 0.303131
Epoch 11.33: Loss = 0.349899
Epoch 11.34: Loss = 0.393631
Epoch 11.35: Loss = 0.401764
Epoch 11.36: Loss = 0.286057
Epoch 11.37: Loss = 0.383453
Epoch 11.38: Loss = 0.35408
Epoch 11.39: Loss = 0.349518
Epoch 11.40: Loss = 0.360489
Epoch 11.41: Loss = 0.347382
Epoch 11.42: Loss = 0.314728
Epoch 11.43: Loss = 0.355316
Epoch 11.44: Loss = 0.365906
Epoch 11.45: Loss = 0.313934
Epoch 11.46: Loss = 0.341415
Epoch 11.47: Loss = 0.359375
Epoch 11.48: Loss = 0.378403
Epoch 11.49: Loss = 0.351959
Epoch 11.50: Loss = 0.36882
Epoch 11.51: Loss = 0.360107
Epoch 11.52: Loss = 0.410675
Epoch 11.53: Loss = 0.407669
Epoch 11.54: Loss = 0.303848
Epoch 11.55: Loss = 0.333939
Epoch 11.56: Loss = 0.381195
Epoch 11.57: Loss = 0.336365
Epoch 11.58: Loss = 0.478088
Epoch 11.59: Loss = 0.360275
Epoch 11.60: Loss = 0.343231
Epoch 11.61: Loss = 0.407715
Epoch 11.62: Loss = 0.349213
Epoch 11.63: Loss = 0.336914
Epoch 11.64: Loss = 0.321243
Epoch 11.65: Loss = 0.30574
Epoch 11.66: Loss = 0.329086
Epoch 11.67: Loss = 0.409927
Epoch 11.68: Loss = 0.317291
Epoch 11.69: Loss = 0.365143
Epoch 11.70: Loss = 0.385803
Epoch 11.71: Loss = 0.394119
Epoch 11.72: Loss = 0.375626
Epoch 11.73: Loss = 0.312927
Epoch 11.74: Loss = 0.314636
Epoch 11.75: Loss = 0.364243
Epoch 11.76: Loss = 0.335281
Epoch 11.77: Loss = 0.41745
Epoch 11.78: Loss = 0.335281
Epoch 11.79: Loss = 0.426407
Epoch 11.80: Loss = 0.386032
Epoch 11.81: Loss = 0.427307
Epoch 11.82: Loss = 0.363419
Epoch 11.83: Loss = 0.261551
Epoch 11.84: Loss = 0.292419
Epoch 11.85: Loss = 0.418274
Epoch 11.86: Loss = 0.293503
Epoch 11.87: Loss = 0.344879
Epoch 11.88: Loss = 0.294067
Epoch 11.89: Loss = 0.264984
Epoch 11.90: Loss = 0.39183
Epoch 11.91: Loss = 0.330856
Epoch 11.92: Loss = 0.390457
Epoch 11.93: Loss = 0.289017
Epoch 11.94: Loss = 0.373993
Epoch 11.95: Loss = 0.321228
Epoch 11.96: Loss = 0.25238
Epoch 11.97: Loss = 0.318756
Epoch 11.98: Loss = 0.391876
Epoch 11.99: Loss = 0.384171
Epoch 11.100: Loss = 0.308182
TRAIN LOSS = 0.353394
TRAIN ACC = 89.9658 % (53982/60000)
Loss = 0.343185
Loss = 0.407654
Loss = 0.519897
Loss = 0.488281
Loss = 0.339264
Loss = 0.339569
Loss = 0.508682
Loss = 0.432083
Loss = 0.270752
Loss = 0.225983
Loss = 0.269791
Loss = 0.227417
Loss = 0.159851
Loss = 0.287384
Loss = 0.0614777
Loss = 0.212555
Loss = 0.537094
TEST LOSS = 0.327113
TEST ACC = 539.819 % (9073/10000)
Epoch 12.1: Loss = 0.383469
Epoch 12.2: Loss = 0.326126
Epoch 12.3: Loss = 0.30838
Epoch 12.4: Loss = 0.379501
Epoch 12.5: Loss = 0.368683
Epoch 12.6: Loss = 0.38942
Epoch 12.7: Loss = 0.307449
Epoch 12.8: Loss = 0.322296
Epoch 12.9: Loss = 0.354385
Epoch 12.10: Loss = 0.368408
Epoch 12.11: Loss = 0.341507
Epoch 12.12: Loss = 0.39328
Epoch 12.13: Loss = 0.362244
Epoch 12.14: Loss = 0.286545
Epoch 12.15: Loss = 0.309235
Epoch 12.16: Loss = 0.327698
Epoch 12.17: Loss = 0.250443
Epoch 12.18: Loss = 0.379868
Epoch 12.19: Loss = 0.354736
Epoch 12.20: Loss = 0.425842
Epoch 12.21: Loss = 0.369064
Epoch 12.22: Loss = 0.319199
Epoch 12.23: Loss = 0.37355
Epoch 12.24: Loss = 0.328705
Epoch 12.25: Loss = 0.437424
Epoch 12.26: Loss = 0.423355
Epoch 12.27: Loss = 0.39328
Epoch 12.28: Loss = 0.331772
Epoch 12.29: Loss = 0.360596
Epoch 12.30: Loss = 0.405792
Epoch 12.31: Loss = 0.321091
Epoch 12.32: Loss = 0.315552
Epoch 12.33: Loss = 0.32048
Epoch 12.34: Loss = 0.345749
Epoch 12.35: Loss = 0.298355
Epoch 12.36: Loss = 0.287796
Epoch 12.37: Loss = 0.316742
Epoch 12.38: Loss = 0.31366
Epoch 12.39: Loss = 0.312912
Epoch 12.40: Loss = 0.416077
Epoch 12.41: Loss = 0.386078
Epoch 12.42: Loss = 0.314255
Epoch 12.43: Loss = 0.436905
Epoch 12.44: Loss = 0.337814
Epoch 12.45: Loss = 0.312988
Epoch 12.46: Loss = 0.480789
Epoch 12.47: Loss = 0.307556
Epoch 12.48: Loss = 0.323486
Epoch 12.49: Loss = 0.380051
Epoch 12.50: Loss = 0.373978
Epoch 12.51: Loss = 0.394485
Epoch 12.52: Loss = 0.310303
Epoch 12.53: Loss = 0.32338
Epoch 12.54: Loss = 0.369476
Epoch 12.55: Loss = 0.347244
Epoch 12.56: Loss = 0.332443
Epoch 12.57: Loss = 0.248428
Epoch 12.58: Loss = 0.412231
Epoch 12.59: Loss = 0.363632
Epoch 12.60: Loss = 0.341995
Epoch 12.61: Loss = 0.316376
Epoch 12.62: Loss = 0.296906
Epoch 12.63: Loss = 0.344406
Epoch 12.64: Loss = 0.391556
Epoch 12.65: Loss = 0.41507
Epoch 12.66: Loss = 0.399872
Epoch 12.67: Loss = 0.480103
Epoch 12.68: Loss = 0.306915
Epoch 12.69: Loss = 0.356018
Epoch 12.70: Loss = 0.438232
Epoch 12.71: Loss = 0.398071
Epoch 12.72: Loss = 0.372314
Epoch 12.73: Loss = 0.277481
Epoch 12.74: Loss = 0.389633
Epoch 12.75: Loss = 0.358582
Epoch 12.76: Loss = 0.369339
Epoch 12.77: Loss = 0.434341
Epoch 12.78: Loss = 0.323578
Epoch 12.79: Loss = 0.468933
Epoch 12.80: Loss = 0.327713
Epoch 12.81: Loss = 0.312958
Epoch 12.82: Loss = 0.417099
Epoch 12.83: Loss = 0.223999
Epoch 12.84: Loss = 0.30571
Epoch 12.85: Loss = 0.267365
Epoch 12.86: Loss = 0.338028
Epoch 12.87: Loss = 0.370697
Epoch 12.88: Loss = 0.338211
Epoch 12.89: Loss = 0.340225
Epoch 12.90: Loss = 0.326538
Epoch 12.91: Loss = 0.349182
Epoch 12.92: Loss = 0.381287
Epoch 12.93: Loss = 0.296082
Epoch 12.94: Loss = 0.38063
Epoch 12.95: Loss = 0.311218
Epoch 12.96: Loss = 0.277267
Epoch 12.97: Loss = 0.361145
Epoch 12.98: Loss = 0.37204
Epoch 12.99: Loss = 0.349945
Epoch 12.100: Loss = 0.355515
TRAIN LOSS = 0.3517
TRAIN ACC = 90.062 % (54039/60000)
Loss = 0.341003
Loss = 0.402817
Loss = 0.516144
Loss = 0.491653
Loss = 0.341995
Loss = 0.337341
Loss = 0.514252
Loss = 0.426437
Loss = 0.264389
Loss = 0.227615
Loss = 0.274109
Loss = 0.225555
Loss = 0.162155
Loss = 0.288467
Loss = 0.0599213
Loss = 0.218033
Loss = 0.531494
TEST LOSS = 0.326773
TEST ACC = 540.388 % (9081/10000)
Epoch 13.1: Loss = 0.408218
Epoch 13.2: Loss = 0.342743
Epoch 13.3: Loss = 0.359314
Epoch 13.4: Loss = 0.421265
Epoch 13.5: Loss = 0.351608
Epoch 13.6: Loss = 0.373337
Epoch 13.7: Loss = 0.239349
Epoch 13.8: Loss = 0.409225
Epoch 13.9: Loss = 0.318176
Epoch 13.10: Loss = 0.437256
Epoch 13.11: Loss = 0.379883
Epoch 13.12: Loss = 0.375137
Epoch 13.13: Loss = 0.283127
Epoch 13.14: Loss = 0.373978
Epoch 13.15: Loss = 0.322662
Epoch 13.16: Loss = 0.356094
Epoch 13.17: Loss = 0.423065
Epoch 13.18: Loss = 0.3255
Epoch 13.19: Loss = 0.327896
Epoch 13.20: Loss = 0.318253
Epoch 13.21: Loss = 0.307129
Epoch 13.22: Loss = 0.363266
Epoch 13.23: Loss = 0.354599
Epoch 13.24: Loss = 0.308853
Epoch 13.25: Loss = 0.327469
Epoch 13.26: Loss = 0.354004
Epoch 13.27: Loss = 0.312088
Epoch 13.28: Loss = 0.32254
Epoch 13.29: Loss = 0.301041
Epoch 13.30: Loss = 0.392136
Epoch 13.31: Loss = 0.382019
Epoch 13.32: Loss = 0.249298
Epoch 13.33: Loss = 0.362656
Epoch 13.34: Loss = 0.396774
Epoch 13.35: Loss = 0.380325
Epoch 13.36: Loss = 0.395569
Epoch 13.37: Loss = 0.320114
Epoch 13.38: Loss = 0.276398
Epoch 13.39: Loss = 0.319519
Epoch 13.40: Loss = 0.382126
Epoch 13.41: Loss = 0.344803
Epoch 13.42: Loss = 0.356445
Epoch 13.43: Loss = 0.358658
Epoch 13.44: Loss = 0.450012
Epoch 13.45: Loss = 0.327988
Epoch 13.46: Loss = 0.41954
Epoch 13.47: Loss = 0.302872
Epoch 13.48: Loss = 0.396347
Epoch 13.49: Loss = 0.248459
Epoch 13.50: Loss = 0.391205
Epoch 13.51: Loss = 0.360275
Epoch 13.52: Loss = 0.408554
Epoch 13.53: Loss = 0.363647
Epoch 13.54: Loss = 0.338455
Epoch 13.55: Loss = 0.443878
Epoch 13.56: Loss = 0.401932
Epoch 13.57: Loss = 0.376312
Epoch 13.58: Loss = 0.273392
Epoch 13.59: Loss = 0.338974
Epoch 13.60: Loss = 0.298035
Epoch 13.61: Loss = 0.287766
Epoch 13.62: Loss = 0.350922
Epoch 13.63: Loss = 0.353516
Epoch 13.64: Loss = 0.297699
Epoch 13.65: Loss = 0.321564
Epoch 13.66: Loss = 0.341476
Epoch 13.67: Loss = 0.415909
Epoch 13.68: Loss = 0.289108
Epoch 13.69: Loss = 0.400223
Epoch 13.70: Loss = 0.304276
Epoch 13.71: Loss = 0.349182
Epoch 13.72: Loss = 0.357208
Epoch 13.73: Loss = 0.312286
Epoch 13.74: Loss = 0.449066
Epoch 13.75: Loss = 0.384338
Epoch 13.76: Loss = 0.329956
Epoch 13.77: Loss = 0.318863
Epoch 13.78: Loss = 0.377396
Epoch 13.79: Loss = 0.367615
Epoch 13.80: Loss = 0.286774
Epoch 13.81: Loss = 0.342133
Epoch 13.82: Loss = 0.328262
Epoch 13.83: Loss = 0.409439
Epoch 13.84: Loss = 0.325195
Epoch 13.85: Loss = 0.30687
Epoch 13.86: Loss = 0.299011
Epoch 13.87: Loss = 0.416565
Epoch 13.88: Loss = 0.308929
Epoch 13.89: Loss = 0.421356
Epoch 13.90: Loss = 0.315506
Epoch 13.91: Loss = 0.354477
Epoch 13.92: Loss = 0.316544
Epoch 13.93: Loss = 0.263199
Epoch 13.94: Loss = 0.348267
Epoch 13.95: Loss = 0.340286
Epoch 13.96: Loss = 0.379532
Epoch 13.97: Loss = 0.381149
Epoch 13.98: Loss = 0.345428
Epoch 13.99: Loss = 0.313309
Epoch 13.100: Loss = 0.406876
TRAIN LOSS = 0.349716
TRAIN ACC = 90.1764 % (54109/60000)
Loss = 0.348022
Loss = 0.398972
Loss = 0.514832
Loss = 0.489136
Loss = 0.342178
Loss = 0.332108
Loss = 0.501755
Loss = 0.417404
Loss = 0.267288
Loss = 0.221039
Loss = 0.272964
Loss = 0.218155
Loss = 0.154526
Loss = 0.291901
Loss = 0.0571442
Loss = 0.215302
Loss = 0.512802
TEST LOSS = 0.323075
TEST ACC = 541.089 % (9095/10000)
Epoch 14.1: Loss = 0.408112
Epoch 14.2: Loss = 0.261108
Epoch 14.3: Loss = 0.322067
Epoch 14.4: Loss = 0.310059
Epoch 14.5: Loss = 0.360107
Epoch 14.6: Loss = 0.426178
Epoch 14.7: Loss = 0.26947
Epoch 14.8: Loss = 0.256897
Epoch 14.9: Loss = 0.3293
Epoch 14.10: Loss = 0.339371
Epoch 14.11: Loss = 0.378448
Epoch 14.12: Loss = 0.342041
Epoch 14.13: Loss = 0.452682
Epoch 14.14: Loss = 0.345062
Epoch 14.15: Loss = 0.387909
Epoch 14.16: Loss = 0.421463
Epoch 14.17: Loss = 0.318314
Epoch 14.18: Loss = 0.367264
Epoch 14.19: Loss = 0.317795
Epoch 14.20: Loss = 0.329651
Epoch 14.21: Loss = 0.320816
Epoch 14.22: Loss = 0.455154
Epoch 14.23: Loss = 0.356354
Epoch 14.24: Loss = 0.391769
Epoch 14.25: Loss = 0.342758
Epoch 14.26: Loss = 0.365067
Epoch 14.27: Loss = 0.315125
Epoch 14.28: Loss = 0.348129
Epoch 14.29: Loss = 0.374207
Epoch 14.30: Loss = 0.324387
Epoch 14.31: Loss = 0.41951
Epoch 14.32: Loss = 0.42627
Epoch 14.33: Loss = 0.354828
Epoch 14.34: Loss = 0.35025
Epoch 14.35: Loss = 0.347778
Epoch 14.36: Loss = 0.383408
Epoch 14.37: Loss = 0.313095
Epoch 14.38: Loss = 0.248245
Epoch 14.39: Loss = 0.292892
Epoch 14.40: Loss = 0.333313
Epoch 14.41: Loss = 0.343414
Epoch 14.42: Loss = 0.419678
Epoch 14.43: Loss = 0.332413
Epoch 14.44: Loss = 0.299622
Epoch 14.45: Loss = 0.270096
Epoch 14.46: Loss = 0.342987
Epoch 14.47: Loss = 0.353561
Epoch 14.48: Loss = 0.323654
Epoch 14.49: Loss = 0.355835
Epoch 14.50: Loss = 0.318008
Epoch 14.51: Loss = 0.472412
Epoch 14.52: Loss = 0.282272
Epoch 14.53: Loss = 0.360641
Epoch 14.54: Loss = 0.438538
Epoch 14.55: Loss = 0.357391
Epoch 14.56: Loss = 0.262253
Epoch 14.57: Loss = 0.413528
Epoch 14.58: Loss = 0.311691
Epoch 14.59: Loss = 0.379929
Epoch 14.60: Loss = 0.390823
Epoch 14.61: Loss = 0.299698
Epoch 14.62: Loss = 0.238312
Epoch 14.63: Loss = 0.314163
Epoch 14.64: Loss = 0.349106
Epoch 14.65: Loss = 0.444992
Epoch 14.66: Loss = 0.374435
Epoch 14.67: Loss = 0.340149
Epoch 14.68: Loss = 0.321671
Epoch 14.69: Loss = 0.359558
Epoch 14.70: Loss = 0.317505
Epoch 14.71: Loss = 0.385651
Epoch 14.72: Loss = 0.35968
Epoch 14.73: Loss = 0.352936
Epoch 14.74: Loss = 0.325409
Epoch 14.75: Loss = 0.3629
Epoch 14.76: Loss = 0.35289
Epoch 14.77: Loss = 0.34967
Epoch 14.78: Loss = 0.257599
Epoch 14.79: Loss = 0.355576
Epoch 14.80: Loss = 0.282974
Epoch 14.81: Loss = 0.312134
Epoch 14.82: Loss = 0.318039
Epoch 14.83: Loss = 0.346375
Epoch 14.84: Loss = 0.310699
Epoch 14.85: Loss = 0.393982
Epoch 14.86: Loss = 0.369186
Epoch 14.87: Loss = 0.384888
Epoch 14.88: Loss = 0.343597
Epoch 14.89: Loss = 0.250626
Epoch 14.90: Loss = 0.295654
Epoch 14.91: Loss = 0.402649
Epoch 14.92: Loss = 0.249573
Epoch 14.93: Loss = 0.330399
Epoch 14.94: Loss = 0.352722
Epoch 14.95: Loss = 0.364471
Epoch 14.96: Loss = 0.330002
Epoch 14.97: Loss = 0.349976
Epoch 14.98: Loss = 0.359985
Epoch 14.99: Loss = 0.374573
Epoch 14.100: Loss = 0.326508
TRAIN LOSS = 0.345444
TRAIN ACC = 90.3412 % (54207/60000)
Loss = 0.347046
Loss = 0.399033
Loss = 0.519363
Loss = 0.495163
Loss = 0.333664
Loss = 0.33931
Loss = 0.497009
Loss = 0.407043
Loss = 0.269348
Loss = 0.231201
Loss = 0.270309
Loss = 0.209473
Loss = 0.153336
Loss = 0.267914
Loss = 0.061554
Loss = 0.211334
Loss = 0.512146
TEST LOSS = 0.321212
TEST ACC = 542.068 % (9108/10000)
Epoch 15.1: Loss = 0.368851
Epoch 15.2: Loss = 0.350311
Epoch 15.3: Loss = 0.337372
Epoch 15.4: Loss = 0.345367
Epoch 15.5: Loss = 0.366486
Epoch 15.6: Loss = 0.261169
Epoch 15.7: Loss = 0.3965
Epoch 15.8: Loss = 0.345795
Epoch 15.9: Loss = 0.369614
Epoch 15.10: Loss = 0.340332
Epoch 15.11: Loss = 0.31868
Epoch 15.12: Loss = 0.33548
Epoch 15.13: Loss = 0.35994
Epoch 15.14: Loss = 0.389175
Epoch 15.15: Loss = 0.287918
Epoch 15.16: Loss = 0.430756
Epoch 15.17: Loss = 0.310974
Epoch 15.18: Loss = 0.460251
Epoch 15.19: Loss = 0.276703
Epoch 15.20: Loss = 0.33168
Epoch 15.21: Loss = 0.37674
Epoch 15.22: Loss = 0.401581
Epoch 15.23: Loss = 0.298752
Epoch 15.24: Loss = 0.279297
Epoch 15.25: Loss = 0.295197
Epoch 15.26: Loss = 0.362122
Epoch 15.27: Loss = 0.406555
Epoch 15.28: Loss = 0.394424
Epoch 15.29: Loss = 0.368439
Epoch 15.30: Loss = 0.358826
Epoch 15.31: Loss = 0.344849
Epoch 15.32: Loss = 0.257263
Epoch 15.33: Loss = 0.351929
Epoch 15.34: Loss = 0.347168
Epoch 15.35: Loss = 0.267151
Epoch 15.36: Loss = 0.295929
Epoch 15.37: Loss = 0.291306
Epoch 15.38: Loss = 0.381897
Epoch 15.39: Loss = 0.418274
Epoch 15.40: Loss = 0.271255
Epoch 15.41: Loss = 0.352066
Epoch 15.42: Loss = 0.316589
Epoch 15.43: Loss = 0.365707
Epoch 15.44: Loss = 0.327637
Epoch 15.45: Loss = 0.359421
Epoch 15.46: Loss = 0.358551
Epoch 15.47: Loss = 0.347672
Epoch 15.48: Loss = 0.283951
Epoch 15.49: Loss = 0.348892
Epoch 15.50: Loss = 0.252899
Epoch 15.51: Loss = 0.448685
Epoch 15.52: Loss = 0.26535
Epoch 15.53: Loss = 0.277237
Epoch 15.54: Loss = 0.295822
Epoch 15.55: Loss = 0.303497
Epoch 15.56: Loss = 0.34137
Epoch 15.57: Loss = 0.266205
Epoch 15.58: Loss = 0.342133
Epoch 15.59: Loss = 0.327454
Epoch 15.60: Loss = 0.318527
Epoch 15.61: Loss = 0.385651
Epoch 15.62: Loss = 0.288574
Epoch 15.63: Loss = 0.494675
Epoch 15.64: Loss = 0.372269
Epoch 15.65: Loss = 0.344193
Epoch 15.66: Loss = 0.454483
Epoch 15.67: Loss = 0.410233
Epoch 15.68: Loss = 0.300491
Epoch 15.69: Loss = 0.313248
Epoch 15.70: Loss = 0.439423
Epoch 15.71: Loss = 0.321991
Epoch 15.72: Loss = 0.343887
Epoch 15.73: Loss = 0.338104
Epoch 15.74: Loss = 0.353577
Epoch 15.75: Loss = 0.349182
Epoch 15.76: Loss = 0.321365
terminate called after throwing an instance of 'std::runtime_error'
  what():  client 0 already connected
