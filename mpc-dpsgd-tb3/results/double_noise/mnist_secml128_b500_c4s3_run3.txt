Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.2816
Epoch 1.2: Loss = 2.27296
Epoch 1.3: Loss = 2.25
Epoch 1.4: Loss = 2.19655
Epoch 1.5: Loss = 2.18498
Epoch 1.6: Loss = 2.15869
Epoch 1.7: Loss = 2.1613
Epoch 1.8: Loss = 2.1086
Epoch 1.9: Loss = 2.09801
Epoch 1.10: Loss = 2.0468
Epoch 1.11: Loss = 2.01465
Epoch 1.12: Loss = 2.01105
Epoch 1.13: Loss = 1.94495
Epoch 1.14: Loss = 1.96272
Epoch 1.15: Loss = 1.98981
Epoch 1.16: Loss = 1.91786
Epoch 1.17: Loss = 1.86292
Epoch 1.18: Loss = 1.83803
Epoch 1.19: Loss = 1.80348
Epoch 1.20: Loss = 1.79663
Epoch 1.21: Loss = 1.71983
Epoch 1.22: Loss = 1.71877
Epoch 1.23: Loss = 1.66159
Epoch 1.24: Loss = 1.71617
Epoch 1.25: Loss = 1.66521
Epoch 1.26: Loss = 1.67253
Epoch 1.27: Loss = 1.60217
Epoch 1.28: Loss = 1.5918
Epoch 1.29: Loss = 1.59085
Epoch 1.30: Loss = 1.6203
Epoch 1.31: Loss = 1.5005
Epoch 1.32: Loss = 1.50708
Epoch 1.33: Loss = 1.46635
Epoch 1.34: Loss = 1.47368
Epoch 1.35: Loss = 1.40981
Epoch 1.36: Loss = 1.51794
Epoch 1.37: Loss = 1.34511
Epoch 1.38: Loss = 1.28622
Epoch 1.39: Loss = 1.28658
Epoch 1.40: Loss = 1.19839
Epoch 1.41: Loss = 1.26631
Epoch 1.42: Loss = 1.21342
Epoch 1.43: Loss = 1.17015
Epoch 1.44: Loss = 1.06021
Epoch 1.45: Loss = 1.1989
Epoch 1.46: Loss = 1.13913
Epoch 1.47: Loss = 1.07347
Epoch 1.48: Loss = 1.15967
Epoch 1.49: Loss = 1.0694
Epoch 1.50: Loss = 1.16446
Epoch 1.51: Loss = 0.96611
Epoch 1.52: Loss = 0.979721
Epoch 1.53: Loss = 1.02594
Epoch 1.54: Loss = 1.04041
Epoch 1.55: Loss = 1.01759
Epoch 1.56: Loss = 0.970764
Epoch 1.57: Loss = 0.864914
Epoch 1.58: Loss = 0.927887
Epoch 1.59: Loss = 0.916763
Epoch 1.60: Loss = 1.03128
Epoch 1.61: Loss = 0.986145
Epoch 1.62: Loss = 0.994019
Epoch 1.63: Loss = 0.981506
Epoch 1.64: Loss = 0.983673
Epoch 1.65: Loss = 1.03792
Epoch 1.66: Loss = 0.850388
Epoch 1.67: Loss = 0.866837
Epoch 1.68: Loss = 0.710114
Epoch 1.69: Loss = 0.803848
Epoch 1.70: Loss = 0.872162
Epoch 1.71: Loss = 0.827194
Epoch 1.72: Loss = 0.796021
Epoch 1.73: Loss = 0.800293
Epoch 1.74: Loss = 0.678391
Epoch 1.75: Loss = 0.830566
Epoch 1.76: Loss = 0.813278
Epoch 1.77: Loss = 0.751465
Epoch 1.78: Loss = 0.709015
Epoch 1.79: Loss = 0.730591
Epoch 1.80: Loss = 0.870682
Epoch 1.81: Loss = 0.695526
Epoch 1.82: Loss = 0.686066
Epoch 1.83: Loss = 0.851883
Epoch 1.84: Loss = 0.759598
Epoch 1.85: Loss = 0.837402
Epoch 1.86: Loss = 0.741379
Epoch 1.87: Loss = 0.66629
Epoch 1.88: Loss = 0.697937
Epoch 1.89: Loss = 0.801605
Epoch 1.90: Loss = 0.640488
Epoch 1.91: Loss = 0.725479
Epoch 1.92: Loss = 0.713547
Epoch 1.93: Loss = 0.751663
Epoch 1.94: Loss = 0.582947
Epoch 1.95: Loss = 0.720352
Epoch 1.96: Loss = 0.684753
Epoch 1.97: Loss = 0.514175
Epoch 1.98: Loss = 0.621887
Epoch 1.99: Loss = 0.728622
Epoch 1.100: Loss = 0.843506
Epoch 1.101: Loss = 0.727005
Epoch 1.102: Loss = 0.612259
Epoch 1.103: Loss = 0.606415
Epoch 1.104: Loss = 0.570267
Epoch 1.105: Loss = 0.68576
Epoch 1.106: Loss = 0.689651
Epoch 1.107: Loss = 0.576202
Epoch 1.108: Loss = 0.625549
Epoch 1.109: Loss = 0.58931
Epoch 1.110: Loss = 0.575821
Epoch 1.111: Loss = 0.505798
Epoch 1.112: Loss = 0.47493
Epoch 1.113: Loss = 0.5961
Epoch 1.114: Loss = 0.511398
Epoch 1.115: Loss = 0.575745
Epoch 1.116: Loss = 0.570251
Epoch 1.117: Loss = 0.478485
Epoch 1.118: Loss = 0.399902
Epoch 1.119: Loss = 0.451385
Epoch 1.120: Loss = 0.48288
TRAIN LOSS = 1.12646
TRAIN ACC = 68.5669 % (41142/60000)
Loss = 0.579895
Loss = 0.620193
Loss = 0.76683
Loss = 0.687149
Loss = 0.738373
Loss = 0.616653
Loss = 0.568207
Loss = 0.74971
Loss = 0.694458
Loss = 0.66803
Loss = 0.327789
Loss = 0.505478
Loss = 0.362381
Loss = 0.583908
Loss = 0.445999
Loss = 0.416092
Loss = 0.429169
Loss = 0.236969
Loss = 0.430832
Loss = 0.707184
TEST LOSS = 0.556765
TEST ACC = 411.42 % (8303/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.551743
Epoch 2.2: Loss = 0.670639
Epoch 2.3: Loss = 0.62146
Epoch 2.4: Loss = 0.489426
Epoch 2.5: Loss = 0.515167
Epoch 2.6: Loss = 0.523544
Epoch 2.7: Loss = 0.589035
Epoch 2.8: Loss = 0.525818
Epoch 2.9: Loss = 0.555161
Epoch 2.10: Loss = 0.529434
Epoch 2.11: Loss = 0.534439
Epoch 2.12: Loss = 0.518356
Epoch 2.13: Loss = 0.434921
Epoch 2.14: Loss = 0.502411
Epoch 2.15: Loss = 0.626846
Epoch 2.16: Loss = 0.583588
Epoch 2.17: Loss = 0.59343
Epoch 2.18: Loss = 0.66597
Epoch 2.19: Loss = 0.539474
Epoch 2.20: Loss = 0.489563
Epoch 2.21: Loss = 0.460678
Epoch 2.22: Loss = 0.460617
Epoch 2.23: Loss = 0.470032
Epoch 2.24: Loss = 0.666809
Epoch 2.25: Loss = 0.544693
Epoch 2.26: Loss = 0.581985
Epoch 2.27: Loss = 0.595932
Epoch 2.28: Loss = 0.612381
Epoch 2.29: Loss = 0.686417
Epoch 2.30: Loss = 0.678421
Epoch 2.31: Loss = 0.473419
Epoch 2.32: Loss = 0.613724
Epoch 2.33: Loss = 0.521606
Epoch 2.34: Loss = 0.583572
Epoch 2.35: Loss = 0.540619
Epoch 2.36: Loss = 0.643967
Epoch 2.37: Loss = 0.470856
Epoch 2.38: Loss = 0.445053
Epoch 2.39: Loss = 0.510284
Epoch 2.40: Loss = 0.4655
Epoch 2.41: Loss = 0.539352
Epoch 2.42: Loss = 0.602234
Epoch 2.43: Loss = 0.455093
Epoch 2.44: Loss = 0.393127
Epoch 2.45: Loss = 0.508331
Epoch 2.46: Loss = 0.557037
Epoch 2.47: Loss = 0.475754
Epoch 2.48: Loss = 0.52034
Epoch 2.49: Loss = 0.498886
Epoch 2.50: Loss = 0.620056
Epoch 2.51: Loss = 0.432571
Epoch 2.52: Loss = 0.426422
Epoch 2.53: Loss = 0.480423
Epoch 2.54: Loss = 0.556824
Epoch 2.55: Loss = 0.496887
Epoch 2.56: Loss = 0.469894
Epoch 2.57: Loss = 0.443451
Epoch 2.58: Loss = 0.490402
Epoch 2.59: Loss = 0.503906
Epoch 2.60: Loss = 0.592819
Epoch 2.61: Loss = 0.591644
Epoch 2.62: Loss = 0.563202
Epoch 2.63: Loss = 0.594467
Epoch 2.64: Loss = 0.612686
Epoch 2.65: Loss = 0.701065
Epoch 2.66: Loss = 0.464783
Epoch 2.67: Loss = 0.514603
Epoch 2.68: Loss = 0.322678
Epoch 2.69: Loss = 0.420822
Epoch 2.70: Loss = 0.586777
Epoch 2.71: Loss = 0.446381
Epoch 2.72: Loss = 0.431534
Epoch 2.73: Loss = 0.495056
Epoch 2.74: Loss = 0.355072
Epoch 2.75: Loss = 0.644852
Epoch 2.76: Loss = 0.522873
Epoch 2.77: Loss = 0.443832
Epoch 2.78: Loss = 0.431885
Epoch 2.79: Loss = 0.539825
Epoch 2.80: Loss = 0.590866
Epoch 2.81: Loss = 0.42392
Epoch 2.82: Loss = 0.394241
Epoch 2.83: Loss = 0.566376
Epoch 2.84: Loss = 0.479065
Epoch 2.85: Loss = 0.664886
Epoch 2.86: Loss = 0.539581
Epoch 2.87: Loss = 0.405502
Epoch 2.88: Loss = 0.445114
Epoch 2.89: Loss = 0.571274
Epoch 2.90: Loss = 0.387787
Epoch 2.91: Loss = 0.535355
Epoch 2.92: Loss = 0.532349
Epoch 2.93: Loss = 0.540436
Epoch 2.94: Loss = 0.387695
Epoch 2.95: Loss = 0.510101
Epoch 2.96: Loss = 0.514359
Epoch 2.97: Loss = 0.361847
Epoch 2.98: Loss = 0.432907
Epoch 2.99: Loss = 0.568375
Epoch 2.100: Loss = 0.613403
Epoch 2.101: Loss = 0.575882
Epoch 2.102: Loss = 0.456345
Epoch 2.103: Loss = 0.454208
Epoch 2.104: Loss = 0.383591
Epoch 2.105: Loss = 0.545334
Epoch 2.106: Loss = 0.571457
Epoch 2.107: Loss = 0.416824
Epoch 2.108: Loss = 0.48233
Epoch 2.109: Loss = 0.432251
Epoch 2.110: Loss = 0.4552
Epoch 2.111: Loss = 0.36586
Epoch 2.112: Loss = 0.355347
Epoch 2.113: Loss = 0.43512
Epoch 2.114: Loss = 0.384949
Epoch 2.115: Loss = 0.412033
Epoch 2.116: Loss = 0.437485
Epoch 2.117: Loss = 0.2939
Epoch 2.118: Loss = 0.234665
Epoch 2.119: Loss = 0.336411
Epoch 2.120: Loss = 0.357651
TRAIN LOSS = 0.505737
TRAIN ACC = 84.4391 % (50666/60000)
Loss = 0.458374
Loss = 0.534012
Loss = 0.674667
Loss = 0.611145
Loss = 0.630814
Loss = 0.49559
Loss = 0.448944
Loss = 0.689056
Loss = 0.619308
Loss = 0.587784
Loss = 0.238022
Loss = 0.44899
Loss = 0.321716
Loss = 0.450256
Loss = 0.283646
Loss = 0.323212
Loss = 0.317871
Loss = 0.114655
Loss = 0.301819
Loss = 0.59259
TEST LOSS = 0.457123
TEST ACC = 506.659 % (8598/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.435425
Epoch 3.2: Loss = 0.535461
Epoch 3.3: Loss = 0.512192
Epoch 3.4: Loss = 0.362198
Epoch 3.5: Loss = 0.418381
Epoch 3.6: Loss = 0.395813
Epoch 3.7: Loss = 0.455948
Epoch 3.8: Loss = 0.426071
Epoch 3.9: Loss = 0.412979
Epoch 3.10: Loss = 0.440826
Epoch 3.11: Loss = 0.456573
Epoch 3.12: Loss = 0.426331
Epoch 3.13: Loss = 0.360123
Epoch 3.14: Loss = 0.417633
Epoch 3.15: Loss = 0.480621
Epoch 3.16: Loss = 0.459747
Epoch 3.17: Loss = 0.509216
Epoch 3.18: Loss = 0.610352
Epoch 3.19: Loss = 0.492889
Epoch 3.20: Loss = 0.425873
Epoch 3.21: Loss = 0.398178
Epoch 3.22: Loss = 0.354172
Epoch 3.23: Loss = 0.38121
Epoch 3.24: Loss = 0.571136
Epoch 3.25: Loss = 0.455353
Epoch 3.26: Loss = 0.530502
Epoch 3.27: Loss = 0.494873
Epoch 3.28: Loss = 0.554306
Epoch 3.29: Loss = 0.624237
Epoch 3.30: Loss = 0.617783
Epoch 3.31: Loss = 0.429459
Epoch 3.32: Loss = 0.515747
Epoch 3.33: Loss = 0.459091
Epoch 3.34: Loss = 0.523544
Epoch 3.35: Loss = 0.482483
Epoch 3.36: Loss = 0.585007
Epoch 3.37: Loss = 0.377731
Epoch 3.38: Loss = 0.396881
Epoch 3.39: Loss = 0.43074
Epoch 3.40: Loss = 0.411545
Epoch 3.41: Loss = 0.463699
Epoch 3.42: Loss = 0.607117
Epoch 3.43: Loss = 0.406998
Epoch 3.44: Loss = 0.342926
Epoch 3.45: Loss = 0.439316
Epoch 3.46: Loss = 0.496078
Epoch 3.47: Loss = 0.456543
Epoch 3.48: Loss = 0.452408
Epoch 3.49: Loss = 0.456406
Epoch 3.50: Loss = 0.573608
Epoch 3.51: Loss = 0.399582
Epoch 3.52: Loss = 0.393112
Epoch 3.53: Loss = 0.433853
Epoch 3.54: Loss = 0.523697
Epoch 3.55: Loss = 0.406998
Epoch 3.56: Loss = 0.438919
Epoch 3.57: Loss = 0.382919
Epoch 3.58: Loss = 0.454666
Epoch 3.59: Loss = 0.4823
Epoch 3.60: Loss = 0.553741
Epoch 3.61: Loss = 0.516464
Epoch 3.62: Loss = 0.558655
Epoch 3.63: Loss = 0.588013
Epoch 3.64: Loss = 0.561813
Epoch 3.65: Loss = 0.688065
Epoch 3.66: Loss = 0.434647
Epoch 3.67: Loss = 0.467606
Epoch 3.68: Loss = 0.25882
Epoch 3.69: Loss = 0.378433
Epoch 3.70: Loss = 0.545486
Epoch 3.71: Loss = 0.401184
Epoch 3.72: Loss = 0.387421
Epoch 3.73: Loss = 0.465866
Epoch 3.74: Loss = 0.331116
Epoch 3.75: Loss = 0.63913
Epoch 3.76: Loss = 0.453339
Epoch 3.77: Loss = 0.383774
Epoch 3.78: Loss = 0.417938
Epoch 3.79: Loss = 0.506531
Epoch 3.80: Loss = 0.516983
Epoch 3.81: Loss = 0.353821
Epoch 3.82: Loss = 0.342834
Epoch 3.83: Loss = 0.505524
Epoch 3.84: Loss = 0.466324
Epoch 3.85: Loss = 0.603546
Epoch 3.86: Loss = 0.527435
Epoch 3.87: Loss = 0.381821
Epoch 3.88: Loss = 0.456787
Epoch 3.89: Loss = 0.530182
Epoch 3.90: Loss = 0.363403
Epoch 3.91: Loss = 0.502838
Epoch 3.92: Loss = 0.517838
Epoch 3.93: Loss = 0.533279
Epoch 3.94: Loss = 0.358551
Epoch 3.95: Loss = 0.498886
Epoch 3.96: Loss = 0.503479
Epoch 3.97: Loss = 0.351852
Epoch 3.98: Loss = 0.403076
Epoch 3.99: Loss = 0.54155
Epoch 3.100: Loss = 0.623352
Epoch 3.101: Loss = 0.594284
Epoch 3.102: Loss = 0.418167
Epoch 3.103: Loss = 0.426605
Epoch 3.104: Loss = 0.36792
Epoch 3.105: Loss = 0.550461
Epoch 3.106: Loss = 0.559814
Epoch 3.107: Loss = 0.361115
Epoch 3.108: Loss = 0.499939
Epoch 3.109: Loss = 0.384827
Epoch 3.110: Loss = 0.429291
Epoch 3.111: Loss = 0.338104
Epoch 3.112: Loss = 0.320053
Epoch 3.113: Loss = 0.418533
Epoch 3.114: Loss = 0.344742
Epoch 3.115: Loss = 0.353149
Epoch 3.116: Loss = 0.391479
Epoch 3.117: Loss = 0.256775
Epoch 3.118: Loss = 0.211182
Epoch 3.119: Loss = 0.3255
Epoch 3.120: Loss = 0.327881
TRAIN LOSS = 0.454498
TRAIN ACC = 86.4471 % (51870/60000)
Loss = 0.404282
Loss = 0.473068
Loss = 0.637848
Loss = 0.595947
Loss = 0.607635
Loss = 0.464813
Loss = 0.410721
Loss = 0.680008
Loss = 0.574722
Loss = 0.550461
Loss = 0.196274
Loss = 0.370636
Loss = 0.319519
Loss = 0.436935
Loss = 0.240723
Loss = 0.322983
Loss = 0.26593
Loss = 0.0786743
Loss = 0.265106
Loss = 0.585236
TEST LOSS = 0.424076
TEST ACC = 518.7 % (8763/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.412537
Epoch 4.2: Loss = 0.507599
Epoch 4.3: Loss = 0.491455
Epoch 4.4: Loss = 0.319382
Epoch 4.5: Loss = 0.367065
Epoch 4.6: Loss = 0.364273
Epoch 4.7: Loss = 0.39975
Epoch 4.8: Loss = 0.389236
Epoch 4.9: Loss = 0.392426
Epoch 4.10: Loss = 0.415237
Epoch 4.11: Loss = 0.431961
Epoch 4.12: Loss = 0.395905
Epoch 4.13: Loss = 0.32933
Epoch 4.14: Loss = 0.368652
Epoch 4.15: Loss = 0.441238
Epoch 4.16: Loss = 0.429214
Epoch 4.17: Loss = 0.494141
Epoch 4.18: Loss = 0.605362
Epoch 4.19: Loss = 0.47496
Epoch 4.20: Loss = 0.388504
Epoch 4.21: Loss = 0.416718
Epoch 4.22: Loss = 0.326569
Epoch 4.23: Loss = 0.358414
Epoch 4.24: Loss = 0.591278
Epoch 4.25: Loss = 0.45726
Epoch 4.26: Loss = 0.493073
Epoch 4.27: Loss = 0.481461
Epoch 4.28: Loss = 0.517166
Epoch 4.29: Loss = 0.59877
Epoch 4.30: Loss = 0.573013
Epoch 4.31: Loss = 0.408096
Epoch 4.32: Loss = 0.510086
Epoch 4.33: Loss = 0.408066
Epoch 4.34: Loss = 0.532761
Epoch 4.35: Loss = 0.449875
Epoch 4.36: Loss = 0.563263
Epoch 4.37: Loss = 0.326675
Epoch 4.38: Loss = 0.389313
Epoch 4.39: Loss = 0.397369
Epoch 4.40: Loss = 0.393005
Epoch 4.41: Loss = 0.438919
Epoch 4.42: Loss = 0.590332
Epoch 4.43: Loss = 0.370178
Epoch 4.44: Loss = 0.316223
Epoch 4.45: Loss = 0.413055
Epoch 4.46: Loss = 0.509018
Epoch 4.47: Loss = 0.432846
Epoch 4.48: Loss = 0.457947
Epoch 4.49: Loss = 0.417328
Epoch 4.50: Loss = 0.547653
Epoch 4.51: Loss = 0.374985
Epoch 4.52: Loss = 0.352173
Epoch 4.53: Loss = 0.418289
Epoch 4.54: Loss = 0.546341
Epoch 4.55: Loss = 0.384903
Epoch 4.56: Loss = 0.413483
Epoch 4.57: Loss = 0.404007
Epoch 4.58: Loss = 0.455551
Epoch 4.59: Loss = 0.475266
Epoch 4.60: Loss = 0.546585
Epoch 4.61: Loss = 0.489365
Epoch 4.62: Loss = 0.545517
Epoch 4.63: Loss = 0.626465
Epoch 4.64: Loss = 0.547638
Epoch 4.65: Loss = 0.663193
Epoch 4.66: Loss = 0.429047
Epoch 4.67: Loss = 0.465012
Epoch 4.68: Loss = 0.265442
Epoch 4.69: Loss = 0.372864
Epoch 4.70: Loss = 0.564728
Epoch 4.71: Loss = 0.404221
Epoch 4.72: Loss = 0.363007
Epoch 4.73: Loss = 0.473251
Epoch 4.74: Loss = 0.324265
Epoch 4.75: Loss = 0.640259
Epoch 4.76: Loss = 0.472458
Epoch 4.77: Loss = 0.364563
Epoch 4.78: Loss = 0.44368
Epoch 4.79: Loss = 0.535507
Epoch 4.80: Loss = 0.487885
Epoch 4.81: Loss = 0.3461
Epoch 4.82: Loss = 0.326172
Epoch 4.83: Loss = 0.494553
Epoch 4.84: Loss = 0.440964
Epoch 4.85: Loss = 0.624817
Epoch 4.86: Loss = 0.5327
Epoch 4.87: Loss = 0.37886
Epoch 4.88: Loss = 0.429443
Epoch 4.89: Loss = 0.508377
Epoch 4.90: Loss = 0.343353
Epoch 4.91: Loss = 0.526901
Epoch 4.92: Loss = 0.525787
Epoch 4.93: Loss = 0.55011
Epoch 4.94: Loss = 0.347931
Epoch 4.95: Loss = 0.471619
Epoch 4.96: Loss = 0.499283
Epoch 4.97: Loss = 0.366867
Epoch 4.98: Loss = 0.402695
Epoch 4.99: Loss = 0.529312
Epoch 4.100: Loss = 0.654602
Epoch 4.101: Loss = 0.611038
Epoch 4.102: Loss = 0.412918
Epoch 4.103: Loss = 0.414215
Epoch 4.104: Loss = 0.362366
Epoch 4.105: Loss = 0.562469
Epoch 4.106: Loss = 0.554657
Epoch 4.107: Loss = 0.364319
Epoch 4.108: Loss = 0.492676
Epoch 4.109: Loss = 0.3853
Epoch 4.110: Loss = 0.425964
Epoch 4.111: Loss = 0.335938
Epoch 4.112: Loss = 0.302521
Epoch 4.113: Loss = 0.410217
Epoch 4.114: Loss = 0.325836
Epoch 4.115: Loss = 0.354736
Epoch 4.116: Loss = 0.389465
Epoch 4.117: Loss = 0.25061
Epoch 4.118: Loss = 0.217178
Epoch 4.119: Loss = 0.340302
Epoch 4.120: Loss = 0.320709
TRAIN LOSS = 0.442398
TRAIN ACC = 87.3169 % (52392/60000)
Loss = 0.407883
Loss = 0.471375
Loss = 0.660706
Loss = 0.598206
Loss = 0.616013
Loss = 0.469788
Loss = 0.40094
Loss = 0.724564
Loss = 0.57637
Loss = 0.563644
Loss = 0.166534
Loss = 0.387726
Loss = 0.3302
Loss = 0.423584
Loss = 0.216171
Loss = 0.310989
Loss = 0.269958
Loss = 0.0692291
Loss = 0.268341
Loss = 0.592194
TEST LOSS = 0.426221
TEST ACC = 523.92 % (8779/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.436722
Epoch 5.2: Loss = 0.50705
Epoch 5.3: Loss = 0.483917
Epoch 5.4: Loss = 0.33252
Epoch 5.5: Loss = 0.348328
Epoch 5.6: Loss = 0.380478
Epoch 5.7: Loss = 0.397079
Epoch 5.8: Loss = 0.422806
Epoch 5.9: Loss = 0.400528
Epoch 5.10: Loss = 0.413788
Epoch 5.11: Loss = 0.438522
Epoch 5.12: Loss = 0.41864
Epoch 5.13: Loss = 0.33902
Epoch 5.14: Loss = 0.36441
Epoch 5.15: Loss = 0.447769
Epoch 5.16: Loss = 0.437134
Epoch 5.17: Loss = 0.522873
Epoch 5.18: Loss = 0.643356
Epoch 5.19: Loss = 0.49501
Epoch 5.20: Loss = 0.396179
Epoch 5.21: Loss = 0.419571
Epoch 5.22: Loss = 0.303558
Epoch 5.23: Loss = 0.379471
Epoch 5.24: Loss = 0.639023
Epoch 5.25: Loss = 0.466827
Epoch 5.26: Loss = 0.508102
Epoch 5.27: Loss = 0.490479
Epoch 5.28: Loss = 0.511246
Epoch 5.29: Loss = 0.586136
Epoch 5.30: Loss = 0.592056
Epoch 5.31: Loss = 0.401901
Epoch 5.32: Loss = 0.500137
Epoch 5.33: Loss = 0.397263
Epoch 5.34: Loss = 0.528458
Epoch 5.35: Loss = 0.452911
Epoch 5.36: Loss = 0.560013
Epoch 5.37: Loss = 0.33078
Epoch 5.38: Loss = 0.399597
Epoch 5.39: Loss = 0.396484
Epoch 5.40: Loss = 0.404861
Epoch 5.41: Loss = 0.431992
Epoch 5.42: Loss = 0.641068
Epoch 5.43: Loss = 0.41774
Epoch 5.44: Loss = 0.328461
Epoch 5.45: Loss = 0.415253
Epoch 5.46: Loss = 0.515976
Epoch 5.47: Loss = 0.448257
Epoch 5.48: Loss = 0.47377
Epoch 5.49: Loss = 0.428192
Epoch 5.50: Loss = 0.563965
Epoch 5.51: Loss = 0.36763
Epoch 5.52: Loss = 0.362854
Epoch 5.53: Loss = 0.426666
Epoch 5.54: Loss = 0.537155
Epoch 5.55: Loss = 0.397263
Epoch 5.56: Loss = 0.440842
Epoch 5.57: Loss = 0.405228
Epoch 5.58: Loss = 0.452179
Epoch 5.59: Loss = 0.498917
Epoch 5.60: Loss = 0.584366
Epoch 5.61: Loss = 0.488205
Epoch 5.62: Loss = 0.54454
Epoch 5.63: Loss = 0.617142
Epoch 5.64: Loss = 0.55011
Epoch 5.65: Loss = 0.675049
Epoch 5.66: Loss = 0.462509
Epoch 5.67: Loss = 0.461716
Epoch 5.68: Loss = 0.242584
Epoch 5.69: Loss = 0.380341
Epoch 5.70: Loss = 0.550217
Epoch 5.71: Loss = 0.395996
Epoch 5.72: Loss = 0.348267
Epoch 5.73: Loss = 0.484741
Epoch 5.74: Loss = 0.293091
Epoch 5.75: Loss = 0.666428
Epoch 5.76: Loss = 0.451416
Epoch 5.77: Loss = 0.37001
Epoch 5.78: Loss = 0.418549
Epoch 5.79: Loss = 0.544998
Epoch 5.80: Loss = 0.492142
Epoch 5.81: Loss = 0.345444
Epoch 5.82: Loss = 0.315338
Epoch 5.83: Loss = 0.520096
Epoch 5.84: Loss = 0.446564
Epoch 5.85: Loss = 0.61673
Epoch 5.86: Loss = 0.549072
Epoch 5.87: Loss = 0.363998
Epoch 5.88: Loss = 0.426468
Epoch 5.89: Loss = 0.485916
Epoch 5.90: Loss = 0.330566
Epoch 5.91: Loss = 0.520187
Epoch 5.92: Loss = 0.504166
Epoch 5.93: Loss = 0.543732
Epoch 5.94: Loss = 0.33786
Epoch 5.95: Loss = 0.461151
Epoch 5.96: Loss = 0.523712
Epoch 5.97: Loss = 0.350418
Epoch 5.98: Loss = 0.392761
Epoch 5.99: Loss = 0.529114
Epoch 5.100: Loss = 0.684402
Epoch 5.101: Loss = 0.630768
Epoch 5.102: Loss = 0.399536
Epoch 5.103: Loss = 0.423508
Epoch 5.104: Loss = 0.371048
Epoch 5.105: Loss = 0.559128
Epoch 5.106: Loss = 0.562958
Epoch 5.107: Loss = 0.347351
Epoch 5.108: Loss = 0.494858
Epoch 5.109: Loss = 0.397385
Epoch 5.110: Loss = 0.425049
Epoch 5.111: Loss = 0.353485
Epoch 5.112: Loss = 0.330307
Epoch 5.113: Loss = 0.417389
Epoch 5.114: Loss = 0.337875
Epoch 5.115: Loss = 0.335236
Epoch 5.116: Loss = 0.361328
Epoch 5.117: Loss = 0.254227
Epoch 5.118: Loss = 0.224335
Epoch 5.119: Loss = 0.328522
Epoch 5.120: Loss = 0.342987
TRAIN LOSS = 0.446793
TRAIN ACC = 87.7747 % (52667/60000)
Loss = 0.399414
Loss = 0.473785
Loss = 0.644608
Loss = 0.590668
Loss = 0.56955
Loss = 0.479614
Loss = 0.371216
Loss = 0.70842
Loss = 0.583389
Loss = 0.555847
Loss = 0.168762
Loss = 0.379364
Loss = 0.385681
Loss = 0.408539
Loss = 0.201981
Loss = 0.331726
Loss = 0.271027
Loss = 0.0592804
Loss = 0.287338
Loss = 0.622543
TEST LOSS = 0.424637
TEST ACC = 526.669 % (8851/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.438232
Epoch 6.2: Loss = 0.551361
Epoch 6.3: Loss = 0.5009
Epoch 6.4: Loss = 0.344894
Epoch 6.5: Loss = 0.351563
Epoch 6.6: Loss = 0.374664
Epoch 6.7: Loss = 0.408752
Epoch 6.8: Loss = 0.41861
Epoch 6.9: Loss = 0.407425
Epoch 6.10: Loss = 0.405991
Epoch 6.11: Loss = 0.452942
Epoch 6.12: Loss = 0.401932
Epoch 6.13: Loss = 0.329498
Epoch 6.14: Loss = 0.342911
Epoch 6.15: Loss = 0.43045
Epoch 6.16: Loss = 0.442825
Epoch 6.17: Loss = 0.571152
Epoch 6.18: Loss = 0.642319
Epoch 6.19: Loss = 0.525513
Epoch 6.20: Loss = 0.361832
Epoch 6.21: Loss = 0.410339
Epoch 6.22: Loss = 0.323166
Epoch 6.23: Loss = 0.389801
Epoch 6.24: Loss = 0.622665
Epoch 6.25: Loss = 0.4198
Epoch 6.26: Loss = 0.538315
Epoch 6.27: Loss = 0.483292
Epoch 6.28: Loss = 0.506439
Epoch 6.29: Loss = 0.594742
Epoch 6.30: Loss = 0.609406
Epoch 6.31: Loss = 0.402267
Epoch 6.32: Loss = 0.507187
Epoch 6.33: Loss = 0.379532
Epoch 6.34: Loss = 0.522095
Epoch 6.35: Loss = 0.439346
Epoch 6.36: Loss = 0.539017
Epoch 6.37: Loss = 0.314972
Epoch 6.38: Loss = 0.401337
Epoch 6.39: Loss = 0.391083
Epoch 6.40: Loss = 0.393753
Epoch 6.41: Loss = 0.389694
Epoch 6.42: Loss = 0.665466
Epoch 6.43: Loss = 0.387909
Epoch 6.44: Loss = 0.320145
Epoch 6.45: Loss = 0.419327
Epoch 6.46: Loss = 0.489059
Epoch 6.47: Loss = 0.43924
Epoch 6.48: Loss = 0.46199
Epoch 6.49: Loss = 0.422592
Epoch 6.50: Loss = 0.569
Epoch 6.51: Loss = 0.372879
Epoch 6.52: Loss = 0.370056
Epoch 6.53: Loss = 0.402283
Epoch 6.54: Loss = 0.557449
Epoch 6.55: Loss = 0.408936
Epoch 6.56: Loss = 0.435501
Epoch 6.57: Loss = 0.431671
Epoch 6.58: Loss = 0.484589
Epoch 6.59: Loss = 0.46553
Epoch 6.60: Loss = 0.592224
Epoch 6.61: Loss = 0.471558
Epoch 6.62: Loss = 0.544083
Epoch 6.63: Loss = 0.686676
Epoch 6.64: Loss = 0.555023
Epoch 6.65: Loss = 0.669754
Epoch 6.66: Loss = 0.451141
Epoch 6.67: Loss = 0.459793
Epoch 6.68: Loss = 0.249878
Epoch 6.69: Loss = 0.397354
Epoch 6.70: Loss = 0.54808
Epoch 6.71: Loss = 0.396683
Epoch 6.72: Loss = 0.357315
Epoch 6.73: Loss = 0.477997
Epoch 6.74: Loss = 0.293228
Epoch 6.75: Loss = 0.676666
Epoch 6.76: Loss = 0.450821
Epoch 6.77: Loss = 0.343521
Epoch 6.78: Loss = 0.422562
Epoch 6.79: Loss = 0.521103
Epoch 6.80: Loss = 0.542984
Epoch 6.81: Loss = 0.339157
Epoch 6.82: Loss = 0.315659
Epoch 6.83: Loss = 0.525665
Epoch 6.84: Loss = 0.455917
Epoch 6.85: Loss = 0.646103
Epoch 6.86: Loss = 0.532578
Epoch 6.87: Loss = 0.370651
Epoch 6.88: Loss = 0.454956
Epoch 6.89: Loss = 0.498245
Epoch 6.90: Loss = 0.357544
Epoch 6.91: Loss = 0.517288
Epoch 6.92: Loss = 0.504593
Epoch 6.93: Loss = 0.545502
Epoch 6.94: Loss = 0.349045
Epoch 6.95: Loss = 0.455017
Epoch 6.96: Loss = 0.548264
Epoch 6.97: Loss = 0.361877
Epoch 6.98: Loss = 0.383286
Epoch 6.99: Loss = 0.504623
Epoch 6.100: Loss = 0.700989
Epoch 6.101: Loss = 0.639999
Epoch 6.102: Loss = 0.361389
Epoch 6.103: Loss = 0.437943
Epoch 6.104: Loss = 0.37854
Epoch 6.105: Loss = 0.586685
Epoch 6.106: Loss = 0.572449
Epoch 6.107: Loss = 0.347504
Epoch 6.108: Loss = 0.521927
Epoch 6.109: Loss = 0.383484
Epoch 6.110: Loss = 0.449921
Epoch 6.111: Loss = 0.373795
Epoch 6.112: Loss = 0.339783
Epoch 6.113: Loss = 0.40329
Epoch 6.114: Loss = 0.345749
Epoch 6.115: Loss = 0.331253
Epoch 6.116: Loss = 0.39093
Epoch 6.117: Loss = 0.233719
Epoch 6.118: Loss = 0.215546
Epoch 6.119: Loss = 0.335327
Epoch 6.120: Loss = 0.34938
TRAIN LOSS = 0.448822
TRAIN ACC = 88.2767 % (52969/60000)
Loss = 0.37825
Loss = 0.485962
Loss = 0.667984
Loss = 0.602936
Loss = 0.633789
Loss = 0.451385
Loss = 0.374252
Loss = 0.748901
Loss = 0.598236
Loss = 0.59137
Loss = 0.165604
Loss = 0.400314
Loss = 0.435822
Loss = 0.444519
Loss = 0.182739
Loss = 0.333847
Loss = 0.298752
Loss = 0.0760193
Loss = 0.292419
Loss = 0.644363
TEST LOSS = 0.440373
TEST ACC = 529.689 % (8827/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.441605
Epoch 7.2: Loss = 0.545792
Epoch 7.3: Loss = 0.522141
Epoch 7.4: Loss = 0.348389
Epoch 7.5: Loss = 0.354935
Epoch 7.6: Loss = 0.415695
Epoch 7.7: Loss = 0.415863
Epoch 7.8: Loss = 0.414398
Epoch 7.9: Loss = 0.414642
Epoch 7.10: Loss = 0.391708
Epoch 7.11: Loss = 0.48497
Epoch 7.12: Loss = 0.43573
Epoch 7.13: Loss = 0.346771
Epoch 7.14: Loss = 0.355438
Epoch 7.15: Loss = 0.428955
Epoch 7.16: Loss = 0.447479
Epoch 7.17: Loss = 0.566498
Epoch 7.18: Loss = 0.710785
Epoch 7.19: Loss = 0.544418
Epoch 7.20: Loss = 0.424301
Epoch 7.21: Loss = 0.424255
Epoch 7.22: Loss = 0.314865
Epoch 7.23: Loss = 0.37233
Epoch 7.24: Loss = 0.666901
Epoch 7.25: Loss = 0.488022
Epoch 7.26: Loss = 0.553818
Epoch 7.27: Loss = 0.485016
Epoch 7.28: Loss = 0.541687
Epoch 7.29: Loss = 0.608215
Epoch 7.30: Loss = 0.607712
Epoch 7.31: Loss = 0.430771
Epoch 7.32: Loss = 0.527237
Epoch 7.33: Loss = 0.408844
Epoch 7.34: Loss = 0.563629
Epoch 7.35: Loss = 0.461395
Epoch 7.36: Loss = 0.567947
Epoch 7.37: Loss = 0.327744
Epoch 7.38: Loss = 0.429855
Epoch 7.39: Loss = 0.381332
Epoch 7.40: Loss = 0.412506
Epoch 7.41: Loss = 0.400757
Epoch 7.42: Loss = 0.700745
Epoch 7.43: Loss = 0.42041
Epoch 7.44: Loss = 0.350189
Epoch 7.45: Loss = 0.482895
Epoch 7.46: Loss = 0.521042
Epoch 7.47: Loss = 0.485947
Epoch 7.48: Loss = 0.475723
Epoch 7.49: Loss = 0.473129
Epoch 7.50: Loss = 0.637863
Epoch 7.51: Loss = 0.3573
Epoch 7.52: Loss = 0.391327
Epoch 7.53: Loss = 0.444794
Epoch 7.54: Loss = 0.595581
Epoch 7.55: Loss = 0.410126
Epoch 7.56: Loss = 0.458038
Epoch 7.57: Loss = 0.42421
Epoch 7.58: Loss = 0.494888
Epoch 7.59: Loss = 0.48735
Epoch 7.60: Loss = 0.628738
Epoch 7.61: Loss = 0.51239
Epoch 7.62: Loss = 0.571594
Epoch 7.63: Loss = 0.745224
Epoch 7.64: Loss = 0.615448
Epoch 7.65: Loss = 0.701477
Epoch 7.66: Loss = 0.466522
Epoch 7.67: Loss = 0.449966
Epoch 7.68: Loss = 0.24733
Epoch 7.69: Loss = 0.396225
Epoch 7.70: Loss = 0.536377
Epoch 7.71: Loss = 0.415558
Epoch 7.72: Loss = 0.384232
Epoch 7.73: Loss = 0.539337
Epoch 7.74: Loss = 0.331467
Epoch 7.75: Loss = 0.710815
Epoch 7.76: Loss = 0.471207
Epoch 7.77: Loss = 0.347855
Epoch 7.78: Loss = 0.444275
Epoch 7.79: Loss = 0.543106
Epoch 7.80: Loss = 0.540314
Epoch 7.81: Loss = 0.342606
Epoch 7.82: Loss = 0.341156
Epoch 7.83: Loss = 0.520447
Epoch 7.84: Loss = 0.479538
Epoch 7.85: Loss = 0.647293
Epoch 7.86: Loss = 0.538406
Epoch 7.87: Loss = 0.350601
Epoch 7.88: Loss = 0.453339
Epoch 7.89: Loss = 0.506546
Epoch 7.90: Loss = 0.395889
Epoch 7.91: Loss = 0.52536
Epoch 7.92: Loss = 0.522278
Epoch 7.93: Loss = 0.590073
Epoch 7.94: Loss = 0.366089
Epoch 7.95: Loss = 0.495163
Epoch 7.96: Loss = 0.549835
Epoch 7.97: Loss = 0.359344
Epoch 7.98: Loss = 0.385925
Epoch 7.99: Loss = 0.486908
Epoch 7.100: Loss = 0.694855
Epoch 7.101: Loss = 0.686218
Epoch 7.102: Loss = 0.432266
Epoch 7.103: Loss = 0.432266
Epoch 7.104: Loss = 0.400894
Epoch 7.105: Loss = 0.593857
Epoch 7.106: Loss = 0.640167
Epoch 7.107: Loss = 0.338608
Epoch 7.108: Loss = 0.522934
Epoch 7.109: Loss = 0.435059
Epoch 7.110: Loss = 0.471497
Epoch 7.111: Loss = 0.389786
Epoch 7.112: Loss = 0.349075
Epoch 7.113: Loss = 0.406799
Epoch 7.114: Loss = 0.304794
Epoch 7.115: Loss = 0.316284
Epoch 7.116: Loss = 0.402969
Epoch 7.117: Loss = 0.263855
Epoch 7.118: Loss = 0.222549
Epoch 7.119: Loss = 0.318451
Epoch 7.120: Loss = 0.377319
TRAIN LOSS = 0.467102
TRAIN ACC = 88.1943 % (52919/60000)
Loss = 0.399231
Loss = 0.512711
Loss = 0.675659
Loss = 0.616531
Loss = 0.662247
Loss = 0.517303
Loss = 0.433899
Loss = 0.765869
Loss = 0.587723
Loss = 0.588394
Loss = 0.161285
Loss = 0.44397
Loss = 0.416046
Loss = 0.438126
Loss = 0.175217
Loss = 0.323105
Loss = 0.31044
Loss = 0.068573
Loss = 0.329391
Loss = 0.662552
TEST LOSS = 0.454413
TEST ACC = 529.189 % (8853/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.416504
Epoch 8.2: Loss = 0.561478
Epoch 8.3: Loss = 0.53508
Epoch 8.4: Loss = 0.359711
Epoch 8.5: Loss = 0.356491
Epoch 8.6: Loss = 0.412781
Epoch 8.7: Loss = 0.380569
Epoch 8.8: Loss = 0.398743
Epoch 8.9: Loss = 0.42662
Epoch 8.10: Loss = 0.398499
Epoch 8.11: Loss = 0.490952
Epoch 8.12: Loss = 0.409943
Epoch 8.13: Loss = 0.32901
Epoch 8.14: Loss = 0.340561
Epoch 8.15: Loss = 0.470551
Epoch 8.16: Loss = 0.45192
Epoch 8.17: Loss = 0.574936
Epoch 8.18: Loss = 0.762054
Epoch 8.19: Loss = 0.548325
Epoch 8.20: Loss = 0.422562
Epoch 8.21: Loss = 0.440002
Epoch 8.22: Loss = 0.315643
Epoch 8.23: Loss = 0.369019
Epoch 8.24: Loss = 0.68898
Epoch 8.25: Loss = 0.482483
Epoch 8.26: Loss = 0.576202
Epoch 8.27: Loss = 0.508987
Epoch 8.28: Loss = 0.601532
Epoch 8.29: Loss = 0.603577
Epoch 8.30: Loss = 0.611664
Epoch 8.31: Loss = 0.463745
Epoch 8.32: Loss = 0.536789
Epoch 8.33: Loss = 0.399597
Epoch 8.34: Loss = 0.509369
Epoch 8.35: Loss = 0.497543
Epoch 8.36: Loss = 0.553238
Epoch 8.37: Loss = 0.332413
Epoch 8.38: Loss = 0.42099
Epoch 8.39: Loss = 0.374985
Epoch 8.40: Loss = 0.393127
Epoch 8.41: Loss = 0.405792
Epoch 8.42: Loss = 0.698914
Epoch 8.43: Loss = 0.408951
Epoch 8.44: Loss = 0.361237
Epoch 8.45: Loss = 0.478607
Epoch 8.46: Loss = 0.522415
Epoch 8.47: Loss = 0.473831
Epoch 8.48: Loss = 0.466537
Epoch 8.49: Loss = 0.442047
Epoch 8.50: Loss = 0.613068
Epoch 8.51: Loss = 0.368179
Epoch 8.52: Loss = 0.373138
Epoch 8.53: Loss = 0.465012
Epoch 8.54: Loss = 0.613754
Epoch 8.55: Loss = 0.42186
Epoch 8.56: Loss = 0.436966
Epoch 8.57: Loss = 0.431229
Epoch 8.58: Loss = 0.47905
Epoch 8.59: Loss = 0.502625
Epoch 8.60: Loss = 0.643723
Epoch 8.61: Loss = 0.556213
Epoch 8.62: Loss = 0.587814
Epoch 8.63: Loss = 0.757324
Epoch 8.64: Loss = 0.626114
Epoch 8.65: Loss = 0.673172
Epoch 8.66: Loss = 0.496368
Epoch 8.67: Loss = 0.385422
Epoch 8.68: Loss = 0.244202
Epoch 8.69: Loss = 0.444275
Epoch 8.70: Loss = 0.520264
Epoch 8.71: Loss = 0.403152
Epoch 8.72: Loss = 0.3871
Epoch 8.73: Loss = 0.552139
Epoch 8.74: Loss = 0.334488
Epoch 8.75: Loss = 0.743576
Epoch 8.76: Loss = 0.433182
Epoch 8.77: Loss = 0.349579
Epoch 8.78: Loss = 0.467331
Epoch 8.79: Loss = 0.553864
Epoch 8.80: Loss = 0.545914
Epoch 8.81: Loss = 0.334747
Epoch 8.82: Loss = 0.341629
Epoch 8.83: Loss = 0.539139
Epoch 8.84: Loss = 0.443558
Epoch 8.85: Loss = 0.624451
Epoch 8.86: Loss = 0.567108
Epoch 8.87: Loss = 0.351074
Epoch 8.88: Loss = 0.457779
Epoch 8.89: Loss = 0.472321
Epoch 8.90: Loss = 0.385376
Epoch 8.91: Loss = 0.545975
Epoch 8.92: Loss = 0.592606
Epoch 8.93: Loss = 0.574081
Epoch 8.94: Loss = 0.360245
Epoch 8.95: Loss = 0.465988
Epoch 8.96: Loss = 0.524475
Epoch 8.97: Loss = 0.322021
Epoch 8.98: Loss = 0.403336
Epoch 8.99: Loss = 0.501511
Epoch 8.100: Loss = 0.682068
Epoch 8.101: Loss = 0.705612
Epoch 8.102: Loss = 0.422989
Epoch 8.103: Loss = 0.405411
Epoch 8.104: Loss = 0.399796
Epoch 8.105: Loss = 0.580994
Epoch 8.106: Loss = 0.66275
Epoch 8.107: Loss = 0.330627
Epoch 8.108: Loss = 0.509781
Epoch 8.109: Loss = 0.434982
Epoch 8.110: Loss = 0.460678
Epoch 8.111: Loss = 0.40123
Epoch 8.112: Loss = 0.336029
Epoch 8.113: Loss = 0.408722
Epoch 8.114: Loss = 0.315887
Epoch 8.115: Loss = 0.366302
Epoch 8.116: Loss = 0.463974
Epoch 8.117: Loss = 0.259811
Epoch 8.118: Loss = 0.252258
Epoch 8.119: Loss = 0.347092
Epoch 8.120: Loss = 0.361099
TRAIN LOSS = 0.469269
TRAIN ACC = 88.6246 % (53177/60000)
Loss = 0.383804
Loss = 0.515717
Loss = 0.666092
Loss = 0.640945
Loss = 0.668549
Loss = 0.513489
Loss = 0.457825
Loss = 0.74884
Loss = 0.595718
Loss = 0.568985
Loss = 0.173416
Loss = 0.464783
Loss = 0.419083
Loss = 0.421829
Loss = 0.180328
Loss = 0.336853
Loss = 0.300507
Loss = 0.0448303
Loss = 0.328812
Loss = 0.698685
TEST LOSS = 0.456454
TEST ACC = 531.769 % (8888/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.431122
Epoch 9.2: Loss = 0.559937
Epoch 9.3: Loss = 0.517853
Epoch 9.4: Loss = 0.35437
Epoch 9.5: Loss = 0.3638
Epoch 9.6: Loss = 0.425873
Epoch 9.7: Loss = 0.352585
Epoch 9.8: Loss = 0.411407
Epoch 9.9: Loss = 0.430084
Epoch 9.10: Loss = 0.404999
Epoch 9.11: Loss = 0.479034
Epoch 9.12: Loss = 0.423492
Epoch 9.13: Loss = 0.337189
Epoch 9.14: Loss = 0.309814
Epoch 9.15: Loss = 0.478928
Epoch 9.16: Loss = 0.439194
Epoch 9.17: Loss = 0.538986
Epoch 9.18: Loss = 0.672226
Epoch 9.19: Loss = 0.580902
Epoch 9.20: Loss = 0.382919
Epoch 9.21: Loss = 0.441147
Epoch 9.22: Loss = 0.310226
Epoch 9.23: Loss = 0.379089
Epoch 9.24: Loss = 0.671646
Epoch 9.25: Loss = 0.518402
Epoch 9.26: Loss = 0.567062
Epoch 9.27: Loss = 0.530334
Epoch 9.28: Loss = 0.559906
Epoch 9.29: Loss = 0.594833
Epoch 9.30: Loss = 0.580475
Epoch 9.31: Loss = 0.447342
Epoch 9.32: Loss = 0.541687
Epoch 9.33: Loss = 0.41684
Epoch 9.34: Loss = 0.481567
Epoch 9.35: Loss = 0.486145
Epoch 9.36: Loss = 0.576294
Epoch 9.37: Loss = 0.281647
Epoch 9.38: Loss = 0.450516
Epoch 9.39: Loss = 0.389862
Epoch 9.40: Loss = 0.433014
Epoch 9.41: Loss = 0.387115
Epoch 9.42: Loss = 0.72229
Epoch 9.43: Loss = 0.39476
Epoch 9.44: Loss = 0.341019
Epoch 9.45: Loss = 0.458786
Epoch 9.46: Loss = 0.534958
Epoch 9.47: Loss = 0.492172
Epoch 9.48: Loss = 0.463425
Epoch 9.49: Loss = 0.419189
Epoch 9.50: Loss = 0.62822
Epoch 9.51: Loss = 0.355377
Epoch 9.52: Loss = 0.381256
Epoch 9.53: Loss = 0.480286
Epoch 9.54: Loss = 0.652817
Epoch 9.55: Loss = 0.438629
Epoch 9.56: Loss = 0.460373
Epoch 9.57: Loss = 0.431213
Epoch 9.58: Loss = 0.488174
Epoch 9.59: Loss = 0.513245
Epoch 9.60: Loss = 0.64769
Epoch 9.61: Loss = 0.538208
Epoch 9.62: Loss = 0.579849
Epoch 9.63: Loss = 0.72226
Epoch 9.64: Loss = 0.601364
Epoch 9.65: Loss = 0.685654
Epoch 9.66: Loss = 0.483047
Epoch 9.67: Loss = 0.410172
Epoch 9.68: Loss = 0.274963
Epoch 9.69: Loss = 0.442413
Epoch 9.70: Loss = 0.531082
Epoch 9.71: Loss = 0.422531
Epoch 9.72: Loss = 0.361267
Epoch 9.73: Loss = 0.525238
Epoch 9.74: Loss = 0.356918
Epoch 9.75: Loss = 0.779205
Epoch 9.76: Loss = 0.43898
Epoch 9.77: Loss = 0.340805
Epoch 9.78: Loss = 0.451157
Epoch 9.79: Loss = 0.597382
Epoch 9.80: Loss = 0.496643
Epoch 9.81: Loss = 0.357239
Epoch 9.82: Loss = 0.336197
Epoch 9.83: Loss = 0.536438
Epoch 9.84: Loss = 0.433136
Epoch 9.85: Loss = 0.670944
Epoch 9.86: Loss = 0.564606
Epoch 9.87: Loss = 0.352692
Epoch 9.88: Loss = 0.471283
Epoch 9.89: Loss = 0.452866
Epoch 9.90: Loss = 0.398087
Epoch 9.91: Loss = 0.581604
Epoch 9.92: Loss = 0.540512
Epoch 9.93: Loss = 0.636276
Epoch 9.94: Loss = 0.327698
Epoch 9.95: Loss = 0.480789
Epoch 9.96: Loss = 0.529419
Epoch 9.97: Loss = 0.326385
Epoch 9.98: Loss = 0.396942
Epoch 9.99: Loss = 0.514252
Epoch 9.100: Loss = 0.746567
Epoch 9.101: Loss = 0.688446
Epoch 9.102: Loss = 0.446503
Epoch 9.103: Loss = 0.447128
Epoch 9.104: Loss = 0.38768
Epoch 9.105: Loss = 0.577942
Epoch 9.106: Loss = 0.666168
Epoch 9.107: Loss = 0.335434
Epoch 9.108: Loss = 0.54834
Epoch 9.109: Loss = 0.440872
Epoch 9.110: Loss = 0.492813
Epoch 9.111: Loss = 0.411423
Epoch 9.112: Loss = 0.370529
Epoch 9.113: Loss = 0.456451
Epoch 9.114: Loss = 0.293625
Epoch 9.115: Loss = 0.337219
Epoch 9.116: Loss = 0.459763
Epoch 9.117: Loss = 0.274399
Epoch 9.118: Loss = 0.236191
Epoch 9.119: Loss = 0.33551
Epoch 9.120: Loss = 0.364777
TRAIN LOSS = 0.470901
TRAIN ACC = 88.7527 % (53254/60000)
Loss = 0.393967
Loss = 0.507339
Loss = 0.701645
Loss = 0.673904
Loss = 0.657425
Loss = 0.499344
Loss = 0.448547
Loss = 0.77858
Loss = 0.57663
Loss = 0.580246
Loss = 0.190079
Loss = 0.501007
Loss = 0.443588
Loss = 0.431503
Loss = 0.160995
Loss = 0.349503
Loss = 0.304855
Loss = 0.0476685
Loss = 0.356659
Loss = 0.690628
TEST LOSS = 0.464706
TEST ACC = 532.539 % (8866/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.427933
Epoch 10.2: Loss = 0.569244
Epoch 10.3: Loss = 0.583679
Epoch 10.4: Loss = 0.382538
Epoch 10.5: Loss = 0.375412
Epoch 10.6: Loss = 0.419128
Epoch 10.7: Loss = 0.396774
Epoch 10.8: Loss = 0.422882
Epoch 10.9: Loss = 0.474701
Epoch 10.10: Loss = 0.429581
Epoch 10.11: Loss = 0.479706
Epoch 10.12: Loss = 0.421524
Epoch 10.13: Loss = 0.328705
Epoch 10.14: Loss = 0.314346
Epoch 10.15: Loss = 0.482605
Epoch 10.16: Loss = 0.474365
Epoch 10.17: Loss = 0.560394
Epoch 10.18: Loss = 0.673157
Epoch 10.19: Loss = 0.582214
Epoch 10.20: Loss = 0.412888
Epoch 10.21: Loss = 0.47908
Epoch 10.22: Loss = 0.319733
Epoch 10.23: Loss = 0.396988
Epoch 10.24: Loss = 0.660507
Epoch 10.25: Loss = 0.491226
Epoch 10.26: Loss = 0.58287
Epoch 10.27: Loss = 0.568237
Epoch 10.28: Loss = 0.553375
Epoch 10.29: Loss = 0.613617
Epoch 10.30: Loss = 0.613419
Epoch 10.31: Loss = 0.468201
Epoch 10.32: Loss = 0.556381
Epoch 10.33: Loss = 0.39209
Epoch 10.34: Loss = 0.520691
Epoch 10.35: Loss = 0.488083
Epoch 10.36: Loss = 0.579422
Epoch 10.37: Loss = 0.32637
Epoch 10.38: Loss = 0.450989
Epoch 10.39: Loss = 0.386627
Epoch 10.40: Loss = 0.443954
Epoch 10.41: Loss = 0.385468
Epoch 10.42: Loss = 0.783447
Epoch 10.43: Loss = 0.43187
Epoch 10.44: Loss = 0.369507
Epoch 10.45: Loss = 0.448318
Epoch 10.46: Loss = 0.546173
Epoch 10.47: Loss = 0.513962
Epoch 10.48: Loss = 0.439423
Epoch 10.49: Loss = 0.424576
Epoch 10.50: Loss = 0.66214
Epoch 10.51: Loss = 0.315063
Epoch 10.52: Loss = 0.380661
Epoch 10.53: Loss = 0.493561
Epoch 10.54: Loss = 0.595444
Epoch 10.55: Loss = 0.435272
Epoch 10.56: Loss = 0.436554
Epoch 10.57: Loss = 0.436859
Epoch 10.58: Loss = 0.495499
Epoch 10.59: Loss = 0.547241
Epoch 10.60: Loss = 0.654373
Epoch 10.61: Loss = 0.524673
Epoch 10.62: Loss = 0.606262
Epoch 10.63: Loss = 0.724945
Epoch 10.64: Loss = 0.604324
Epoch 10.65: Loss = 0.670456
Epoch 10.66: Loss = 0.475967
Epoch 10.67: Loss = 0.41098
Epoch 10.68: Loss = 0.259293
Epoch 10.69: Loss = 0.378738
Epoch 10.70: Loss = 0.504288
Epoch 10.71: Loss = 0.402969
Epoch 10.72: Loss = 0.335587
Epoch 10.73: Loss = 0.511902
Epoch 10.74: Loss = 0.354767
Epoch 10.75: Loss = 0.811951
Epoch 10.76: Loss = 0.414063
Epoch 10.77: Loss = 0.355362
Epoch 10.78: Loss = 0.432312
Epoch 10.79: Loss = 0.582291
Epoch 10.80: Loss = 0.523926
Epoch 10.81: Loss = 0.332169
Epoch 10.82: Loss = 0.30983
Epoch 10.83: Loss = 0.529022
Epoch 10.84: Loss = 0.4189
Epoch 10.85: Loss = 0.668121
Epoch 10.86: Loss = 0.539276
Epoch 10.87: Loss = 0.356201
Epoch 10.88: Loss = 0.466629
Epoch 10.89: Loss = 0.467972
Epoch 10.90: Loss = 0.390594
Epoch 10.91: Loss = 0.553314
Epoch 10.92: Loss = 0.598999
Epoch 10.93: Loss = 0.587799
Epoch 10.94: Loss = 0.373764
Epoch 10.95: Loss = 0.494125
Epoch 10.96: Loss = 0.556778
Epoch 10.97: Loss = 0.336197
Epoch 10.98: Loss = 0.381592
Epoch 10.99: Loss = 0.479996
Epoch 10.100: Loss = 0.760498
Epoch 10.101: Loss = 0.672668
Epoch 10.102: Loss = 0.44458
Epoch 10.103: Loss = 0.382294
Epoch 10.104: Loss = 0.363754
Epoch 10.105: Loss = 0.608139
Epoch 10.106: Loss = 0.67131
Epoch 10.107: Loss = 0.334442
Epoch 10.108: Loss = 0.529922
Epoch 10.109: Loss = 0.37883
Epoch 10.110: Loss = 0.479553
Epoch 10.111: Loss = 0.372635
Epoch 10.112: Loss = 0.317596
Epoch 10.113: Loss = 0.449768
Epoch 10.114: Loss = 0.302078
Epoch 10.115: Loss = 0.35733
Epoch 10.116: Loss = 0.436462
Epoch 10.117: Loss = 0.266617
Epoch 10.118: Loss = 0.220703
Epoch 10.119: Loss = 0.331528
Epoch 10.120: Loss = 0.350479
TRAIN LOSS = 0.472137
TRAIN ACC = 88.7558 % (53256/60000)
Loss = 0.3918
Loss = 0.490265
Loss = 0.66011
Loss = 0.624329
Loss = 0.659958
Loss = 0.487473
Loss = 0.409988
Loss = 0.753464
Loss = 0.531113
Loss = 0.542633
Loss = 0.18338
Loss = 0.446869
Loss = 0.468399
Loss = 0.432587
Loss = 0.14119
Loss = 0.371033
Loss = 0.315948
Loss = 0.0561981
Loss = 0.296417
Loss = 0.700378
TEST LOSS = 0.448176
TEST ACC = 532.559 % (8944/10000)
