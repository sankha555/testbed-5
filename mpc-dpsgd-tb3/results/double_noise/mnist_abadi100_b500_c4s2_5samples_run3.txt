Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.35548
Epoch 1.2: Loss = 2.29333
Epoch 1.3: Loss = 2.27026
Epoch 1.4: Loss = 2.25119
Epoch 1.5: Loss = 2.1817
Epoch 1.6: Loss = 2.14651
Epoch 1.7: Loss = 2.14934
Epoch 1.8: Loss = 2.08049
Epoch 1.9: Loss = 2.03165
Epoch 1.10: Loss = 1.98045
Epoch 1.11: Loss = 1.92899
Epoch 1.12: Loss = 1.94005
Epoch 1.13: Loss = 1.82005
Epoch 1.14: Loss = 1.83995
Epoch 1.15: Loss = 1.89429
Epoch 1.16: Loss = 1.7955
Epoch 1.17: Loss = 1.73048
Epoch 1.18: Loss = 1.68707
Epoch 1.19: Loss = 1.67802
Epoch 1.20: Loss = 1.62389
Epoch 1.21: Loss = 1.56854
Epoch 1.22: Loss = 1.54149
Epoch 1.23: Loss = 1.49271
Epoch 1.24: Loss = 1.59604
Epoch 1.25: Loss = 1.49464
Epoch 1.26: Loss = 1.53111
Epoch 1.27: Loss = 1.4679
Epoch 1.28: Loss = 1.4492
Epoch 1.29: Loss = 1.43889
Epoch 1.30: Loss = 1.50259
Epoch 1.31: Loss = 1.36894
Epoch 1.32: Loss = 1.37836
Epoch 1.33: Loss = 1.33551
Epoch 1.34: Loss = 1.37082
Epoch 1.35: Loss = 1.28204
Epoch 1.36: Loss = 1.38423
Epoch 1.37: Loss = 1.25684
Epoch 1.38: Loss = 1.21741
Epoch 1.39: Loss = 1.17801
Epoch 1.40: Loss = 1.10768
Epoch 1.41: Loss = 1.15898
Epoch 1.42: Loss = 1.15829
Epoch 1.43: Loss = 1.0905
Epoch 1.44: Loss = 1.01936
Epoch 1.45: Loss = 1.12802
Epoch 1.46: Loss = 1.06894
Epoch 1.47: Loss = 0.989746
Epoch 1.48: Loss = 1.05843
Epoch 1.49: Loss = 0.998825
Epoch 1.50: Loss = 1.07817
Epoch 1.51: Loss = 0.915726
Epoch 1.52: Loss = 0.937408
Epoch 1.53: Loss = 0.988174
Epoch 1.54: Loss = 0.999786
Epoch 1.55: Loss = 0.984665
Epoch 1.56: Loss = 0.912704
Epoch 1.57: Loss = 0.817276
Epoch 1.58: Loss = 0.899979
Epoch 1.59: Loss = 0.914825
Epoch 1.60: Loss = 1.01263
Epoch 1.61: Loss = 0.939224
Epoch 1.62: Loss = 0.976196
Epoch 1.63: Loss = 0.9888
Epoch 1.64: Loss = 0.974792
Epoch 1.65: Loss = 1.00459
Epoch 1.66: Loss = 0.880951
Epoch 1.67: Loss = 0.84465
Epoch 1.68: Loss = 0.715286
Epoch 1.69: Loss = 0.778
Epoch 1.70: Loss = 0.834122
Epoch 1.71: Loss = 0.783905
Epoch 1.72: Loss = 0.799927
Epoch 1.73: Loss = 0.810501
Epoch 1.74: Loss = 0.665878
Epoch 1.75: Loss = 0.817612
Epoch 1.76: Loss = 0.776489
Epoch 1.77: Loss = 0.740112
Epoch 1.78: Loss = 0.751602
Epoch 1.79: Loss = 0.704376
Epoch 1.80: Loss = 0.815994
Epoch 1.81: Loss = 0.706482
Epoch 1.82: Loss = 0.671936
Epoch 1.83: Loss = 0.83168
Epoch 1.84: Loss = 0.735214
Epoch 1.85: Loss = 0.800827
Epoch 1.86: Loss = 0.734741
Epoch 1.87: Loss = 0.6604
Epoch 1.88: Loss = 0.691086
Epoch 1.89: Loss = 0.77121
Epoch 1.90: Loss = 0.659058
Epoch 1.91: Loss = 0.71904
Epoch 1.92: Loss = 0.719818
Epoch 1.93: Loss = 0.757156
Epoch 1.94: Loss = 0.589432
Epoch 1.95: Loss = 0.692947
Epoch 1.96: Loss = 0.679993
Epoch 1.97: Loss = 0.513763
Epoch 1.98: Loss = 0.623367
Epoch 1.99: Loss = 0.716171
Epoch 1.100: Loss = 0.832458
Epoch 1.101: Loss = 0.738998
Epoch 1.102: Loss = 0.650711
Epoch 1.103: Loss = 0.607391
Epoch 1.104: Loss = 0.593948
Epoch 1.105: Loss = 0.696472
Epoch 1.106: Loss = 0.681854
Epoch 1.107: Loss = 0.572906
Epoch 1.108: Loss = 0.626068
Epoch 1.109: Loss = 0.58699
Epoch 1.110: Loss = 0.594208
Epoch 1.111: Loss = 0.527344
Epoch 1.112: Loss = 0.501633
Epoch 1.113: Loss = 0.561096
Epoch 1.114: Loss = 0.511139
Epoch 1.115: Loss = 0.57663
Epoch 1.116: Loss = 0.575226
Epoch 1.117: Loss = 0.461746
Epoch 1.118: Loss = 0.416336
Epoch 1.119: Loss = 0.414246
Epoch 1.120: Loss = 0.464661
TRAIN LOSS = 1.08205
TRAIN ACC = 69.8624 % (41919/60000)
Loss = 0.607437
Loss = 0.63443
Loss = 0.738983
Loss = 0.696976
Loss = 0.719131
Loss = 0.634033
Loss = 0.60408
Loss = 0.762802
Loss = 0.720505
Loss = 0.653793
Loss = 0.366043
Loss = 0.491516
Loss = 0.336456
Loss = 0.515396
Loss = 0.433884
Loss = 0.443512
Loss = 0.391891
Loss = 0.244339
Loss = 0.417023
Loss = 0.692291
TEST LOSS = 0.555226
TEST ACC = 419.189 % (8391/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.523621
Epoch 2.2: Loss = 0.66626
Epoch 2.3: Loss = 0.626938
Epoch 2.4: Loss = 0.494873
Epoch 2.5: Loss = 0.509857
Epoch 2.6: Loss = 0.50705
Epoch 2.7: Loss = 0.587296
Epoch 2.8: Loss = 0.541016
Epoch 2.9: Loss = 0.535126
Epoch 2.10: Loss = 0.536072
Epoch 2.11: Loss = 0.530991
Epoch 2.12: Loss = 0.528183
Epoch 2.13: Loss = 0.471207
Epoch 2.14: Loss = 0.506561
Epoch 2.15: Loss = 0.654633
Epoch 2.16: Loss = 0.576416
Epoch 2.17: Loss = 0.592117
Epoch 2.18: Loss = 0.632523
Epoch 2.19: Loss = 0.520187
Epoch 2.20: Loss = 0.468277
Epoch 2.21: Loss = 0.447037
Epoch 2.22: Loss = 0.450729
Epoch 2.23: Loss = 0.42308
Epoch 2.24: Loss = 0.684326
Epoch 2.25: Loss = 0.557846
Epoch 2.26: Loss = 0.630157
Epoch 2.27: Loss = 0.591629
Epoch 2.28: Loss = 0.57576
Epoch 2.29: Loss = 0.610641
Epoch 2.30: Loss = 0.706421
Epoch 2.31: Loss = 0.474854
Epoch 2.32: Loss = 0.595932
Epoch 2.33: Loss = 0.503342
Epoch 2.34: Loss = 0.592178
Epoch 2.35: Loss = 0.545395
Epoch 2.36: Loss = 0.630539
Epoch 2.37: Loss = 0.458649
Epoch 2.38: Loss = 0.431396
Epoch 2.39: Loss = 0.525528
Epoch 2.40: Loss = 0.459229
Epoch 2.41: Loss = 0.526367
Epoch 2.42: Loss = 0.579544
Epoch 2.43: Loss = 0.462082
Epoch 2.44: Loss = 0.410141
Epoch 2.45: Loss = 0.515656
Epoch 2.46: Loss = 0.568848
Epoch 2.47: Loss = 0.436279
Epoch 2.48: Loss = 0.54628
Epoch 2.49: Loss = 0.493423
Epoch 2.50: Loss = 0.588409
Epoch 2.51: Loss = 0.451889
Epoch 2.52: Loss = 0.446106
Epoch 2.53: Loss = 0.496719
Epoch 2.54: Loss = 0.559021
Epoch 2.55: Loss = 0.492874
Epoch 2.56: Loss = 0.451904
Epoch 2.57: Loss = 0.411835
Epoch 2.58: Loss = 0.483658
Epoch 2.59: Loss = 0.547287
Epoch 2.60: Loss = 0.615753
Epoch 2.61: Loss = 0.568726
Epoch 2.62: Loss = 0.56102
Epoch 2.63: Loss = 0.636871
Epoch 2.64: Loss = 0.573196
Epoch 2.65: Loss = 0.681137
Epoch 2.66: Loss = 0.506744
Epoch 2.67: Loss = 0.530243
Epoch 2.68: Loss = 0.360809
Epoch 2.69: Loss = 0.430206
Epoch 2.70: Loss = 0.580521
Epoch 2.71: Loss = 0.441544
Epoch 2.72: Loss = 0.480423
Epoch 2.73: Loss = 0.493225
Epoch 2.74: Loss = 0.379364
Epoch 2.75: Loss = 0.596893
Epoch 2.76: Loss = 0.502426
Epoch 2.77: Loss = 0.453323
Epoch 2.78: Loss = 0.486481
Epoch 2.79: Loss = 0.496002
Epoch 2.80: Loss = 0.533966
Epoch 2.81: Loss = 0.447723
Epoch 2.82: Loss = 0.407715
Epoch 2.83: Loss = 0.588516
Epoch 2.84: Loss = 0.487915
Epoch 2.85: Loss = 0.631119
Epoch 2.86: Loss = 0.51413
Epoch 2.87: Loss = 0.399094
Epoch 2.88: Loss = 0.443619
Epoch 2.89: Loss = 0.567184
Epoch 2.90: Loss = 0.41925
Epoch 2.91: Loss = 0.51445
Epoch 2.92: Loss = 0.519958
Epoch 2.93: Loss = 0.564972
Epoch 2.94: Loss = 0.391113
Epoch 2.95: Loss = 0.490204
Epoch 2.96: Loss = 0.531311
Epoch 2.97: Loss = 0.349457
Epoch 2.98: Loss = 0.441391
Epoch 2.99: Loss = 0.545654
Epoch 2.100: Loss = 0.640656
Epoch 2.101: Loss = 0.591003
Epoch 2.102: Loss = 0.446777
Epoch 2.103: Loss = 0.433838
Epoch 2.104: Loss = 0.405075
Epoch 2.105: Loss = 0.564835
Epoch 2.106: Loss = 0.54158
Epoch 2.107: Loss = 0.392441
Epoch 2.108: Loss = 0.493988
Epoch 2.109: Loss = 0.430054
Epoch 2.110: Loss = 0.472931
Epoch 2.111: Loss = 0.371201
Epoch 2.112: Loss = 0.382095
Epoch 2.113: Loss = 0.40004
Epoch 2.114: Loss = 0.361984
Epoch 2.115: Loss = 0.39595
Epoch 2.116: Loss = 0.443634
Epoch 2.117: Loss = 0.283005
Epoch 2.118: Loss = 0.258972
Epoch 2.119: Loss = 0.311874
Epoch 2.120: Loss = 0.34726
TRAIN LOSS = 0.504227
TRAIN ACC = 84.9182 % (50953/60000)
Loss = 0.447998
Loss = 0.509949
Loss = 0.612717
Loss = 0.579193
Loss = 0.602692
Loss = 0.478363
Loss = 0.454483
Loss = 0.643951
Loss = 0.578018
Loss = 0.541992
Loss = 0.247086
Loss = 0.353607
Loss = 0.266632
Loss = 0.417923
Loss = 0.301102
Loss = 0.358337
Loss = 0.291504
Loss = 0.128235
Loss = 0.288712
Loss = 0.568604
TEST LOSS = 0.433555
TEST ACC = 509.529 % (8687/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.415222
Epoch 3.2: Loss = 0.555847
Epoch 3.3: Loss = 0.5186
Epoch 3.4: Loss = 0.377594
Epoch 3.5: Loss = 0.382828
Epoch 3.6: Loss = 0.386185
Epoch 3.7: Loss = 0.444199
Epoch 3.8: Loss = 0.423477
Epoch 3.9: Loss = 0.417694
Epoch 3.10: Loss = 0.444992
Epoch 3.11: Loss = 0.427933
Epoch 3.12: Loss = 0.427612
Epoch 3.13: Loss = 0.366272
Epoch 3.14: Loss = 0.396851
Epoch 3.15: Loss = 0.509811
Epoch 3.16: Loss = 0.484802
Epoch 3.17: Loss = 0.488876
Epoch 3.18: Loss = 0.590912
Epoch 3.19: Loss = 0.431625
Epoch 3.20: Loss = 0.379089
Epoch 3.21: Loss = 0.352921
Epoch 3.22: Loss = 0.34874
Epoch 3.23: Loss = 0.33963
Epoch 3.24: Loss = 0.592468
Epoch 3.25: Loss = 0.47406
Epoch 3.26: Loss = 0.540283
Epoch 3.27: Loss = 0.519592
Epoch 3.28: Loss = 0.489365
Epoch 3.29: Loss = 0.546326
Epoch 3.30: Loss = 0.62204
Epoch 3.31: Loss = 0.397827
Epoch 3.32: Loss = 0.512985
Epoch 3.33: Loss = 0.411179
Epoch 3.34: Loss = 0.505844
Epoch 3.35: Loss = 0.461594
Epoch 3.36: Loss = 0.527084
Epoch 3.37: Loss = 0.349991
Epoch 3.38: Loss = 0.373489
Epoch 3.39: Loss = 0.44397
Epoch 3.40: Loss = 0.386871
Epoch 3.41: Loss = 0.429779
Epoch 3.42: Loss = 0.559586
Epoch 3.43: Loss = 0.381699
Epoch 3.44: Loss = 0.331482
Epoch 3.45: Loss = 0.424377
Epoch 3.46: Loss = 0.506058
Epoch 3.47: Loss = 0.383896
Epoch 3.48: Loss = 0.474854
Epoch 3.49: Loss = 0.434662
Epoch 3.50: Loss = 0.504684
Epoch 3.51: Loss = 0.385223
Epoch 3.52: Loss = 0.365555
Epoch 3.53: Loss = 0.421005
Epoch 3.54: Loss = 0.507523
Epoch 3.55: Loss = 0.425522
Epoch 3.56: Loss = 0.38707
Epoch 3.57: Loss = 0.365128
Epoch 3.58: Loss = 0.428467
Epoch 3.59: Loss = 0.491318
Epoch 3.60: Loss = 0.546783
Epoch 3.61: Loss = 0.493515
Epoch 3.62: Loss = 0.520325
Epoch 3.63: Loss = 0.594818
Epoch 3.64: Loss = 0.522507
Epoch 3.65: Loss = 0.62941
Epoch 3.66: Loss = 0.448471
Epoch 3.67: Loss = 0.459656
Epoch 3.68: Loss = 0.290268
Epoch 3.69: Loss = 0.37616
Epoch 3.70: Loss = 0.525696
Epoch 3.71: Loss = 0.381134
Epoch 3.72: Loss = 0.384094
Epoch 3.73: Loss = 0.435089
Epoch 3.74: Loss = 0.339706
Epoch 3.75: Loss = 0.603058
Epoch 3.76: Loss = 0.460602
Epoch 3.77: Loss = 0.369263
Epoch 3.78: Loss = 0.460129
Epoch 3.79: Loss = 0.475754
Epoch 3.80: Loss = 0.477554
Epoch 3.81: Loss = 0.396439
Epoch 3.82: Loss = 0.344086
Epoch 3.83: Loss = 0.523621
Epoch 3.84: Loss = 0.442841
Epoch 3.85: Loss = 0.572876
Epoch 3.86: Loss = 0.487411
Epoch 3.87: Loss = 0.344162
Epoch 3.88: Loss = 0.416245
Epoch 3.89: Loss = 0.512131
Epoch 3.90: Loss = 0.363083
Epoch 3.91: Loss = 0.477921
Epoch 3.92: Loss = 0.484299
Epoch 3.93: Loss = 0.51767
Epoch 3.94: Loss = 0.351151
Epoch 3.95: Loss = 0.44342
Epoch 3.96: Loss = 0.502502
Epoch 3.97: Loss = 0.319656
Epoch 3.98: Loss = 0.403183
Epoch 3.99: Loss = 0.497528
Epoch 3.100: Loss = 0.603287
Epoch 3.101: Loss = 0.553894
Epoch 3.102: Loss = 0.396576
Epoch 3.103: Loss = 0.383987
Epoch 3.104: Loss = 0.360229
Epoch 3.105: Loss = 0.523071
Epoch 3.106: Loss = 0.514648
Epoch 3.107: Loss = 0.340164
Epoch 3.108: Loss = 0.458878
Epoch 3.109: Loss = 0.387634
Epoch 3.110: Loss = 0.446976
Epoch 3.111: Loss = 0.353638
Epoch 3.112: Loss = 0.367249
Epoch 3.113: Loss = 0.375107
Epoch 3.114: Loss = 0.319366
Epoch 3.115: Loss = 0.341217
Epoch 3.116: Loss = 0.39827
Epoch 3.117: Loss = 0.239502
Epoch 3.118: Loss = 0.227554
Epoch 3.119: Loss = 0.286102
Epoch 3.120: Loss = 0.340546
TRAIN LOSS = 0.439056
TRAIN ACC = 86.7966 % (52081/60000)
Loss = 0.406387
Loss = 0.453735
Loss = 0.568085
Loss = 0.527512
Loss = 0.563156
Loss = 0.446213
Loss = 0.405167
Loss = 0.60968
Loss = 0.522095
Loss = 0.498245
Loss = 0.210236
Loss = 0.327103
Loss = 0.272781
Loss = 0.383667
Loss = 0.258133
Loss = 0.342865
Loss = 0.264343
Loss = 0.100891
Loss = 0.250183
Loss = 0.529465
TEST LOSS = 0.396997
TEST ACC = 520.808 % (8802/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.374069
Epoch 4.2: Loss = 0.519257
Epoch 4.3: Loss = 0.516174
Epoch 4.4: Loss = 0.345062
Epoch 4.5: Loss = 0.352676
Epoch 4.6: Loss = 0.340088
Epoch 4.7: Loss = 0.387985
Epoch 4.8: Loss = 0.369583
Epoch 4.9: Loss = 0.392838
Epoch 4.10: Loss = 0.399017
Epoch 4.11: Loss = 0.399124
Epoch 4.12: Loss = 0.374435
Epoch 4.13: Loss = 0.328766
Epoch 4.14: Loss = 0.362549
Epoch 4.15: Loss = 0.439224
Epoch 4.16: Loss = 0.438812
Epoch 4.17: Loss = 0.447388
Epoch 4.18: Loss = 0.565369
Epoch 4.19: Loss = 0.408859
Epoch 4.20: Loss = 0.344437
Epoch 4.21: Loss = 0.325745
Epoch 4.22: Loss = 0.288422
Epoch 4.23: Loss = 0.296234
Epoch 4.24: Loss = 0.565933
Epoch 4.25: Loss = 0.432251
Epoch 4.26: Loss = 0.489395
Epoch 4.27: Loss = 0.456589
Epoch 4.28: Loss = 0.445358
Epoch 4.29: Loss = 0.527878
Epoch 4.30: Loss = 0.570786
Epoch 4.31: Loss = 0.391708
Epoch 4.32: Loss = 0.476044
Epoch 4.33: Loss = 0.366333
Epoch 4.34: Loss = 0.441086
Epoch 4.35: Loss = 0.426147
Epoch 4.36: Loss = 0.4879
Epoch 4.37: Loss = 0.300568
Epoch 4.38: Loss = 0.362152
Epoch 4.39: Loss = 0.41391
Epoch 4.40: Loss = 0.353958
Epoch 4.41: Loss = 0.383438
Epoch 4.42: Loss = 0.564423
Epoch 4.43: Loss = 0.351822
Epoch 4.44: Loss = 0.299301
Epoch 4.45: Loss = 0.378494
Epoch 4.46: Loss = 0.478119
Epoch 4.47: Loss = 0.374954
Epoch 4.48: Loss = 0.428665
Epoch 4.49: Loss = 0.405441
Epoch 4.50: Loss = 0.478546
Epoch 4.51: Loss = 0.368683
Epoch 4.52: Loss = 0.345215
Epoch 4.53: Loss = 0.39769
Epoch 4.54: Loss = 0.496368
Epoch 4.55: Loss = 0.394653
Epoch 4.56: Loss = 0.385559
Epoch 4.57: Loss = 0.353745
Epoch 4.58: Loss = 0.411163
Epoch 4.59: Loss = 0.462234
Epoch 4.60: Loss = 0.543961
Epoch 4.61: Loss = 0.462311
Epoch 4.62: Loss = 0.482269
Epoch 4.63: Loss = 0.566666
Epoch 4.64: Loss = 0.492203
Epoch 4.65: Loss = 0.599182
Epoch 4.66: Loss = 0.418182
Epoch 4.67: Loss = 0.432632
Epoch 4.68: Loss = 0.266769
Epoch 4.69: Loss = 0.355713
Epoch 4.70: Loss = 0.487915
Epoch 4.71: Loss = 0.358215
Epoch 4.72: Loss = 0.371826
Epoch 4.73: Loss = 0.412552
Epoch 4.74: Loss = 0.312454
Epoch 4.75: Loss = 0.617233
Epoch 4.76: Loss = 0.424026
Epoch 4.77: Loss = 0.340271
Epoch 4.78: Loss = 0.425156
Epoch 4.79: Loss = 0.458054
Epoch 4.80: Loss = 0.474579
Epoch 4.81: Loss = 0.381699
Epoch 4.82: Loss = 0.320236
Epoch 4.83: Loss = 0.483566
Epoch 4.84: Loss = 0.412399
Epoch 4.85: Loss = 0.604904
Epoch 4.86: Loss = 0.478043
Epoch 4.87: Loss = 0.324692
Epoch 4.88: Loss = 0.39241
Epoch 4.89: Loss = 0.494492
Epoch 4.90: Loss = 0.333115
Epoch 4.91: Loss = 0.457916
Epoch 4.92: Loss = 0.465622
Epoch 4.93: Loss = 0.502701
Epoch 4.94: Loss = 0.308182
Epoch 4.95: Loss = 0.426788
Epoch 4.96: Loss = 0.474609
Epoch 4.97: Loss = 0.295105
Epoch 4.98: Loss = 0.378723
Epoch 4.99: Loss = 0.448059
Epoch 4.100: Loss = 0.5392
Epoch 4.101: Loss = 0.550125
Epoch 4.102: Loss = 0.387573
Epoch 4.103: Loss = 0.364624
Epoch 4.104: Loss = 0.342438
Epoch 4.105: Loss = 0.502686
Epoch 4.106: Loss = 0.509094
Epoch 4.107: Loss = 0.324234
Epoch 4.108: Loss = 0.435654
Epoch 4.109: Loss = 0.374283
Epoch 4.110: Loss = 0.430099
Epoch 4.111: Loss = 0.341049
Epoch 4.112: Loss = 0.340195
Epoch 4.113: Loss = 0.358078
Epoch 4.114: Loss = 0.300766
Epoch 4.115: Loss = 0.323517
Epoch 4.116: Loss = 0.386932
Epoch 4.117: Loss = 0.232208
Epoch 4.118: Loss = 0.202454
Epoch 4.119: Loss = 0.242447
Epoch 4.120: Loss = 0.333054
TRAIN LOSS = 0.411575
TRAIN ACC = 87.9807 % (52791/60000)
Loss = 0.391602
Loss = 0.45668
Loss = 0.532639
Loss = 0.529556
Loss = 0.545547
Loss = 0.427673
Loss = 0.379288
Loss = 0.610657
Loss = 0.517426
Loss = 0.485428
Loss = 0.19519
Loss = 0.299973
Loss = 0.255905
Loss = 0.3638
Loss = 0.223053
Loss = 0.321121
Loss = 0.225906
Loss = 0.0771637
Loss = 0.254166
Loss = 0.514175
TEST LOSS = 0.380347
TEST ACC = 527.91 % (8877/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.363495
Epoch 5.2: Loss = 0.475235
Epoch 5.3: Loss = 0.488007
Epoch 5.4: Loss = 0.33046
Epoch 5.5: Loss = 0.338577
Epoch 5.6: Loss = 0.338104
Epoch 5.7: Loss = 0.37822
Epoch 5.8: Loss = 0.360458
Epoch 5.9: Loss = 0.371353
Epoch 5.10: Loss = 0.369766
Epoch 5.11: Loss = 0.391739
Epoch 5.12: Loss = 0.375046
Epoch 5.13: Loss = 0.284225
Epoch 5.14: Loss = 0.342514
Epoch 5.15: Loss = 0.442307
Epoch 5.16: Loss = 0.408737
Epoch 5.17: Loss = 0.445068
Epoch 5.18: Loss = 0.589127
Epoch 5.19: Loss = 0.415604
Epoch 5.20: Loss = 0.330093
Epoch 5.21: Loss = 0.317657
Epoch 5.22: Loss = 0.285217
Epoch 5.23: Loss = 0.300674
Epoch 5.24: Loss = 0.547485
Epoch 5.25: Loss = 0.439621
Epoch 5.26: Loss = 0.47876
Epoch 5.27: Loss = 0.462112
Epoch 5.28: Loss = 0.442841
Epoch 5.29: Loss = 0.495834
Epoch 5.30: Loss = 0.555115
Epoch 5.31: Loss = 0.381393
Epoch 5.32: Loss = 0.459763
Epoch 5.33: Loss = 0.343369
Epoch 5.34: Loss = 0.441055
Epoch 5.35: Loss = 0.423538
Epoch 5.36: Loss = 0.460297
Epoch 5.37: Loss = 0.289536
Epoch 5.38: Loss = 0.321579
Epoch 5.39: Loss = 0.381973
Epoch 5.40: Loss = 0.345001
Epoch 5.41: Loss = 0.386215
Epoch 5.42: Loss = 0.568985
Epoch 5.43: Loss = 0.329178
Epoch 5.44: Loss = 0.305054
Epoch 5.45: Loss = 0.358078
Epoch 5.46: Loss = 0.465149
Epoch 5.47: Loss = 0.378204
Epoch 5.48: Loss = 0.422348
Epoch 5.49: Loss = 0.3992
Epoch 5.50: Loss = 0.479782
Epoch 5.51: Loss = 0.345108
Epoch 5.52: Loss = 0.3358
Epoch 5.53: Loss = 0.393463
Epoch 5.54: Loss = 0.477966
Epoch 5.55: Loss = 0.408768
Epoch 5.56: Loss = 0.367111
Epoch 5.57: Loss = 0.335556
Epoch 5.58: Loss = 0.401276
Epoch 5.59: Loss = 0.463959
Epoch 5.60: Loss = 0.536041
Epoch 5.61: Loss = 0.444504
Epoch 5.62: Loss = 0.476395
Epoch 5.63: Loss = 0.564346
Epoch 5.64: Loss = 0.496429
Epoch 5.65: Loss = 0.597595
Epoch 5.66: Loss = 0.420395
Epoch 5.67: Loss = 0.424561
Epoch 5.68: Loss = 0.255753
Epoch 5.69: Loss = 0.35997
Epoch 5.70: Loss = 0.505035
Epoch 5.71: Loss = 0.36438
Epoch 5.72: Loss = 0.35466
Epoch 5.73: Loss = 0.402878
Epoch 5.74: Loss = 0.323532
Epoch 5.75: Loss = 0.65509
Epoch 5.76: Loss = 0.407959
Epoch 5.77: Loss = 0.325745
Epoch 5.78: Loss = 0.435013
Epoch 5.79: Loss = 0.4496
Epoch 5.80: Loss = 0.445923
Epoch 5.81: Loss = 0.373184
Epoch 5.82: Loss = 0.321136
Epoch 5.83: Loss = 0.491562
Epoch 5.84: Loss = 0.390274
Epoch 5.85: Loss = 0.58754
Epoch 5.86: Loss = 0.500824
Epoch 5.87: Loss = 0.301636
Epoch 5.88: Loss = 0.379883
Epoch 5.89: Loss = 0.486923
Epoch 5.90: Loss = 0.316757
Epoch 5.91: Loss = 0.445206
Epoch 5.92: Loss = 0.458115
Epoch 5.93: Loss = 0.503769
Epoch 5.94: Loss = 0.295303
Epoch 5.95: Loss = 0.423904
Epoch 5.96: Loss = 0.467621
Epoch 5.97: Loss = 0.307877
Epoch 5.98: Loss = 0.383148
Epoch 5.99: Loss = 0.45015
Epoch 5.100: Loss = 0.55806
Epoch 5.101: Loss = 0.542725
Epoch 5.102: Loss = 0.357788
Epoch 5.103: Loss = 0.350601
Epoch 5.104: Loss = 0.325821
Epoch 5.105: Loss = 0.509216
Epoch 5.106: Loss = 0.526184
Epoch 5.107: Loss = 0.308105
Epoch 5.108: Loss = 0.418365
Epoch 5.109: Loss = 0.367462
Epoch 5.110: Loss = 0.421677
Epoch 5.111: Loss = 0.324341
Epoch 5.112: Loss = 0.336777
Epoch 5.113: Loss = 0.363434
Epoch 5.114: Loss = 0.296494
Epoch 5.115: Loss = 0.297577
Epoch 5.116: Loss = 0.361679
Epoch 5.117: Loss = 0.230347
Epoch 5.118: Loss = 0.196457
Epoch 5.119: Loss = 0.242462
Epoch 5.120: Loss = 0.330505
TRAIN LOSS = 0.403564
TRAIN ACC = 88.3041 % (52985/60000)
Loss = 0.387177
Loss = 0.451508
Loss = 0.518723
Loss = 0.554718
Loss = 0.55722
Loss = 0.438492
Loss = 0.37616
Loss = 0.637344
Loss = 0.514877
Loss = 0.491714
Loss = 0.180511
Loss = 0.297562
Loss = 0.303787
Loss = 0.370468
Loss = 0.19606
Loss = 0.315521
Loss = 0.220093
Loss = 0.0751495
Loss = 0.236862
Loss = 0.525299
TEST LOSS = 0.382462
TEST ACC = 529.849 % (8904/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.349823
Epoch 6.2: Loss = 0.463013
Epoch 6.3: Loss = 0.480377
Epoch 6.4: Loss = 0.329697
Epoch 6.5: Loss = 0.30896
Epoch 6.6: Loss = 0.33757
Epoch 6.7: Loss = 0.357071
Epoch 6.8: Loss = 0.34819
Epoch 6.9: Loss = 0.364426
Epoch 6.10: Loss = 0.368484
Epoch 6.11: Loss = 0.395401
Epoch 6.12: Loss = 0.370392
Epoch 6.13: Loss = 0.272675
Epoch 6.14: Loss = 0.333481
Epoch 6.15: Loss = 0.421951
Epoch 6.16: Loss = 0.402298
Epoch 6.17: Loss = 0.453384
Epoch 6.18: Loss = 0.596527
Epoch 6.19: Loss = 0.419937
Epoch 6.20: Loss = 0.314163
Epoch 6.21: Loss = 0.331665
Epoch 6.22: Loss = 0.273407
Epoch 6.23: Loss = 0.287292
Epoch 6.24: Loss = 0.547501
Epoch 6.25: Loss = 0.423798
Epoch 6.26: Loss = 0.490875
Epoch 6.27: Loss = 0.461609
Epoch 6.28: Loss = 0.450668
Epoch 6.29: Loss = 0.51178
Epoch 6.30: Loss = 0.54422
Epoch 6.31: Loss = 0.391373
Epoch 6.32: Loss = 0.441589
Epoch 6.33: Loss = 0.342682
Epoch 6.34: Loss = 0.422821
Epoch 6.35: Loss = 0.4151
Epoch 6.36: Loss = 0.456711
Epoch 6.37: Loss = 0.298325
Epoch 6.38: Loss = 0.324554
Epoch 6.39: Loss = 0.388733
Epoch 6.40: Loss = 0.359634
Epoch 6.41: Loss = 0.380981
Epoch 6.42: Loss = 0.572235
Epoch 6.43: Loss = 0.328583
Epoch 6.44: Loss = 0.336075
Epoch 6.45: Loss = 0.362076
Epoch 6.46: Loss = 0.464905
Epoch 6.47: Loss = 0.398224
Epoch 6.48: Loss = 0.412384
Epoch 6.49: Loss = 0.396622
Epoch 6.50: Loss = 0.477737
Epoch 6.51: Loss = 0.356323
Epoch 6.52: Loss = 0.320877
Epoch 6.53: Loss = 0.384018
Epoch 6.54: Loss = 0.473175
Epoch 6.55: Loss = 0.396149
Epoch 6.56: Loss = 0.378189
Epoch 6.57: Loss = 0.348526
Epoch 6.58: Loss = 0.403061
Epoch 6.59: Loss = 0.447083
Epoch 6.60: Loss = 0.500702
Epoch 6.61: Loss = 0.425415
Epoch 6.62: Loss = 0.474777
Epoch 6.63: Loss = 0.559021
Epoch 6.64: Loss = 0.494202
Epoch 6.65: Loss = 0.625916
Epoch 6.66: Loss = 0.415604
Epoch 6.67: Loss = 0.424255
Epoch 6.68: Loss = 0.243362
Epoch 6.69: Loss = 0.350891
Epoch 6.70: Loss = 0.51683
Epoch 6.71: Loss = 0.363724
Epoch 6.72: Loss = 0.335297
Epoch 6.73: Loss = 0.391403
Epoch 6.74: Loss = 0.316406
Epoch 6.75: Loss = 0.664886
Epoch 6.76: Loss = 0.424896
Epoch 6.77: Loss = 0.315491
Epoch 6.78: Loss = 0.421936
Epoch 6.79: Loss = 0.458893
Epoch 6.80: Loss = 0.428024
Epoch 6.81: Loss = 0.344193
Epoch 6.82: Loss = 0.314896
Epoch 6.83: Loss = 0.496078
Epoch 6.84: Loss = 0.39415
Epoch 6.85: Loss = 0.58725
Epoch 6.86: Loss = 0.492981
Epoch 6.87: Loss = 0.291977
Epoch 6.88: Loss = 0.375595
Epoch 6.89: Loss = 0.478333
Epoch 6.90: Loss = 0.329559
Epoch 6.91: Loss = 0.432846
Epoch 6.92: Loss = 0.462036
Epoch 6.93: Loss = 0.522491
Epoch 6.94: Loss = 0.303375
Epoch 6.95: Loss = 0.407852
Epoch 6.96: Loss = 0.464966
Epoch 6.97: Loss = 0.305252
Epoch 6.98: Loss = 0.384109
Epoch 6.99: Loss = 0.472183
Epoch 6.100: Loss = 0.548569
Epoch 6.101: Loss = 0.555313
Epoch 6.102: Loss = 0.397842
Epoch 6.103: Loss = 0.358246
Epoch 6.104: Loss = 0.332748
Epoch 6.105: Loss = 0.523331
Epoch 6.106: Loss = 0.530289
Epoch 6.107: Loss = 0.301254
Epoch 6.108: Loss = 0.430984
Epoch 6.109: Loss = 0.368454
Epoch 6.110: Loss = 0.433243
Epoch 6.111: Loss = 0.313019
Epoch 6.112: Loss = 0.336426
Epoch 6.113: Loss = 0.351334
Epoch 6.114: Loss = 0.296249
Epoch 6.115: Loss = 0.285675
Epoch 6.116: Loss = 0.385239
Epoch 6.117: Loss = 0.215271
Epoch 6.118: Loss = 0.198776
Epoch 6.119: Loss = 0.246368
Epoch 6.120: Loss = 0.346573
TRAIN LOSS = 0.40213
TRAIN ACC = 88.5742 % (53147/60000)
Loss = 0.3797
Loss = 0.461884
Loss = 0.506683
Loss = 0.55603
Loss = 0.565735
Loss = 0.425278
Loss = 0.379776
Loss = 0.644012
Loss = 0.511841
Loss = 0.495651
Loss = 0.177078
Loss = 0.299606
Loss = 0.259689
Loss = 0.345764
Loss = 0.18132
Loss = 0.303711
Loss = 0.19046
Loss = 0.0719299
Loss = 0.232452
Loss = 0.525543
TEST LOSS = 0.375707
TEST ACC = 531.47 % (8933/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.344742
Epoch 7.2: Loss = 0.487259
Epoch 7.3: Loss = 0.454987
Epoch 7.4: Loss = 0.331024
Epoch 7.5: Loss = 0.318558
Epoch 7.6: Loss = 0.319855
Epoch 7.7: Loss = 0.356857
Epoch 7.8: Loss = 0.345123
Epoch 7.9: Loss = 0.363495
Epoch 7.10: Loss = 0.361069
Epoch 7.11: Loss = 0.379593
Epoch 7.12: Loss = 0.345657
Epoch 7.13: Loss = 0.262161
Epoch 7.14: Loss = 0.338852
Epoch 7.15: Loss = 0.417313
Epoch 7.16: Loss = 0.395432
Epoch 7.17: Loss = 0.434402
Epoch 7.18: Loss = 0.58609
Epoch 7.19: Loss = 0.41394
Epoch 7.20: Loss = 0.300964
Epoch 7.21: Loss = 0.334274
Epoch 7.22: Loss = 0.285278
Epoch 7.23: Loss = 0.285416
Epoch 7.24: Loss = 0.540817
Epoch 7.25: Loss = 0.43782
Epoch 7.26: Loss = 0.48053
Epoch 7.27: Loss = 0.45546
Epoch 7.28: Loss = 0.442291
Epoch 7.29: Loss = 0.524216
Epoch 7.30: Loss = 0.517517
Epoch 7.31: Loss = 0.382462
Epoch 7.32: Loss = 0.436707
Epoch 7.33: Loss = 0.335861
Epoch 7.34: Loss = 0.40802
Epoch 7.35: Loss = 0.412918
Epoch 7.36: Loss = 0.485718
Epoch 7.37: Loss = 0.288879
Epoch 7.38: Loss = 0.335739
Epoch 7.39: Loss = 0.352325
Epoch 7.40: Loss = 0.354721
Epoch 7.41: Loss = 0.388229
Epoch 7.42: Loss = 0.553864
Epoch 7.43: Loss = 0.313507
Epoch 7.44: Loss = 0.312302
Epoch 7.45: Loss = 0.355301
Epoch 7.46: Loss = 0.474884
Epoch 7.47: Loss = 0.398331
Epoch 7.48: Loss = 0.409866
Epoch 7.49: Loss = 0.371902
Epoch 7.50: Loss = 0.484161
Epoch 7.51: Loss = 0.334824
Epoch 7.52: Loss = 0.317215
Epoch 7.53: Loss = 0.393539
Epoch 7.54: Loss = 0.496384
Epoch 7.55: Loss = 0.394836
Epoch 7.56: Loss = 0.382614
Epoch 7.57: Loss = 0.342606
Epoch 7.58: Loss = 0.385513
Epoch 7.59: Loss = 0.450455
Epoch 7.60: Loss = 0.490601
Epoch 7.61: Loss = 0.423279
Epoch 7.62: Loss = 0.469116
Epoch 7.63: Loss = 0.570724
Epoch 7.64: Loss = 0.475494
Epoch 7.65: Loss = 0.631027
Epoch 7.66: Loss = 0.414413
Epoch 7.67: Loss = 0.433197
Epoch 7.68: Loss = 0.233719
Epoch 7.69: Loss = 0.3517
Epoch 7.70: Loss = 0.509735
Epoch 7.71: Loss = 0.354828
Epoch 7.72: Loss = 0.327835
Epoch 7.73: Loss = 0.394485
Epoch 7.74: Loss = 0.317596
Epoch 7.75: Loss = 0.649979
Epoch 7.76: Loss = 0.430756
Epoch 7.77: Loss = 0.314377
Epoch 7.78: Loss = 0.419647
Epoch 7.79: Loss = 0.446655
Epoch 7.80: Loss = 0.427689
Epoch 7.81: Loss = 0.338318
Epoch 7.82: Loss = 0.31723
Epoch 7.83: Loss = 0.492676
Epoch 7.84: Loss = 0.399384
Epoch 7.85: Loss = 0.580643
Epoch 7.86: Loss = 0.49437
Epoch 7.87: Loss = 0.287888
Epoch 7.88: Loss = 0.385147
Epoch 7.89: Loss = 0.449814
Epoch 7.90: Loss = 0.33551
Epoch 7.91: Loss = 0.435669
Epoch 7.92: Loss = 0.462906
Epoch 7.93: Loss = 0.525345
Epoch 7.94: Loss = 0.283417
Epoch 7.95: Loss = 0.396622
Epoch 7.96: Loss = 0.461334
Epoch 7.97: Loss = 0.299332
Epoch 7.98: Loss = 0.381973
Epoch 7.99: Loss = 0.454483
Epoch 7.100: Loss = 0.56189
Epoch 7.101: Loss = 0.579544
Epoch 7.102: Loss = 0.384186
Epoch 7.103: Loss = 0.321457
Epoch 7.104: Loss = 0.317398
Epoch 7.105: Loss = 0.481186
Epoch 7.106: Loss = 0.514206
Epoch 7.107: Loss = 0.304596
Epoch 7.108: Loss = 0.440399
Epoch 7.109: Loss = 0.360321
Epoch 7.110: Loss = 0.410172
Epoch 7.111: Loss = 0.311417
Epoch 7.112: Loss = 0.33252
Epoch 7.113: Loss = 0.354599
Epoch 7.114: Loss = 0.291656
Epoch 7.115: Loss = 0.273758
Epoch 7.116: Loss = 0.364792
Epoch 7.117: Loss = 0.220932
Epoch 7.118: Loss = 0.191513
Epoch 7.119: Loss = 0.233978
Epoch 7.120: Loss = 0.342224
TRAIN LOSS = 0.397293
TRAIN ACC = 88.8306 % (53301/60000)
Loss = 0.381363
Loss = 0.459183
Loss = 0.509354
Loss = 0.546906
Loss = 0.572128
Loss = 0.398315
Loss = 0.36264
Loss = 0.644455
Loss = 0.521408
Loss = 0.476242
Loss = 0.168732
Loss = 0.307175
Loss = 0.274597
Loss = 0.353317
Loss = 0.182846
Loss = 0.304092
Loss = 0.196259
Loss = 0.0662994
Loss = 0.241714
Loss = 0.504623
TEST LOSS = 0.373582
TEST ACC = 533.009 % (8951/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.331772
Epoch 8.2: Loss = 0.476334
Epoch 8.3: Loss = 0.467926
Epoch 8.4: Loss = 0.324768
Epoch 8.5: Loss = 0.314087
Epoch 8.6: Loss = 0.327316
Epoch 8.7: Loss = 0.350143
Epoch 8.8: Loss = 0.333267
Epoch 8.9: Loss = 0.358185
Epoch 8.10: Loss = 0.365341
Epoch 8.11: Loss = 0.385986
Epoch 8.12: Loss = 0.331818
Epoch 8.13: Loss = 0.277084
Epoch 8.14: Loss = 0.34227
Epoch 8.15: Loss = 0.430435
Epoch 8.16: Loss = 0.392944
Epoch 8.17: Loss = 0.430557
Epoch 8.18: Loss = 0.609558
Epoch 8.19: Loss = 0.446121
Epoch 8.20: Loss = 0.301056
Epoch 8.21: Loss = 0.334
Epoch 8.22: Loss = 0.281006
Epoch 8.23: Loss = 0.293015
Epoch 8.24: Loss = 0.544876
Epoch 8.25: Loss = 0.444138
Epoch 8.26: Loss = 0.536285
Epoch 8.27: Loss = 0.465622
Epoch 8.28: Loss = 0.4599
Epoch 8.29: Loss = 0.527542
Epoch 8.30: Loss = 0.549652
Epoch 8.31: Loss = 0.361664
Epoch 8.32: Loss = 0.44194
Epoch 8.33: Loss = 0.339752
Epoch 8.34: Loss = 0.403366
Epoch 8.35: Loss = 0.407822
Epoch 8.36: Loss = 0.490143
Epoch 8.37: Loss = 0.283371
Epoch 8.38: Loss = 0.335907
Epoch 8.39: Loss = 0.361908
Epoch 8.40: Loss = 0.359894
Epoch 8.41: Loss = 0.399506
Epoch 8.42: Loss = 0.593018
Epoch 8.43: Loss = 0.307404
Epoch 8.44: Loss = 0.312073
Epoch 8.45: Loss = 0.383026
Epoch 8.46: Loss = 0.442337
Epoch 8.47: Loss = 0.394882
Epoch 8.48: Loss = 0.394669
Epoch 8.49: Loss = 0.373123
Epoch 8.50: Loss = 0.514511
Epoch 8.51: Loss = 0.349854
Epoch 8.52: Loss = 0.316605
Epoch 8.53: Loss = 0.40477
Epoch 8.54: Loss = 0.494904
Epoch 8.55: Loss = 0.417725
Epoch 8.56: Loss = 0.375626
Epoch 8.57: Loss = 0.358124
Epoch 8.58: Loss = 0.398132
Epoch 8.59: Loss = 0.46051
Epoch 8.60: Loss = 0.507904
Epoch 8.61: Loss = 0.4133
Epoch 8.62: Loss = 0.489258
Epoch 8.63: Loss = 0.59874
Epoch 8.64: Loss = 0.477158
Epoch 8.65: Loss = 0.605438
Epoch 8.66: Loss = 0.408279
Epoch 8.67: Loss = 0.412766
Epoch 8.68: Loss = 0.257233
Epoch 8.69: Loss = 0.365143
Epoch 8.70: Loss = 0.510223
Epoch 8.71: Loss = 0.370346
Epoch 8.72: Loss = 0.32515
Epoch 8.73: Loss = 0.415619
Epoch 8.74: Loss = 0.323059
Epoch 8.75: Loss = 0.662155
Epoch 8.76: Loss = 0.452332
Epoch 8.77: Loss = 0.325256
Epoch 8.78: Loss = 0.415024
Epoch 8.79: Loss = 0.445633
Epoch 8.80: Loss = 0.43425
Epoch 8.81: Loss = 0.345001
Epoch 8.82: Loss = 0.301544
Epoch 8.83: Loss = 0.493973
Epoch 8.84: Loss = 0.392807
Epoch 8.85: Loss = 0.593201
Epoch 8.86: Loss = 0.512222
Epoch 8.87: Loss = 0.294907
Epoch 8.88: Loss = 0.390762
Epoch 8.89: Loss = 0.446579
Epoch 8.90: Loss = 0.343719
Epoch 8.91: Loss = 0.444656
Epoch 8.92: Loss = 0.449829
Epoch 8.93: Loss = 0.537094
Epoch 8.94: Loss = 0.285202
Epoch 8.95: Loss = 0.402451
Epoch 8.96: Loss = 0.457214
Epoch 8.97: Loss = 0.291351
Epoch 8.98: Loss = 0.3936
Epoch 8.99: Loss = 0.471649
Epoch 8.100: Loss = 0.584473
Epoch 8.101: Loss = 0.606171
Epoch 8.102: Loss = 0.393616
Epoch 8.103: Loss = 0.340469
Epoch 8.104: Loss = 0.322388
Epoch 8.105: Loss = 0.491501
Epoch 8.106: Loss = 0.546219
Epoch 8.107: Loss = 0.324036
Epoch 8.108: Loss = 0.435501
Epoch 8.109: Loss = 0.356384
Epoch 8.110: Loss = 0.449875
Epoch 8.111: Loss = 0.331116
Epoch 8.112: Loss = 0.339584
Epoch 8.113: Loss = 0.378571
Epoch 8.114: Loss = 0.294159
Epoch 8.115: Loss = 0.280823
Epoch 8.116: Loss = 0.401291
Epoch 8.117: Loss = 0.220886
Epoch 8.118: Loss = 0.202286
Epoch 8.119: Loss = 0.239273
Epoch 8.120: Loss = 0.343719
TRAIN LOSS = 0.403778
TRAIN ACC = 88.9252 % (53358/60000)
Loss = 0.385788
Loss = 0.459534
Loss = 0.533478
Loss = 0.559525
Loss = 0.574936
Loss = 0.407516
Loss = 0.386963
Loss = 0.656067
Loss = 0.550781
Loss = 0.498535
Loss = 0.17897
Loss = 0.288696
Loss = 0.267639
Loss = 0.361603
Loss = 0.166931
Loss = 0.294647
Loss = 0.194931
Loss = 0.0641785
Loss = 0.225128
Loss = 0.48967
TEST LOSS = 0.377276
TEST ACC = 533.578 % (8972/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.330612
Epoch 9.2: Loss = 0.495834
Epoch 9.3: Loss = 0.476608
Epoch 9.4: Loss = 0.32486
Epoch 9.5: Loss = 0.28215
Epoch 9.6: Loss = 0.318771
Epoch 9.7: Loss = 0.351273
Epoch 9.8: Loss = 0.352661
Epoch 9.9: Loss = 0.370758
Epoch 9.10: Loss = 0.387039
Epoch 9.11: Loss = 0.406158
Epoch 9.12: Loss = 0.342575
Epoch 9.13: Loss = 0.28894
Epoch 9.14: Loss = 0.336182
Epoch 9.15: Loss = 0.426178
Epoch 9.16: Loss = 0.399857
Epoch 9.17: Loss = 0.415146
Epoch 9.18: Loss = 0.654526
Epoch 9.19: Loss = 0.454498
Epoch 9.20: Loss = 0.298798
Epoch 9.21: Loss = 0.338959
Epoch 9.22: Loss = 0.297211
Epoch 9.23: Loss = 0.306274
Epoch 9.24: Loss = 0.505066
Epoch 9.25: Loss = 0.412827
Epoch 9.26: Loss = 0.561295
Epoch 9.27: Loss = 0.459976
Epoch 9.28: Loss = 0.458954
Epoch 9.29: Loss = 0.507965
Epoch 9.30: Loss = 0.534485
Epoch 9.31: Loss = 0.370483
Epoch 9.32: Loss = 0.429291
Epoch 9.33: Loss = 0.341354
Epoch 9.34: Loss = 0.406097
Epoch 9.35: Loss = 0.395691
Epoch 9.36: Loss = 0.500854
Epoch 9.37: Loss = 0.286377
Epoch 9.38: Loss = 0.337418
Epoch 9.39: Loss = 0.379608
Epoch 9.40: Loss = 0.382813
Epoch 9.41: Loss = 0.405731
Epoch 9.42: Loss = 0.581497
Epoch 9.43: Loss = 0.310425
Epoch 9.44: Loss = 0.307755
Epoch 9.45: Loss = 0.369537
Epoch 9.46: Loss = 0.470444
Epoch 9.47: Loss = 0.374557
Epoch 9.48: Loss = 0.397217
Epoch 9.49: Loss = 0.359818
Epoch 9.50: Loss = 0.51416
Epoch 9.51: Loss = 0.345703
Epoch 9.52: Loss = 0.310791
Epoch 9.53: Loss = 0.398041
Epoch 9.54: Loss = 0.5289
Epoch 9.55: Loss = 0.43309
Epoch 9.56: Loss = 0.389938
Epoch 9.57: Loss = 0.361542
Epoch 9.58: Loss = 0.403931
Epoch 9.59: Loss = 0.472794
Epoch 9.60: Loss = 0.506104
Epoch 9.61: Loss = 0.416092
Epoch 9.62: Loss = 0.467346
Epoch 9.63: Loss = 0.603439
Epoch 9.64: Loss = 0.501862
Epoch 9.65: Loss = 0.591675
Epoch 9.66: Loss = 0.412613
Epoch 9.67: Loss = 0.406952
Epoch 9.68: Loss = 0.269073
Epoch 9.69: Loss = 0.352066
Epoch 9.70: Loss = 0.510254
Epoch 9.71: Loss = 0.365356
Epoch 9.72: Loss = 0.321533
Epoch 9.73: Loss = 0.421036
Epoch 9.74: Loss = 0.331039
Epoch 9.75: Loss = 0.651672
Epoch 9.76: Loss = 0.43959
Epoch 9.77: Loss = 0.318695
Epoch 9.78: Loss = 0.435898
Epoch 9.79: Loss = 0.433044
Epoch 9.80: Loss = 0.4077
Epoch 9.81: Loss = 0.349319
Epoch 9.82: Loss = 0.307266
Epoch 9.83: Loss = 0.471313
Epoch 9.84: Loss = 0.404541
Epoch 9.85: Loss = 0.608398
Epoch 9.86: Loss = 0.517532
Epoch 9.87: Loss = 0.298431
Epoch 9.88: Loss = 0.369812
Epoch 9.89: Loss = 0.462341
Epoch 9.90: Loss = 0.35202
Epoch 9.91: Loss = 0.443451
Epoch 9.92: Loss = 0.464951
Epoch 9.93: Loss = 0.531143
Epoch 9.94: Loss = 0.305923
Epoch 9.95: Loss = 0.390854
Epoch 9.96: Loss = 0.414169
Epoch 9.97: Loss = 0.28598
Epoch 9.98: Loss = 0.405991
Epoch 9.99: Loss = 0.439316
Epoch 9.100: Loss = 0.568222
Epoch 9.101: Loss = 0.621887
Epoch 9.102: Loss = 0.410248
Epoch 9.103: Loss = 0.342606
Epoch 9.104: Loss = 0.332703
Epoch 9.105: Loss = 0.470551
Epoch 9.106: Loss = 0.54747
Epoch 9.107: Loss = 0.321243
Epoch 9.108: Loss = 0.437592
Epoch 9.109: Loss = 0.371109
Epoch 9.110: Loss = 0.441605
Epoch 9.111: Loss = 0.338486
Epoch 9.112: Loss = 0.334793
Epoch 9.113: Loss = 0.373886
Epoch 9.114: Loss = 0.265701
Epoch 9.115: Loss = 0.293182
Epoch 9.116: Loss = 0.389084
Epoch 9.117: Loss = 0.216278
Epoch 9.118: Loss = 0.197281
Epoch 9.119: Loss = 0.242737
Epoch 9.120: Loss = 0.327805
TRAIN LOSS = 0.404099
TRAIN ACC = 88.9664 % (53383/60000)
Loss = 0.400375
Loss = 0.457382
Loss = 0.531906
Loss = 0.55925
Loss = 0.590347
Loss = 0.418152
Loss = 0.381439
Loss = 0.67337
Loss = 0.537323
Loss = 0.491333
Loss = 0.159332
Loss = 0.3517
Loss = 0.276352
Loss = 0.362152
Loss = 0.170883
Loss = 0.279236
Loss = 0.195114
Loss = 0.0742645
Loss = 0.245041
Loss = 0.496536
TEST LOSS = 0.382574
TEST ACC = 533.829 % (8979/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.34375
Epoch 10.2: Loss = 0.494431
Epoch 10.3: Loss = 0.460464
Epoch 10.4: Loss = 0.313492
Epoch 10.5: Loss = 0.30159
Epoch 10.6: Loss = 0.34259
Epoch 10.7: Loss = 0.375183
Epoch 10.8: Loss = 0.342758
Epoch 10.9: Loss = 0.366943
Epoch 10.10: Loss = 0.376816
Epoch 10.11: Loss = 0.403564
Epoch 10.12: Loss = 0.331161
Epoch 10.13: Loss = 0.288071
Epoch 10.14: Loss = 0.347748
Epoch 10.15: Loss = 0.402161
Epoch 10.16: Loss = 0.430298
Epoch 10.17: Loss = 0.416916
Epoch 10.18: Loss = 0.640121
Epoch 10.19: Loss = 0.447357
Epoch 10.20: Loss = 0.3004
Epoch 10.21: Loss = 0.329758
Epoch 10.22: Loss = 0.295853
Epoch 10.23: Loss = 0.316452
Epoch 10.24: Loss = 0.499069
Epoch 10.25: Loss = 0.380173
Epoch 10.26: Loss = 0.544098
Epoch 10.27: Loss = 0.430847
Epoch 10.28: Loss = 0.453995
Epoch 10.29: Loss = 0.522537
Epoch 10.30: Loss = 0.522232
Epoch 10.31: Loss = 0.367859
Epoch 10.32: Loss = 0.417084
Epoch 10.33: Loss = 0.325272
Epoch 10.34: Loss = 0.417526
Epoch 10.35: Loss = 0.390823
Epoch 10.36: Loss = 0.490753
Epoch 10.37: Loss = 0.292191
Epoch 10.38: Loss = 0.3349
Epoch 10.39: Loss = 0.377838
Epoch 10.40: Loss = 0.374603
Epoch 10.41: Loss = 0.406723
Epoch 10.42: Loss = 0.589783
Epoch 10.43: Loss = 0.327332
Epoch 10.44: Loss = 0.305405
Epoch 10.45: Loss = 0.337677
Epoch 10.46: Loss = 0.469635
Epoch 10.47: Loss = 0.372665
Epoch 10.48: Loss = 0.405533
Epoch 10.49: Loss = 0.362366
Epoch 10.50: Loss = 0.504868
Epoch 10.51: Loss = 0.348267
Epoch 10.52: Loss = 0.312592
Epoch 10.53: Loss = 0.387711
Epoch 10.54: Loss = 0.513092
Epoch 10.55: Loss = 0.405136
Epoch 10.56: Loss = 0.380096
Epoch 10.57: Loss = 0.361282
Epoch 10.58: Loss = 0.393982
Epoch 10.59: Loss = 0.479019
Epoch 10.60: Loss = 0.519806
Epoch 10.61: Loss = 0.383896
Epoch 10.62: Loss = 0.466446
Epoch 10.63: Loss = 0.628006
Epoch 10.64: Loss = 0.511078
Epoch 10.65: Loss = 0.646149
Epoch 10.66: Loss = 0.444458
Epoch 10.67: Loss = 0.412537
Epoch 10.68: Loss = 0.269653
Epoch 10.69: Loss = 0.373001
Epoch 10.70: Loss = 0.503815
Epoch 10.71: Loss = 0.369614
Epoch 10.72: Loss = 0.30986
Epoch 10.73: Loss = 0.451981
Epoch 10.74: Loss = 0.312531
Epoch 10.75: Loss = 0.655548
Epoch 10.76: Loss = 0.45459
Epoch 10.77: Loss = 0.338699
Epoch 10.78: Loss = 0.453247
Epoch 10.79: Loss = 0.439117
Epoch 10.80: Loss = 0.408524
Epoch 10.81: Loss = 0.331787
Epoch 10.82: Loss = 0.341934
Epoch 10.83: Loss = 0.46373
Epoch 10.84: Loss = 0.413651
Epoch 10.85: Loss = 0.587143
Epoch 10.86: Loss = 0.516174
Epoch 10.87: Loss = 0.309341
Epoch 10.88: Loss = 0.37851
Epoch 10.89: Loss = 0.458328
Epoch 10.90: Loss = 0.343018
Epoch 10.91: Loss = 0.448959
Epoch 10.92: Loss = 0.498581
Epoch 10.93: Loss = 0.520172
Epoch 10.94: Loss = 0.304001
Epoch 10.95: Loss = 0.38327
Epoch 10.96: Loss = 0.440338
Epoch 10.97: Loss = 0.288055
Epoch 10.98: Loss = 0.400787
Epoch 10.99: Loss = 0.436035
Epoch 10.100: Loss = 0.571075
Epoch 10.101: Loss = 0.630737
Epoch 10.102: Loss = 0.398849
Epoch 10.103: Loss = 0.32933
Epoch 10.104: Loss = 0.336563
Epoch 10.105: Loss = 0.492386
Epoch 10.106: Loss = 0.558182
Epoch 10.107: Loss = 0.324814
Epoch 10.108: Loss = 0.442368
Epoch 10.109: Loss = 0.368912
Epoch 10.110: Loss = 0.462997
Epoch 10.111: Loss = 0.338974
Epoch 10.112: Loss = 0.339584
Epoch 10.113: Loss = 0.369858
Epoch 10.114: Loss = 0.271454
Epoch 10.115: Loss = 0.29628
Epoch 10.116: Loss = 0.407135
Epoch 10.117: Loss = 0.212662
Epoch 10.118: Loss = 0.210342
Epoch 10.119: Loss = 0.239594
Epoch 10.120: Loss = 0.333435
TRAIN LOSS = 0.405197
TRAIN ACC = 89.1174 % (53473/60000)
Loss = 0.408859
Loss = 0.441391
Loss = 0.523041
Loss = 0.539429
Loss = 0.57814
Loss = 0.435043
Loss = 0.388
Loss = 0.671768
Loss = 0.5298
Loss = 0.474915
Loss = 0.15535
Loss = 0.384598
Loss = 0.261932
Loss = 0.367386
Loss = 0.174911
Loss = 0.328506
Loss = 0.222717
Loss = 0.067276
Loss = 0.263443
Loss = 0.511902
TEST LOSS = 0.38642
TEST ACC = 534.729 % (8959/10000)
