Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.35477
Epoch 1.2: Loss = 2.28847
Epoch 1.3: Loss = 2.26157
Epoch 1.4: Loss = 2.21475
Epoch 1.5: Loss = 2.17023
Epoch 1.6: Loss = 2.10258
Epoch 1.7: Loss = 2.1389
Epoch 1.8: Loss = 2.08621
Epoch 1.9: Loss = 2.02739
Epoch 1.10: Loss = 1.98563
Epoch 1.11: Loss = 1.925
Epoch 1.12: Loss = 1.90834
Epoch 1.13: Loss = 1.85095
Epoch 1.14: Loss = 1.84312
Epoch 1.15: Loss = 1.89336
Epoch 1.16: Loss = 1.79218
Epoch 1.17: Loss = 1.75253
Epoch 1.18: Loss = 1.6985
Epoch 1.19: Loss = 1.64787
Epoch 1.20: Loss = 1.62485
Epoch 1.21: Loss = 1.55696
Epoch 1.22: Loss = 1.55267
Epoch 1.23: Loss = 1.48735
Epoch 1.24: Loss = 1.60809
Epoch 1.25: Loss = 1.48657
Epoch 1.26: Loss = 1.52458
Epoch 1.27: Loss = 1.44763
Epoch 1.28: Loss = 1.42778
Epoch 1.29: Loss = 1.42236
Epoch 1.30: Loss = 1.4873
Epoch 1.31: Loss = 1.34331
Epoch 1.32: Loss = 1.37314
Epoch 1.33: Loss = 1.33984
Epoch 1.34: Loss = 1.34863
Epoch 1.35: Loss = 1.28838
Epoch 1.36: Loss = 1.405
Epoch 1.37: Loss = 1.22763
Epoch 1.38: Loss = 1.18584
Epoch 1.39: Loss = 1.16423
Epoch 1.40: Loss = 1.11372
Epoch 1.41: Loss = 1.14751
Epoch 1.42: Loss = 1.14758
Epoch 1.43: Loss = 1.09038
Epoch 1.44: Loss = 1.00072
Epoch 1.45: Loss = 1.13666
Epoch 1.46: Loss = 1.05075
Epoch 1.47: Loss = 1.00443
Epoch 1.48: Loss = 1.06145
Epoch 1.49: Loss = 1.01041
Epoch 1.50: Loss = 1.07692
Epoch 1.51: Loss = 0.938232
Epoch 1.52: Loss = 0.946671
Epoch 1.53: Loss = 1.0162
Epoch 1.54: Loss = 0.987213
Epoch 1.55: Loss = 0.997772
Epoch 1.56: Loss = 0.896118
Epoch 1.57: Loss = 0.854279
Epoch 1.58: Loss = 0.896164
Epoch 1.59: Loss = 0.946213
Epoch 1.60: Loss = 1.03453
Epoch 1.61: Loss = 0.974075
Epoch 1.62: Loss = 0.960739
Epoch 1.63: Loss = 0.99614
Epoch 1.64: Loss = 0.962753
Epoch 1.65: Loss = 0.960587
Epoch 1.66: Loss = 0.888977
Epoch 1.67: Loss = 0.879959
Epoch 1.68: Loss = 0.730118
Epoch 1.69: Loss = 0.791397
Epoch 1.70: Loss = 0.858719
Epoch 1.71: Loss = 0.790115
Epoch 1.72: Loss = 0.805588
Epoch 1.73: Loss = 0.855881
Epoch 1.74: Loss = 0.695419
Epoch 1.75: Loss = 0.808868
Epoch 1.76: Loss = 0.796524
Epoch 1.77: Loss = 0.744492
Epoch 1.78: Loss = 0.715805
Epoch 1.79: Loss = 0.710373
Epoch 1.80: Loss = 0.818909
Epoch 1.81: Loss = 0.669815
Epoch 1.82: Loss = 0.658051
Epoch 1.83: Loss = 0.818207
Epoch 1.84: Loss = 0.710693
Epoch 1.85: Loss = 0.817581
Epoch 1.86: Loss = 0.717834
Epoch 1.87: Loss = 0.661514
Epoch 1.88: Loss = 0.688217
Epoch 1.89: Loss = 0.797241
Epoch 1.90: Loss = 0.69577
Epoch 1.91: Loss = 0.720276
Epoch 1.92: Loss = 0.745911
Epoch 1.93: Loss = 0.760086
Epoch 1.94: Loss = 0.61879
Epoch 1.95: Loss = 0.72963
Epoch 1.96: Loss = 0.667145
Epoch 1.97: Loss = 0.550705
Epoch 1.98: Loss = 0.664795
Epoch 1.99: Loss = 0.734222
Epoch 1.100: Loss = 0.859695
Epoch 1.101: Loss = 0.751312
Epoch 1.102: Loss = 0.732376
Epoch 1.103: Loss = 0.600067
Epoch 1.104: Loss = 0.595642
Epoch 1.105: Loss = 0.685883
Epoch 1.106: Loss = 0.684036
Epoch 1.107: Loss = 0.5914
Epoch 1.108: Loss = 0.658829
Epoch 1.109: Loss = 0.625076
Epoch 1.110: Loss = 0.648209
Epoch 1.111: Loss = 0.556366
Epoch 1.112: Loss = 0.527863
Epoch 1.113: Loss = 0.62587
Epoch 1.114: Loss = 0.570908
Epoch 1.115: Loss = 0.605804
Epoch 1.116: Loss = 0.611343
Epoch 1.117: Loss = 0.473053
Epoch 1.118: Loss = 0.424484
Epoch 1.119: Loss = 0.425323
Epoch 1.120: Loss = 0.479385
TRAIN LOSS = 1.08754
TRAIN ACC = 68.7576 % (41256/60000)
Loss = 0.630646
Loss = 0.685486
Loss = 0.76387
Loss = 0.738373
Loss = 0.762878
Loss = 0.650223
Loss = 0.630783
Loss = 0.78334
Loss = 0.789383
Loss = 0.706406
Loss = 0.373016
Loss = 0.535385
Loss = 0.359985
Loss = 0.581146
Loss = 0.460449
Loss = 0.471954
Loss = 0.409393
Loss = 0.231369
Loss = 0.434784
Loss = 0.683594
TEST LOSS = 0.584123
TEST ACC = 412.56 % (8203/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.533218
Epoch 2.2: Loss = 0.685822
Epoch 2.3: Loss = 0.656113
Epoch 2.4: Loss = 0.506363
Epoch 2.5: Loss = 0.555801
Epoch 2.6: Loss = 0.489975
Epoch 2.7: Loss = 0.602692
Epoch 2.8: Loss = 0.588455
Epoch 2.9: Loss = 0.537735
Epoch 2.10: Loss = 0.56218
Epoch 2.11: Loss = 0.538834
Epoch 2.12: Loss = 0.51918
Epoch 2.13: Loss = 0.492508
Epoch 2.14: Loss = 0.49852
Epoch 2.15: Loss = 0.661652
Epoch 2.16: Loss = 0.625412
Epoch 2.17: Loss = 0.623352
Epoch 2.18: Loss = 0.66835
Epoch 2.19: Loss = 0.527359
Epoch 2.20: Loss = 0.47049
Epoch 2.21: Loss = 0.50148
Epoch 2.22: Loss = 0.490524
Epoch 2.23: Loss = 0.441177
Epoch 2.24: Loss = 0.696945
Epoch 2.25: Loss = 0.573929
Epoch 2.26: Loss = 0.676666
Epoch 2.27: Loss = 0.645782
Epoch 2.28: Loss = 0.614365
Epoch 2.29: Loss = 0.67746
Epoch 2.30: Loss = 0.783936
Epoch 2.31: Loss = 0.506027
Epoch 2.32: Loss = 0.650406
Epoch 2.33: Loss = 0.571518
Epoch 2.34: Loss = 0.634354
Epoch 2.35: Loss = 0.592194
Epoch 2.36: Loss = 0.653305
Epoch 2.37: Loss = 0.482849
Epoch 2.38: Loss = 0.453186
Epoch 2.39: Loss = 0.516052
Epoch 2.40: Loss = 0.502792
Epoch 2.41: Loss = 0.531982
Epoch 2.42: Loss = 0.668854
Epoch 2.43: Loss = 0.472412
Epoch 2.44: Loss = 0.412506
Epoch 2.45: Loss = 0.521423
Epoch 2.46: Loss = 0.573868
Epoch 2.47: Loss = 0.490051
Epoch 2.48: Loss = 0.516144
Epoch 2.49: Loss = 0.491119
Epoch 2.50: Loss = 0.60762
Epoch 2.51: Loss = 0.461853
Epoch 2.52: Loss = 0.439789
Epoch 2.53: Loss = 0.507736
Epoch 2.54: Loss = 0.585114
Epoch 2.55: Loss = 0.53476
Epoch 2.56: Loss = 0.460632
Epoch 2.57: Loss = 0.462631
Epoch 2.58: Loss = 0.535202
Epoch 2.59: Loss = 0.562225
Epoch 2.60: Loss = 0.656586
Epoch 2.61: Loss = 0.637009
Epoch 2.62: Loss = 0.596268
Epoch 2.63: Loss = 0.646729
Epoch 2.64: Loss = 0.608109
Epoch 2.65: Loss = 0.695938
Epoch 2.66: Loss = 0.488129
Epoch 2.67: Loss = 0.572891
Epoch 2.68: Loss = 0.364426
Epoch 2.69: Loss = 0.448563
Epoch 2.70: Loss = 0.642136
Epoch 2.71: Loss = 0.498032
Epoch 2.72: Loss = 0.488525
Epoch 2.73: Loss = 0.590744
Epoch 2.74: Loss = 0.367294
Epoch 2.75: Loss = 0.669937
Epoch 2.76: Loss = 0.557587
Epoch 2.77: Loss = 0.473068
Epoch 2.78: Loss = 0.521606
Epoch 2.79: Loss = 0.542908
Epoch 2.80: Loss = 0.627533
Epoch 2.81: Loss = 0.476425
Epoch 2.82: Loss = 0.430038
Epoch 2.83: Loss = 0.613968
Epoch 2.84: Loss = 0.525299
Epoch 2.85: Loss = 0.666367
Epoch 2.86: Loss = 0.554855
Epoch 2.87: Loss = 0.43779
Epoch 2.88: Loss = 0.50853
Epoch 2.89: Loss = 0.621201
Epoch 2.90: Loss = 0.418243
Epoch 2.91: Loss = 0.549576
Epoch 2.92: Loss = 0.538177
Epoch 2.93: Loss = 0.588104
Epoch 2.94: Loss = 0.412857
Epoch 2.95: Loss = 0.544754
Epoch 2.96: Loss = 0.528732
Epoch 2.97: Loss = 0.406509
Epoch 2.98: Loss = 0.500687
Epoch 2.99: Loss = 0.587311
Epoch 2.100: Loss = 0.668503
Epoch 2.101: Loss = 0.62944
Epoch 2.102: Loss = 0.531631
Epoch 2.103: Loss = 0.459381
Epoch 2.104: Loss = 0.417831
Epoch 2.105: Loss = 0.559814
Epoch 2.106: Loss = 0.570236
Epoch 2.107: Loss = 0.432007
Epoch 2.108: Loss = 0.519211
Epoch 2.109: Loss = 0.483078
Epoch 2.110: Loss = 0.521667
Epoch 2.111: Loss = 0.445358
Epoch 2.112: Loss = 0.372635
Epoch 2.113: Loss = 0.494141
Epoch 2.114: Loss = 0.402832
Epoch 2.115: Loss = 0.427582
Epoch 2.116: Loss = 0.468338
Epoch 2.117: Loss = 0.350113
Epoch 2.118: Loss = 0.258957
Epoch 2.119: Loss = 0.330765
Epoch 2.120: Loss = 0.396103
TRAIN LOSS = 0.534927
TRAIN ACC = 83.5907 % (50157/60000)
Loss = 0.459656
Loss = 0.57283
Loss = 0.659714
Loss = 0.58989
Loss = 0.653809
Loss = 0.511047
Loss = 0.485336
Loss = 0.651657
Loss = 0.626282
Loss = 0.566711
Loss = 0.268738
Loss = 0.384384
Loss = 0.302826
Loss = 0.444565
Loss = 0.33284
Loss = 0.346436
Loss = 0.295883
Loss = 0.126663
Loss = 0.324738
Loss = 0.599152
TEST LOSS = 0.460158
TEST ACC = 501.569 % (8630/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.396973
Epoch 3.2: Loss = 0.652405
Epoch 3.3: Loss = 0.593918
Epoch 3.4: Loss = 0.40387
Epoch 3.5: Loss = 0.437897
Epoch 3.6: Loss = 0.401825
Epoch 3.7: Loss = 0.461548
Epoch 3.8: Loss = 0.509705
Epoch 3.9: Loss = 0.450256
Epoch 3.10: Loss = 0.486099
Epoch 3.11: Loss = 0.468155
Epoch 3.12: Loss = 0.466721
Epoch 3.13: Loss = 0.396835
Epoch 3.14: Loss = 0.384857
Epoch 3.15: Loss = 0.551804
Epoch 3.16: Loss = 0.582733
Epoch 3.17: Loss = 0.580292
Epoch 3.18: Loss = 0.638474
Epoch 3.19: Loss = 0.423706
Epoch 3.20: Loss = 0.401688
Epoch 3.21: Loss = 0.416931
Epoch 3.22: Loss = 0.408157
Epoch 3.23: Loss = 0.406311
Epoch 3.24: Loss = 0.648163
Epoch 3.25: Loss = 0.514526
Epoch 3.26: Loss = 0.638077
Epoch 3.27: Loss = 0.593643
Epoch 3.28: Loss = 0.5578
Epoch 3.29: Loss = 0.644058
Epoch 3.30: Loss = 0.721741
Epoch 3.31: Loss = 0.470978
Epoch 3.32: Loss = 0.583908
Epoch 3.33: Loss = 0.481674
Epoch 3.34: Loss = 0.556427
Epoch 3.35: Loss = 0.548782
Epoch 3.36: Loss = 0.619598
Epoch 3.37: Loss = 0.42836
Epoch 3.38: Loss = 0.422272
Epoch 3.39: Loss = 0.487152
Epoch 3.40: Loss = 0.483841
Epoch 3.41: Loss = 0.457031
Epoch 3.42: Loss = 0.655289
Epoch 3.43: Loss = 0.403397
Epoch 3.44: Loss = 0.351776
Epoch 3.45: Loss = 0.459412
Epoch 3.46: Loss = 0.564453
Epoch 3.47: Loss = 0.473465
Epoch 3.48: Loss = 0.498688
Epoch 3.49: Loss = 0.453156
Epoch 3.50: Loss = 0.552597
Epoch 3.51: Loss = 0.397034
Epoch 3.52: Loss = 0.406876
Epoch 3.53: Loss = 0.445541
Epoch 3.54: Loss = 0.55304
Epoch 3.55: Loss = 0.477402
Epoch 3.56: Loss = 0.405457
Epoch 3.57: Loss = 0.390732
Epoch 3.58: Loss = 0.46402
Epoch 3.59: Loss = 0.528687
Epoch 3.60: Loss = 0.578751
Epoch 3.61: Loss = 0.529297
Epoch 3.62: Loss = 0.547592
Epoch 3.63: Loss = 0.611771
Epoch 3.64: Loss = 0.57103
Epoch 3.65: Loss = 0.673325
Epoch 3.66: Loss = 0.452225
Epoch 3.67: Loss = 0.534409
Epoch 3.68: Loss = 0.320175
Epoch 3.69: Loss = 0.394867
Epoch 3.70: Loss = 0.626068
Epoch 3.71: Loss = 0.453796
Epoch 3.72: Loss = 0.418869
Epoch 3.73: Loss = 0.556091
Epoch 3.74: Loss = 0.377548
Epoch 3.75: Loss = 0.672043
Epoch 3.76: Loss = 0.543594
Epoch 3.77: Loss = 0.446625
Epoch 3.78: Loss = 0.491272
Epoch 3.79: Loss = 0.516647
Epoch 3.80: Loss = 0.560822
Epoch 3.81: Loss = 0.428177
Epoch 3.82: Loss = 0.408554
Epoch 3.83: Loss = 0.591461
Epoch 3.84: Loss = 0.468979
Epoch 3.85: Loss = 0.664169
Epoch 3.86: Loss = 0.557632
Epoch 3.87: Loss = 0.382217
Epoch 3.88: Loss = 0.468124
Epoch 3.89: Loss = 0.586853
Epoch 3.90: Loss = 0.397141
Epoch 3.91: Loss = 0.553543
Epoch 3.92: Loss = 0.51886
Epoch 3.93: Loss = 0.595154
Epoch 3.94: Loss = 0.367798
Epoch 3.95: Loss = 0.518799
Epoch 3.96: Loss = 0.580032
Epoch 3.97: Loss = 0.404739
Epoch 3.98: Loss = 0.456055
Epoch 3.99: Loss = 0.549866
Epoch 3.100: Loss = 0.655273
Epoch 3.101: Loss = 0.604309
Epoch 3.102: Loss = 0.49173
Epoch 3.103: Loss = 0.460037
Epoch 3.104: Loss = 0.404053
Epoch 3.105: Loss = 0.565384
Epoch 3.106: Loss = 0.566956
Epoch 3.107: Loss = 0.370865
Epoch 3.108: Loss = 0.519363
Epoch 3.109: Loss = 0.418777
Epoch 3.110: Loss = 0.499176
Epoch 3.111: Loss = 0.400345
Epoch 3.112: Loss = 0.363617
Epoch 3.113: Loss = 0.451324
Epoch 3.114: Loss = 0.375427
Epoch 3.115: Loss = 0.421097
Epoch 3.116: Loss = 0.447739
Epoch 3.117: Loss = 0.317902
Epoch 3.118: Loss = 0.231979
Epoch 3.119: Loss = 0.355789
Epoch 3.120: Loss = 0.373444
TRAIN LOSS = 0.491089
TRAIN ACC = 85.2158 % (51132/60000)
Loss = 0.473755
Loss = 0.5858
Loss = 0.647034
Loss = 0.618866
Loss = 0.652557
Loss = 0.485901
Loss = 0.469116
Loss = 0.673172
Loss = 0.608231
Loss = 0.588562
Loss = 0.246552
Loss = 0.363785
Loss = 0.303223
Loss = 0.456177
Loss = 0.290451
Loss = 0.382339
Loss = 0.295532
Loss = 0.134903
Loss = 0.286209
Loss = 0.619415
TEST LOSS = 0.459079
TEST ACC = 511.319 % (8632/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.382721
Epoch 4.2: Loss = 0.614624
Epoch 4.3: Loss = 0.576797
Epoch 4.4: Loss = 0.39801
Epoch 4.5: Loss = 0.39711
Epoch 4.6: Loss = 0.410385
Epoch 4.7: Loss = 0.45845
Epoch 4.8: Loss = 0.488617
Epoch 4.9: Loss = 0.436798
Epoch 4.10: Loss = 0.419907
Epoch 4.11: Loss = 0.476913
Epoch 4.12: Loss = 0.454834
Epoch 4.13: Loss = 0.361649
Epoch 4.14: Loss = 0.378357
Epoch 4.15: Loss = 0.530304
Epoch 4.16: Loss = 0.575394
Epoch 4.17: Loss = 0.577408
Epoch 4.18: Loss = 0.650421
Epoch 4.19: Loss = 0.435852
Epoch 4.20: Loss = 0.401627
Epoch 4.21: Loss = 0.393677
Epoch 4.22: Loss = 0.387466
Epoch 4.23: Loss = 0.387665
Epoch 4.24: Loss = 0.594757
Epoch 4.25: Loss = 0.530746
Epoch 4.26: Loss = 0.653091
Epoch 4.27: Loss = 0.602783
Epoch 4.28: Loss = 0.519882
Epoch 4.29: Loss = 0.637177
Epoch 4.30: Loss = 0.688614
Epoch 4.31: Loss = 0.448639
Epoch 4.32: Loss = 0.529175
Epoch 4.33: Loss = 0.452484
Epoch 4.34: Loss = 0.537933
Epoch 4.35: Loss = 0.555023
Epoch 4.36: Loss = 0.563965
Epoch 4.37: Loss = 0.389191
Epoch 4.38: Loss = 0.409592
Epoch 4.39: Loss = 0.482132
Epoch 4.40: Loss = 0.433548
Epoch 4.41: Loss = 0.466385
Epoch 4.42: Loss = 0.631561
Epoch 4.43: Loss = 0.397141
Epoch 4.44: Loss = 0.353226
Epoch 4.45: Loss = 0.414093
Epoch 4.46: Loss = 0.56163
Epoch 4.47: Loss = 0.463486
Epoch 4.48: Loss = 0.488937
Epoch 4.49: Loss = 0.385559
Epoch 4.50: Loss = 0.541321
Epoch 4.51: Loss = 0.39032
Epoch 4.52: Loss = 0.41188
Epoch 4.53: Loss = 0.453766
Epoch 4.54: Loss = 0.562012
Epoch 4.55: Loss = 0.542709
Epoch 4.56: Loss = 0.414459
Epoch 4.57: Loss = 0.401962
Epoch 4.58: Loss = 0.447418
Epoch 4.59: Loss = 0.551575
Epoch 4.60: Loss = 0.610474
Epoch 4.61: Loss = 0.510651
Epoch 4.62: Loss = 0.523392
Epoch 4.63: Loss = 0.618378
Epoch 4.64: Loss = 0.604156
Epoch 4.65: Loss = 0.712296
Epoch 4.66: Loss = 0.472214
Epoch 4.67: Loss = 0.490356
Epoch 4.68: Loss = 0.299606
Epoch 4.69: Loss = 0.373535
Epoch 4.70: Loss = 0.586899
Epoch 4.71: Loss = 0.462967
Epoch 4.72: Loss = 0.421356
Epoch 4.73: Loss = 0.552185
Epoch 4.74: Loss = 0.370438
Epoch 4.75: Loss = 0.741486
Epoch 4.76: Loss = 0.529938
Epoch 4.77: Loss = 0.470139
Epoch 4.78: Loss = 0.482391
Epoch 4.79: Loss = 0.528229
Epoch 4.80: Loss = 0.572052
Epoch 4.81: Loss = 0.463989
Epoch 4.82: Loss = 0.367905
Epoch 4.83: Loss = 0.529037
Epoch 4.84: Loss = 0.44519
Epoch 4.85: Loss = 0.694992
Epoch 4.86: Loss = 0.607712
Epoch 4.87: Loss = 0.378387
Epoch 4.88: Loss = 0.483719
Epoch 4.89: Loss = 0.604553
Epoch 4.90: Loss = 0.371307
Epoch 4.91: Loss = 0.584915
Epoch 4.92: Loss = 0.533096
Epoch 4.93: Loss = 0.578018
Epoch 4.94: Loss = 0.352142
Epoch 4.95: Loss = 0.504318
Epoch 4.96: Loss = 0.583801
Epoch 4.97: Loss = 0.391693
Epoch 4.98: Loss = 0.426682
Epoch 4.99: Loss = 0.511185
Epoch 4.100: Loss = 0.650009
Epoch 4.101: Loss = 0.592102
Epoch 4.102: Loss = 0.479523
Epoch 4.103: Loss = 0.48616
Epoch 4.104: Loss = 0.369492
Epoch 4.105: Loss = 0.59079
Epoch 4.106: Loss = 0.625961
Epoch 4.107: Loss = 0.370728
Epoch 4.108: Loss = 0.525711
Epoch 4.109: Loss = 0.440109
Epoch 4.110: Loss = 0.52504
Epoch 4.111: Loss = 0.402405
Epoch 4.112: Loss = 0.393845
Epoch 4.113: Loss = 0.483215
Epoch 4.114: Loss = 0.378235
Epoch 4.115: Loss = 0.379898
Epoch 4.116: Loss = 0.439697
Epoch 4.117: Loss = 0.332489
Epoch 4.118: Loss = 0.235962
Epoch 4.119: Loss = 0.367035
Epoch 4.120: Loss = 0.412949
TRAIN LOSS = 0.486084
TRAIN ACC = 85.8017 % (51483/60000)
Loss = 0.456573
Loss = 0.575577
Loss = 0.654861
Loss = 0.627548
Loss = 0.661346
Loss = 0.502533
Loss = 0.467331
Loss = 0.706543
Loss = 0.598434
Loss = 0.601379
Loss = 0.266281
Loss = 0.36261
Loss = 0.364883
Loss = 0.439621
Loss = 0.273804
Loss = 0.358032
Loss = 0.245941
Loss = 0.123734
Loss = 0.296066
Loss = 0.578308
TEST LOSS = 0.45807
TEST ACC = 514.828 % (8684/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.378342
Epoch 5.2: Loss = 0.614105
Epoch 5.3: Loss = 0.604996
Epoch 5.4: Loss = 0.405319
Epoch 5.5: Loss = 0.408066
Epoch 5.6: Loss = 0.427887
Epoch 5.7: Loss = 0.471329
Epoch 5.8: Loss = 0.455032
Epoch 5.9: Loss = 0.437973
Epoch 5.10: Loss = 0.434799
Epoch 5.11: Loss = 0.474442
Epoch 5.12: Loss = 0.431625
Epoch 5.13: Loss = 0.382751
Epoch 5.14: Loss = 0.370422
Epoch 5.15: Loss = 0.555786
Epoch 5.16: Loss = 0.563446
Epoch 5.17: Loss = 0.525101
Epoch 5.18: Loss = 0.676605
Epoch 5.19: Loss = 0.477371
Epoch 5.20: Loss = 0.397141
Epoch 5.21: Loss = 0.408752
Epoch 5.22: Loss = 0.370621
Epoch 5.23: Loss = 0.407928
Epoch 5.24: Loss = 0.666794
Epoch 5.25: Loss = 0.509781
Epoch 5.26: Loss = 0.671661
Epoch 5.27: Loss = 0.605576
Epoch 5.28: Loss = 0.553207
Epoch 5.29: Loss = 0.64772
Epoch 5.30: Loss = 0.652878
Epoch 5.31: Loss = 0.470657
Epoch 5.32: Loss = 0.515701
Epoch 5.33: Loss = 0.441025
Epoch 5.34: Loss = 0.516235
Epoch 5.35: Loss = 0.535782
Epoch 5.36: Loss = 0.546463
Epoch 5.37: Loss = 0.365112
Epoch 5.38: Loss = 0.429367
Epoch 5.39: Loss = 0.511841
Epoch 5.40: Loss = 0.43541
Epoch 5.41: Loss = 0.482117
Epoch 5.42: Loss = 0.687744
Epoch 5.43: Loss = 0.415619
Epoch 5.44: Loss = 0.333694
Epoch 5.45: Loss = 0.423721
Epoch 5.46: Loss = 0.555252
Epoch 5.47: Loss = 0.471161
Epoch 5.48: Loss = 0.471039
Epoch 5.49: Loss = 0.401398
Epoch 5.50: Loss = 0.565933
Epoch 5.51: Loss = 0.468735
Epoch 5.52: Loss = 0.425461
Epoch 5.53: Loss = 0.468475
Epoch 5.54: Loss = 0.596542
Epoch 5.55: Loss = 0.563202
Epoch 5.56: Loss = 0.418732
Epoch 5.57: Loss = 0.419739
Epoch 5.58: Loss = 0.460464
Epoch 5.59: Loss = 0.561829
Epoch 5.60: Loss = 0.631409
Epoch 5.61: Loss = 0.551437
Epoch 5.62: Loss = 0.552597
Epoch 5.63: Loss = 0.682159
Epoch 5.64: Loss = 0.594147
Epoch 5.65: Loss = 0.725891
Epoch 5.66: Loss = 0.468048
Epoch 5.67: Loss = 0.476181
Epoch 5.68: Loss = 0.300049
Epoch 5.69: Loss = 0.386307
Epoch 5.70: Loss = 0.597534
Epoch 5.71: Loss = 0.473221
Epoch 5.72: Loss = 0.395706
Epoch 5.73: Loss = 0.532013
Epoch 5.74: Loss = 0.374268
Epoch 5.75: Loss = 0.752731
Epoch 5.76: Loss = 0.514877
Epoch 5.77: Loss = 0.443405
Epoch 5.78: Loss = 0.510818
Epoch 5.79: Loss = 0.55368
Epoch 5.80: Loss = 0.542679
Epoch 5.81: Loss = 0.461578
Epoch 5.82: Loss = 0.341232
Epoch 5.83: Loss = 0.605728
Epoch 5.84: Loss = 0.479034
Epoch 5.85: Loss = 0.669846
Epoch 5.86: Loss = 0.633652
Epoch 5.87: Loss = 0.382965
Epoch 5.88: Loss = 0.478302
Epoch 5.89: Loss = 0.590149
Epoch 5.90: Loss = 0.388611
Epoch 5.91: Loss = 0.584702
Epoch 5.92: Loss = 0.571289
Epoch 5.93: Loss = 0.602432
Epoch 5.94: Loss = 0.311752
Epoch 5.95: Loss = 0.530457
Epoch 5.96: Loss = 0.584305
Epoch 5.97: Loss = 0.359619
Epoch 5.98: Loss = 0.460098
Epoch 5.99: Loss = 0.563858
Epoch 5.100: Loss = 0.658249
Epoch 5.101: Loss = 0.622559
Epoch 5.102: Loss = 0.461823
Epoch 5.103: Loss = 0.458328
Epoch 5.104: Loss = 0.323883
Epoch 5.105: Loss = 0.621262
Epoch 5.106: Loss = 0.572723
Epoch 5.107: Loss = 0.37114
Epoch 5.108: Loss = 0.485321
Epoch 5.109: Loss = 0.426743
Epoch 5.110: Loss = 0.486771
Epoch 5.111: Loss = 0.405579
Epoch 5.112: Loss = 0.362625
Epoch 5.113: Loss = 0.479172
Epoch 5.114: Loss = 0.35762
Epoch 5.115: Loss = 0.398682
Epoch 5.116: Loss = 0.447067
Epoch 5.117: Loss = 0.316208
Epoch 5.118: Loss = 0.248154
Epoch 5.119: Loss = 0.345322
Epoch 5.120: Loss = 0.396942
TRAIN LOSS = 0.490433
TRAIN ACC = 86.0855 % (51654/60000)
Loss = 0.437836
Loss = 0.57312
Loss = 0.63945
Loss = 0.633759
Loss = 0.635117
Loss = 0.497253
Loss = 0.438828
Loss = 0.715561
Loss = 0.594345
Loss = 0.594147
Loss = 0.241425
Loss = 0.345428
Loss = 0.429794
Loss = 0.462463
Loss = 0.284256
Loss = 0.331894
Loss = 0.265152
Loss = 0.0822601
Loss = 0.252594
Loss = 0.55928
TEST LOSS = 0.450698
TEST ACC = 516.539 % (8710/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.382507
Epoch 6.2: Loss = 0.606323
Epoch 6.3: Loss = 0.611557
Epoch 6.4: Loss = 0.399872
Epoch 6.5: Loss = 0.367645
Epoch 6.6: Loss = 0.410126
Epoch 6.7: Loss = 0.444778
Epoch 6.8: Loss = 0.434235
Epoch 6.9: Loss = 0.424973
Epoch 6.10: Loss = 0.447357
Epoch 6.11: Loss = 0.4729
Epoch 6.12: Loss = 0.453049
Epoch 6.13: Loss = 0.397629
Epoch 6.14: Loss = 0.39122
Epoch 6.15: Loss = 0.525497
Epoch 6.16: Loss = 0.571121
Epoch 6.17: Loss = 0.590927
Epoch 6.18: Loss = 0.711899
Epoch 6.19: Loss = 0.479614
Epoch 6.20: Loss = 0.385437
Epoch 6.21: Loss = 0.374359
Epoch 6.22: Loss = 0.422638
Epoch 6.23: Loss = 0.423981
Epoch 6.24: Loss = 0.699585
Epoch 6.25: Loss = 0.519348
Epoch 6.26: Loss = 0.632919
Epoch 6.27: Loss = 0.608185
Epoch 6.28: Loss = 0.599854
Epoch 6.29: Loss = 0.611862
Epoch 6.30: Loss = 0.704666
Epoch 6.31: Loss = 0.484085
Epoch 6.32: Loss = 0.511749
Epoch 6.33: Loss = 0.439087
Epoch 6.34: Loss = 0.539352
Epoch 6.35: Loss = 0.521622
Epoch 6.36: Loss = 0.583878
Epoch 6.37: Loss = 0.36795
Epoch 6.38: Loss = 0.425583
Epoch 6.39: Loss = 0.484558
Epoch 6.40: Loss = 0.488022
Epoch 6.41: Loss = 0.456009
Epoch 6.42: Loss = 0.7164
Epoch 6.43: Loss = 0.403564
Epoch 6.44: Loss = 0.372162
Epoch 6.45: Loss = 0.427917
Epoch 6.46: Loss = 0.525009
Epoch 6.47: Loss = 0.493607
Epoch 6.48: Loss = 0.524902
Epoch 6.49: Loss = 0.4077
Epoch 6.50: Loss = 0.585831
Epoch 6.51: Loss = 0.483734
Epoch 6.52: Loss = 0.409927
Epoch 6.53: Loss = 0.540802
Epoch 6.54: Loss = 0.60376
Epoch 6.55: Loss = 0.540817
Epoch 6.56: Loss = 0.468643
Epoch 6.57: Loss = 0.449524
Epoch 6.58: Loss = 0.446671
Epoch 6.59: Loss = 0.618378
Epoch 6.60: Loss = 0.634109
Epoch 6.61: Loss = 0.555283
Epoch 6.62: Loss = 0.552444
Epoch 6.63: Loss = 0.695114
Epoch 6.64: Loss = 0.594818
Epoch 6.65: Loss = 0.788147
Epoch 6.66: Loss = 0.430908
Epoch 6.67: Loss = 0.46376
Epoch 6.68: Loss = 0.312866
Epoch 6.69: Loss = 0.377167
Epoch 6.70: Loss = 0.56102
Epoch 6.71: Loss = 0.446808
Epoch 6.72: Loss = 0.363617
Epoch 6.73: Loss = 0.559418
Epoch 6.74: Loss = 0.380356
Epoch 6.75: Loss = 0.732224
Epoch 6.76: Loss = 0.509903
Epoch 6.77: Loss = 0.441742
Epoch 6.78: Loss = 0.514465
Epoch 6.79: Loss = 0.567566
Epoch 6.80: Loss = 0.525528
Epoch 6.81: Loss = 0.421249
Epoch 6.82: Loss = 0.343552
Epoch 6.83: Loss = 0.621582
Epoch 6.84: Loss = 0.457703
Epoch 6.85: Loss = 0.677444
Epoch 6.86: Loss = 0.655853
Epoch 6.87: Loss = 0.377274
Epoch 6.88: Loss = 0.501419
Epoch 6.89: Loss = 0.553986
Epoch 6.90: Loss = 0.471085
Epoch 6.91: Loss = 0.593658
Epoch 6.92: Loss = 0.619934
Epoch 6.93: Loss = 0.684647
Epoch 6.94: Loss = 0.345306
Epoch 6.95: Loss = 0.549316
Epoch 6.96: Loss = 0.567261
Epoch 6.97: Loss = 0.356979
Epoch 6.98: Loss = 0.429001
Epoch 6.99: Loss = 0.550674
Epoch 6.100: Loss = 0.711533
Epoch 6.101: Loss = 0.629013
Epoch 6.102: Loss = 0.474869
Epoch 6.103: Loss = 0.507278
Epoch 6.104: Loss = 0.353851
Epoch 6.105: Loss = 0.618607
Epoch 6.106: Loss = 0.596588
Epoch 6.107: Loss = 0.385666
Epoch 6.108: Loss = 0.539932
Epoch 6.109: Loss = 0.425476
Epoch 6.110: Loss = 0.515045
Epoch 6.111: Loss = 0.427765
Epoch 6.112: Loss = 0.392059
Epoch 6.113: Loss = 0.49411
Epoch 6.114: Loss = 0.366318
Epoch 6.115: Loss = 0.459427
Epoch 6.116: Loss = 0.481079
Epoch 6.117: Loss = 0.358765
Epoch 6.118: Loss = 0.261154
Epoch 6.119: Loss = 0.371307
Epoch 6.120: Loss = 0.412354
TRAIN LOSS = 0.499954
TRAIN ACC = 86.2015 % (51723/60000)
Loss = 0.431641
Loss = 0.569092
Loss = 0.663422
Loss = 0.65593
Loss = 0.667587
Loss = 0.497925
Loss = 0.46579
Loss = 0.758469
Loss = 0.62973
Loss = 0.602356
Loss = 0.268311
Loss = 0.398331
Loss = 0.43718
Loss = 0.492874
Loss = 0.27684
Loss = 0.390503
Loss = 0.273529
Loss = 0.0950012
Loss = 0.273071
Loss = 0.696304
TEST LOSS = 0.477194
TEST ACC = 517.229 % (8707/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.395157
Epoch 7.2: Loss = 0.63739
Epoch 7.3: Loss = 0.620087
Epoch 7.4: Loss = 0.384003
Epoch 7.5: Loss = 0.403198
Epoch 7.6: Loss = 0.448975
Epoch 7.7: Loss = 0.461166
Epoch 7.8: Loss = 0.469986
Epoch 7.9: Loss = 0.475571
Epoch 7.10: Loss = 0.416306
Epoch 7.11: Loss = 0.483643
Epoch 7.12: Loss = 0.486069
Epoch 7.13: Loss = 0.349152
Epoch 7.14: Loss = 0.424393
Epoch 7.15: Loss = 0.555893
Epoch 7.16: Loss = 0.557327
Epoch 7.17: Loss = 0.591019
Epoch 7.18: Loss = 0.742157
Epoch 7.19: Loss = 0.512268
Epoch 7.20: Loss = 0.413315
Epoch 7.21: Loss = 0.439407
Epoch 7.22: Loss = 0.442841
Epoch 7.23: Loss = 0.429962
Epoch 7.24: Loss = 0.64534
Epoch 7.25: Loss = 0.537521
Epoch 7.26: Loss = 0.71904
Epoch 7.27: Loss = 0.6306
Epoch 7.28: Loss = 0.643005
Epoch 7.29: Loss = 0.639053
Epoch 7.30: Loss = 0.761673
Epoch 7.31: Loss = 0.508423
Epoch 7.32: Loss = 0.563644
Epoch 7.33: Loss = 0.463974
Epoch 7.34: Loss = 0.56424
Epoch 7.35: Loss = 0.561249
Epoch 7.36: Loss = 0.579391
Epoch 7.37: Loss = 0.379395
Epoch 7.38: Loss = 0.439987
Epoch 7.39: Loss = 0.500351
Epoch 7.40: Loss = 0.499756
Epoch 7.41: Loss = 0.501602
Epoch 7.42: Loss = 0.757355
Epoch 7.43: Loss = 0.438202
Epoch 7.44: Loss = 0.380402
Epoch 7.45: Loss = 0.493118
Epoch 7.46: Loss = 0.6203
Epoch 7.47: Loss = 0.498413
Epoch 7.48: Loss = 0.558609
Epoch 7.49: Loss = 0.440811
Epoch 7.50: Loss = 0.630859
Epoch 7.51: Loss = 0.448349
Epoch 7.52: Loss = 0.424438
Epoch 7.53: Loss = 0.553177
Epoch 7.54: Loss = 0.666138
Epoch 7.55: Loss = 0.568359
Epoch 7.56: Loss = 0.483612
Epoch 7.57: Loss = 0.468781
Epoch 7.58: Loss = 0.444794
Epoch 7.59: Loss = 0.650116
Epoch 7.60: Loss = 0.678467
Epoch 7.61: Loss = 0.521378
Epoch 7.62: Loss = 0.597687
Epoch 7.63: Loss = 0.747498
Epoch 7.64: Loss = 0.617233
Epoch 7.65: Loss = 0.798706
Epoch 7.66: Loss = 0.488388
Epoch 7.67: Loss = 0.509048
Epoch 7.68: Loss = 0.357819
Epoch 7.69: Loss = 0.401764
Epoch 7.70: Loss = 0.592941
Epoch 7.71: Loss = 0.498535
Epoch 7.72: Loss = 0.372559
Epoch 7.73: Loss = 0.62674
Epoch 7.74: Loss = 0.429001
Epoch 7.75: Loss = 0.83107
Epoch 7.76: Loss = 0.587784
Epoch 7.77: Loss = 0.486603
Epoch 7.78: Loss = 0.537354
Epoch 7.79: Loss = 0.637177
Epoch 7.80: Loss = 0.577652
Epoch 7.81: Loss = 0.46492
Epoch 7.82: Loss = 0.364151
Epoch 7.83: Loss = 0.644318
Epoch 7.84: Loss = 0.48616
Epoch 7.85: Loss = 0.699402
Epoch 7.86: Loss = 0.673187
Epoch 7.87: Loss = 0.413757
Epoch 7.88: Loss = 0.56427
Epoch 7.89: Loss = 0.55777
Epoch 7.90: Loss = 0.462418
Epoch 7.91: Loss = 0.592209
Epoch 7.92: Loss = 0.676132
Epoch 7.93: Loss = 0.699875
Epoch 7.94: Loss = 0.383026
Epoch 7.95: Loss = 0.645752
Epoch 7.96: Loss = 0.669571
Epoch 7.97: Loss = 0.386017
Epoch 7.98: Loss = 0.466339
Epoch 7.99: Loss = 0.62236
Epoch 7.100: Loss = 0.825089
Epoch 7.101: Loss = 0.681412
Epoch 7.102: Loss = 0.472672
Epoch 7.103: Loss = 0.555481
Epoch 7.104: Loss = 0.36377
Epoch 7.105: Loss = 0.656158
Epoch 7.106: Loss = 0.623947
Epoch 7.107: Loss = 0.399216
Epoch 7.108: Loss = 0.591019
Epoch 7.109: Loss = 0.467102
Epoch 7.110: Loss = 0.548965
Epoch 7.111: Loss = 0.447708
Epoch 7.112: Loss = 0.426346
Epoch 7.113: Loss = 0.560379
Epoch 7.114: Loss = 0.396667
Epoch 7.115: Loss = 0.513046
Epoch 7.116: Loss = 0.511841
Epoch 7.117: Loss = 0.383026
Epoch 7.118: Loss = 0.274658
Epoch 7.119: Loss = 0.383606
Epoch 7.120: Loss = 0.459427
TRAIN LOSS = 0.530945
TRAIN ACC = 86.0718 % (51645/60000)
Loss = 0.493637
Loss = 0.672134
Loss = 0.72641
Loss = 0.743866
Loss = 0.748077
Loss = 0.52269
Loss = 0.516159
Loss = 0.809952
Loss = 0.710663
Loss = 0.723907
Loss = 0.337173
Loss = 0.410034
Loss = 0.438568
Loss = 0.516815
Loss = 0.274017
Loss = 0.414307
Loss = 0.314224
Loss = 0.101166
Loss = 0.306885
Loss = 0.780121
TEST LOSS = 0.52804
TEST ACC = 516.449 % (8642/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.447662
Epoch 8.2: Loss = 0.630981
Epoch 8.3: Loss = 0.669601
Epoch 8.4: Loss = 0.42569
Epoch 8.5: Loss = 0.420303
Epoch 8.6: Loss = 0.453934
Epoch 8.7: Loss = 0.461243
Epoch 8.8: Loss = 0.476807
Epoch 8.9: Loss = 0.51001
Epoch 8.10: Loss = 0.442719
Epoch 8.11: Loss = 0.496552
Epoch 8.12: Loss = 0.522064
Epoch 8.13: Loss = 0.413422
Epoch 8.14: Loss = 0.472839
Epoch 8.15: Loss = 0.610336
Epoch 8.16: Loss = 0.636337
Epoch 8.17: Loss = 0.686752
Epoch 8.18: Loss = 0.756973
Epoch 8.19: Loss = 0.554855
Epoch 8.20: Loss = 0.451935
Epoch 8.21: Loss = 0.422455
Epoch 8.22: Loss = 0.463196
Epoch 8.23: Loss = 0.479172
Epoch 8.24: Loss = 0.605469
Epoch 8.25: Loss = 0.491089
Epoch 8.26: Loss = 0.76329
Epoch 8.27: Loss = 0.688629
Epoch 8.28: Loss = 0.677628
Epoch 8.29: Loss = 0.713058
Epoch 8.30: Loss = 0.915115
Epoch 8.31: Loss = 0.524872
Epoch 8.32: Loss = 0.574951
Epoch 8.33: Loss = 0.467834
Epoch 8.34: Loss = 0.594528
Epoch 8.35: Loss = 0.663422
Epoch 8.36: Loss = 0.62468
Epoch 8.37: Loss = 0.385513
Epoch 8.38: Loss = 0.48291
Epoch 8.39: Loss = 0.575134
Epoch 8.40: Loss = 0.559647
Epoch 8.41: Loss = 0.544662
Epoch 8.42: Loss = 0.842087
Epoch 8.43: Loss = 0.443863
Epoch 8.44: Loss = 0.41362
Epoch 8.45: Loss = 0.525284
Epoch 8.46: Loss = 0.722107
Epoch 8.47: Loss = 0.582489
Epoch 8.48: Loss = 0.603683
Epoch 8.49: Loss = 0.433899
Epoch 8.50: Loss = 0.654144
Epoch 8.51: Loss = 0.445435
Epoch 8.52: Loss = 0.42128
Epoch 8.53: Loss = 0.588638
Epoch 8.54: Loss = 0.760391
Epoch 8.55: Loss = 0.617615
Epoch 8.56: Loss = 0.557724
Epoch 8.57: Loss = 0.487137
Epoch 8.58: Loss = 0.514893
Epoch 8.59: Loss = 0.708084
Epoch 8.60: Loss = 0.698853
Epoch 8.61: Loss = 0.597229
Epoch 8.62: Loss = 0.707626
Epoch 8.63: Loss = 0.787704
Epoch 8.64: Loss = 0.702835
Epoch 8.65: Loss = 0.870377
Epoch 8.66: Loss = 0.53183
Epoch 8.67: Loss = 0.557678
Epoch 8.68: Loss = 0.380051
Epoch 8.69: Loss = 0.44519
Epoch 8.70: Loss = 0.644455
Epoch 8.71: Loss = 0.522293
Epoch 8.72: Loss = 0.407761
Epoch 8.73: Loss = 0.639023
Epoch 8.74: Loss = 0.44809
Epoch 8.75: Loss = 0.901932
Epoch 8.76: Loss = 0.624359
Epoch 8.77: Loss = 0.494629
Epoch 8.78: Loss = 0.528992
Epoch 8.79: Loss = 0.663162
Epoch 8.80: Loss = 0.618164
Epoch 8.81: Loss = 0.534317
Epoch 8.82: Loss = 0.440811
Epoch 8.83: Loss = 0.719299
Epoch 8.84: Loss = 0.564041
Epoch 8.85: Loss = 0.739685
Epoch 8.86: Loss = 0.768463
Epoch 8.87: Loss = 0.434891
Epoch 8.88: Loss = 0.591171
Epoch 8.89: Loss = 0.620361
Epoch 8.90: Loss = 0.505386
Epoch 8.91: Loss = 0.660324
Epoch 8.92: Loss = 0.716187
Epoch 8.93: Loss = 0.783539
Epoch 8.94: Loss = 0.456787
Epoch 8.95: Loss = 0.684189
Epoch 8.96: Loss = 0.607773
Epoch 8.97: Loss = 0.417145
Epoch 8.98: Loss = 0.4505
Epoch 8.99: Loss = 0.620972
Epoch 8.100: Loss = 0.978912
Epoch 8.101: Loss = 0.702637
Epoch 8.102: Loss = 0.613144
Epoch 8.103: Loss = 0.57634
Epoch 8.104: Loss = 0.368622
Epoch 8.105: Loss = 0.667725
Epoch 8.106: Loss = 0.651672
Epoch 8.107: Loss = 0.415619
Epoch 8.108: Loss = 0.643036
Epoch 8.109: Loss = 0.467667
Epoch 8.110: Loss = 0.584122
Epoch 8.111: Loss = 0.449783
Epoch 8.112: Loss = 0.426773
Epoch 8.113: Loss = 0.581192
Epoch 8.114: Loss = 0.385635
Epoch 8.115: Loss = 0.497238
Epoch 8.116: Loss = 0.552216
Epoch 8.117: Loss = 0.370407
Epoch 8.118: Loss = 0.312851
Epoch 8.119: Loss = 0.438095
Epoch 8.120: Loss = 0.466751
TRAIN LOSS = 0.569336
TRAIN ACC = 85.8978 % (51541/60000)
Loss = 0.514038
Loss = 0.683426
Loss = 0.787933
Loss = 0.749023
Loss = 0.742813
Loss = 0.548843
Loss = 0.536179
Loss = 0.843399
Loss = 0.735641
Loss = 0.711456
Loss = 0.319992
Loss = 0.487701
Loss = 0.45459
Loss = 0.521637
Loss = 0.300385
Loss = 0.38385
Loss = 0.32724
Loss = 0.120239
Loss = 0.320007
Loss = 0.813782
TEST LOSS = 0.545109
TEST ACC = 515.41 % (8633/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.506058
Epoch 9.2: Loss = 0.709869
Epoch 9.3: Loss = 0.649887
Epoch 9.4: Loss = 0.428711
Epoch 9.5: Loss = 0.441376
Epoch 9.6: Loss = 0.470596
Epoch 9.7: Loss = 0.508652
Epoch 9.8: Loss = 0.547302
Epoch 9.9: Loss = 0.474991
Epoch 9.10: Loss = 0.470428
Epoch 9.11: Loss = 0.460785
Epoch 9.12: Loss = 0.538193
Epoch 9.13: Loss = 0.409409
Epoch 9.14: Loss = 0.450516
Epoch 9.15: Loss = 0.648712
Epoch 9.16: Loss = 0.668884
Epoch 9.17: Loss = 0.743469
Epoch 9.18: Loss = 0.81218
Epoch 9.19: Loss = 0.565277
Epoch 9.20: Loss = 0.439362
Epoch 9.21: Loss = 0.458481
Epoch 9.22: Loss = 0.496964
Epoch 9.23: Loss = 0.443008
Epoch 9.24: Loss = 0.725708
Epoch 9.25: Loss = 0.529846
Epoch 9.26: Loss = 0.746979
Epoch 9.27: Loss = 0.651001
Epoch 9.28: Loss = 0.67366
Epoch 9.29: Loss = 0.720932
Epoch 9.30: Loss = 0.951828
Epoch 9.31: Loss = 0.520386
Epoch 9.32: Loss = 0.653961
Epoch 9.33: Loss = 0.514313
Epoch 9.34: Loss = 0.623886
Epoch 9.35: Loss = 0.661621
Epoch 9.36: Loss = 0.623688
Epoch 9.37: Loss = 0.414169
Epoch 9.38: Loss = 0.493988
Epoch 9.39: Loss = 0.575027
Epoch 9.40: Loss = 0.586273
Epoch 9.41: Loss = 0.547409
Epoch 9.42: Loss = 0.890503
Epoch 9.43: Loss = 0.435516
Epoch 9.44: Loss = 0.404709
Epoch 9.45: Loss = 0.557755
Epoch 9.46: Loss = 0.678818
Epoch 9.47: Loss = 0.562393
Epoch 9.48: Loss = 0.648056
Epoch 9.49: Loss = 0.501541
Epoch 9.50: Loss = 0.719986
Epoch 9.51: Loss = 0.473404
Epoch 9.52: Loss = 0.434143
Epoch 9.53: Loss = 0.583099
Epoch 9.54: Loss = 0.73642
Epoch 9.55: Loss = 0.534393
Epoch 9.56: Loss = 0.579056
Epoch 9.57: Loss = 0.491684
Epoch 9.58: Loss = 0.509796
Epoch 9.59: Loss = 0.715347
Epoch 9.60: Loss = 0.690582
Epoch 9.61: Loss = 0.64566
Epoch 9.62: Loss = 0.634995
Epoch 9.63: Loss = 0.825562
Epoch 9.64: Loss = 0.722153
Epoch 9.65: Loss = 0.861084
Epoch 9.66: Loss = 0.538986
Epoch 9.67: Loss = 0.554306
Epoch 9.68: Loss = 0.38327
Epoch 9.69: Loss = 0.461731
Epoch 9.70: Loss = 0.743332
Epoch 9.71: Loss = 0.51123
Epoch 9.72: Loss = 0.426956
Epoch 9.73: Loss = 0.630753
Epoch 9.74: Loss = 0.469116
Epoch 9.75: Loss = 0.808075
Epoch 9.76: Loss = 0.632095
Epoch 9.77: Loss = 0.478424
Epoch 9.78: Loss = 0.536118
Epoch 9.79: Loss = 0.676498
Epoch 9.80: Loss = 0.63942
Epoch 9.81: Loss = 0.464447
Epoch 9.82: Loss = 0.426788
Epoch 9.83: Loss = 0.728378
Epoch 9.84: Loss = 0.550491
Epoch 9.85: Loss = 0.725891
Epoch 9.86: Loss = 0.74852
Epoch 9.87: Loss = 0.413406
Epoch 9.88: Loss = 0.566422
Epoch 9.89: Loss = 0.629242
Epoch 9.90: Loss = 0.474396
Epoch 9.91: Loss = 0.681747
Epoch 9.92: Loss = 0.676147
Epoch 9.93: Loss = 0.791931
Epoch 9.94: Loss = 0.420761
Epoch 9.95: Loss = 0.645096
Epoch 9.96: Loss = 0.630142
Epoch 9.97: Loss = 0.405609
Epoch 9.98: Loss = 0.447525
Epoch 9.99: Loss = 0.615402
Epoch 9.100: Loss = 0.937988
Epoch 9.101: Loss = 0.739761
Epoch 9.102: Loss = 0.529541
Epoch 9.103: Loss = 0.534134
Epoch 9.104: Loss = 0.357437
Epoch 9.105: Loss = 0.723068
Epoch 9.106: Loss = 0.677094
Epoch 9.107: Loss = 0.445038
Epoch 9.108: Loss = 0.681961
Epoch 9.109: Loss = 0.437531
Epoch 9.110: Loss = 0.637527
Epoch 9.111: Loss = 0.478531
Epoch 9.112: Loss = 0.405991
Epoch 9.113: Loss = 0.615005
Epoch 9.114: Loss = 0.404449
Epoch 9.115: Loss = 0.525024
Epoch 9.116: Loss = 0.545502
Epoch 9.117: Loss = 0.354935
Epoch 9.118: Loss = 0.28064
Epoch 9.119: Loss = 0.413818
Epoch 9.120: Loss = 0.449646
TRAIN LOSS = 0.575867
TRAIN ACC = 86.467 % (51882/60000)
Loss = 0.58905
Loss = 0.730682
Loss = 0.861404
Loss = 0.736359
Loss = 0.804214
Loss = 0.601974
Loss = 0.576675
Loss = 0.908279
Loss = 0.780457
Loss = 0.76767
Loss = 0.295563
Loss = 0.588364
Loss = 0.475464
Loss = 0.553391
Loss = 0.316879
Loss = 0.406357
Loss = 0.312485
Loss = 0.106598
Loss = 0.32019
Loss = 0.824005
TEST LOSS = 0.577803
TEST ACC = 518.819 % (8661/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.513412
Epoch 10.2: Loss = 0.743759
Epoch 10.3: Loss = 0.671722
Epoch 10.4: Loss = 0.401184
Epoch 10.5: Loss = 0.488281
Epoch 10.6: Loss = 0.459549
Epoch 10.7: Loss = 0.504684
Epoch 10.8: Loss = 0.560654
Epoch 10.9: Loss = 0.507339
Epoch 10.10: Loss = 0.483978
Epoch 10.11: Loss = 0.508026
Epoch 10.12: Loss = 0.53772
Epoch 10.13: Loss = 0.412292
Epoch 10.14: Loss = 0.488083
Epoch 10.15: Loss = 0.636429
Epoch 10.16: Loss = 0.642227
Epoch 10.17: Loss = 0.759918
Epoch 10.18: Loss = 0.896698
Epoch 10.19: Loss = 0.611832
Epoch 10.20: Loss = 0.451309
Epoch 10.21: Loss = 0.503784
Epoch 10.22: Loss = 0.489288
Epoch 10.23: Loss = 0.494553
Epoch 10.24: Loss = 0.800262
Epoch 10.25: Loss = 0.590927
Epoch 10.26: Loss = 0.82164
Epoch 10.27: Loss = 0.688675
Epoch 10.28: Loss = 0.720581
Epoch 10.29: Loss = 0.766174
Epoch 10.30: Loss = 0.948868
Epoch 10.31: Loss = 0.567673
Epoch 10.32: Loss = 0.666183
Epoch 10.33: Loss = 0.503799
Epoch 10.34: Loss = 0.661255
Epoch 10.35: Loss = 0.692963
Epoch 10.36: Loss = 0.660507
Epoch 10.37: Loss = 0.437668
Epoch 10.38: Loss = 0.44223
Epoch 10.39: Loss = 0.548126
Epoch 10.40: Loss = 0.511459
Epoch 10.41: Loss = 0.587769
Epoch 10.42: Loss = 0.919556
Epoch 10.43: Loss = 0.492538
Epoch 10.44: Loss = 0.41481
Epoch 10.45: Loss = 0.577194
Epoch 10.46: Loss = 0.691498
Epoch 10.47: Loss = 0.606781
Epoch 10.48: Loss = 0.731537
Epoch 10.49: Loss = 0.55275
Epoch 10.50: Loss = 0.710571
Epoch 10.51: Loss = 0.508469
Epoch 10.52: Loss = 0.469254
Epoch 10.53: Loss = 0.576523
Epoch 10.54: Loss = 0.710831
Epoch 10.55: Loss = 0.617935
Epoch 10.56: Loss = 0.571167
Epoch 10.57: Loss = 0.523422
Epoch 10.58: Loss = 0.52356
Epoch 10.59: Loss = 0.715103
Epoch 10.60: Loss = 0.689621
Epoch 10.61: Loss = 0.71701
Epoch 10.62: Loss = 0.663086
Epoch 10.63: Loss = 0.872696
Epoch 10.64: Loss = 0.78476
Epoch 10.65: Loss = 0.81366
Epoch 10.66: Loss = 0.574219
Epoch 10.67: Loss = 0.621353
Epoch 10.68: Loss = 0.377884
Epoch 10.69: Loss = 0.528625
Epoch 10.70: Loss = 0.786194
Epoch 10.71: Loss = 0.535645
Epoch 10.72: Loss = 0.408569
Epoch 10.73: Loss = 0.64917
Epoch 10.74: Loss = 0.472443
Epoch 10.75: Loss = 0.929123
Epoch 10.76: Loss = 0.637741
Epoch 10.77: Loss = 0.523712
Epoch 10.78: Loss = 0.548706
Epoch 10.79: Loss = 0.714294
Epoch 10.80: Loss = 0.674957
Epoch 10.81: Loss = 0.421509
Epoch 10.82: Loss = 0.425003
Epoch 10.83: Loss = 0.796555
Epoch 10.84: Loss = 0.575485
Epoch 10.85: Loss = 0.817596
Epoch 10.86: Loss = 0.755951
Epoch 10.87: Loss = 0.416656
Epoch 10.88: Loss = 0.579453
Epoch 10.89: Loss = 0.666656
Epoch 10.90: Loss = 0.466522
Epoch 10.91: Loss = 0.714966
Epoch 10.92: Loss = 0.70282
Epoch 10.93: Loss = 0.832001
Epoch 10.94: Loss = 0.440918
Epoch 10.95: Loss = 0.703644
Epoch 10.96: Loss = 0.646332
Epoch 10.97: Loss = 0.46524
Epoch 10.98: Loss = 0.516815
Epoch 10.99: Loss = 0.680573
Epoch 10.100: Loss = 0.948746
Epoch 10.101: Loss = 0.812561
Epoch 10.102: Loss = 0.54335
Epoch 10.103: Loss = 0.482132
Epoch 10.104: Loss = 0.389618
Epoch 10.105: Loss = 0.802765
Epoch 10.106: Loss = 0.739975
Epoch 10.107: Loss = 0.485153
Epoch 10.108: Loss = 0.717819
Epoch 10.109: Loss = 0.493393
Epoch 10.110: Loss = 0.638229
Epoch 10.111: Loss = 0.428299
Epoch 10.112: Loss = 0.428848
Epoch 10.113: Loss = 0.569901
Epoch 10.114: Loss = 0.468735
Epoch 10.115: Loss = 0.563385
Epoch 10.116: Loss = 0.530899
Epoch 10.117: Loss = 0.382172
Epoch 10.118: Loss = 0.27034
Epoch 10.119: Loss = 0.40773
Epoch 10.120: Loss = 0.48793
TRAIN LOSS = 0.600327
TRAIN ACC = 86.467 % (51882/60000)
Loss = 0.588928
Loss = 0.791183
Loss = 0.864609
Loss = 0.816666
Loss = 0.889511
Loss = 0.5858
Loss = 0.600769
Loss = 0.96106
Loss = 0.815399
Loss = 0.756104
Loss = 0.28653
Loss = 0.603821
Loss = 0.530472
Loss = 0.572189
Loss = 0.385544
Loss = 0.510834
Loss = 0.36438
Loss = 0.089035
Loss = 0.321945
Loss = 0.802032
TEST LOSS = 0.60684
TEST ACC = 518.819 % (8659/10000)
