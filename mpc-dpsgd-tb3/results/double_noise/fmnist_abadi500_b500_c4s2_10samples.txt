Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 500]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.48254
Epoch 1.2: Loss = 2.36171
Epoch 1.3: Loss = 2.28278
Epoch 1.4: Loss = 2.16348
Epoch 1.5: Loss = 2.07071
Epoch 1.6: Loss = 1.98502
Epoch 1.7: Loss = 1.88443
Epoch 1.8: Loss = 1.85477
Epoch 1.9: Loss = 1.81081
Epoch 1.10: Loss = 1.70761
Epoch 1.11: Loss = 1.69312
Epoch 1.12: Loss = 1.64343
Epoch 1.13: Loss = 1.59183
Epoch 1.14: Loss = 1.55019
Epoch 1.15: Loss = 1.48965
Epoch 1.16: Loss = 1.51485
Epoch 1.17: Loss = 1.44513
Epoch 1.18: Loss = 1.37991
Epoch 1.19: Loss = 1.34351
Epoch 1.20: Loss = 1.37532
Epoch 1.21: Loss = 1.30745
Epoch 1.22: Loss = 1.27295
Epoch 1.23: Loss = 1.27692
Epoch 1.24: Loss = 1.34552
Epoch 1.25: Loss = 1.27338
Epoch 1.26: Loss = 1.19902
Epoch 1.27: Loss = 1.16153
Epoch 1.28: Loss = 1.17508
Epoch 1.29: Loss = 1.16187
Epoch 1.30: Loss = 1.1279
Epoch 1.31: Loss = 1.15854
Epoch 1.32: Loss = 1.11845
Epoch 1.33: Loss = 1.03136
Epoch 1.34: Loss = 1.12148
Epoch 1.35: Loss = 1.13315
Epoch 1.36: Loss = 1.10698
Epoch 1.37: Loss = 1.07892
Epoch 1.38: Loss = 1.0526
Epoch 1.39: Loss = 1.05107
Epoch 1.40: Loss = 1.02834
Epoch 1.41: Loss = 1.0665
Epoch 1.42: Loss = 1.03531
Epoch 1.43: Loss = 0.994843
Epoch 1.44: Loss = 0.959915
Epoch 1.45: Loss = 1.00121
Epoch 1.46: Loss = 0.994385
Epoch 1.47: Loss = 0.949921
Epoch 1.48: Loss = 0.935455
Epoch 1.49: Loss = 0.982071
Epoch 1.50: Loss = 0.908173
Epoch 1.51: Loss = 0.892578
Epoch 1.52: Loss = 0.972412
Epoch 1.53: Loss = 0.979614
Epoch 1.54: Loss = 0.830963
Epoch 1.55: Loss = 0.944
Epoch 1.56: Loss = 0.939163
Epoch 1.57: Loss = 0.937698
Epoch 1.58: Loss = 0.917053
Epoch 1.59: Loss = 0.90744
Epoch 1.60: Loss = 0.955978
Epoch 1.61: Loss = 0.824951
Epoch 1.62: Loss = 0.918152
Epoch 1.63: Loss = 0.772766
Epoch 1.64: Loss = 0.821487
Epoch 1.65: Loss = 0.823441
Epoch 1.66: Loss = 0.871078
Epoch 1.67: Loss = 0.801743
Epoch 1.68: Loss = 0.909378
Epoch 1.69: Loss = 0.884033
Epoch 1.70: Loss = 0.855057
Epoch 1.71: Loss = 0.769257
Epoch 1.72: Loss = 0.79689
Epoch 1.73: Loss = 0.890106
Epoch 1.74: Loss = 0.865387
Epoch 1.75: Loss = 0.803528
Epoch 1.76: Loss = 0.812866
Epoch 1.77: Loss = 0.799927
Epoch 1.78: Loss = 0.839325
Epoch 1.79: Loss = 0.771317
Epoch 1.80: Loss = 0.831177
Epoch 1.81: Loss = 0.765213
Epoch 1.82: Loss = 0.797684
Epoch 1.83: Loss = 0.847153
Epoch 1.84: Loss = 0.786163
Epoch 1.85: Loss = 0.774094
Epoch 1.86: Loss = 0.836258
Epoch 1.87: Loss = 0.85611
Epoch 1.88: Loss = 0.710663
Epoch 1.89: Loss = 0.849533
Epoch 1.90: Loss = 0.768829
Epoch 1.91: Loss = 0.859009
Epoch 1.92: Loss = 0.793884
Epoch 1.93: Loss = 0.815643
Epoch 1.94: Loss = 0.801697
Epoch 1.95: Loss = 0.820923
Epoch 1.96: Loss = 0.764481
Epoch 1.97: Loss = 0.677734
Epoch 1.98: Loss = 0.769226
Epoch 1.99: Loss = 0.776886
Epoch 1.100: Loss = 0.741119
Epoch 1.101: Loss = 0.810715
Epoch 1.102: Loss = 0.799072
Epoch 1.103: Loss = 0.784027
Epoch 1.104: Loss = 0.78035
Epoch 1.105: Loss = 0.703018
Epoch 1.106: Loss = 0.850632
Epoch 1.107: Loss = 0.788254
Epoch 1.108: Loss = 0.783798
Epoch 1.109: Loss = 0.757782
Epoch 1.110: Loss = 0.790344
Epoch 1.111: Loss = 0.706131
Epoch 1.112: Loss = 0.698196
Epoch 1.113: Loss = 0.785492
Epoch 1.114: Loss = 0.741699
Epoch 1.115: Loss = 0.745407
Epoch 1.116: Loss = 0.683792
Epoch 1.117: Loss = 0.803696
Epoch 1.118: Loss = 0.693283
Epoch 1.119: Loss = 0.763641
Epoch 1.120: Loss = 0.712738
TRAIN LOSS = 1.04646
TRAIN ACC = 65.6647 % (39401/60000)
Loss = 0.69754
Loss = 0.834625
Loss = 0.802505
Loss = 0.683258
Loss = 0.711639
Loss = 0.879105
Loss = 0.875275
Loss = 0.851532
Loss = 0.766174
Loss = 0.71698
Loss = 0.835464
Loss = 0.818054
Loss = 0.782379
Loss = 0.790649
Loss = 0.744034
Loss = 0.798706
Loss = 0.723648
Loss = 0.77536
Loss = 0.865295
Loss = 0.756424
TEST LOSS = 0.785432
TEST ACC = 394.009 % (7246/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.777679
Epoch 2.2: Loss = 0.750977
Epoch 2.3: Loss = 0.795624
Epoch 2.4: Loss = 0.692596
Epoch 2.5: Loss = 0.744354
Epoch 2.6: Loss = 0.833298
Epoch 2.7: Loss = 0.74646
Epoch 2.8: Loss = 0.792557
Epoch 2.9: Loss = 0.695297
Epoch 2.10: Loss = 0.583252
Epoch 2.11: Loss = 0.842621
Epoch 2.12: Loss = 0.769623
Epoch 2.13: Loss = 0.740646
Epoch 2.14: Loss = 0.715042
Epoch 2.15: Loss = 0.70166
Epoch 2.16: Loss = 0.774765
Epoch 2.17: Loss = 0.667145
Epoch 2.18: Loss = 0.716522
Epoch 2.19: Loss = 0.706619
Epoch 2.20: Loss = 0.853928
Epoch 2.21: Loss = 0.689743
Epoch 2.22: Loss = 0.640945
Epoch 2.23: Loss = 0.723999
Epoch 2.24: Loss = 0.840088
Epoch 2.25: Loss = 0.715027
Epoch 2.26: Loss = 0.637054
Epoch 2.27: Loss = 0.672211
Epoch 2.28: Loss = 0.74501
Epoch 2.29: Loss = 0.739639
Epoch 2.30: Loss = 0.695755
Epoch 2.31: Loss = 0.745102
Epoch 2.32: Loss = 0.707703
Epoch 2.33: Loss = 0.614792
Epoch 2.34: Loss = 0.805817
Epoch 2.35: Loss = 0.760513
Epoch 2.36: Loss = 0.797028
Epoch 2.37: Loss = 0.740158
Epoch 2.38: Loss = 0.729126
Epoch 2.39: Loss = 0.775452
Epoch 2.40: Loss = 0.712631
Epoch 2.41: Loss = 0.769699
Epoch 2.42: Loss = 0.750198
Epoch 2.43: Loss = 0.729507
Epoch 2.44: Loss = 0.648987
Epoch 2.45: Loss = 0.726624
Epoch 2.46: Loss = 0.813995
Epoch 2.47: Loss = 0.690353
Epoch 2.48: Loss = 0.692673
Epoch 2.49: Loss = 0.777908
Epoch 2.50: Loss = 0.706421
Epoch 2.51: Loss = 0.613098
Epoch 2.52: Loss = 0.798523
Epoch 2.53: Loss = 0.817078
Epoch 2.54: Loss = 0.638962
Epoch 2.55: Loss = 0.782425
Epoch 2.56: Loss = 0.761841
Epoch 2.57: Loss = 0.768967
Epoch 2.58: Loss = 0.733261
Epoch 2.59: Loss = 0.739655
Epoch 2.60: Loss = 0.788177
Epoch 2.61: Loss = 0.650696
Epoch 2.62: Loss = 0.786362
Epoch 2.63: Loss = 0.623932
Epoch 2.64: Loss = 0.636963
Epoch 2.65: Loss = 0.715073
Epoch 2.66: Loss = 0.709381
Epoch 2.67: Loss = 0.694214
Epoch 2.68: Loss = 0.822083
Epoch 2.69: Loss = 0.749542
Epoch 2.70: Loss = 0.763748
Epoch 2.71: Loss = 0.625198
Epoch 2.72: Loss = 0.710587
Epoch 2.73: Loss = 0.801392
Epoch 2.74: Loss = 0.7742
Epoch 2.75: Loss = 0.704453
Epoch 2.76: Loss = 0.71167
Epoch 2.77: Loss = 0.697342
Epoch 2.78: Loss = 0.732925
Epoch 2.79: Loss = 0.692719
Epoch 2.80: Loss = 0.682358
Epoch 2.81: Loss = 0.689468
Epoch 2.82: Loss = 0.674408
Epoch 2.83: Loss = 0.836319
Epoch 2.84: Loss = 0.680374
Epoch 2.85: Loss = 0.715378
Epoch 2.86: Loss = 0.783493
Epoch 2.87: Loss = 0.746887
Epoch 2.88: Loss = 0.613968
Epoch 2.89: Loss = 0.812592
Epoch 2.90: Loss = 0.708557
Epoch 2.91: Loss = 0.794403
Epoch 2.92: Loss = 0.735992
Epoch 2.93: Loss = 0.768539
Epoch 2.94: Loss = 0.740372
Epoch 2.95: Loss = 0.731598
Epoch 2.96: Loss = 0.720947
Epoch 2.97: Loss = 0.633301
Epoch 2.98: Loss = 0.732086
Epoch 2.99: Loss = 0.707855
Epoch 2.100: Loss = 0.702591
Epoch 2.101: Loss = 0.78331
Epoch 2.102: Loss = 0.726196
Epoch 2.103: Loss = 0.719452
Epoch 2.104: Loss = 0.680145
Epoch 2.105: Loss = 0.659775
Epoch 2.106: Loss = 0.852661
Epoch 2.107: Loss = 0.747986
Epoch 2.108: Loss = 0.775497
Epoch 2.109: Loss = 0.745087
Epoch 2.110: Loss = 0.679398
Epoch 2.111: Loss = 0.655701
Epoch 2.112: Loss = 0.681168
Epoch 2.113: Loss = 0.712067
Epoch 2.114: Loss = 0.729019
Epoch 2.115: Loss = 0.712936
Epoch 2.116: Loss = 0.640167
Epoch 2.117: Loss = 0.771103
Epoch 2.118: Loss = 0.682266
Epoch 2.119: Loss = 0.733994
Epoch 2.120: Loss = 0.663422
TRAIN LOSS = 0.727539
TRAIN ACC = 75.5783 % (45349/60000)
Loss = 0.700287
Loss = 0.827301
Loss = 0.779022
Loss = 0.656845
Loss = 0.677155
Loss = 0.835999
Loss = 0.889069
Loss = 0.843185
Loss = 0.765305
Loss = 0.711273
Loss = 0.838669
Loss = 0.856171
Loss = 0.789368
Loss = 0.816772
Loss = 0.764755
Loss = 0.79332
Loss = 0.704788
Loss = 0.806488
Loss = 0.855423
Loss = 0.737808
TEST LOSS = 0.78245
TEST ACC = 453.49 % (7515/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.742355
Epoch 3.2: Loss = 0.74437
Epoch 3.3: Loss = 0.75032
Epoch 3.4: Loss = 0.628571
Epoch 3.5: Loss = 0.717178
Epoch 3.6: Loss = 0.787079
Epoch 3.7: Loss = 0.700928
Epoch 3.8: Loss = 0.792023
Epoch 3.9: Loss = 0.608871
Epoch 3.10: Loss = 0.542786
Epoch 3.11: Loss = 0.824951
Epoch 3.12: Loss = 0.739319
Epoch 3.13: Loss = 0.737289
Epoch 3.14: Loss = 0.682053
Epoch 3.15: Loss = 0.760788
Epoch 3.16: Loss = 0.770462
Epoch 3.17: Loss = 0.676636
Epoch 3.18: Loss = 0.721863
Epoch 3.19: Loss = 0.711411
Epoch 3.20: Loss = 0.839401
Epoch 3.21: Loss = 0.637756
Epoch 3.22: Loss = 0.600616
Epoch 3.23: Loss = 0.743347
Epoch 3.24: Loss = 0.83284
Epoch 3.25: Loss = 0.651566
Epoch 3.26: Loss = 0.583786
Epoch 3.27: Loss = 0.688873
Epoch 3.28: Loss = 0.701111
Epoch 3.29: Loss = 0.747894
Epoch 3.30: Loss = 0.692902
Epoch 3.31: Loss = 0.779358
Epoch 3.32: Loss = 0.666763
Epoch 3.33: Loss = 0.612595
Epoch 3.34: Loss = 0.799759
Epoch 3.35: Loss = 0.702911
Epoch 3.36: Loss = 0.810577
Epoch 3.37: Loss = 0.733643
Epoch 3.38: Loss = 0.690247
Epoch 3.39: Loss = 0.771011
Epoch 3.40: Loss = 0.676102
Epoch 3.41: Loss = 0.720062
Epoch 3.42: Loss = 0.680969
Epoch 3.43: Loss = 0.763962
Epoch 3.44: Loss = 0.627136
Epoch 3.45: Loss = 0.691162
Epoch 3.46: Loss = 0.810623
Epoch 3.47: Loss = 0.628891
Epoch 3.48: Loss = 0.640961
Epoch 3.49: Loss = 0.765472
Epoch 3.50: Loss = 0.700409
Epoch 3.51: Loss = 0.578186
Epoch 3.52: Loss = 0.806854
Epoch 3.53: Loss = 0.898438
Epoch 3.54: Loss = 0.608322
Epoch 3.55: Loss = 0.744919
Epoch 3.56: Loss = 0.768127
Epoch 3.57: Loss = 0.785797
Epoch 3.58: Loss = 0.706146
Epoch 3.59: Loss = 0.788116
Epoch 3.60: Loss = 0.752625
Epoch 3.61: Loss = 0.691345
Epoch 3.62: Loss = 0.763062
Epoch 3.63: Loss = 0.608047
Epoch 3.64: Loss = 0.577591
Epoch 3.65: Loss = 0.705841
Epoch 3.66: Loss = 0.709244
Epoch 3.67: Loss = 0.707321
Epoch 3.68: Loss = 0.850891
Epoch 3.69: Loss = 0.729919
Epoch 3.70: Loss = 0.793091
Epoch 3.71: Loss = 0.619446
Epoch 3.72: Loss = 0.7547
Epoch 3.73: Loss = 0.839767
Epoch 3.74: Loss = 0.775009
Epoch 3.75: Loss = 0.687363
Epoch 3.76: Loss = 0.693481
Epoch 3.77: Loss = 0.676254
Epoch 3.78: Loss = 0.781219
Epoch 3.79: Loss = 0.711105
Epoch 3.80: Loss = 0.712524
Epoch 3.81: Loss = 0.653107
Epoch 3.82: Loss = 0.690079
Epoch 3.83: Loss = 0.760727
Epoch 3.84: Loss = 0.667267
Epoch 3.85: Loss = 0.685318
Epoch 3.86: Loss = 0.771225
Epoch 3.87: Loss = 0.739822
Epoch 3.88: Loss = 0.640884
Epoch 3.89: Loss = 0.793869
Epoch 3.90: Loss = 0.726151
Epoch 3.91: Loss = 0.798065
Epoch 3.92: Loss = 0.698425
Epoch 3.93: Loss = 0.739136
Epoch 3.94: Loss = 0.742371
Epoch 3.95: Loss = 0.728989
Epoch 3.96: Loss = 0.736359
Epoch 3.97: Loss = 0.60704
Epoch 3.98: Loss = 0.751602
Epoch 3.99: Loss = 0.725433
Epoch 3.100: Loss = 0.719635
Epoch 3.101: Loss = 0.762726
Epoch 3.102: Loss = 0.733337
Epoch 3.103: Loss = 0.723846
Epoch 3.104: Loss = 0.689331
Epoch 3.105: Loss = 0.655136
Epoch 3.106: Loss = 0.793442
Epoch 3.107: Loss = 0.784271
Epoch 3.108: Loss = 0.788498
Epoch 3.109: Loss = 0.796112
Epoch 3.110: Loss = 0.73732
Epoch 3.111: Loss = 0.654373
Epoch 3.112: Loss = 0.689148
Epoch 3.113: Loss = 0.683884
Epoch 3.114: Loss = 0.759857
Epoch 3.115: Loss = 0.764709
Epoch 3.116: Loss = 0.684601
Epoch 3.117: Loss = 0.806931
Epoch 3.118: Loss = 0.655853
Epoch 3.119: Loss = 0.780014
Epoch 3.120: Loss = 0.671616
TRAIN LOSS = 0.720413
TRAIN ACC = 77.0584 % (46237/60000)
Loss = 0.666824
Loss = 0.819427
Loss = 0.748932
Loss = 0.616501
Loss = 0.641998
Loss = 0.85527
Loss = 0.902725
Loss = 0.853851
Loss = 0.753113
Loss = 0.697418
Loss = 0.898544
Loss = 0.860718
Loss = 0.755051
Loss = 0.755127
Loss = 0.769806
Loss = 0.805771
Loss = 0.729874
Loss = 0.783142
Loss = 0.861862
Loss = 0.746689
TEST LOSS = 0.776132
TEST ACC = 462.369 % (7589/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.72905
Epoch 4.2: Loss = 0.700592
Epoch 4.3: Loss = 0.769913
Epoch 4.4: Loss = 0.658737
Epoch 4.5: Loss = 0.752213
Epoch 4.6: Loss = 0.789276
Epoch 4.7: Loss = 0.729935
Epoch 4.8: Loss = 0.848022
Epoch 4.9: Loss = 0.632568
Epoch 4.10: Loss = 0.568726
Epoch 4.11: Loss = 0.846924
Epoch 4.12: Loss = 0.758499
Epoch 4.13: Loss = 0.79454
Epoch 4.14: Loss = 0.674576
Epoch 4.15: Loss = 0.783997
Epoch 4.16: Loss = 0.766113
Epoch 4.17: Loss = 0.679031
Epoch 4.18: Loss = 0.831375
Epoch 4.19: Loss = 0.706879
Epoch 4.20: Loss = 0.839813
Epoch 4.21: Loss = 0.657974
Epoch 4.22: Loss = 0.592651
Epoch 4.23: Loss = 0.73378
Epoch 4.24: Loss = 0.83197
Epoch 4.25: Loss = 0.695892
Epoch 4.26: Loss = 0.608383
Epoch 4.27: Loss = 0.679596
Epoch 4.28: Loss = 0.741165
Epoch 4.29: Loss = 0.746811
Epoch 4.30: Loss = 0.694214
Epoch 4.31: Loss = 0.761246
Epoch 4.32: Loss = 0.713211
Epoch 4.33: Loss = 0.6082
Epoch 4.34: Loss = 0.834076
Epoch 4.35: Loss = 0.744568
Epoch 4.36: Loss = 0.824478
Epoch 4.37: Loss = 0.778381
Epoch 4.38: Loss = 0.781479
Epoch 4.39: Loss = 0.811859
Epoch 4.40: Loss = 0.787827
Epoch 4.41: Loss = 0.746002
Epoch 4.42: Loss = 0.763901
Epoch 4.43: Loss = 0.753311
Epoch 4.44: Loss = 0.624023
Epoch 4.45: Loss = 0.702255
Epoch 4.46: Loss = 0.81218
Epoch 4.47: Loss = 0.703384
Epoch 4.48: Loss = 0.645325
Epoch 4.49: Loss = 0.766495
Epoch 4.50: Loss = 0.681061
Epoch 4.51: Loss = 0.56868
Epoch 4.52: Loss = 0.79512
Epoch 4.53: Loss = 0.874023
Epoch 4.54: Loss = 0.619598
Epoch 4.55: Loss = 0.710236
Epoch 4.56: Loss = 0.781128
Epoch 4.57: Loss = 0.776413
Epoch 4.58: Loss = 0.731781
Epoch 4.59: Loss = 0.799927
Epoch 4.60: Loss = 0.777634
Epoch 4.61: Loss = 0.67244
Epoch 4.62: Loss = 0.824448
Epoch 4.63: Loss = 0.636963
Epoch 4.64: Loss = 0.637161
Epoch 4.65: Loss = 0.765884
Epoch 4.66: Loss = 0.67215
Epoch 4.67: Loss = 0.686264
Epoch 4.68: Loss = 0.869156
Epoch 4.69: Loss = 0.74852
Epoch 4.70: Loss = 0.757828
Epoch 4.71: Loss = 0.61882
Epoch 4.72: Loss = 0.734604
Epoch 4.73: Loss = 0.849869
Epoch 4.74: Loss = 0.721161
Epoch 4.75: Loss = 0.704971
Epoch 4.76: Loss = 0.730194
Epoch 4.77: Loss = 0.684799
Epoch 4.78: Loss = 0.781799
Epoch 4.79: Loss = 0.712494
Epoch 4.80: Loss = 0.687164
Epoch 4.81: Loss = 0.678802
Epoch 4.82: Loss = 0.674164
Epoch 4.83: Loss = 0.789978
Epoch 4.84: Loss = 0.6763
Epoch 4.85: Loss = 0.704971
Epoch 4.86: Loss = 0.785995
Epoch 4.87: Loss = 0.748276
Epoch 4.88: Loss = 0.68576
Epoch 4.89: Loss = 0.814453
Epoch 4.90: Loss = 0.736847
Epoch 4.91: Loss = 0.806046
Epoch 4.92: Loss = 0.711365
Epoch 4.93: Loss = 0.779892
Epoch 4.94: Loss = 0.744827
Epoch 4.95: Loss = 0.774689
Epoch 4.96: Loss = 0.726395
Epoch 4.97: Loss = 0.633392
Epoch 4.98: Loss = 0.713211
Epoch 4.99: Loss = 0.70195
Epoch 4.100: Loss = 0.720139
Epoch 4.101: Loss = 0.755478
Epoch 4.102: Loss = 0.781174
Epoch 4.103: Loss = 0.723618
Epoch 4.104: Loss = 0.64212
Epoch 4.105: Loss = 0.703583
Epoch 4.106: Loss = 0.818924
Epoch 4.107: Loss = 0.728394
Epoch 4.108: Loss = 0.812195
Epoch 4.109: Loss = 0.781067
Epoch 4.110: Loss = 0.777145
Epoch 4.111: Loss = 0.72818
Epoch 4.112: Loss = 0.725037
Epoch 4.113: Loss = 0.687027
Epoch 4.114: Loss = 0.798203
Epoch 4.115: Loss = 0.747925
Epoch 4.116: Loss = 0.61673
Epoch 4.117: Loss = 0.785553
Epoch 4.118: Loss = 0.621109
Epoch 4.119: Loss = 0.741409
Epoch 4.120: Loss = 0.655289
TRAIN LOSS = 0.732147
TRAIN ACC = 77.9297 % (46760/60000)
Loss = 0.659775
Loss = 0.791061
Loss = 0.726852
Loss = 0.588455
Loss = 0.6539
Loss = 0.834
Loss = 0.910233
Loss = 0.840744
Loss = 0.737274
Loss = 0.678955
Loss = 0.912018
Loss = 0.878128
Loss = 0.774033
Loss = 0.796097
Loss = 0.75267
Loss = 0.796265
Loss = 0.759659
Loss = 0.808273
Loss = 0.873718
Loss = 0.721237
TEST LOSS = 0.774667
TEST ACC = 467.599 % (7812/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.663116
Epoch 5.2: Loss = 0.704208
Epoch 5.3: Loss = 0.752167
Epoch 5.4: Loss = 0.678543
Epoch 5.5: Loss = 0.774673
Epoch 5.6: Loss = 0.812714
Epoch 5.7: Loss = 0.748566
Epoch 5.8: Loss = 0.831604
Epoch 5.9: Loss = 0.59996
Epoch 5.10: Loss = 0.526886
Epoch 5.11: Loss = 0.843658
Epoch 5.12: Loss = 0.735168
Epoch 5.13: Loss = 0.801392
Epoch 5.14: Loss = 0.672913
Epoch 5.15: Loss = 0.818726
Epoch 5.16: Loss = 0.785385
Epoch 5.17: Loss = 0.681381
Epoch 5.18: Loss = 0.796143
Epoch 5.19: Loss = 0.732132
Epoch 5.20: Loss = 0.840347
Epoch 5.21: Loss = 0.677963
Epoch 5.22: Loss = 0.585251
Epoch 5.23: Loss = 0.711121
Epoch 5.24: Loss = 0.897934
Epoch 5.25: Loss = 0.706573
Epoch 5.26: Loss = 0.633667
Epoch 5.27: Loss = 0.713867
Epoch 5.28: Loss = 0.722824
Epoch 5.29: Loss = 0.723434
Epoch 5.30: Loss = 0.732285
Epoch 5.31: Loss = 0.838669
Epoch 5.32: Loss = 0.701492
Epoch 5.33: Loss = 0.600571
Epoch 5.34: Loss = 0.807449
Epoch 5.35: Loss = 0.763962
Epoch 5.36: Loss = 0.8647
Epoch 5.37: Loss = 0.79715
Epoch 5.38: Loss = 0.748962
Epoch 5.39: Loss = 0.811218
Epoch 5.40: Loss = 0.746933
Epoch 5.41: Loss = 0.72847
Epoch 5.42: Loss = 0.742462
Epoch 5.43: Loss = 0.753235
Epoch 5.44: Loss = 0.649506
Epoch 5.45: Loss = 0.698364
Epoch 5.46: Loss = 0.86647
Epoch 5.47: Loss = 0.741943
Epoch 5.48: Loss = 0.637039
Epoch 5.49: Loss = 0.792984
Epoch 5.50: Loss = 0.726303
Epoch 5.51: Loss = 0.562744
Epoch 5.52: Loss = 0.780228
Epoch 5.53: Loss = 0.825623
Epoch 5.54: Loss = 0.616348
Epoch 5.55: Loss = 0.721619
Epoch 5.56: Loss = 0.772202
Epoch 5.57: Loss = 0.821091
Epoch 5.58: Loss = 0.728302
Epoch 5.59: Loss = 0.843323
Epoch 5.60: Loss = 0.718018
Epoch 5.61: Loss = 0.711777
Epoch 5.62: Loss = 0.79213
Epoch 5.63: Loss = 0.619492
Epoch 5.64: Loss = 0.627563
Epoch 5.65: Loss = 0.783951
Epoch 5.66: Loss = 0.708542
Epoch 5.67: Loss = 0.714996
Epoch 5.68: Loss = 0.91246
Epoch 5.69: Loss = 0.75177
Epoch 5.70: Loss = 0.765625
Epoch 5.71: Loss = 0.608688
Epoch 5.72: Loss = 0.734238
Epoch 5.73: Loss = 0.898758
Epoch 5.74: Loss = 0.790619
Epoch 5.75: Loss = 0.709641
Epoch 5.76: Loss = 0.708481
Epoch 5.77: Loss = 0.723587
Epoch 5.78: Loss = 0.804703
Epoch 5.79: Loss = 0.681564
Epoch 5.80: Loss = 0.72261
Epoch 5.81: Loss = 0.658829
Epoch 5.82: Loss = 0.7146
Epoch 5.83: Loss = 0.820572
Epoch 5.84: Loss = 0.714325
Epoch 5.85: Loss = 0.71756
Epoch 5.86: Loss = 0.81752
Epoch 5.87: Loss = 0.684418
Epoch 5.88: Loss = 0.72551
Epoch 5.89: Loss = 0.84259
Epoch 5.90: Loss = 0.782196
Epoch 5.91: Loss = 0.815598
Epoch 5.92: Loss = 0.695236
Epoch 5.93: Loss = 0.766113
Epoch 5.94: Loss = 0.806473
Epoch 5.95: Loss = 0.782349
Epoch 5.96: Loss = 0.734818
Epoch 5.97: Loss = 0.634903
Epoch 5.98: Loss = 0.779648
Epoch 5.99: Loss = 0.68338
Epoch 5.100: Loss = 0.803772
Epoch 5.101: Loss = 0.83522
Epoch 5.102: Loss = 0.80574
Epoch 5.103: Loss = 0.742706
Epoch 5.104: Loss = 0.675934
Epoch 5.105: Loss = 0.731934
Epoch 5.106: Loss = 0.825851
Epoch 5.107: Loss = 0.737656
Epoch 5.108: Loss = 0.79921
Epoch 5.109: Loss = 0.834763
Epoch 5.110: Loss = 0.759537
Epoch 5.111: Loss = 0.760239
Epoch 5.112: Loss = 0.754578
Epoch 5.113: Loss = 0.733582
Epoch 5.114: Loss = 0.76825
Epoch 5.115: Loss = 0.75148
Epoch 5.116: Loss = 0.638596
Epoch 5.117: Loss = 0.828629
Epoch 5.118: Loss = 0.656982
Epoch 5.119: Loss = 0.718506
Epoch 5.120: Loss = 0.714417
TRAIN LOSS = 0.742828
TRAIN ACC = 78.6453 % (47190/60000)
Loss = 0.657104
Loss = 0.780106
Loss = 0.708313
Loss = 0.642456
Loss = 0.656326
Loss = 0.875778
Loss = 0.974518
Loss = 0.866821
Loss = 0.738983
Loss = 0.66507
Loss = 0.931259
Loss = 0.873459
Loss = 0.788132
Loss = 0.814774
Loss = 0.744232
Loss = 0.774185
Loss = 0.763824
Loss = 0.823181
Loss = 0.873642
Loss = 0.725952
TEST LOSS = 0.783906
TEST ACC = 471.899 % (7822/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.709274
Epoch 6.2: Loss = 0.769226
Epoch 6.3: Loss = 0.784988
Epoch 6.4: Loss = 0.70079
Epoch 6.5: Loss = 0.804535
Epoch 6.6: Loss = 0.882751
Epoch 6.7: Loss = 0.759781
Epoch 6.8: Loss = 0.887711
Epoch 6.9: Loss = 0.630829
Epoch 6.10: Loss = 0.559128
Epoch 6.11: Loss = 0.839752
Epoch 6.12: Loss = 0.774353
Epoch 6.13: Loss = 0.855713
Epoch 6.14: Loss = 0.659103
Epoch 6.15: Loss = 0.878799
Epoch 6.16: Loss = 0.862411
Epoch 6.17: Loss = 0.727097
Epoch 6.18: Loss = 0.813309
Epoch 6.19: Loss = 0.758499
Epoch 6.20: Loss = 0.901459
Epoch 6.21: Loss = 0.653885
Epoch 6.22: Loss = 0.581131
Epoch 6.23: Loss = 0.74823
Epoch 6.24: Loss = 0.941376
Epoch 6.25: Loss = 0.731293
Epoch 6.26: Loss = 0.665054
Epoch 6.27: Loss = 0.794327
Epoch 6.28: Loss = 0.811035
Epoch 6.29: Loss = 0.752075
Epoch 6.30: Loss = 0.752075
Epoch 6.31: Loss = 0.830917
Epoch 6.32: Loss = 0.738602
Epoch 6.33: Loss = 0.662109
Epoch 6.34: Loss = 0.821335
Epoch 6.35: Loss = 0.786224
Epoch 6.36: Loss = 0.907639
Epoch 6.37: Loss = 0.85878
Epoch 6.38: Loss = 0.80368
Epoch 6.39: Loss = 0.881119
Epoch 6.40: Loss = 0.775421
Epoch 6.41: Loss = 0.833115
Epoch 6.42: Loss = 0.805283
Epoch 6.43: Loss = 0.826645
Epoch 6.44: Loss = 0.667099
Epoch 6.45: Loss = 0.768326
Epoch 6.46: Loss = 0.927887
Epoch 6.47: Loss = 0.738632
Epoch 6.48: Loss = 0.689209
Epoch 6.49: Loss = 0.850021
Epoch 6.50: Loss = 0.780731
Epoch 6.51: Loss = 0.585022
Epoch 6.52: Loss = 0.793045
Epoch 6.53: Loss = 0.880417
Epoch 6.54: Loss = 0.635406
Epoch 6.55: Loss = 0.816071
Epoch 6.56: Loss = 0.861969
Epoch 6.57: Loss = 0.856308
Epoch 6.58: Loss = 0.737778
Epoch 6.59: Loss = 0.93573
Epoch 6.60: Loss = 0.819717
Epoch 6.61: Loss = 0.734177
Epoch 6.62: Loss = 0.814743
Epoch 6.63: Loss = 0.70134
Epoch 6.64: Loss = 0.667847
Epoch 6.65: Loss = 0.843323
Epoch 6.66: Loss = 0.778305
Epoch 6.67: Loss = 0.84166
Epoch 6.68: Loss = 0.96022
Epoch 6.69: Loss = 0.81369
Epoch 6.70: Loss = 0.838852
Epoch 6.71: Loss = 0.704758
Epoch 6.72: Loss = 0.773743
Epoch 6.73: Loss = 0.939117
Epoch 6.74: Loss = 0.825287
Epoch 6.75: Loss = 0.755554
Epoch 6.76: Loss = 0.735291
Epoch 6.77: Loss = 0.775009
Epoch 6.78: Loss = 0.869019
Epoch 6.79: Loss = 0.75563
Epoch 6.80: Loss = 0.78714
Epoch 6.81: Loss = 0.658829
Epoch 6.82: Loss = 0.735214
Epoch 6.83: Loss = 0.864059
Epoch 6.84: Loss = 0.786926
Epoch 6.85: Loss = 0.779221
Epoch 6.86: Loss = 0.858643
Epoch 6.87: Loss = 0.736267
Epoch 6.88: Loss = 0.807236
Epoch 6.89: Loss = 0.885605
Epoch 6.90: Loss = 0.795227
Epoch 6.91: Loss = 0.852676
Epoch 6.92: Loss = 0.765488
Epoch 6.93: Loss = 0.808273
Epoch 6.94: Loss = 0.767456
Epoch 6.95: Loss = 0.816803
Epoch 6.96: Loss = 0.749832
Epoch 6.97: Loss = 0.68866
Epoch 6.98: Loss = 0.849045
Epoch 6.99: Loss = 0.742126
Epoch 6.100: Loss = 0.852707
Epoch 6.101: Loss = 0.885559
Epoch 6.102: Loss = 0.78743
Epoch 6.103: Loss = 0.801071
Epoch 6.104: Loss = 0.746796
Epoch 6.105: Loss = 0.758057
Epoch 6.106: Loss = 0.882126
Epoch 6.107: Loss = 0.721771
Epoch 6.108: Loss = 0.868652
Epoch 6.109: Loss = 0.878693
Epoch 6.110: Loss = 0.773407
Epoch 6.111: Loss = 0.807693
Epoch 6.112: Loss = 0.787842
Epoch 6.113: Loss = 0.73764
Epoch 6.114: Loss = 0.868423
Epoch 6.115: Loss = 0.830185
Epoch 6.116: Loss = 0.750519
Epoch 6.117: Loss = 0.936447
Epoch 6.118: Loss = 0.690399
Epoch 6.119: Loss = 0.780579
Epoch 6.120: Loss = 0.802078
TRAIN LOSS = 0.788971
TRAIN ACC = 78.9597 % (47378/60000)
Loss = 0.669388
Loss = 0.804565
Loss = 0.806122
Loss = 0.688339
Loss = 0.707458
Loss = 0.921036
Loss = 1.01974
Loss = 0.92749
Loss = 0.804474
Loss = 0.764694
Loss = 0.969254
Loss = 0.976257
Loss = 0.824173
Loss = 0.906357
Loss = 0.794449
Loss = 0.88205
Loss = 0.787628
Loss = 0.878052
Loss = 0.972107
Loss = 0.776199
TEST LOSS = 0.843992
TEST ACC = 473.779 % (7802/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.816177
Epoch 7.2: Loss = 0.862976
Epoch 7.3: Loss = 0.799423
Epoch 7.4: Loss = 0.738861
Epoch 7.5: Loss = 0.841537
Epoch 7.6: Loss = 0.905334
Epoch 7.7: Loss = 0.805496
Epoch 7.8: Loss = 0.925095
Epoch 7.9: Loss = 0.668198
Epoch 7.10: Loss = 0.633926
Epoch 7.11: Loss = 0.882645
Epoch 7.12: Loss = 0.811569
Epoch 7.13: Loss = 0.854263
Epoch 7.14: Loss = 0.728439
Epoch 7.15: Loss = 0.876266
Epoch 7.16: Loss = 0.847839
Epoch 7.17: Loss = 0.734726
Epoch 7.18: Loss = 0.801895
Epoch 7.19: Loss = 0.713409
Epoch 7.20: Loss = 0.942505
Epoch 7.21: Loss = 0.711548
Epoch 7.22: Loss = 0.54715
Epoch 7.23: Loss = 0.754288
Epoch 7.24: Loss = 0.941925
Epoch 7.25: Loss = 0.788025
Epoch 7.26: Loss = 0.654587
Epoch 7.27: Loss = 0.797882
Epoch 7.28: Loss = 0.796753
Epoch 7.29: Loss = 0.789169
Epoch 7.30: Loss = 0.799271
Epoch 7.31: Loss = 0.896011
Epoch 7.32: Loss = 0.772552
Epoch 7.33: Loss = 0.735458
Epoch 7.34: Loss = 0.842972
Epoch 7.35: Loss = 0.792084
Epoch 7.36: Loss = 0.935944
Epoch 7.37: Loss = 0.869568
Epoch 7.38: Loss = 0.861206
Epoch 7.39: Loss = 0.897476
Epoch 7.40: Loss = 0.817551
Epoch 7.41: Loss = 0.857468
Epoch 7.42: Loss = 0.793472
Epoch 7.43: Loss = 0.806992
Epoch 7.44: Loss = 0.726547
Epoch 7.45: Loss = 0.795517
Epoch 7.46: Loss = 0.909103
Epoch 7.47: Loss = 0.800171
Epoch 7.48: Loss = 0.69899
Epoch 7.49: Loss = 0.936279
Epoch 7.50: Loss = 0.840302
Epoch 7.51: Loss = 0.625107
Epoch 7.52: Loss = 0.8228
Epoch 7.53: Loss = 0.889542
Epoch 7.54: Loss = 0.631317
Epoch 7.55: Loss = 0.798019
Epoch 7.56: Loss = 0.855133
Epoch 7.57: Loss = 0.85759
Epoch 7.58: Loss = 0.803162
Epoch 7.59: Loss = 0.951248
Epoch 7.60: Loss = 0.869156
Epoch 7.61: Loss = 0.721542
Epoch 7.62: Loss = 0.821518
Epoch 7.63: Loss = 0.706619
Epoch 7.64: Loss = 0.695465
Epoch 7.65: Loss = 0.901733
Epoch 7.66: Loss = 0.837479
Epoch 7.67: Loss = 0.77916
Epoch 7.68: Loss = 0.988663
Epoch 7.69: Loss = 0.775406
Epoch 7.70: Loss = 0.80867
Epoch 7.71: Loss = 0.675934
Epoch 7.72: Loss = 0.823502
Epoch 7.73: Loss = 0.928162
Epoch 7.74: Loss = 0.825928
Epoch 7.75: Loss = 0.770096
Epoch 7.76: Loss = 0.748108
Epoch 7.77: Loss = 0.775635
Epoch 7.78: Loss = 0.831772
Epoch 7.79: Loss = 0.783829
Epoch 7.80: Loss = 0.780853
Epoch 7.81: Loss = 0.754944
Epoch 7.82: Loss = 0.733414
Epoch 7.83: Loss = 0.851044
Epoch 7.84: Loss = 0.731125
Epoch 7.85: Loss = 0.779861
Epoch 7.86: Loss = 0.913849
Epoch 7.87: Loss = 0.76326
Epoch 7.88: Loss = 0.815765
Epoch 7.89: Loss = 0.917358
Epoch 7.90: Loss = 0.90567
Epoch 7.91: Loss = 0.89679
Epoch 7.92: Loss = 0.774719
Epoch 7.93: Loss = 0.851837
Epoch 7.94: Loss = 0.822525
Epoch 7.95: Loss = 0.87883
Epoch 7.96: Loss = 0.748291
Epoch 7.97: Loss = 0.728256
Epoch 7.98: Loss = 0.841385
Epoch 7.99: Loss = 0.756943
Epoch 7.100: Loss = 0.799835
Epoch 7.101: Loss = 0.901154
Epoch 7.102: Loss = 0.818527
Epoch 7.103: Loss = 0.821381
Epoch 7.104: Loss = 0.754013
Epoch 7.105: Loss = 0.762497
Epoch 7.106: Loss = 0.964752
Epoch 7.107: Loss = 0.777023
Epoch 7.108: Loss = 0.875137
Epoch 7.109: Loss = 0.884384
Epoch 7.110: Loss = 0.867203
Epoch 7.111: Loss = 0.82077
Epoch 7.112: Loss = 0.780136
Epoch 7.113: Loss = 0.827179
Epoch 7.114: Loss = 0.895401
Epoch 7.115: Loss = 0.834015
Epoch 7.116: Loss = 0.687317
Epoch 7.117: Loss = 0.936371
Epoch 7.118: Loss = 0.727081
Epoch 7.119: Loss = 0.78624
Epoch 7.120: Loss = 0.807541
TRAIN LOSS = 0.810303
TRAIN ACC = 78.7735 % (47266/60000)
Loss = 0.667969
Loss = 0.775604
Loss = 0.777924
Loss = 0.709259
Loss = 0.703934
Loss = 0.897888
Loss = 0.99884
Loss = 0.925949
Loss = 0.827408
Loss = 0.757217
Loss = 0.964279
Loss = 0.868149
Loss = 0.847122
Loss = 0.856522
Loss = 0.804504
Loss = 0.822357
Loss = 0.7957
Loss = 0.829971
Loss = 0.944458
Loss = 0.757858
TEST LOSS = 0.826645
TEST ACC = 472.659 % (7828/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.782898
Epoch 8.2: Loss = 0.889114
Epoch 8.3: Loss = 0.848694
Epoch 8.4: Loss = 0.738159
Epoch 8.5: Loss = 0.846771
Epoch 8.6: Loss = 0.971359
Epoch 8.7: Loss = 0.790466
Epoch 8.8: Loss = 0.932755
Epoch 8.9: Loss = 0.713257
Epoch 8.10: Loss = 0.670212
Epoch 8.11: Loss = 0.883606
Epoch 8.12: Loss = 0.841873
Epoch 8.13: Loss = 0.914658
Epoch 8.14: Loss = 0.727936
Epoch 8.15: Loss = 0.871719
Epoch 8.16: Loss = 0.910492
Epoch 8.17: Loss = 0.729019
Epoch 8.18: Loss = 0.908493
Epoch 8.19: Loss = 0.769516
Epoch 8.20: Loss = 0.935211
Epoch 8.21: Loss = 0.725021
Epoch 8.22: Loss = 0.601959
Epoch 8.23: Loss = 0.796555
Epoch 8.24: Loss = 0.982346
Epoch 8.25: Loss = 0.771942
Epoch 8.26: Loss = 0.668091
Epoch 8.27: Loss = 0.85733
Epoch 8.28: Loss = 0.815216
Epoch 8.29: Loss = 0.87851
Epoch 8.30: Loss = 0.844208
Epoch 8.31: Loss = 0.96109
Epoch 8.32: Loss = 0.830063
Epoch 8.33: Loss = 0.739716
Epoch 8.34: Loss = 0.900528
Epoch 8.35: Loss = 0.830032
Epoch 8.36: Loss = 0.934052
Epoch 8.37: Loss = 0.873489
Epoch 8.38: Loss = 0.870453
Epoch 8.39: Loss = 0.94043
Epoch 8.40: Loss = 0.794983
Epoch 8.41: Loss = 0.879974
Epoch 8.42: Loss = 0.818527
Epoch 8.43: Loss = 0.826874
Epoch 8.44: Loss = 0.809158
Epoch 8.45: Loss = 0.813614
Epoch 8.46: Loss = 0.933289
Epoch 8.47: Loss = 0.728912
Epoch 8.48: Loss = 0.764282
Epoch 8.49: Loss = 0.956726
Epoch 8.50: Loss = 0.873474
Epoch 8.51: Loss = 0.628296
Epoch 8.52: Loss = 0.839462
Epoch 8.53: Loss = 0.892822
Epoch 8.54: Loss = 0.635818
Epoch 8.55: Loss = 0.819717
Epoch 8.56: Loss = 0.823532
Epoch 8.57: Loss = 0.855652
Epoch 8.58: Loss = 0.767471
Epoch 8.59: Loss = 0.997025
Epoch 8.60: Loss = 0.877106
Epoch 8.61: Loss = 0.75296
Epoch 8.62: Loss = 0.865921
Epoch 8.63: Loss = 0.714432
Epoch 8.64: Loss = 0.694504
Epoch 8.65: Loss = 0.93605
Epoch 8.66: Loss = 0.82872
Epoch 8.67: Loss = 0.826813
Epoch 8.68: Loss = 1.06599
Epoch 8.69: Loss = 0.805588
Epoch 8.70: Loss = 0.903946
Epoch 8.71: Loss = 0.70755
Epoch 8.72: Loss = 0.865356
Epoch 8.73: Loss = 0.985016
Epoch 8.74: Loss = 0.851074
Epoch 8.75: Loss = 0.772095
Epoch 8.76: Loss = 0.757858
Epoch 8.77: Loss = 0.849396
Epoch 8.78: Loss = 0.811966
Epoch 8.79: Loss = 0.827728
Epoch 8.80: Loss = 0.832367
Epoch 8.81: Loss = 0.832123
Epoch 8.82: Loss = 0.779099
Epoch 8.83: Loss = 0.899139
Epoch 8.84: Loss = 0.708618
Epoch 8.85: Loss = 0.767761
Epoch 8.86: Loss = 0.892471
Epoch 8.87: Loss = 0.769073
Epoch 8.88: Loss = 0.791245
Epoch 8.89: Loss = 0.894043
Epoch 8.90: Loss = 0.914337
Epoch 8.91: Loss = 0.942993
Epoch 8.92: Loss = 0.824036
Epoch 8.93: Loss = 0.89064
Epoch 8.94: Loss = 0.838959
Epoch 8.95: Loss = 0.867798
Epoch 8.96: Loss = 0.782898
Epoch 8.97: Loss = 0.730896
Epoch 8.98: Loss = 0.766205
Epoch 8.99: Loss = 0.765671
Epoch 8.100: Loss = 0.810699
Epoch 8.101: Loss = 0.938889
Epoch 8.102: Loss = 0.821732
Epoch 8.103: Loss = 0.861923
Epoch 8.104: Loss = 0.794495
Epoch 8.105: Loss = 0.793137
Epoch 8.106: Loss = 0.988525
Epoch 8.107: Loss = 0.797104
Epoch 8.108: Loss = 0.904373
Epoch 8.109: Loss = 0.962906
Epoch 8.110: Loss = 0.816116
Epoch 8.111: Loss = 0.819656
Epoch 8.112: Loss = 0.782043
Epoch 8.113: Loss = 0.776398
Epoch 8.114: Loss = 0.865311
Epoch 8.115: Loss = 0.865768
Epoch 8.116: Loss = 0.672012
Epoch 8.117: Loss = 0.995087
Epoch 8.118: Loss = 0.767654
Epoch 8.119: Loss = 0.832825
Epoch 8.120: Loss = 0.825668
TRAIN LOSS = 0.832001
TRAIN ACC = 78.8452 % (47310/60000)
Loss = 0.721725
Loss = 0.805725
Loss = 0.823547
Loss = 0.75441
Loss = 0.768417
Loss = 0.898148
Loss = 1.01057
Loss = 0.908127
Loss = 0.880051
Loss = 0.81015
Loss = 0.953552
Loss = 0.911743
Loss = 0.907242
Loss = 0.917877
Loss = 0.872086
Loss = 0.835571
Loss = 0.761124
Loss = 0.891449
Loss = 0.970001
Loss = 0.814316
TEST LOSS = 0.860792
TEST ACC = 473.099 % (7802/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.844101
Epoch 9.2: Loss = 0.880249
Epoch 9.3: Loss = 0.82074
Epoch 9.4: Loss = 0.769012
Epoch 9.5: Loss = 0.863281
Epoch 9.6: Loss = 0.933975
Epoch 9.7: Loss = 0.773697
Epoch 9.8: Loss = 0.974304
Epoch 9.9: Loss = 0.699203
Epoch 9.10: Loss = 0.696609
Epoch 9.11: Loss = 0.986221
Epoch 9.12: Loss = 0.901932
Epoch 9.13: Loss = 0.940369
Epoch 9.14: Loss = 0.8069
Epoch 9.15: Loss = 0.932236
Epoch 9.16: Loss = 1.02151
Epoch 9.17: Loss = 0.831329
Epoch 9.18: Loss = 0.923935
Epoch 9.19: Loss = 0.796432
Epoch 9.20: Loss = 0.998825
Epoch 9.21: Loss = 0.739426
Epoch 9.22: Loss = 0.617416
Epoch 9.23: Loss = 0.750488
Epoch 9.24: Loss = 0.963364
Epoch 9.25: Loss = 0.790039
Epoch 9.26: Loss = 0.704727
Epoch 9.27: Loss = 0.8535
Epoch 9.28: Loss = 0.819153
Epoch 9.29: Loss = 0.870865
Epoch 9.30: Loss = 0.810516
Epoch 9.31: Loss = 0.971619
Epoch 9.32: Loss = 0.81395
Epoch 9.33: Loss = 0.698135
Epoch 9.34: Loss = 0.890656
Epoch 9.35: Loss = 0.800079
Epoch 9.36: Loss = 0.911591
Epoch 9.37: Loss = 0.878372
Epoch 9.38: Loss = 0.899246
Epoch 9.39: Loss = 0.971893
Epoch 9.40: Loss = 0.801559
Epoch 9.41: Loss = 0.875336
Epoch 9.42: Loss = 0.840622
Epoch 9.43: Loss = 0.852386
Epoch 9.44: Loss = 0.764893
Epoch 9.45: Loss = 0.871597
Epoch 9.46: Loss = 0.948853
Epoch 9.47: Loss = 0.79689
Epoch 9.48: Loss = 0.852066
Epoch 9.49: Loss = 0.936737
Epoch 9.50: Loss = 0.905045
Epoch 9.51: Loss = 0.685135
Epoch 9.52: Loss = 0.871475
Epoch 9.53: Loss = 0.935638
Epoch 9.54: Loss = 0.636505
Epoch 9.55: Loss = 0.773972
Epoch 9.56: Loss = 0.863083
Epoch 9.57: Loss = 0.878067
Epoch 9.58: Loss = 0.800613
Epoch 9.59: Loss = 1.08443
Epoch 9.60: Loss = 0.882813
Epoch 9.61: Loss = 0.78389
Epoch 9.62: Loss = 0.881317
Epoch 9.63: Loss = 0.771835
Epoch 9.64: Loss = 0.76973
Epoch 9.65: Loss = 0.984161
Epoch 9.66: Loss = 0.875351
Epoch 9.67: Loss = 0.869003
Epoch 9.68: Loss = 1.10791
Epoch 9.69: Loss = 0.803284
Epoch 9.70: Loss = 0.865036
Epoch 9.71: Loss = 0.761185
Epoch 9.72: Loss = 0.83165
Epoch 9.73: Loss = 1.00659
Epoch 9.74: Loss = 0.915604
Epoch 9.75: Loss = 0.817551
Epoch 9.76: Loss = 0.781784
Epoch 9.77: Loss = 0.932144
Epoch 9.78: Loss = 0.845734
Epoch 9.79: Loss = 0.835907
Epoch 9.80: Loss = 0.875244
Epoch 9.81: Loss = 0.808945
Epoch 9.82: Loss = 0.803009
Epoch 9.83: Loss = 0.989517
Epoch 9.84: Loss = 0.817245
Epoch 9.85: Loss = 0.777603
Epoch 9.86: Loss = 0.959442
Epoch 9.87: Loss = 0.799026
Epoch 9.88: Loss = 0.888474
Epoch 9.89: Loss = 0.943375
Epoch 9.90: Loss = 0.912766
Epoch 9.91: Loss = 0.987488
Epoch 9.92: Loss = 0.839966
Epoch 9.93: Loss = 0.894699
Epoch 9.94: Loss = 0.870972
Epoch 9.95: Loss = 0.878922
Epoch 9.96: Loss = 0.817413
Epoch 9.97: Loss = 0.715683
Epoch 9.98: Loss = 0.822357
Epoch 9.99: Loss = 0.765884
Epoch 9.100: Loss = 0.848145
Epoch 9.101: Loss = 0.912277
Epoch 9.102: Loss = 0.848862
Epoch 9.103: Loss = 0.871735
Epoch 9.104: Loss = 0.775818
Epoch 9.105: Loss = 0.885132
Epoch 9.106: Loss = 0.979843
Epoch 9.107: Loss = 0.830185
Epoch 9.108: Loss = 0.930573
Epoch 9.109: Loss = 1.00447
Epoch 9.110: Loss = 0.872437
Epoch 9.111: Loss = 0.914627
Epoch 9.112: Loss = 0.761063
Epoch 9.113: Loss = 0.839783
Epoch 9.114: Loss = 0.920044
Epoch 9.115: Loss = 0.858398
Epoch 9.116: Loss = 0.722473
Epoch 9.117: Loss = 0.968048
Epoch 9.118: Loss = 0.814178
Epoch 9.119: Loss = 0.849045
Epoch 9.120: Loss = 0.814362
TRAIN LOSS = 0.856995
TRAIN ACC = 78.8757 % (47327/60000)
Loss = 0.77298
Loss = 0.918839
Loss = 0.953247
Loss = 0.820145
Loss = 0.780869
Loss = 1.06027
Loss = 1.10443
Loss = 0.995728
Loss = 0.973618
Loss = 0.938904
Loss = 1.14218
Loss = 1.04964
Loss = 0.942078
Loss = 1.00597
Loss = 0.993637
Loss = 0.895126
Loss = 0.846619
Loss = 0.963745
Loss = 1.01767
Loss = 0.902237
TEST LOSS = 0.953896
TEST ACC = 473.27 % (7796/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.949524
Epoch 10.2: Loss = 0.902634
Epoch 10.3: Loss = 0.936569
Epoch 10.4: Loss = 0.815735
Epoch 10.5: Loss = 0.912354
Epoch 10.6: Loss = 0.921082
Epoch 10.7: Loss = 0.773849
Epoch 10.8: Loss = 0.980408
Epoch 10.9: Loss = 0.788483
Epoch 10.10: Loss = 0.719864
Epoch 10.11: Loss = 0.989227
Epoch 10.12: Loss = 0.901031
Epoch 10.13: Loss = 0.902328
Epoch 10.14: Loss = 0.799515
Epoch 10.15: Loss = 0.965668
Epoch 10.16: Loss = 1.04054
Epoch 10.17: Loss = 0.799789
Epoch 10.18: Loss = 0.953995
Epoch 10.19: Loss = 0.769089
Epoch 10.20: Loss = 0.936096
Epoch 10.21: Loss = 0.793518
Epoch 10.22: Loss = 0.700974
Epoch 10.23: Loss = 0.834991
Epoch 10.24: Loss = 0.883743
Epoch 10.25: Loss = 0.847748
Epoch 10.26: Loss = 0.687668
Epoch 10.27: Loss = 0.902847
Epoch 10.28: Loss = 0.835648
Epoch 10.29: Loss = 0.890488
Epoch 10.30: Loss = 0.862503
Epoch 10.31: Loss = 1.03067
Epoch 10.32: Loss = 0.906372
Epoch 10.33: Loss = 0.796082
Epoch 10.34: Loss = 0.929214
Epoch 10.35: Loss = 0.925339
Epoch 10.36: Loss = 0.992371
Epoch 10.37: Loss = 0.936768
Epoch 10.38: Loss = 0.923615
Epoch 10.39: Loss = 0.910797
Epoch 10.40: Loss = 0.921875
Epoch 10.41: Loss = 0.920532
Epoch 10.42: Loss = 0.920929
Epoch 10.43: Loss = 0.903152
Epoch 10.44: Loss = 0.805328
Epoch 10.45: Loss = 0.904984
Epoch 10.46: Loss = 1.02374
Epoch 10.47: Loss = 0.79155
Epoch 10.48: Loss = 0.873611
Epoch 10.49: Loss = 0.987228
Epoch 10.50: Loss = 0.891708
Epoch 10.51: Loss = 0.764908
Epoch 10.52: Loss = 0.938095
Epoch 10.53: Loss = 0.995941
Epoch 10.54: Loss = 0.67897
Epoch 10.55: Loss = 0.855133
Epoch 10.56: Loss = 0.860001
Epoch 10.57: Loss = 0.967514
Epoch 10.58: Loss = 0.822601
Epoch 10.59: Loss = 1.05585
Epoch 10.60: Loss = 0.912155
Epoch 10.61: Loss = 0.73967
Epoch 10.62: Loss = 0.934128
Epoch 10.63: Loss = 0.808014
Epoch 10.64: Loss = 0.787155
Epoch 10.65: Loss = 1.04784
Epoch 10.66: Loss = 0.895477
Epoch 10.67: Loss = 0.856064
Epoch 10.68: Loss = 1.20045
Epoch 10.69: Loss = 0.83699
Epoch 10.70: Loss = 0.927521
Epoch 10.71: Loss = 0.812668
Epoch 10.72: Loss = 0.790253
Epoch 10.73: Loss = 1.03421
Epoch 10.74: Loss = 0.875458
Epoch 10.75: Loss = 0.785828
Epoch 10.76: Loss = 0.800232
Epoch 10.77: Loss = 0.992966
Epoch 10.78: Loss = 0.892654
Epoch 10.79: Loss = 0.829865
Epoch 10.80: Loss = 0.861511
Epoch 10.81: Loss = 0.870972
Epoch 10.82: Loss = 0.805191
Epoch 10.83: Loss = 0.985321
Epoch 10.84: Loss = 0.860825
Epoch 10.85: Loss = 0.833954
Epoch 10.86: Loss = 0.955688
Epoch 10.87: Loss = 0.84343
Epoch 10.88: Loss = 0.961548
Epoch 10.89: Loss = 1.08263
Epoch 10.90: Loss = 0.968765
Epoch 10.91: Loss = 0.925598
Epoch 10.92: Loss = 0.918579
Epoch 10.93: Loss = 0.950668
Epoch 10.94: Loss = 0.934586
Epoch 10.95: Loss = 0.96463
Epoch 10.96: Loss = 0.862091
Epoch 10.97: Loss = 0.743378
Epoch 10.98: Loss = 0.816254
Epoch 10.99: Loss = 0.83754
Epoch 10.100: Loss = 0.930481
Epoch 10.101: Loss = 0.984818
Epoch 10.102: Loss = 0.977966
Epoch 10.103: Loss = 0.93515
Epoch 10.104: Loss = 0.801666
Epoch 10.105: Loss = 0.930695
Epoch 10.106: Loss = 1.04372
Epoch 10.107: Loss = 0.849518
Epoch 10.108: Loss = 1.0123
Epoch 10.109: Loss = 1.10832
Epoch 10.110: Loss = 0.868835
Epoch 10.111: Loss = 0.954437
Epoch 10.112: Loss = 0.895706
Epoch 10.113: Loss = 0.818878
Epoch 10.114: Loss = 0.971497
Epoch 10.115: Loss = 0.869293
Epoch 10.116: Loss = 0.830414
Epoch 10.117: Loss = 1.01299
Epoch 10.118: Loss = 0.803406
Epoch 10.119: Loss = 0.849548
Epoch 10.120: Loss = 0.837601
TRAIN LOSS = 0.89386
TRAIN ACC = 78.5767 % (47148/60000)
Loss = 0.793686
Loss = 0.993301
Loss = 0.968796
Loss = 0.865952
Loss = 0.777145
Loss = 1.10071
Loss = 1.18962
Loss = 1.04558
Loss = 1.0318
Loss = 0.982788
Loss = 1.22539
Loss = 1.07396
Loss = 0.975647
Loss = 1.0876
Loss = 1.03296
Loss = 0.96019
Loss = 0.864838
Loss = 1.01506
Loss = 1.04503
Loss = 0.905563
TEST LOSS = 0.99678
TEST ACC = 471.48 % (7787/10000)
