Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.37758
Epoch 1.2: Loss = 2.31799
Epoch 1.3: Loss = 2.26703
Epoch 1.4: Loss = 2.19824
Epoch 1.5: Loss = 2.15239
Epoch 1.6: Loss = 2.13997
Epoch 1.7: Loss = 2.10217
Epoch 1.8: Loss = 2.07927
Epoch 1.9: Loss = 2.01277
Epoch 1.10: Loss = 1.99257
Epoch 1.11: Loss = 1.9012
Epoch 1.12: Loss = 1.90007
Epoch 1.13: Loss = 1.84966
Epoch 1.14: Loss = 1.81779
Epoch 1.15: Loss = 1.87068
Epoch 1.16: Loss = 1.79642
Epoch 1.17: Loss = 1.73784
Epoch 1.18: Loss = 1.73199
Epoch 1.19: Loss = 1.64642
Epoch 1.20: Loss = 1.63972
Epoch 1.21: Loss = 1.57497
Epoch 1.22: Loss = 1.55505
Epoch 1.23: Loss = 1.49898
Epoch 1.24: Loss = 1.59258
Epoch 1.25: Loss = 1.53577
Epoch 1.26: Loss = 1.53766
Epoch 1.27: Loss = 1.48241
Epoch 1.28: Loss = 1.45416
Epoch 1.29: Loss = 1.49722
Epoch 1.30: Loss = 1.49617
Epoch 1.31: Loss = 1.36098
Epoch 1.32: Loss = 1.38527
Epoch 1.33: Loss = 1.34406
Epoch 1.34: Loss = 1.35699
Epoch 1.35: Loss = 1.30533
Epoch 1.36: Loss = 1.40318
Epoch 1.37: Loss = 1.24359
Epoch 1.38: Loss = 1.22232
Epoch 1.39: Loss = 1.20456
Epoch 1.40: Loss = 1.12961
Epoch 1.41: Loss = 1.18689
Epoch 1.42: Loss = 1.17108
Epoch 1.43: Loss = 1.10609
Epoch 1.44: Loss = 1.01913
Epoch 1.45: Loss = 1.16563
Epoch 1.46: Loss = 1.11929
Epoch 1.47: Loss = 1.03107
Epoch 1.48: Loss = 1.11273
Epoch 1.49: Loss = 1.05333
Epoch 1.50: Loss = 1.1076
Epoch 1.51: Loss = 0.948853
Epoch 1.52: Loss = 0.948685
Epoch 1.53: Loss = 1.00229
Epoch 1.54: Loss = 1.01189
Epoch 1.55: Loss = 1.01248
Epoch 1.56: Loss = 0.936386
Epoch 1.57: Loss = 0.84964
Epoch 1.58: Loss = 0.89888
Epoch 1.59: Loss = 0.920715
Epoch 1.60: Loss = 1.03453
Epoch 1.61: Loss = 0.96759
Epoch 1.62: Loss = 0.989197
Epoch 1.63: Loss = 0.991821
Epoch 1.64: Loss = 0.967819
Epoch 1.65: Loss = 1.03607
Epoch 1.66: Loss = 0.85849
Epoch 1.67: Loss = 0.908096
Epoch 1.68: Loss = 0.714951
Epoch 1.69: Loss = 0.791183
Epoch 1.70: Loss = 0.896957
Epoch 1.71: Loss = 0.832581
Epoch 1.72: Loss = 0.805038
Epoch 1.73: Loss = 0.826965
Epoch 1.74: Loss = 0.689789
Epoch 1.75: Loss = 0.838959
Epoch 1.76: Loss = 0.804306
Epoch 1.77: Loss = 0.775803
Epoch 1.78: Loss = 0.751297
Epoch 1.79: Loss = 0.748611
Epoch 1.80: Loss = 0.868332
Epoch 1.81: Loss = 0.70163
Epoch 1.82: Loss = 0.684052
Epoch 1.83: Loss = 0.855179
Epoch 1.84: Loss = 0.775604
Epoch 1.85: Loss = 0.845581
Epoch 1.86: Loss = 0.759155
Epoch 1.87: Loss = 0.677933
Epoch 1.88: Loss = 0.693207
Epoch 1.89: Loss = 0.790482
Epoch 1.90: Loss = 0.66008
Epoch 1.91: Loss = 0.753494
Epoch 1.92: Loss = 0.71637
Epoch 1.93: Loss = 0.753113
Epoch 1.94: Loss = 0.603424
Epoch 1.95: Loss = 0.713821
Epoch 1.96: Loss = 0.69603
Epoch 1.97: Loss = 0.542999
Epoch 1.98: Loss = 0.65654
Epoch 1.99: Loss = 0.726105
Epoch 1.100: Loss = 0.822617
Epoch 1.101: Loss = 0.724701
Epoch 1.102: Loss = 0.647491
Epoch 1.103: Loss = 0.607132
Epoch 1.104: Loss = 0.575562
Epoch 1.105: Loss = 0.699844
Epoch 1.106: Loss = 0.694611
Epoch 1.107: Loss = 0.58606
Epoch 1.108: Loss = 0.636002
Epoch 1.109: Loss = 0.595856
Epoch 1.110: Loss = 0.620911
Epoch 1.111: Loss = 0.538742
Epoch 1.112: Loss = 0.516602
Epoch 1.113: Loss = 0.612869
Epoch 1.114: Loss = 0.520721
Epoch 1.115: Loss = 0.591553
Epoch 1.116: Loss = 0.578812
Epoch 1.117: Loss = 0.47673
Epoch 1.118: Loss = 0.43071
Epoch 1.119: Loss = 0.426193
Epoch 1.120: Loss = 0.463104
TRAIN LOSS = 1.09489
TRAIN ACC = 70.0409 % (42027/60000)
Loss = 0.610168
Loss = 0.638077
Loss = 0.75946
Loss = 0.69136
Loss = 0.729248
Loss = 0.619339
Loss = 0.589035
Loss = 0.749542
Loss = 0.731979
Loss = 0.682449
Loss = 0.348984
Loss = 0.51123
Loss = 0.383484
Loss = 0.576462
Loss = 0.466827
Loss = 0.445999
Loss = 0.455719
Loss = 0.2491
Loss = 0.434784
Loss = 0.718964
TEST LOSS = 0.56961
TEST ACC = 420.27 % (8368/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.556274
Epoch 2.2: Loss = 0.682953
Epoch 2.3: Loss = 0.673447
Epoch 2.4: Loss = 0.514679
Epoch 2.5: Loss = 0.537552
Epoch 2.6: Loss = 0.529068
Epoch 2.7: Loss = 0.5849
Epoch 2.8: Loss = 0.555374
Epoch 2.9: Loss = 0.543045
Epoch 2.10: Loss = 0.564758
Epoch 2.11: Loss = 0.539276
Epoch 2.12: Loss = 0.536514
Epoch 2.13: Loss = 0.467712
Epoch 2.14: Loss = 0.51416
Epoch 2.15: Loss = 0.654129
Epoch 2.16: Loss = 0.60524
Epoch 2.17: Loss = 0.605576
Epoch 2.18: Loss = 0.670639
Epoch 2.19: Loss = 0.521683
Epoch 2.20: Loss = 0.477814
Epoch 2.21: Loss = 0.449722
Epoch 2.22: Loss = 0.465851
Epoch 2.23: Loss = 0.444916
Epoch 2.24: Loss = 0.660339
Epoch 2.25: Loss = 0.559616
Epoch 2.26: Loss = 0.62207
Epoch 2.27: Loss = 0.605179
Epoch 2.28: Loss = 0.592346
Epoch 2.29: Loss = 0.666519
Epoch 2.30: Loss = 0.69104
Epoch 2.31: Loss = 0.494125
Epoch 2.32: Loss = 0.619095
Epoch 2.33: Loss = 0.529114
Epoch 2.34: Loss = 0.598251
Epoch 2.35: Loss = 0.553299
Epoch 2.36: Loss = 0.642441
Epoch 2.37: Loss = 0.464615
Epoch 2.38: Loss = 0.452942
Epoch 2.39: Loss = 0.505737
Epoch 2.40: Loss = 0.459137
Epoch 2.41: Loss = 0.526764
Epoch 2.42: Loss = 0.60173
Epoch 2.43: Loss = 0.471085
Epoch 2.44: Loss = 0.40802
Epoch 2.45: Loss = 0.53392
Epoch 2.46: Loss = 0.561462
Epoch 2.47: Loss = 0.473373
Epoch 2.48: Loss = 0.533463
Epoch 2.49: Loss = 0.511719
Epoch 2.50: Loss = 0.58078
Epoch 2.51: Loss = 0.441315
Epoch 2.52: Loss = 0.4366
Epoch 2.53: Loss = 0.500061
Epoch 2.54: Loss = 0.562241
Epoch 2.55: Loss = 0.519257
Epoch 2.56: Loss = 0.455032
Epoch 2.57: Loss = 0.423508
Epoch 2.58: Loss = 0.474136
Epoch 2.59: Loss = 0.53006
Epoch 2.60: Loss = 0.610336
Epoch 2.61: Loss = 0.568466
Epoch 2.62: Loss = 0.58197
Epoch 2.63: Loss = 0.618896
Epoch 2.64: Loss = 0.585297
Epoch 2.65: Loss = 0.664093
Epoch 2.66: Loss = 0.498489
Epoch 2.67: Loss = 0.543442
Epoch 2.68: Loss = 0.344193
Epoch 2.69: Loss = 0.423279
Epoch 2.70: Loss = 0.585892
Epoch 2.71: Loss = 0.44339
Epoch 2.72: Loss = 0.448608
Epoch 2.73: Loss = 0.482254
Epoch 2.74: Loss = 0.35791
Epoch 2.75: Loss = 0.583328
Epoch 2.76: Loss = 0.504669
Epoch 2.77: Loss = 0.453903
Epoch 2.78: Loss = 0.467133
Epoch 2.79: Loss = 0.507477
Epoch 2.80: Loss = 0.552795
Epoch 2.81: Loss = 0.417038
Epoch 2.82: Loss = 0.399338
Epoch 2.83: Loss = 0.577271
Epoch 2.84: Loss = 0.481018
Epoch 2.85: Loss = 0.631287
Epoch 2.86: Loss = 0.529251
Epoch 2.87: Loss = 0.388916
Epoch 2.88: Loss = 0.442902
Epoch 2.89: Loss = 0.556198
Epoch 2.90: Loss = 0.394516
Epoch 2.91: Loss = 0.516586
Epoch 2.92: Loss = 0.509567
Epoch 2.93: Loss = 0.549423
Epoch 2.94: Loss = 0.384888
Epoch 2.95: Loss = 0.482681
Epoch 2.96: Loss = 0.521896
Epoch 2.97: Loss = 0.368271
Epoch 2.98: Loss = 0.445328
Epoch 2.99: Loss = 0.53389
Epoch 2.100: Loss = 0.607697
Epoch 2.101: Loss = 0.557831
Epoch 2.102: Loss = 0.450882
Epoch 2.103: Loss = 0.410995
Epoch 2.104: Loss = 0.380981
Epoch 2.105: Loss = 0.542252
Epoch 2.106: Loss = 0.531891
Epoch 2.107: Loss = 0.396332
Epoch 2.108: Loss = 0.472916
Epoch 2.109: Loss = 0.41069
Epoch 2.110: Loss = 0.466476
Epoch 2.111: Loss = 0.366867
Epoch 2.112: Loss = 0.370499
Epoch 2.113: Loss = 0.436172
Epoch 2.114: Loss = 0.364517
Epoch 2.115: Loss = 0.397018
Epoch 2.116: Loss = 0.408066
Epoch 2.117: Loss = 0.304153
Epoch 2.118: Loss = 0.242447
Epoch 2.119: Loss = 0.300812
Epoch 2.120: Loss = 0.326141
TRAIN LOSS = 0.505493
TRAIN ACC = 85.2325 % (51142/60000)
Loss = 0.443542
Loss = 0.500595
Loss = 0.587509
Loss = 0.55188
Loss = 0.592148
Loss = 0.457779
Loss = 0.428116
Loss = 0.617081
Loss = 0.575806
Loss = 0.537491
Loss = 0.223129
Loss = 0.359451
Loss = 0.292007
Loss = 0.424805
Loss = 0.296768
Loss = 0.322708
Loss = 0.304825
Loss = 0.116165
Loss = 0.290665
Loss = 0.573807
TEST LOSS = 0.424814
TEST ACC = 511.42 % (8768/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.414474
Epoch 3.2: Loss = 0.53093
Epoch 3.3: Loss = 0.525146
Epoch 3.4: Loss = 0.358261
Epoch 3.5: Loss = 0.38623
Epoch 3.6: Loss = 0.390869
Epoch 3.7: Loss = 0.417969
Epoch 3.8: Loss = 0.416
Epoch 3.9: Loss = 0.392899
Epoch 3.10: Loss = 0.442841
Epoch 3.11: Loss = 0.432922
Epoch 3.12: Loss = 0.415359
Epoch 3.13: Loss = 0.350281
Epoch 3.14: Loss = 0.389618
Epoch 3.15: Loss = 0.491898
Epoch 3.16: Loss = 0.484283
Epoch 3.17: Loss = 0.502701
Epoch 3.18: Loss = 0.580811
Epoch 3.19: Loss = 0.418472
Epoch 3.20: Loss = 0.375107
Epoch 3.21: Loss = 0.34346
Epoch 3.22: Loss = 0.350571
Epoch 3.23: Loss = 0.342834
Epoch 3.24: Loss = 0.551865
Epoch 3.25: Loss = 0.456009
Epoch 3.26: Loss = 0.531052
Epoch 3.27: Loss = 0.505783
Epoch 3.28: Loss = 0.48996
Epoch 3.29: Loss = 0.568802
Epoch 3.30: Loss = 0.587479
Epoch 3.31: Loss = 0.399216
Epoch 3.32: Loss = 0.516602
Epoch 3.33: Loss = 0.418976
Epoch 3.34: Loss = 0.501907
Epoch 3.35: Loss = 0.452637
Epoch 3.36: Loss = 0.526947
Epoch 3.37: Loss = 0.35199
Epoch 3.38: Loss = 0.36557
Epoch 3.39: Loss = 0.409027
Epoch 3.40: Loss = 0.376129
Epoch 3.41: Loss = 0.424133
Epoch 3.42: Loss = 0.555008
Epoch 3.43: Loss = 0.378876
Epoch 3.44: Loss = 0.334625
Epoch 3.45: Loss = 0.436676
Epoch 3.46: Loss = 0.47934
Epoch 3.47: Loss = 0.40538
Epoch 3.48: Loss = 0.444916
Epoch 3.49: Loss = 0.430374
Epoch 3.50: Loss = 0.499634
Epoch 3.51: Loss = 0.361115
Epoch 3.52: Loss = 0.349396
Epoch 3.53: Loss = 0.416534
Epoch 3.54: Loss = 0.489426
Epoch 3.55: Loss = 0.424469
Epoch 3.56: Loss = 0.381653
Epoch 3.57: Loss = 0.364151
Epoch 3.58: Loss = 0.408081
Epoch 3.59: Loss = 0.470306
Epoch 3.60: Loss = 0.527618
Epoch 3.61: Loss = 0.482361
Epoch 3.62: Loss = 0.507187
Epoch 3.63: Loss = 0.567719
Epoch 3.64: Loss = 0.514313
Epoch 3.65: Loss = 0.587723
Epoch 3.66: Loss = 0.429092
Epoch 3.67: Loss = 0.466187
Epoch 3.68: Loss = 0.272202
Epoch 3.69: Loss = 0.355881
Epoch 3.70: Loss = 0.528687
Epoch 3.71: Loss = 0.36412
Epoch 3.72: Loss = 0.37001
Epoch 3.73: Loss = 0.420944
Epoch 3.74: Loss = 0.304245
Epoch 3.75: Loss = 0.567322
Epoch 3.76: Loss = 0.433624
Epoch 3.77: Loss = 0.376877
Epoch 3.78: Loss = 0.416
Epoch 3.79: Loss = 0.465942
Epoch 3.80: Loss = 0.466431
Epoch 3.81: Loss = 0.359375
Epoch 3.82: Loss = 0.334534
Epoch 3.83: Loss = 0.507416
Epoch 3.84: Loss = 0.416061
Epoch 3.85: Loss = 0.578964
Epoch 3.86: Loss = 0.486954
Epoch 3.87: Loss = 0.319214
Epoch 3.88: Loss = 0.388458
Epoch 3.89: Loss = 0.493927
Epoch 3.90: Loss = 0.344086
Epoch 3.91: Loss = 0.460663
Epoch 3.92: Loss = 0.463013
Epoch 3.93: Loss = 0.507904
Epoch 3.94: Loss = 0.32515
Epoch 3.95: Loss = 0.417587
Epoch 3.96: Loss = 0.481552
Epoch 3.97: Loss = 0.327866
Epoch 3.98: Loss = 0.388306
Epoch 3.99: Loss = 0.479187
Epoch 3.100: Loss = 0.554916
Epoch 3.101: Loss = 0.524399
Epoch 3.102: Loss = 0.399429
Epoch 3.103: Loss = 0.361755
Epoch 3.104: Loss = 0.3367
Epoch 3.105: Loss = 0.498718
Epoch 3.106: Loss = 0.49852
Epoch 3.107: Loss = 0.343384
Epoch 3.108: Loss = 0.433823
Epoch 3.109: Loss = 0.359238
Epoch 3.110: Loss = 0.422318
Epoch 3.111: Loss = 0.327377
Epoch 3.112: Loss = 0.332947
Epoch 3.113: Loss = 0.391815
Epoch 3.114: Loss = 0.319168
Epoch 3.115: Loss = 0.343018
Epoch 3.116: Loss = 0.362335
Epoch 3.117: Loss = 0.250107
Epoch 3.118: Loss = 0.196671
Epoch 3.119: Loss = 0.264252
Epoch 3.120: Loss = 0.305344
TRAIN LOSS = 0.426239
TRAIN ACC = 87.3291 % (52400/60000)
Loss = 0.390823
Loss = 0.46257
Loss = 0.53476
Loss = 0.521515
Loss = 0.553818
Loss = 0.411728
Loss = 0.375687
Loss = 0.586395
Loss = 0.533707
Loss = 0.492401
Loss = 0.193268
Loss = 0.310669
Loss = 0.275818
Loss = 0.380142
Loss = 0.244843
Loss = 0.289963
Loss = 0.255966
Loss = 0.0808411
Loss = 0.243469
Loss = 0.538376
TEST LOSS = 0.383838
TEST ACC = 523.999 % (8893/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.372238
Epoch 4.2: Loss = 0.482529
Epoch 4.3: Loss = 0.477112
Epoch 4.4: Loss = 0.314545
Epoch 4.5: Loss = 0.348709
Epoch 4.6: Loss = 0.351852
Epoch 4.7: Loss = 0.361679
Epoch 4.8: Loss = 0.373199
Epoch 4.9: Loss = 0.346832
Epoch 4.10: Loss = 0.40065
Epoch 4.11: Loss = 0.398422
Epoch 4.12: Loss = 0.38179
Epoch 4.13: Loss = 0.307083
Epoch 4.14: Loss = 0.34848
Epoch 4.15: Loss = 0.437027
Epoch 4.16: Loss = 0.44017
Epoch 4.17: Loss = 0.4673
Epoch 4.18: Loss = 0.558289
Epoch 4.19: Loss = 0.393021
Epoch 4.20: Loss = 0.334167
Epoch 4.21: Loss = 0.311508
Epoch 4.22: Loss = 0.309677
Epoch 4.23: Loss = 0.311691
Epoch 4.24: Loss = 0.518661
Epoch 4.25: Loss = 0.421555
Epoch 4.26: Loss = 0.496887
Epoch 4.27: Loss = 0.470291
Epoch 4.28: Loss = 0.453598
Epoch 4.29: Loss = 0.536499
Epoch 4.30: Loss = 0.542511
Epoch 4.31: Loss = 0.362198
Epoch 4.32: Loss = 0.470657
Epoch 4.33: Loss = 0.372467
Epoch 4.34: Loss = 0.462601
Epoch 4.35: Loss = 0.408707
Epoch 4.36: Loss = 0.480865
Epoch 4.37: Loss = 0.311417
Epoch 4.38: Loss = 0.33783
Epoch 4.39: Loss = 0.367294
Epoch 4.40: Loss = 0.348145
Epoch 4.41: Loss = 0.385071
Epoch 4.42: Loss = 0.54361
Epoch 4.43: Loss = 0.336319
Epoch 4.44: Loss = 0.306015
Epoch 4.45: Loss = 0.384811
Epoch 4.46: Loss = 0.437317
Epoch 4.47: Loss = 0.377228
Epoch 4.48: Loss = 0.407104
Epoch 4.49: Loss = 0.393356
Epoch 4.50: Loss = 0.461075
Epoch 4.51: Loss = 0.330063
Epoch 4.52: Loss = 0.315781
Epoch 4.53: Loss = 0.384079
Epoch 4.54: Loss = 0.463608
Epoch 4.55: Loss = 0.389435
Epoch 4.56: Loss = 0.352463
Epoch 4.57: Loss = 0.348938
Epoch 4.58: Loss = 0.386398
Epoch 4.59: Loss = 0.446915
Epoch 4.60: Loss = 0.494415
Epoch 4.61: Loss = 0.43512
Epoch 4.62: Loss = 0.478073
Epoch 4.63: Loss = 0.536316
Epoch 4.64: Loss = 0.488739
Epoch 4.65: Loss = 0.555847
Epoch 4.66: Loss = 0.398361
Epoch 4.67: Loss = 0.422531
Epoch 4.68: Loss = 0.247269
Epoch 4.69: Loss = 0.323898
Epoch 4.70: Loss = 0.499832
Epoch 4.71: Loss = 0.335022
Epoch 4.72: Loss = 0.329651
Epoch 4.73: Loss = 0.394455
Epoch 4.74: Loss = 0.285614
Epoch 4.75: Loss = 0.556885
Epoch 4.76: Loss = 0.408142
Epoch 4.77: Loss = 0.340637
Epoch 4.78: Loss = 0.383057
Epoch 4.79: Loss = 0.444031
Epoch 4.80: Loss = 0.423889
Epoch 4.81: Loss = 0.328918
Epoch 4.82: Loss = 0.307022
Epoch 4.83: Loss = 0.471848
Epoch 4.84: Loss = 0.378281
Epoch 4.85: Loss = 0.5522
Epoch 4.86: Loss = 0.475021
Epoch 4.87: Loss = 0.287384
Epoch 4.88: Loss = 0.364487
Epoch 4.89: Loss = 0.457489
Epoch 4.90: Loss = 0.317612
Epoch 4.91: Loss = 0.436707
Epoch 4.92: Loss = 0.44458
Epoch 4.93: Loss = 0.491547
Epoch 4.94: Loss = 0.301559
Epoch 4.95: Loss = 0.388855
Epoch 4.96: Loss = 0.455551
Epoch 4.97: Loss = 0.312653
Epoch 4.98: Loss = 0.36087
Epoch 4.99: Loss = 0.450912
Epoch 4.100: Loss = 0.525818
Epoch 4.101: Loss = 0.514725
Epoch 4.102: Loss = 0.374634
Epoch 4.103: Loss = 0.334137
Epoch 4.104: Loss = 0.314072
Epoch 4.105: Loss = 0.479462
Epoch 4.106: Loss = 0.48613
Epoch 4.107: Loss = 0.312378
Epoch 4.108: Loss = 0.415695
Epoch 4.109: Loss = 0.338989
Epoch 4.110: Loss = 0.402832
Epoch 4.111: Loss = 0.307495
Epoch 4.112: Loss = 0.317978
Epoch 4.113: Loss = 0.368637
Epoch 4.114: Loss = 0.299026
Epoch 4.115: Loss = 0.306686
Epoch 4.116: Loss = 0.33847
Epoch 4.117: Loss = 0.22908
Epoch 4.118: Loss = 0.177597
Epoch 4.119: Loss = 0.243317
Epoch 4.120: Loss = 0.290649
TRAIN LOSS = 0.394852
TRAIN ACC = 88.3392 % (53006/60000)
Loss = 0.363846
Loss = 0.440125
Loss = 0.509018
Loss = 0.507278
Loss = 0.531219
Loss = 0.386917
Loss = 0.350082
Loss = 0.573517
Loss = 0.511917
Loss = 0.470413
Loss = 0.175766
Loss = 0.287323
Loss = 0.264313
Loss = 0.354523
Loss = 0.213867
Loss = 0.270798
Loss = 0.226822
Loss = 0.0630646
Loss = 0.223251
Loss = 0.514969
TEST LOSS = 0.361951
TEST ACC = 530.06 % (8960/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.348114
Epoch 5.2: Loss = 0.458313
Epoch 5.3: Loss = 0.457031
Epoch 5.4: Loss = 0.29245
Epoch 5.5: Loss = 0.324463
Epoch 5.6: Loss = 0.330658
Epoch 5.7: Loss = 0.333984
Epoch 5.8: Loss = 0.35376
Epoch 5.9: Loss = 0.328049
Epoch 5.10: Loss = 0.373947
Epoch 5.11: Loss = 0.382767
Epoch 5.12: Loss = 0.364197
Epoch 5.13: Loss = 0.285004
Epoch 5.14: Loss = 0.324768
Epoch 5.15: Loss = 0.406387
Epoch 5.16: Loss = 0.414871
Epoch 5.17: Loss = 0.442657
Epoch 5.18: Loss = 0.54982
Epoch 5.19: Loss = 0.378235
Epoch 5.20: Loss = 0.311661
Epoch 5.21: Loss = 0.300858
Epoch 5.22: Loss = 0.288605
Epoch 5.23: Loss = 0.28537
Epoch 5.24: Loss = 0.505905
Epoch 5.25: Loss = 0.404404
Epoch 5.26: Loss = 0.480972
Epoch 5.27: Loss = 0.452133
Epoch 5.28: Loss = 0.434143
Epoch 5.29: Loss = 0.512009
Epoch 5.30: Loss = 0.520508
Epoch 5.31: Loss = 0.342484
Epoch 5.32: Loss = 0.442749
Epoch 5.33: Loss = 0.345703
Epoch 5.34: Loss = 0.437729
Epoch 5.35: Loss = 0.385101
Epoch 5.36: Loss = 0.456039
Epoch 5.37: Loss = 0.288712
Epoch 5.38: Loss = 0.319992
Epoch 5.39: Loss = 0.340439
Epoch 5.40: Loss = 0.336914
Epoch 5.41: Loss = 0.363327
Epoch 5.42: Loss = 0.537109
Epoch 5.43: Loss = 0.318146
Epoch 5.44: Loss = 0.296326
Epoch 5.45: Loss = 0.369003
Epoch 5.46: Loss = 0.419922
Epoch 5.47: Loss = 0.357056
Epoch 5.48: Loss = 0.387024
Epoch 5.49: Loss = 0.37529
Epoch 5.50: Loss = 0.443573
Epoch 5.51: Loss = 0.310013
Epoch 5.52: Loss = 0.296066
Epoch 5.53: Loss = 0.371307
Epoch 5.54: Loss = 0.455353
Epoch 5.55: Loss = 0.367798
Epoch 5.56: Loss = 0.33577
Epoch 5.57: Loss = 0.338257
Epoch 5.58: Loss = 0.37114
Epoch 5.59: Loss = 0.428482
Epoch 5.60: Loss = 0.481979
Epoch 5.61: Loss = 0.408615
Epoch 5.62: Loss = 0.457565
Epoch 5.63: Loss = 0.526367
Epoch 5.64: Loss = 0.477692
Epoch 5.65: Loss = 0.534561
Epoch 5.66: Loss = 0.383682
Epoch 5.67: Loss = 0.396805
Epoch 5.68: Loss = 0.234268
Epoch 5.69: Loss = 0.306946
Epoch 5.70: Loss = 0.480652
Epoch 5.71: Loss = 0.318726
Epoch 5.72: Loss = 0.303406
Epoch 5.73: Loss = 0.370453
Epoch 5.74: Loss = 0.275482
Epoch 5.75: Loss = 0.559311
Epoch 5.76: Loss = 0.39267
Epoch 5.77: Loss = 0.307693
Epoch 5.78: Loss = 0.372757
Epoch 5.79: Loss = 0.440735
Epoch 5.80: Loss = 0.398636
Epoch 5.81: Loss = 0.313675
Epoch 5.82: Loss = 0.291901
Epoch 5.83: Loss = 0.455719
Epoch 5.84: Loss = 0.361588
Epoch 5.85: Loss = 0.538162
Epoch 5.86: Loss = 0.466171
Epoch 5.87: Loss = 0.26799
Epoch 5.88: Loss = 0.353806
Epoch 5.89: Loss = 0.439941
Epoch 5.90: Loss = 0.307953
Epoch 5.91: Loss = 0.428116
Epoch 5.92: Loss = 0.434998
Epoch 5.93: Loss = 0.482559
Epoch 5.94: Loss = 0.284698
Epoch 5.95: Loss = 0.373474
Epoch 5.96: Loss = 0.44751
Epoch 5.97: Loss = 0.301041
Epoch 5.98: Loss = 0.340866
Epoch 5.99: Loss = 0.431595
Epoch 5.100: Loss = 0.512543
Epoch 5.101: Loss = 0.506989
Epoch 5.102: Loss = 0.360565
Epoch 5.103: Loss = 0.320145
Epoch 5.104: Loss = 0.300613
Epoch 5.105: Loss = 0.459854
Epoch 5.106: Loss = 0.481964
Epoch 5.107: Loss = 0.293915
Epoch 5.108: Loss = 0.409363
Epoch 5.109: Loss = 0.324692
Epoch 5.110: Loss = 0.39003
Epoch 5.111: Loss = 0.291656
Epoch 5.112: Loss = 0.308548
Epoch 5.113: Loss = 0.358765
Epoch 5.114: Loss = 0.288879
Epoch 5.115: Loss = 0.284073
Epoch 5.116: Loss = 0.328552
Epoch 5.117: Loss = 0.212204
Epoch 5.118: Loss = 0.168365
Epoch 5.119: Loss = 0.233398
Epoch 5.120: Loss = 0.289291
TRAIN LOSS = 0.377975
TRAIN ACC = 88.9999 % (53402/60000)
Loss = 0.345245
Loss = 0.423248
Loss = 0.490356
Loss = 0.498337
Loss = 0.517395
Loss = 0.369919
Loss = 0.334137
Loss = 0.563217
Loss = 0.495972
Loss = 0.455093
Loss = 0.168259
Loss = 0.280533
Loss = 0.263214
Loss = 0.339279
Loss = 0.202286
Loss = 0.263138
Loss = 0.215103
Loss = 0.0522766
Loss = 0.209366
Loss = 0.493591
TEST LOSS = 0.348998
TEST ACC = 534.019 % (9007/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.335587
Epoch 6.2: Loss = 0.446259
Epoch 6.3: Loss = 0.448395
Epoch 6.4: Loss = 0.280365
Epoch 6.5: Loss = 0.307068
Epoch 6.6: Loss = 0.314972
Epoch 6.7: Loss = 0.308044
Epoch 6.8: Loss = 0.336655
Epoch 6.9: Loss = 0.317123
Epoch 6.10: Loss = 0.360794
Epoch 6.11: Loss = 0.368011
Epoch 6.12: Loss = 0.352951
Epoch 6.13: Loss = 0.272522
Epoch 6.14: Loss = 0.314453
Epoch 6.15: Loss = 0.389297
Epoch 6.16: Loss = 0.398102
Epoch 6.17: Loss = 0.433029
Epoch 6.18: Loss = 0.5457
Epoch 6.19: Loss = 0.378769
Epoch 6.20: Loss = 0.301651
Epoch 6.21: Loss = 0.288284
Epoch 6.22: Loss = 0.274109
Epoch 6.23: Loss = 0.269257
Epoch 6.24: Loss = 0.492798
Epoch 6.25: Loss = 0.399033
Epoch 6.26: Loss = 0.471756
Epoch 6.27: Loss = 0.44043
Epoch 6.28: Loss = 0.421829
Epoch 6.29: Loss = 0.497421
Epoch 6.30: Loss = 0.503189
Epoch 6.31: Loss = 0.334946
Epoch 6.32: Loss = 0.420105
Epoch 6.33: Loss = 0.328369
Epoch 6.34: Loss = 0.420135
Epoch 6.35: Loss = 0.366928
Epoch 6.36: Loss = 0.444855
Epoch 6.37: Loss = 0.27179
Epoch 6.38: Loss = 0.310303
Epoch 6.39: Loss = 0.323639
Epoch 6.40: Loss = 0.324493
Epoch 6.41: Loss = 0.350082
Epoch 6.42: Loss = 0.536179
Epoch 6.43: Loss = 0.299683
Epoch 6.44: Loss = 0.286591
Epoch 6.45: Loss = 0.356339
Epoch 6.46: Loss = 0.396973
Epoch 6.47: Loss = 0.3452
Epoch 6.48: Loss = 0.378006
Epoch 6.49: Loss = 0.361862
Epoch 6.50: Loss = 0.434998
Epoch 6.51: Loss = 0.293839
Epoch 6.52: Loss = 0.284256
Epoch 6.53: Loss = 0.360916
Epoch 6.54: Loss = 0.443451
Epoch 6.55: Loss = 0.356705
Epoch 6.56: Loss = 0.318695
Epoch 6.57: Loss = 0.333389
Epoch 6.58: Loss = 0.360794
Epoch 6.59: Loss = 0.416122
Epoch 6.60: Loss = 0.461136
Epoch 6.61: Loss = 0.385422
Epoch 6.62: Loss = 0.438538
Epoch 6.63: Loss = 0.52066
Epoch 6.64: Loss = 0.458694
Epoch 6.65: Loss = 0.523483
Epoch 6.66: Loss = 0.366425
Epoch 6.67: Loss = 0.376007
Epoch 6.68: Loss = 0.220825
Epoch 6.69: Loss = 0.291702
Epoch 6.70: Loss = 0.464417
Epoch 6.71: Loss = 0.303787
Epoch 6.72: Loss = 0.285553
Epoch 6.73: Loss = 0.358551
Epoch 6.74: Loss = 0.265808
Epoch 6.75: Loss = 0.564301
Epoch 6.76: Loss = 0.382202
Epoch 6.77: Loss = 0.293777
Epoch 6.78: Loss = 0.363037
Epoch 6.79: Loss = 0.437775
Epoch 6.80: Loss = 0.381744
Epoch 6.81: Loss = 0.297302
Epoch 6.82: Loss = 0.281418
Epoch 6.83: Loss = 0.442734
Epoch 6.84: Loss = 0.341797
Epoch 6.85: Loss = 0.527634
Epoch 6.86: Loss = 0.454498
Epoch 6.87: Loss = 0.258209
Epoch 6.88: Loss = 0.343231
Epoch 6.89: Loss = 0.423218
Epoch 6.90: Loss = 0.293228
Epoch 6.91: Loss = 0.421173
Epoch 6.92: Loss = 0.427826
Epoch 6.93: Loss = 0.470154
Epoch 6.94: Loss = 0.273895
Epoch 6.95: Loss = 0.360565
Epoch 6.96: Loss = 0.433197
Epoch 6.97: Loss = 0.292267
Epoch 6.98: Loss = 0.332657
Epoch 6.99: Loss = 0.415314
Epoch 6.100: Loss = 0.497513
Epoch 6.101: Loss = 0.500854
Epoch 6.102: Loss = 0.353271
Epoch 6.103: Loss = 0.311111
Epoch 6.104: Loss = 0.294464
Epoch 6.105: Loss = 0.45256
Epoch 6.106: Loss = 0.478729
Epoch 6.107: Loss = 0.276917
Epoch 6.108: Loss = 0.404953
Epoch 6.109: Loss = 0.31517
Epoch 6.110: Loss = 0.379395
Epoch 6.111: Loss = 0.285416
Epoch 6.112: Loss = 0.304352
Epoch 6.113: Loss = 0.346481
Epoch 6.114: Loss = 0.279633
Epoch 6.115: Loss = 0.266541
Epoch 6.116: Loss = 0.32222
Epoch 6.117: Loss = 0.204773
Epoch 6.118: Loss = 0.160675
Epoch 6.119: Loss = 0.223816
Epoch 6.120: Loss = 0.284607
TRAIN LOSS = 0.365875
TRAIN ACC = 89.4516 % (53673/60000)
Loss = 0.336411
Loss = 0.414764
Loss = 0.475876
Loss = 0.496323
Loss = 0.506851
Loss = 0.358307
Loss = 0.326233
Loss = 0.560654
Loss = 0.490921
Loss = 0.44751
Loss = 0.156906
Loss = 0.274261
Loss = 0.261093
Loss = 0.327072
Loss = 0.187027
Loss = 0.266144
Loss = 0.199005
Loss = 0.0460052
Loss = 0.190506
Loss = 0.482361
TEST LOSS = 0.340211
TEST ACC = 536.729 % (9030/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.330795
Epoch 7.2: Loss = 0.430817
Epoch 7.3: Loss = 0.437225
Epoch 7.4: Loss = 0.269958
Epoch 7.5: Loss = 0.294037
Epoch 7.6: Loss = 0.308014
Epoch 7.7: Loss = 0.296509
Epoch 7.8: Loss = 0.327667
Epoch 7.9: Loss = 0.307098
Epoch 7.10: Loss = 0.343826
Epoch 7.11: Loss = 0.358932
Epoch 7.12: Loss = 0.341278
Epoch 7.13: Loss = 0.257538
Epoch 7.14: Loss = 0.307281
Epoch 7.15: Loss = 0.370453
Epoch 7.16: Loss = 0.381821
Epoch 7.17: Loss = 0.420074
Epoch 7.18: Loss = 0.54689
Epoch 7.19: Loss = 0.373444
Epoch 7.20: Loss = 0.289536
Epoch 7.21: Loss = 0.277679
Epoch 7.22: Loss = 0.265305
Epoch 7.23: Loss = 0.261215
Epoch 7.24: Loss = 0.477951
Epoch 7.25: Loss = 0.386414
Epoch 7.26: Loss = 0.469376
Epoch 7.27: Loss = 0.433899
Epoch 7.28: Loss = 0.41539
Epoch 7.29: Loss = 0.488556
Epoch 7.30: Loss = 0.494293
Epoch 7.31: Loss = 0.32785
Epoch 7.32: Loss = 0.403503
Epoch 7.33: Loss = 0.316116
Epoch 7.34: Loss = 0.406204
Epoch 7.35: Loss = 0.357788
Epoch 7.36: Loss = 0.432739
Epoch 7.37: Loss = 0.265808
Epoch 7.38: Loss = 0.300903
Epoch 7.39: Loss = 0.311081
Epoch 7.40: Loss = 0.319199
Epoch 7.41: Loss = 0.341049
Epoch 7.42: Loss = 0.532822
Epoch 7.43: Loss = 0.294418
Epoch 7.44: Loss = 0.28418
Epoch 7.45: Loss = 0.347122
Epoch 7.46: Loss = 0.387299
Epoch 7.47: Loss = 0.340088
Epoch 7.48: Loss = 0.373703
Epoch 7.49: Loss = 0.346008
Epoch 7.50: Loss = 0.43158
Epoch 7.51: Loss = 0.284912
Epoch 7.52: Loss = 0.277588
Epoch 7.53: Loss = 0.350403
Epoch 7.54: Loss = 0.435928
Epoch 7.55: Loss = 0.351074
Epoch 7.56: Loss = 0.313065
Epoch 7.57: Loss = 0.327652
Epoch 7.58: Loss = 0.353638
Epoch 7.59: Loss = 0.404312
Epoch 7.60: Loss = 0.451889
Epoch 7.61: Loss = 0.371613
Epoch 7.62: Loss = 0.42836
Epoch 7.63: Loss = 0.516495
Epoch 7.64: Loss = 0.450516
Epoch 7.65: Loss = 0.51004
Epoch 7.66: Loss = 0.360611
Epoch 7.67: Loss = 0.360016
Epoch 7.68: Loss = 0.220383
Epoch 7.69: Loss = 0.283722
Epoch 7.70: Loss = 0.451324
Epoch 7.71: Loss = 0.288864
Epoch 7.72: Loss = 0.275269
Epoch 7.73: Loss = 0.35083
Epoch 7.74: Loss = 0.264618
Epoch 7.75: Loss = 0.555725
Epoch 7.76: Loss = 0.381668
Epoch 7.77: Loss = 0.278732
Epoch 7.78: Loss = 0.353699
Epoch 7.79: Loss = 0.439606
Epoch 7.80: Loss = 0.363724
Epoch 7.81: Loss = 0.290527
Epoch 7.82: Loss = 0.271255
Epoch 7.83: Loss = 0.432236
Epoch 7.84: Loss = 0.332809
Epoch 7.85: Loss = 0.527435
Epoch 7.86: Loss = 0.445374
Epoch 7.87: Loss = 0.246994
Epoch 7.88: Loss = 0.336761
Epoch 7.89: Loss = 0.408127
Epoch 7.90: Loss = 0.284836
Epoch 7.91: Loss = 0.41861
Epoch 7.92: Loss = 0.419556
Epoch 7.93: Loss = 0.464783
Epoch 7.94: Loss = 0.260681
Epoch 7.95: Loss = 0.351028
Epoch 7.96: Loss = 0.414948
Epoch 7.97: Loss = 0.283218
Epoch 7.98: Loss = 0.323837
Epoch 7.99: Loss = 0.405197
Epoch 7.100: Loss = 0.491531
Epoch 7.101: Loss = 0.496811
Epoch 7.102: Loss = 0.343079
Epoch 7.103: Loss = 0.301605
Epoch 7.104: Loss = 0.290802
Epoch 7.105: Loss = 0.441299
Epoch 7.106: Loss = 0.474716
Epoch 7.107: Loss = 0.269836
Epoch 7.108: Loss = 0.396652
Epoch 7.109: Loss = 0.305588
Epoch 7.110: Loss = 0.372803
Epoch 7.111: Loss = 0.280975
Epoch 7.112: Loss = 0.297531
Epoch 7.113: Loss = 0.336395
Epoch 7.114: Loss = 0.266327
Epoch 7.115: Loss = 0.259338
Epoch 7.116: Loss = 0.315765
Epoch 7.117: Loss = 0.192001
Epoch 7.118: Loss = 0.157104
Epoch 7.119: Loss = 0.216064
Epoch 7.120: Loss = 0.288254
TRAIN LOSS = 0.357101
TRAIN ACC = 89.8544 % (53915/60000)
Loss = 0.320114
Loss = 0.407608
Loss = 0.469162
Loss = 0.489456
Loss = 0.495087
Loss = 0.345886
Loss = 0.317535
Loss = 0.555984
Loss = 0.485626
Loss = 0.441071
Loss = 0.155182
Loss = 0.262878
Loss = 0.258682
Loss = 0.325821
Loss = 0.17514
Loss = 0.263489
Loss = 0.187881
Loss = 0.0410614
Loss = 0.190857
Loss = 0.478668
TEST LOSS = 0.333359
TEST ACC = 539.149 % (9079/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.320999
Epoch 8.2: Loss = 0.424789
Epoch 8.3: Loss = 0.432922
Epoch 8.4: Loss = 0.259811
Epoch 8.5: Loss = 0.288971
Epoch 8.6: Loss = 0.301071
Epoch 8.7: Loss = 0.28775
Epoch 8.8: Loss = 0.316788
Epoch 8.9: Loss = 0.298386
Epoch 8.10: Loss = 0.335739
Epoch 8.11: Loss = 0.347977
Epoch 8.12: Loss = 0.336334
Epoch 8.13: Loss = 0.249023
Epoch 8.14: Loss = 0.295563
Epoch 8.15: Loss = 0.362457
Epoch 8.16: Loss = 0.373978
Epoch 8.17: Loss = 0.404861
Epoch 8.18: Loss = 0.548569
Epoch 8.19: Loss = 0.371017
Epoch 8.20: Loss = 0.280624
Epoch 8.21: Loss = 0.271317
Epoch 8.22: Loss = 0.258377
Epoch 8.23: Loss = 0.255539
Epoch 8.24: Loss = 0.466064
Epoch 8.25: Loss = 0.371948
Epoch 8.26: Loss = 0.473099
Epoch 8.27: Loss = 0.426819
Epoch 8.28: Loss = 0.406296
Epoch 8.29: Loss = 0.475311
Epoch 8.30: Loss = 0.485062
Epoch 8.31: Loss = 0.318542
Epoch 8.32: Loss = 0.387482
Epoch 8.33: Loss = 0.304565
Epoch 8.34: Loss = 0.394394
Epoch 8.35: Loss = 0.347672
Epoch 8.36: Loss = 0.42244
Epoch 8.37: Loss = 0.253418
Epoch 8.38: Loss = 0.289261
Epoch 8.39: Loss = 0.299835
Epoch 8.40: Loss = 0.314835
Epoch 8.41: Loss = 0.33609
Epoch 8.42: Loss = 0.530792
Epoch 8.43: Loss = 0.28302
Epoch 8.44: Loss = 0.278854
Epoch 8.45: Loss = 0.333832
Epoch 8.46: Loss = 0.376556
Epoch 8.47: Loss = 0.333298
Epoch 8.48: Loss = 0.369141
Epoch 8.49: Loss = 0.334198
Epoch 8.50: Loss = 0.426804
Epoch 8.51: Loss = 0.276367
Epoch 8.52: Loss = 0.279099
Epoch 8.53: Loss = 0.34198
Epoch 8.54: Loss = 0.437378
Epoch 8.55: Loss = 0.348007
Epoch 8.56: Loss = 0.307938
Epoch 8.57: Loss = 0.320129
Epoch 8.58: Loss = 0.348328
Epoch 8.59: Loss = 0.388229
Epoch 8.60: Loss = 0.440903
Epoch 8.61: Loss = 0.352692
Epoch 8.62: Loss = 0.416595
Epoch 8.63: Loss = 0.511826
Epoch 8.64: Loss = 0.441055
Epoch 8.65: Loss = 0.497833
Epoch 8.66: Loss = 0.355774
Epoch 8.67: Loss = 0.351715
Epoch 8.68: Loss = 0.2108
Epoch 8.69: Loss = 0.278625
Epoch 8.70: Loss = 0.447647
Epoch 8.71: Loss = 0.286835
Epoch 8.72: Loss = 0.264587
Epoch 8.73: Loss = 0.339523
Epoch 8.74: Loss = 0.257004
Epoch 8.75: Loss = 0.563004
Epoch 8.76: Loss = 0.37471
Epoch 8.77: Loss = 0.269211
Epoch 8.78: Loss = 0.348984
Epoch 8.79: Loss = 0.43689
Epoch 8.80: Loss = 0.358932
Epoch 8.81: Loss = 0.281479
Epoch 8.82: Loss = 0.264801
Epoch 8.83: Loss = 0.425217
Epoch 8.84: Loss = 0.325165
Epoch 8.85: Loss = 0.518921
Epoch 8.86: Loss = 0.445831
Epoch 8.87: Loss = 0.244965
Epoch 8.88: Loss = 0.33136
Epoch 8.89: Loss = 0.399429
Epoch 8.90: Loss = 0.282303
Epoch 8.91: Loss = 0.413025
Epoch 8.92: Loss = 0.417389
Epoch 8.93: Loss = 0.469193
Epoch 8.94: Loss = 0.250305
Epoch 8.95: Loss = 0.335419
Epoch 8.96: Loss = 0.409622
Epoch 8.97: Loss = 0.281372
Epoch 8.98: Loss = 0.311996
Epoch 8.99: Loss = 0.394287
Epoch 8.100: Loss = 0.481827
Epoch 8.101: Loss = 0.495804
Epoch 8.102: Loss = 0.327606
Epoch 8.103: Loss = 0.287018
Epoch 8.104: Loss = 0.28569
Epoch 8.105: Loss = 0.432755
Epoch 8.106: Loss = 0.475494
Epoch 8.107: Loss = 0.263519
Epoch 8.108: Loss = 0.395447
Epoch 8.109: Loss = 0.299194
Epoch 8.110: Loss = 0.366943
Epoch 8.111: Loss = 0.274963
Epoch 8.112: Loss = 0.291916
Epoch 8.113: Loss = 0.329926
Epoch 8.114: Loss = 0.267761
Epoch 8.115: Loss = 0.253326
Epoch 8.116: Loss = 0.316605
Epoch 8.117: Loss = 0.186691
Epoch 8.118: Loss = 0.153595
Epoch 8.119: Loss = 0.210098
Epoch 8.120: Loss = 0.289261
TRAIN LOSS = 0.350021
TRAIN ACC = 90.1291 % (54080/60000)
Loss = 0.316711
Loss = 0.405228
Loss = 0.456253
Loss = 0.48999
Loss = 0.492706
Loss = 0.33902
Loss = 0.306549
Loss = 0.556686
Loss = 0.480835
Loss = 0.437561
Loss = 0.145447
Loss = 0.257736
Loss = 0.262146
Loss = 0.315018
Loss = 0.170349
Loss = 0.249237
Loss = 0.180878
Loss = 0.0388794
Loss = 0.182877
Loss = 0.471268
TEST LOSS = 0.327769
TEST ACC = 540.799 % (9096/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.311859
Epoch 9.2: Loss = 0.418732
Epoch 9.3: Loss = 0.429214
Epoch 9.4: Loss = 0.250244
Epoch 9.5: Loss = 0.28627
Epoch 9.6: Loss = 0.294052
Epoch 9.7: Loss = 0.278336
Epoch 9.8: Loss = 0.313354
Epoch 9.9: Loss = 0.29541
Epoch 9.10: Loss = 0.328918
Epoch 9.11: Loss = 0.34906
Epoch 9.12: Loss = 0.333237
Epoch 9.13: Loss = 0.244049
Epoch 9.14: Loss = 0.294998
Epoch 9.15: Loss = 0.356766
Epoch 9.16: Loss = 0.363266
Epoch 9.17: Loss = 0.396439
Epoch 9.18: Loss = 0.550156
Epoch 9.19: Loss = 0.369308
Epoch 9.20: Loss = 0.271973
Epoch 9.21: Loss = 0.269897
Epoch 9.22: Loss = 0.252106
Epoch 9.23: Loss = 0.251694
Epoch 9.24: Loss = 0.46492
Epoch 9.25: Loss = 0.376648
Epoch 9.26: Loss = 0.472946
Epoch 9.27: Loss = 0.4254
Epoch 9.28: Loss = 0.405212
Epoch 9.29: Loss = 0.469193
Epoch 9.30: Loss = 0.47699
Epoch 9.31: Loss = 0.312653
Epoch 9.32: Loss = 0.381622
Epoch 9.33: Loss = 0.297333
Epoch 9.34: Loss = 0.385117
Epoch 9.35: Loss = 0.343292
Epoch 9.36: Loss = 0.416183
Epoch 9.37: Loss = 0.249252
Epoch 9.38: Loss = 0.285156
Epoch 9.39: Loss = 0.292145
Epoch 9.40: Loss = 0.307373
Epoch 9.41: Loss = 0.326157
Epoch 9.42: Loss = 0.531525
Epoch 9.43: Loss = 0.280136
Epoch 9.44: Loss = 0.274155
Epoch 9.45: Loss = 0.33493
Epoch 9.46: Loss = 0.368973
Epoch 9.47: Loss = 0.334106
Epoch 9.48: Loss = 0.365494
Epoch 9.49: Loss = 0.325119
Epoch 9.50: Loss = 0.421814
Epoch 9.51: Loss = 0.26712
Epoch 9.52: Loss = 0.266098
Epoch 9.53: Loss = 0.337967
Epoch 9.54: Loss = 0.434875
Epoch 9.55: Loss = 0.33963
Epoch 9.56: Loss = 0.300842
Epoch 9.57: Loss = 0.320648
Epoch 9.58: Loss = 0.340729
Epoch 9.59: Loss = 0.384567
Epoch 9.60: Loss = 0.434372
Epoch 9.61: Loss = 0.346283
Epoch 9.62: Loss = 0.409561
Epoch 9.63: Loss = 0.505997
Epoch 9.64: Loss = 0.435211
Epoch 9.65: Loss = 0.487961
Epoch 9.66: Loss = 0.342636
Epoch 9.67: Loss = 0.34166
Epoch 9.68: Loss = 0.20668
Epoch 9.69: Loss = 0.267975
Epoch 9.70: Loss = 0.440186
Epoch 9.71: Loss = 0.283981
Epoch 9.72: Loss = 0.251587
Epoch 9.73: Loss = 0.332581
Epoch 9.74: Loss = 0.254608
Epoch 9.75: Loss = 0.56218
Epoch 9.76: Loss = 0.372223
Epoch 9.77: Loss = 0.266586
Epoch 9.78: Loss = 0.340988
Epoch 9.79: Loss = 0.430893
Epoch 9.80: Loss = 0.344376
Epoch 9.81: Loss = 0.27449
Epoch 9.82: Loss = 0.266037
Epoch 9.83: Loss = 0.418686
Epoch 9.84: Loss = 0.308212
Epoch 9.85: Loss = 0.510666
Epoch 9.86: Loss = 0.439774
Epoch 9.87: Loss = 0.239136
Epoch 9.88: Loss = 0.323807
Epoch 9.89: Loss = 0.391754
Epoch 9.90: Loss = 0.27771
Epoch 9.91: Loss = 0.403076
Epoch 9.92: Loss = 0.414093
Epoch 9.93: Loss = 0.467377
Epoch 9.94: Loss = 0.24649
Epoch 9.95: Loss = 0.32843
Epoch 9.96: Loss = 0.399994
Epoch 9.97: Loss = 0.274857
Epoch 9.98: Loss = 0.304504
Epoch 9.99: Loss = 0.386826
Epoch 9.100: Loss = 0.478577
Epoch 9.101: Loss = 0.49678
Epoch 9.102: Loss = 0.324295
Epoch 9.103: Loss = 0.281174
Epoch 9.104: Loss = 0.28508
Epoch 9.105: Loss = 0.42128
Epoch 9.106: Loss = 0.476669
Epoch 9.107: Loss = 0.255722
Epoch 9.108: Loss = 0.394699
Epoch 9.109: Loss = 0.290283
Epoch 9.110: Loss = 0.360825
Epoch 9.111: Loss = 0.273895
Epoch 9.112: Loss = 0.286469
Epoch 9.113: Loss = 0.322021
Epoch 9.114: Loss = 0.258972
Epoch 9.115: Loss = 0.239044
Epoch 9.116: Loss = 0.311539
Epoch 9.117: Loss = 0.181854
Epoch 9.118: Loss = 0.149628
Epoch 9.119: Loss = 0.205551
Epoch 9.120: Loss = 0.292252
TRAIN LOSS = 0.344559
TRAIN ACC = 90.3732 % (54226/60000)
Loss = 0.315704
Loss = 0.400223
Loss = 0.452545
Loss = 0.48671
Loss = 0.487198
Loss = 0.333481
Loss = 0.30574
Loss = 0.554581
Loss = 0.474121
Loss = 0.428146
Loss = 0.140472
Loss = 0.253662
Loss = 0.266388
Loss = 0.307877
Loss = 0.163528
Loss = 0.242569
Loss = 0.175644
Loss = 0.0367889
Loss = 0.178299
Loss = 0.477539
TEST LOSS = 0.324061
TEST ACC = 542.259 % (9106/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.304291
Epoch 10.2: Loss = 0.410187
Epoch 10.3: Loss = 0.422699
Epoch 10.4: Loss = 0.242126
Epoch 10.5: Loss = 0.274475
Epoch 10.6: Loss = 0.291458
Epoch 10.7: Loss = 0.274902
Epoch 10.8: Loss = 0.303879
Epoch 10.9: Loss = 0.294998
Epoch 10.10: Loss = 0.32634
Epoch 10.11: Loss = 0.35022
Epoch 10.12: Loss = 0.332626
Epoch 10.13: Loss = 0.238312
Epoch 10.14: Loss = 0.286041
Epoch 10.15: Loss = 0.343521
Epoch 10.16: Loss = 0.358383
Epoch 10.17: Loss = 0.388443
Epoch 10.18: Loss = 0.542862
Epoch 10.19: Loss = 0.368515
Epoch 10.20: Loss = 0.263031
Epoch 10.21: Loss = 0.26506
Epoch 10.22: Loss = 0.251785
Epoch 10.23: Loss = 0.246796
Epoch 10.24: Loss = 0.465744
Epoch 10.25: Loss = 0.368561
Epoch 10.26: Loss = 0.478256
Epoch 10.27: Loss = 0.421127
Epoch 10.28: Loss = 0.402008
Epoch 10.29: Loss = 0.46698
Epoch 10.30: Loss = 0.466049
Epoch 10.31: Loss = 0.311264
Epoch 10.32: Loss = 0.367325
Epoch 10.33: Loss = 0.290207
Epoch 10.34: Loss = 0.376755
Epoch 10.35: Loss = 0.335098
Epoch 10.36: Loss = 0.409622
Epoch 10.37: Loss = 0.245026
Epoch 10.38: Loss = 0.283035
Epoch 10.39: Loss = 0.286087
Epoch 10.40: Loss = 0.309189
Epoch 10.41: Loss = 0.329269
Epoch 10.42: Loss = 0.532379
Epoch 10.43: Loss = 0.279358
Epoch 10.44: Loss = 0.266388
Epoch 10.45: Loss = 0.323135
Epoch 10.46: Loss = 0.360504
Epoch 10.47: Loss = 0.329315
Epoch 10.48: Loss = 0.360855
Epoch 10.49: Loss = 0.318359
Epoch 10.50: Loss = 0.421051
Epoch 10.51: Loss = 0.26059
Epoch 10.52: Loss = 0.2612
Epoch 10.53: Loss = 0.334442
Epoch 10.54: Loss = 0.42749
Epoch 10.55: Loss = 0.344543
Epoch 10.56: Loss = 0.301834
Epoch 10.57: Loss = 0.320923
Epoch 10.58: Loss = 0.338516
Epoch 10.59: Loss = 0.374466
Epoch 10.60: Loss = 0.426529
Epoch 10.61: Loss = 0.336136
Epoch 10.62: Loss = 0.404449
Epoch 10.63: Loss = 0.508316
Epoch 10.64: Loss = 0.427979
Epoch 10.65: Loss = 0.476212
Epoch 10.66: Loss = 0.337158
Epoch 10.67: Loss = 0.332077
Epoch 10.68: Loss = 0.202957
Epoch 10.69: Loss = 0.265732
Epoch 10.70: Loss = 0.428787
Epoch 10.71: Loss = 0.272903
Epoch 10.72: Loss = 0.244766
Epoch 10.73: Loss = 0.329483
Epoch 10.74: Loss = 0.254532
Epoch 10.75: Loss = 0.567307
Epoch 10.76: Loss = 0.364685
Epoch 10.77: Loss = 0.251251
Epoch 10.78: Loss = 0.337601
Epoch 10.79: Loss = 0.434219
Epoch 10.80: Loss = 0.334976
Epoch 10.81: Loss = 0.266708
Epoch 10.82: Loss = 0.26384
Epoch 10.83: Loss = 0.413986
Epoch 10.84: Loss = 0.303864
Epoch 10.85: Loss = 0.501953
Epoch 10.86: Loss = 0.441803
Epoch 10.87: Loss = 0.235123
Epoch 10.88: Loss = 0.323227
Epoch 10.89: Loss = 0.391708
Epoch 10.90: Loss = 0.271027
Epoch 10.91: Loss = 0.401901
Epoch 10.92: Loss = 0.41275
Epoch 10.93: Loss = 0.46376
Epoch 10.94: Loss = 0.237564
Epoch 10.95: Loss = 0.333038
Epoch 10.96: Loss = 0.400269
Epoch 10.97: Loss = 0.267181
Epoch 10.98: Loss = 0.300781
Epoch 10.99: Loss = 0.381577
Epoch 10.100: Loss = 0.471222
Epoch 10.101: Loss = 0.495041
Epoch 10.102: Loss = 0.322281
Epoch 10.103: Loss = 0.276932
Epoch 10.104: Loss = 0.28331
Epoch 10.105: Loss = 0.41774
Epoch 10.106: Loss = 0.475876
Epoch 10.107: Loss = 0.252869
Epoch 10.108: Loss = 0.390305
Epoch 10.109: Loss = 0.283325
Epoch 10.110: Loss = 0.360626
Epoch 10.111: Loss = 0.275558
Epoch 10.112: Loss = 0.283401
Epoch 10.113: Loss = 0.32251
Epoch 10.114: Loss = 0.254044
Epoch 10.115: Loss = 0.233093
Epoch 10.116: Loss = 0.311661
Epoch 10.117: Loss = 0.180359
Epoch 10.118: Loss = 0.145065
Epoch 10.119: Loss = 0.206863
Epoch 10.120: Loss = 0.290588
TRAIN LOSS = 0.340256
TRAIN ACC = 90.625 % (54378/60000)
Loss = 0.305283
Loss = 0.393417
Loss = 0.447189
Loss = 0.487991
Loss = 0.493835
Loss = 0.331421
Loss = 0.302551
Loss = 0.558563
Loss = 0.474884
Loss = 0.426758
Loss = 0.142471
Loss = 0.249176
Loss = 0.26825
Loss = 0.304199
Loss = 0.150497
Loss = 0.235519
Loss = 0.169479
Loss = 0.0355225
Loss = 0.181351
Loss = 0.46344
TEST LOSS = 0.32109
TEST ACC = 543.779 % (9137/10000)
