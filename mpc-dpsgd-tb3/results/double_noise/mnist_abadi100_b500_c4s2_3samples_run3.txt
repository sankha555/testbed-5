Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.43353
Epoch 1.2: Loss = 2.32607
Epoch 1.3: Loss = 2.2867
Epoch 1.4: Loss = 2.2148
Epoch 1.5: Loss = 2.17863
Epoch 1.6: Loss = 2.15031
Epoch 1.7: Loss = 2.06932
Epoch 1.8: Loss = 2.10213
Epoch 1.9: Loss = 2.02992
Epoch 1.10: Loss = 2.00487
Epoch 1.11: Loss = 1.91911
Epoch 1.12: Loss = 1.90553
Epoch 1.13: Loss = 1.82857
Epoch 1.14: Loss = 1.80655
Epoch 1.15: Loss = 1.84555
Epoch 1.16: Loss = 1.77016
Epoch 1.17: Loss = 1.7318
Epoch 1.18: Loss = 1.70461
Epoch 1.19: Loss = 1.67006
Epoch 1.20: Loss = 1.61945
Epoch 1.21: Loss = 1.54677
Epoch 1.22: Loss = 1.53886
Epoch 1.23: Loss = 1.47057
Epoch 1.24: Loss = 1.55634
Epoch 1.25: Loss = 1.4828
Epoch 1.26: Loss = 1.51212
Epoch 1.27: Loss = 1.43867
Epoch 1.28: Loss = 1.43338
Epoch 1.29: Loss = 1.46115
Epoch 1.30: Loss = 1.52736
Epoch 1.31: Loss = 1.34505
Epoch 1.32: Loss = 1.37904
Epoch 1.33: Loss = 1.29233
Epoch 1.34: Loss = 1.37645
Epoch 1.35: Loss = 1.25076
Epoch 1.36: Loss = 1.38699
Epoch 1.37: Loss = 1.23997
Epoch 1.38: Loss = 1.21193
Epoch 1.39: Loss = 1.21045
Epoch 1.40: Loss = 1.11548
Epoch 1.41: Loss = 1.18564
Epoch 1.42: Loss = 1.13281
Epoch 1.43: Loss = 1.10854
Epoch 1.44: Loss = 1.01242
Epoch 1.45: Loss = 1.1644
Epoch 1.46: Loss = 1.09554
Epoch 1.47: Loss = 1.02977
Epoch 1.48: Loss = 1.08928
Epoch 1.49: Loss = 1.04045
Epoch 1.50: Loss = 1.08769
Epoch 1.51: Loss = 0.922928
Epoch 1.52: Loss = 0.9599
Epoch 1.53: Loss = 1.0056
Epoch 1.54: Loss = 1.05141
Epoch 1.55: Loss = 1.01817
Epoch 1.56: Loss = 0.932083
Epoch 1.57: Loss = 0.862625
Epoch 1.58: Loss = 0.909866
Epoch 1.59: Loss = 0.913986
Epoch 1.60: Loss = 1.00082
Epoch 1.61: Loss = 0.955978
Epoch 1.62: Loss = 0.967941
Epoch 1.63: Loss = 0.993958
Epoch 1.64: Loss = 0.949677
Epoch 1.65: Loss = 0.999985
Epoch 1.66: Loss = 0.864182
Epoch 1.67: Loss = 0.87355
Epoch 1.68: Loss = 0.74469
Epoch 1.69: Loss = 0.797836
Epoch 1.70: Loss = 0.873108
Epoch 1.71: Loss = 0.787659
Epoch 1.72: Loss = 0.802826
Epoch 1.73: Loss = 0.809677
Epoch 1.74: Loss = 0.687759
Epoch 1.75: Loss = 0.812836
Epoch 1.76: Loss = 0.814224
Epoch 1.77: Loss = 0.757599
Epoch 1.78: Loss = 0.740723
Epoch 1.79: Loss = 0.740891
Epoch 1.80: Loss = 0.835587
Epoch 1.81: Loss = 0.69722
Epoch 1.82: Loss = 0.681335
Epoch 1.83: Loss = 0.860992
Epoch 1.84: Loss = 0.751678
Epoch 1.85: Loss = 0.826004
Epoch 1.86: Loss = 0.732086
Epoch 1.87: Loss = 0.64238
Epoch 1.88: Loss = 0.702805
Epoch 1.89: Loss = 0.765259
Epoch 1.90: Loss = 0.650635
Epoch 1.91: Loss = 0.739792
Epoch 1.92: Loss = 0.699066
Epoch 1.93: Loss = 0.745316
Epoch 1.94: Loss = 0.586563
Epoch 1.95: Loss = 0.677795
Epoch 1.96: Loss = 0.668198
Epoch 1.97: Loss = 0.550064
Epoch 1.98: Loss = 0.643814
Epoch 1.99: Loss = 0.730408
Epoch 1.100: Loss = 0.819992
Epoch 1.101: Loss = 0.717728
Epoch 1.102: Loss = 0.649918
Epoch 1.103: Loss = 0.603943
Epoch 1.104: Loss = 0.559891
Epoch 1.105: Loss = 0.678238
Epoch 1.106: Loss = 0.678909
Epoch 1.107: Loss = 0.565979
Epoch 1.108: Loss = 0.620163
Epoch 1.109: Loss = 0.59671
Epoch 1.110: Loss = 0.621185
Epoch 1.111: Loss = 0.509399
Epoch 1.112: Loss = 0.506485
Epoch 1.113: Loss = 0.601578
Epoch 1.114: Loss = 0.500153
Epoch 1.115: Loss = 0.590561
Epoch 1.116: Loss = 0.584015
Epoch 1.117: Loss = 0.478027
Epoch 1.118: Loss = 0.418015
Epoch 1.119: Loss = 0.448288
Epoch 1.120: Loss = 0.460251
TRAIN LOSS = 1.08559
TRAIN ACC = 69.6655 % (41801/60000)
Loss = 0.603699
Loss = 0.634796
Loss = 0.750198
Loss = 0.681213
Loss = 0.738022
Loss = 0.636185
Loss = 0.585785
Loss = 0.746704
Loss = 0.697556
Loss = 0.660034
Loss = 0.336182
Loss = 0.518402
Loss = 0.384674
Loss = 0.537979
Loss = 0.438263
Loss = 0.451202
Loss = 0.404831
Loss = 0.244705
Loss = 0.424133
Loss = 0.689728
TEST LOSS = 0.558214
TEST ACC = 418.01 % (8413/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.549728
Epoch 2.2: Loss = 0.654694
Epoch 2.3: Loss = 0.643097
Epoch 2.4: Loss = 0.481812
Epoch 2.5: Loss = 0.534744
Epoch 2.6: Loss = 0.537048
Epoch 2.7: Loss = 0.560242
Epoch 2.8: Loss = 0.549118
Epoch 2.9: Loss = 0.523682
Epoch 2.10: Loss = 0.544937
Epoch 2.11: Loss = 0.541656
Epoch 2.12: Loss = 0.514389
Epoch 2.13: Loss = 0.46701
Epoch 2.14: Loss = 0.505096
Epoch 2.15: Loss = 0.638702
Epoch 2.16: Loss = 0.59523
Epoch 2.17: Loss = 0.605453
Epoch 2.18: Loss = 0.651077
Epoch 2.19: Loss = 0.530884
Epoch 2.20: Loss = 0.495544
Epoch 2.21: Loss = 0.448532
Epoch 2.22: Loss = 0.455032
Epoch 2.23: Loss = 0.451889
Epoch 2.24: Loss = 0.633774
Epoch 2.25: Loss = 0.550415
Epoch 2.26: Loss = 0.616562
Epoch 2.27: Loss = 0.587296
Epoch 2.28: Loss = 0.569092
Epoch 2.29: Loss = 0.645676
Epoch 2.30: Loss = 0.69075
Epoch 2.31: Loss = 0.473221
Epoch 2.32: Loss = 0.607513
Epoch 2.33: Loss = 0.514648
Epoch 2.34: Loss = 0.594666
Epoch 2.35: Loss = 0.536865
Epoch 2.36: Loss = 0.621033
Epoch 2.37: Loss = 0.433777
Epoch 2.38: Loss = 0.443893
Epoch 2.39: Loss = 0.521912
Epoch 2.40: Loss = 0.453796
Epoch 2.41: Loss = 0.539688
Epoch 2.42: Loss = 0.589874
Epoch 2.43: Loss = 0.474319
Epoch 2.44: Loss = 0.398819
Epoch 2.45: Loss = 0.55217
Epoch 2.46: Loss = 0.544113
Epoch 2.47: Loss = 0.458176
Epoch 2.48: Loss = 0.549026
Epoch 2.49: Loss = 0.509003
Epoch 2.50: Loss = 0.5979
Epoch 2.51: Loss = 0.450531
Epoch 2.52: Loss = 0.445221
Epoch 2.53: Loss = 0.493408
Epoch 2.54: Loss = 0.593323
Epoch 2.55: Loss = 0.528442
Epoch 2.56: Loss = 0.47583
Epoch 2.57: Loss = 0.447479
Epoch 2.58: Loss = 0.494247
Epoch 2.59: Loss = 0.545807
Epoch 2.60: Loss = 0.584686
Epoch 2.61: Loss = 0.592667
Epoch 2.62: Loss = 0.576141
Epoch 2.63: Loss = 0.629211
Epoch 2.64: Loss = 0.586472
Epoch 2.65: Loss = 0.66124
Epoch 2.66: Loss = 0.491959
Epoch 2.67: Loss = 0.540833
Epoch 2.68: Loss = 0.368332
Epoch 2.69: Loss = 0.432495
Epoch 2.70: Loss = 0.589569
Epoch 2.71: Loss = 0.428482
Epoch 2.72: Loss = 0.467087
Epoch 2.73: Loss = 0.491821
Epoch 2.74: Loss = 0.380951
Epoch 2.75: Loss = 0.583344
Epoch 2.76: Loss = 0.506943
Epoch 2.77: Loss = 0.447479
Epoch 2.78: Loss = 0.482651
Epoch 2.79: Loss = 0.506668
Epoch 2.80: Loss = 0.558624
Epoch 2.81: Loss = 0.435974
Epoch 2.82: Loss = 0.4216
Epoch 2.83: Loss = 0.580399
Epoch 2.84: Loss = 0.486176
Epoch 2.85: Loss = 0.622452
Epoch 2.86: Loss = 0.52948
Epoch 2.87: Loss = 0.393311
Epoch 2.88: Loss = 0.471069
Epoch 2.89: Loss = 0.552444
Epoch 2.90: Loss = 0.426956
Epoch 2.91: Loss = 0.535309
Epoch 2.92: Loss = 0.515747
Epoch 2.93: Loss = 0.551346
Epoch 2.94: Loss = 0.392792
Epoch 2.95: Loss = 0.469284
Epoch 2.96: Loss = 0.513321
Epoch 2.97: Loss = 0.395798
Epoch 2.98: Loss = 0.448517
Epoch 2.99: Loss = 0.577286
Epoch 2.100: Loss = 0.64212
Epoch 2.101: Loss = 0.577682
Epoch 2.102: Loss = 0.474075
Epoch 2.103: Loss = 0.436279
Epoch 2.104: Loss = 0.392258
Epoch 2.105: Loss = 0.543762
Epoch 2.106: Loss = 0.538879
Epoch 2.107: Loss = 0.40274
Epoch 2.108: Loss = 0.484726
Epoch 2.109: Loss = 0.429916
Epoch 2.110: Loss = 0.478271
Epoch 2.111: Loss = 0.363113
Epoch 2.112: Loss = 0.370087
Epoch 2.113: Loss = 0.442871
Epoch 2.114: Loss = 0.372635
Epoch 2.115: Loss = 0.414352
Epoch 2.116: Loss = 0.442001
Epoch 2.117: Loss = 0.312515
Epoch 2.118: Loss = 0.26123
Epoch 2.119: Loss = 0.332092
Epoch 2.120: Loss = 0.346481
TRAIN LOSS = 0.507904
TRAIN ACC = 85.0769 % (51049/60000)
Loss = 0.445175
Loss = 0.520325
Loss = 0.609177
Loss = 0.562988
Loss = 0.626694
Loss = 0.477356
Loss = 0.454361
Loss = 0.640808
Loss = 0.565231
Loss = 0.535492
Loss = 0.220703
Loss = 0.374146
Loss = 0.302216
Loss = 0.413666
Loss = 0.3004
Loss = 0.351532
Loss = 0.291611
Loss = 0.126221
Loss = 0.278427
Loss = 0.605164
TEST LOSS = 0.435084
TEST ACC = 510.489 % (8706/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.430817
Epoch 3.2: Loss = 0.537445
Epoch 3.3: Loss = 0.511414
Epoch 3.4: Loss = 0.354736
Epoch 3.5: Loss = 0.416016
Epoch 3.6: Loss = 0.412567
Epoch 3.7: Loss = 0.417084
Epoch 3.8: Loss = 0.429352
Epoch 3.9: Loss = 0.404587
Epoch 3.10: Loss = 0.448425
Epoch 3.11: Loss = 0.460938
Epoch 3.12: Loss = 0.412933
Epoch 3.13: Loss = 0.369812
Epoch 3.14: Loss = 0.404831
Epoch 3.15: Loss = 0.503983
Epoch 3.16: Loss = 0.485886
Epoch 3.17: Loss = 0.508484
Epoch 3.18: Loss = 0.596451
Epoch 3.19: Loss = 0.447479
Epoch 3.20: Loss = 0.406647
Epoch 3.21: Loss = 0.373459
Epoch 3.22: Loss = 0.368561
Epoch 3.23: Loss = 0.378571
Epoch 3.24: Loss = 0.564026
Epoch 3.25: Loss = 0.467438
Epoch 3.26: Loss = 0.543701
Epoch 3.27: Loss = 0.506714
Epoch 3.28: Loss = 0.488266
Epoch 3.29: Loss = 0.56955
Epoch 3.30: Loss = 0.628006
Epoch 3.31: Loss = 0.386719
Epoch 3.32: Loss = 0.529663
Epoch 3.33: Loss = 0.419754
Epoch 3.34: Loss = 0.495087
Epoch 3.35: Loss = 0.422577
Epoch 3.36: Loss = 0.493057
Epoch 3.37: Loss = 0.340332
Epoch 3.38: Loss = 0.368896
Epoch 3.39: Loss = 0.433304
Epoch 3.40: Loss = 0.382599
Epoch 3.41: Loss = 0.464249
Epoch 3.42: Loss = 0.555786
Epoch 3.43: Loss = 0.392746
Epoch 3.44: Loss = 0.337997
Epoch 3.45: Loss = 0.485336
Epoch 3.46: Loss = 0.499146
Epoch 3.47: Loss = 0.388611
Epoch 3.48: Loss = 0.449341
Epoch 3.49: Loss = 0.439453
Epoch 3.50: Loss = 0.513351
Epoch 3.51: Loss = 0.385056
Epoch 3.52: Loss = 0.354568
Epoch 3.53: Loss = 0.419586
Epoch 3.54: Loss = 0.52449
Epoch 3.55: Loss = 0.448502
Epoch 3.56: Loss = 0.410507
Epoch 3.57: Loss = 0.392792
Epoch 3.58: Loss = 0.425903
Epoch 3.59: Loss = 0.49971
Epoch 3.60: Loss = 0.515915
Epoch 3.61: Loss = 0.50174
Epoch 3.62: Loss = 0.515472
Epoch 3.63: Loss = 0.580627
Epoch 3.64: Loss = 0.530991
Epoch 3.65: Loss = 0.603012
Epoch 3.66: Loss = 0.435318
Epoch 3.67: Loss = 0.448029
Epoch 3.68: Loss = 0.293518
Epoch 3.69: Loss = 0.367355
Epoch 3.70: Loss = 0.533463
Epoch 3.71: Loss = 0.369995
Epoch 3.72: Loss = 0.382172
Epoch 3.73: Loss = 0.427765
Epoch 3.74: Loss = 0.349899
Epoch 3.75: Loss = 0.559799
Epoch 3.76: Loss = 0.451462
Epoch 3.77: Loss = 0.391861
Epoch 3.78: Loss = 0.429214
Epoch 3.79: Loss = 0.467072
Epoch 3.80: Loss = 0.497452
Epoch 3.81: Loss = 0.382477
Epoch 3.82: Loss = 0.357208
Epoch 3.83: Loss = 0.51474
Epoch 3.84: Loss = 0.437531
Epoch 3.85: Loss = 0.578903
Epoch 3.86: Loss = 0.48645
Epoch 3.87: Loss = 0.326218
Epoch 3.88: Loss = 0.412521
Epoch 3.89: Loss = 0.497147
Epoch 3.90: Loss = 0.362976
Epoch 3.91: Loss = 0.493881
Epoch 3.92: Loss = 0.464798
Epoch 3.93: Loss = 0.513062
Epoch 3.94: Loss = 0.336655
Epoch 3.95: Loss = 0.424408
Epoch 3.96: Loss = 0.457794
Epoch 3.97: Loss = 0.354416
Epoch 3.98: Loss = 0.387497
Epoch 3.99: Loss = 0.521271
Epoch 3.100: Loss = 0.570206
Epoch 3.101: Loss = 0.546356
Epoch 3.102: Loss = 0.421585
Epoch 3.103: Loss = 0.378876
Epoch 3.104: Loss = 0.345917
Epoch 3.105: Loss = 0.498154
Epoch 3.106: Loss = 0.507858
Epoch 3.107: Loss = 0.359177
Epoch 3.108: Loss = 0.455093
Epoch 3.109: Loss = 0.383301
Epoch 3.110: Loss = 0.432297
Epoch 3.111: Loss = 0.322632
Epoch 3.112: Loss = 0.330063
Epoch 3.113: Loss = 0.399933
Epoch 3.114: Loss = 0.328323
Epoch 3.115: Loss = 0.353241
Epoch 3.116: Loss = 0.3936
Epoch 3.117: Loss = 0.252472
Epoch 3.118: Loss = 0.221909
Epoch 3.119: Loss = 0.291245
Epoch 3.120: Loss = 0.335236
TRAIN LOSS = 0.438309
TRAIN ACC = 86.8896 % (52136/60000)
Loss = 0.390228
Loss = 0.482346
Loss = 0.556335
Loss = 0.51738
Loss = 0.577087
Loss = 0.417007
Loss = 0.396454
Loss = 0.601288
Loss = 0.516052
Loss = 0.485794
Loss = 0.187714
Loss = 0.326874
Loss = 0.280975
Loss = 0.369553
Loss = 0.243195
Loss = 0.312897
Loss = 0.234039
Loss = 0.0899353
Loss = 0.237701
Loss = 0.565262
TEST LOSS = 0.389406
TEST ACC = 521.359 % (8852/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.386734
Epoch 4.2: Loss = 0.492981
Epoch 4.3: Loss = 0.473572
Epoch 4.4: Loss = 0.311279
Epoch 4.5: Loss = 0.369263
Epoch 4.6: Loss = 0.373169
Epoch 4.7: Loss = 0.364548
Epoch 4.8: Loss = 0.385681
Epoch 4.9: Loss = 0.356369
Epoch 4.10: Loss = 0.401627
Epoch 4.11: Loss = 0.436783
Epoch 4.12: Loss = 0.378815
Epoch 4.13: Loss = 0.322327
Epoch 4.14: Loss = 0.363953
Epoch 4.15: Loss = 0.45224
Epoch 4.16: Loss = 0.44606
Epoch 4.17: Loss = 0.479477
Epoch 4.18: Loss = 0.573578
Epoch 4.19: Loss = 0.40892
Epoch 4.20: Loss = 0.365112
Epoch 4.21: Loss = 0.337311
Epoch 4.22: Loss = 0.33107
Epoch 4.23: Loss = 0.348846
Epoch 4.24: Loss = 0.536545
Epoch 4.25: Loss = 0.430023
Epoch 4.26: Loss = 0.514236
Epoch 4.27: Loss = 0.481277
Epoch 4.28: Loss = 0.459274
Epoch 4.29: Loss = 0.537552
Epoch 4.30: Loss = 0.571594
Epoch 4.31: Loss = 0.363525
Epoch 4.32: Loss = 0.482513
Epoch 4.33: Loss = 0.374954
Epoch 4.34: Loss = 0.454086
Epoch 4.35: Loss = 0.384781
Epoch 4.36: Loss = 0.449097
Epoch 4.37: Loss = 0.30246
Epoch 4.38: Loss = 0.345047
Epoch 4.39: Loss = 0.387421
Epoch 4.40: Loss = 0.361267
Epoch 4.41: Loss = 0.428024
Epoch 4.42: Loss = 0.543259
Epoch 4.43: Loss = 0.359497
Epoch 4.44: Loss = 0.305832
Epoch 4.45: Loss = 0.440369
Epoch 4.46: Loss = 0.45108
Epoch 4.47: Loss = 0.363876
Epoch 4.48: Loss = 0.410538
Epoch 4.49: Loss = 0.412628
Epoch 4.50: Loss = 0.483536
Epoch 4.51: Loss = 0.340668
Epoch 4.52: Loss = 0.324814
Epoch 4.53: Loss = 0.384842
Epoch 4.54: Loss = 0.491547
Epoch 4.55: Loss = 0.419327
Epoch 4.56: Loss = 0.375458
Epoch 4.57: Loss = 0.379364
Epoch 4.58: Loss = 0.404205
Epoch 4.59: Loss = 0.476364
Epoch 4.60: Loss = 0.484695
Epoch 4.61: Loss = 0.46199
Epoch 4.62: Loss = 0.479645
Epoch 4.63: Loss = 0.553482
Epoch 4.64: Loss = 0.493637
Epoch 4.65: Loss = 0.567642
Epoch 4.66: Loss = 0.403992
Epoch 4.67: Loss = 0.410812
Epoch 4.68: Loss = 0.261185
Epoch 4.69: Loss = 0.334351
Epoch 4.70: Loss = 0.507034
Epoch 4.71: Loss = 0.335938
Epoch 4.72: Loss = 0.335266
Epoch 4.73: Loss = 0.395645
Epoch 4.74: Loss = 0.329605
Epoch 4.75: Loss = 0.558838
Epoch 4.76: Loss = 0.433228
Epoch 4.77: Loss = 0.348831
Epoch 4.78: Loss = 0.419281
Epoch 4.79: Loss = 0.440567
Epoch 4.80: Loss = 0.459091
Epoch 4.81: Loss = 0.35437
Epoch 4.82: Loss = 0.333237
Epoch 4.83: Loss = 0.500015
Epoch 4.84: Loss = 0.419479
Epoch 4.85: Loss = 0.580322
Epoch 4.86: Loss = 0.468811
Epoch 4.87: Loss = 0.302216
Epoch 4.88: Loss = 0.393967
Epoch 4.89: Loss = 0.461029
Epoch 4.90: Loss = 0.349274
Epoch 4.91: Loss = 0.485626
Epoch 4.92: Loss = 0.450684
Epoch 4.93: Loss = 0.511337
Epoch 4.94: Loss = 0.330383
Epoch 4.95: Loss = 0.396713
Epoch 4.96: Loss = 0.434769
Epoch 4.97: Loss = 0.347519
Epoch 4.98: Loss = 0.355362
Epoch 4.99: Loss = 0.490906
Epoch 4.100: Loss = 0.540695
Epoch 4.101: Loss = 0.53981
Epoch 4.102: Loss = 0.411057
Epoch 4.103: Loss = 0.361755
Epoch 4.104: Loss = 0.33783
Epoch 4.105: Loss = 0.477921
Epoch 4.106: Loss = 0.495041
Epoch 4.107: Loss = 0.331406
Epoch 4.108: Loss = 0.449509
Epoch 4.109: Loss = 0.352615
Epoch 4.110: Loss = 0.401001
Epoch 4.111: Loss = 0.306427
Epoch 4.112: Loss = 0.323624
Epoch 4.113: Loss = 0.374969
Epoch 4.114: Loss = 0.29866
Epoch 4.115: Loss = 0.323471
Epoch 4.116: Loss = 0.376251
Epoch 4.117: Loss = 0.224792
Epoch 4.118: Loss = 0.204071
Epoch 4.119: Loss = 0.305832
Epoch 4.120: Loss = 0.316391
TRAIN LOSS = 0.409378
TRAIN ACC = 87.8922 % (52738/60000)
Loss = 0.375778
Loss = 0.472794
Loss = 0.545395
Loss = 0.509613
Loss = 0.565994
Loss = 0.399811
Loss = 0.385666
Loss = 0.590469
Loss = 0.505081
Loss = 0.471191
Loss = 0.185089
Loss = 0.301727
Loss = 0.27948
Loss = 0.353577
Loss = 0.21405
Loss = 0.300964
Loss = 0.197235
Loss = 0.0796356
Loss = 0.231201
Loss = 0.529373
TEST LOSS = 0.374706
TEST ACC = 527.379 % (8914/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.372238
Epoch 5.2: Loss = 0.476334
Epoch 5.3: Loss = 0.468231
Epoch 5.4: Loss = 0.291245
Epoch 5.5: Loss = 0.34906
Epoch 5.6: Loss = 0.358688
Epoch 5.7: Loss = 0.350128
Epoch 5.8: Loss = 0.366608
Epoch 5.9: Loss = 0.352478
Epoch 5.10: Loss = 0.37738
Epoch 5.11: Loss = 0.416199
Epoch 5.12: Loss = 0.358078
Epoch 5.13: Loss = 0.300262
Epoch 5.14: Loss = 0.340179
Epoch 5.15: Loss = 0.423676
Epoch 5.16: Loss = 0.409058
Epoch 5.17: Loss = 0.462112
Epoch 5.18: Loss = 0.57103
Epoch 5.19: Loss = 0.409088
Epoch 5.20: Loss = 0.343216
Epoch 5.21: Loss = 0.320862
Epoch 5.22: Loss = 0.305786
Epoch 5.23: Loss = 0.327454
Epoch 5.24: Loss = 0.528427
Epoch 5.25: Loss = 0.42244
Epoch 5.26: Loss = 0.506577
Epoch 5.27: Loss = 0.468445
Epoch 5.28: Loss = 0.439865
Epoch 5.29: Loss = 0.547546
Epoch 5.30: Loss = 0.543427
Epoch 5.31: Loss = 0.359283
Epoch 5.32: Loss = 0.441864
Epoch 5.33: Loss = 0.355835
Epoch 5.34: Loss = 0.436844
Epoch 5.35: Loss = 0.374603
Epoch 5.36: Loss = 0.429214
Epoch 5.37: Loss = 0.294022
Epoch 5.38: Loss = 0.326294
Epoch 5.39: Loss = 0.363312
Epoch 5.40: Loss = 0.357101
Epoch 5.41: Loss = 0.391708
Epoch 5.42: Loss = 0.544205
Epoch 5.43: Loss = 0.344482
Epoch 5.44: Loss = 0.290955
Epoch 5.45: Loss = 0.429016
Epoch 5.46: Loss = 0.42923
Epoch 5.47: Loss = 0.348129
Epoch 5.48: Loss = 0.410339
Epoch 5.49: Loss = 0.404449
Epoch 5.50: Loss = 0.470718
Epoch 5.51: Loss = 0.318512
Epoch 5.52: Loss = 0.317978
Epoch 5.53: Loss = 0.382309
Epoch 5.54: Loss = 0.499146
Epoch 5.55: Loss = 0.402008
Epoch 5.56: Loss = 0.381241
Epoch 5.57: Loss = 0.370499
Epoch 5.58: Loss = 0.391739
Epoch 5.59: Loss = 0.459564
Epoch 5.60: Loss = 0.481293
Epoch 5.61: Loss = 0.429382
Epoch 5.62: Loss = 0.475922
Epoch 5.63: Loss = 0.55777
Epoch 5.64: Loss = 0.486679
Epoch 5.65: Loss = 0.564011
Epoch 5.66: Loss = 0.381454
Epoch 5.67: Loss = 0.400009
Epoch 5.68: Loss = 0.257339
Epoch 5.69: Loss = 0.314545
Epoch 5.70: Loss = 0.493073
Epoch 5.71: Loss = 0.321289
Epoch 5.72: Loss = 0.312653
Epoch 5.73: Loss = 0.379288
Epoch 5.74: Loss = 0.321838
Epoch 5.75: Loss = 0.565857
Epoch 5.76: Loss = 0.422287
Epoch 5.77: Loss = 0.328918
Epoch 5.78: Loss = 0.405289
Epoch 5.79: Loss = 0.423004
Epoch 5.80: Loss = 0.438065
Epoch 5.81: Loss = 0.34288
Epoch 5.82: Loss = 0.321609
Epoch 5.83: Loss = 0.495743
Epoch 5.84: Loss = 0.406342
Epoch 5.85: Loss = 0.555908
Epoch 5.86: Loss = 0.468124
Epoch 5.87: Loss = 0.306885
Epoch 5.88: Loss = 0.394852
Epoch 5.89: Loss = 0.439926
Epoch 5.90: Loss = 0.342468
Epoch 5.91: Loss = 0.485977
Epoch 5.92: Loss = 0.452728
Epoch 5.93: Loss = 0.517334
Epoch 5.94: Loss = 0.315445
Epoch 5.95: Loss = 0.378799
Epoch 5.96: Loss = 0.436096
Epoch 5.97: Loss = 0.3349
Epoch 5.98: Loss = 0.357651
Epoch 5.99: Loss = 0.468079
Epoch 5.100: Loss = 0.532516
Epoch 5.101: Loss = 0.539139
Epoch 5.102: Loss = 0.386414
Epoch 5.103: Loss = 0.359818
Epoch 5.104: Loss = 0.340836
Epoch 5.105: Loss = 0.465195
Epoch 5.106: Loss = 0.507614
Epoch 5.107: Loss = 0.331421
Epoch 5.108: Loss = 0.445084
Epoch 5.109: Loss = 0.347015
Epoch 5.110: Loss = 0.400787
Epoch 5.111: Loss = 0.30545
Epoch 5.112: Loss = 0.319
Epoch 5.113: Loss = 0.361557
Epoch 5.114: Loss = 0.280151
Epoch 5.115: Loss = 0.306625
Epoch 5.116: Loss = 0.369125
Epoch 5.117: Loss = 0.22081
Epoch 5.118: Loss = 0.197891
Epoch 5.119: Loss = 0.290878
Epoch 5.120: Loss = 0.306702
TRAIN LOSS = 0.39772
TRAIN ACC = 88.4933 % (53099/60000)
Loss = 0.373489
Loss = 0.46727
Loss = 0.540054
Loss = 0.520203
Loss = 0.563354
Loss = 0.384537
Loss = 0.382431
Loss = 0.574356
Loss = 0.493393
Loss = 0.456436
Loss = 0.186005
Loss = 0.317535
Loss = 0.27272
Loss = 0.34642
Loss = 0.209152
Loss = 0.288742
Loss = 0.182632
Loss = 0.0696259
Loss = 0.214127
Loss = 0.528687
TEST LOSS = 0.368558
TEST ACC = 530.989 % (8950/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.371185
Epoch 6.2: Loss = 0.480682
Epoch 6.3: Loss = 0.452606
Epoch 6.4: Loss = 0.282715
Epoch 6.5: Loss = 0.355804
Epoch 6.6: Loss = 0.359833
Epoch 6.7: Loss = 0.332428
Epoch 6.8: Loss = 0.359406
Epoch 6.9: Loss = 0.339844
Epoch 6.10: Loss = 0.371262
Epoch 6.11: Loss = 0.412384
Epoch 6.12: Loss = 0.342667
Epoch 6.13: Loss = 0.294022
Epoch 6.14: Loss = 0.332153
Epoch 6.15: Loss = 0.409012
Epoch 6.16: Loss = 0.401932
Epoch 6.17: Loss = 0.460632
Epoch 6.18: Loss = 0.574417
Epoch 6.19: Loss = 0.404327
Epoch 6.20: Loss = 0.3367
Epoch 6.21: Loss = 0.306992
Epoch 6.22: Loss = 0.292145
Epoch 6.23: Loss = 0.309906
Epoch 6.24: Loss = 0.532532
Epoch 6.25: Loss = 0.420654
Epoch 6.26: Loss = 0.514862
Epoch 6.27: Loss = 0.455963
Epoch 6.28: Loss = 0.431793
Epoch 6.29: Loss = 0.537674
Epoch 6.30: Loss = 0.51976
Epoch 6.31: Loss = 0.354218
Epoch 6.32: Loss = 0.428146
Epoch 6.33: Loss = 0.350983
Epoch 6.34: Loss = 0.420044
Epoch 6.35: Loss = 0.366226
Epoch 6.36: Loss = 0.419357
Epoch 6.37: Loss = 0.28479
Epoch 6.38: Loss = 0.328003
Epoch 6.39: Loss = 0.349686
Epoch 6.40: Loss = 0.33287
Epoch 6.41: Loss = 0.393646
Epoch 6.42: Loss = 0.553116
Epoch 6.43: Loss = 0.326965
Epoch 6.44: Loss = 0.298157
Epoch 6.45: Loss = 0.41539
Epoch 6.46: Loss = 0.422318
Epoch 6.47: Loss = 0.336899
Epoch 6.48: Loss = 0.400955
Epoch 6.49: Loss = 0.38945
Epoch 6.50: Loss = 0.460449
Epoch 6.51: Loss = 0.291702
Epoch 6.52: Loss = 0.308334
Epoch 6.53: Loss = 0.363571
Epoch 6.54: Loss = 0.4935
Epoch 6.55: Loss = 0.387329
Epoch 6.56: Loss = 0.370361
Epoch 6.57: Loss = 0.357193
Epoch 6.58: Loss = 0.383972
Epoch 6.59: Loss = 0.465302
Epoch 6.60: Loss = 0.465073
Epoch 6.61: Loss = 0.41626
Epoch 6.62: Loss = 0.477264
Epoch 6.63: Loss = 0.561752
Epoch 6.64: Loss = 0.47995
Epoch 6.65: Loss = 0.529297
Epoch 6.66: Loss = 0.370193
Epoch 6.67: Loss = 0.38063
Epoch 6.68: Loss = 0.253311
Epoch 6.69: Loss = 0.312805
Epoch 6.70: Loss = 0.481689
Epoch 6.71: Loss = 0.313873
Epoch 6.72: Loss = 0.288528
Epoch 6.73: Loss = 0.387955
Epoch 6.74: Loss = 0.327225
Epoch 6.75: Loss = 0.559509
Epoch 6.76: Loss = 0.408951
Epoch 6.77: Loss = 0.325134
Epoch 6.78: Loss = 0.393661
Epoch 6.79: Loss = 0.431503
Epoch 6.80: Loss = 0.409927
Epoch 6.81: Loss = 0.325256
Epoch 6.82: Loss = 0.316177
Epoch 6.83: Loss = 0.511459
Epoch 6.84: Loss = 0.401276
Epoch 6.85: Loss = 0.53627
Epoch 6.86: Loss = 0.465332
Epoch 6.87: Loss = 0.292542
Epoch 6.88: Loss = 0.378067
Epoch 6.89: Loss = 0.429947
Epoch 6.90: Loss = 0.34256
Epoch 6.91: Loss = 0.457443
Epoch 6.92: Loss = 0.444122
Epoch 6.93: Loss = 0.494308
Epoch 6.94: Loss = 0.292221
Epoch 6.95: Loss = 0.365967
Epoch 6.96: Loss = 0.4254
Epoch 6.97: Loss = 0.334366
Epoch 6.98: Loss = 0.348053
Epoch 6.99: Loss = 0.459366
Epoch 6.100: Loss = 0.529831
Epoch 6.101: Loss = 0.534424
Epoch 6.102: Loss = 0.397247
Epoch 6.103: Loss = 0.344711
Epoch 6.104: Loss = 0.327194
Epoch 6.105: Loss = 0.470688
Epoch 6.106: Loss = 0.508972
Epoch 6.107: Loss = 0.318756
Epoch 6.108: Loss = 0.426254
Epoch 6.109: Loss = 0.340515
Epoch 6.110: Loss = 0.398209
Epoch 6.111: Loss = 0.302902
Epoch 6.112: Loss = 0.324997
Epoch 6.113: Loss = 0.360825
Epoch 6.114: Loss = 0.264023
Epoch 6.115: Loss = 0.29808
Epoch 6.116: Loss = 0.367523
Epoch 6.117: Loss = 0.216141
Epoch 6.118: Loss = 0.188339
Epoch 6.119: Loss = 0.280624
Epoch 6.120: Loss = 0.296707
TRAIN LOSS = 0.389496
TRAIN ACC = 88.8474 % (53311/60000)
Loss = 0.36586
Loss = 0.441742
Loss = 0.53894
Loss = 0.512665
Loss = 0.555069
Loss = 0.375565
Loss = 0.364731
Loss = 0.581192
Loss = 0.486176
Loss = 0.443985
Loss = 0.194046
Loss = 0.284775
Loss = 0.25087
Loss = 0.342819
Loss = 0.183838
Loss = 0.288956
Loss = 0.173477
Loss = 0.0565338
Loss = 0.210068
Loss = 0.51976
TEST LOSS = 0.358553
TEST ACC = 533.109 % (8995/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.348022
Epoch 7.2: Loss = 0.483215
Epoch 7.3: Loss = 0.470367
Epoch 7.4: Loss = 0.2742
Epoch 7.5: Loss = 0.339905
Epoch 7.6: Loss = 0.353485
Epoch 7.7: Loss = 0.322464
Epoch 7.8: Loss = 0.351303
Epoch 7.9: Loss = 0.333374
Epoch 7.10: Loss = 0.37915
Epoch 7.11: Loss = 0.411194
Epoch 7.12: Loss = 0.33226
Epoch 7.13: Loss = 0.286835
Epoch 7.14: Loss = 0.333328
Epoch 7.15: Loss = 0.405472
Epoch 7.16: Loss = 0.408798
Epoch 7.17: Loss = 0.454254
Epoch 7.18: Loss = 0.563126
Epoch 7.19: Loss = 0.39006
Epoch 7.20: Loss = 0.337601
Epoch 7.21: Loss = 0.302032
Epoch 7.22: Loss = 0.286514
Epoch 7.23: Loss = 0.318192
Epoch 7.24: Loss = 0.533417
Epoch 7.25: Loss = 0.418457
Epoch 7.26: Loss = 0.501907
Epoch 7.27: Loss = 0.467194
Epoch 7.28: Loss = 0.435715
Epoch 7.29: Loss = 0.524582
Epoch 7.30: Loss = 0.516998
Epoch 7.31: Loss = 0.343964
Epoch 7.32: Loss = 0.428711
Epoch 7.33: Loss = 0.342606
Epoch 7.34: Loss = 0.417389
Epoch 7.35: Loss = 0.368057
Epoch 7.36: Loss = 0.440536
Epoch 7.37: Loss = 0.262405
Epoch 7.38: Loss = 0.314026
Epoch 7.39: Loss = 0.3405
Epoch 7.40: Loss = 0.329697
Epoch 7.41: Loss = 0.382584
Epoch 7.42: Loss = 0.558243
Epoch 7.43: Loss = 0.316666
Epoch 7.44: Loss = 0.287415
Epoch 7.45: Loss = 0.401794
Epoch 7.46: Loss = 0.428238
Epoch 7.47: Loss = 0.329712
Epoch 7.48: Loss = 0.406586
Epoch 7.49: Loss = 0.391434
Epoch 7.50: Loss = 0.471802
Epoch 7.51: Loss = 0.274063
Epoch 7.52: Loss = 0.306625
Epoch 7.53: Loss = 0.367645
Epoch 7.54: Loss = 0.483139
Epoch 7.55: Loss = 0.372879
Epoch 7.56: Loss = 0.364624
Epoch 7.57: Loss = 0.374542
Epoch 7.58: Loss = 0.388184
Epoch 7.59: Loss = 0.445969
Epoch 7.60: Loss = 0.46286
Epoch 7.61: Loss = 0.414108
Epoch 7.62: Loss = 0.475388
Epoch 7.63: Loss = 0.543625
Epoch 7.64: Loss = 0.462982
Epoch 7.65: Loss = 0.531311
Epoch 7.66: Loss = 0.360214
Epoch 7.67: Loss = 0.396957
Epoch 7.68: Loss = 0.240311
Epoch 7.69: Loss = 0.304352
Epoch 7.70: Loss = 0.477783
Epoch 7.71: Loss = 0.315247
Epoch 7.72: Loss = 0.291214
Epoch 7.73: Loss = 0.352234
Epoch 7.74: Loss = 0.309952
Epoch 7.75: Loss = 0.562485
Epoch 7.76: Loss = 0.408905
Epoch 7.77: Loss = 0.306046
Epoch 7.78: Loss = 0.381088
Epoch 7.79: Loss = 0.445663
Epoch 7.80: Loss = 0.408722
Epoch 7.81: Loss = 0.332336
Epoch 7.82: Loss = 0.307831
Epoch 7.83: Loss = 0.502792
Epoch 7.84: Loss = 0.399765
Epoch 7.85: Loss = 0.556534
Epoch 7.86: Loss = 0.480774
Epoch 7.87: Loss = 0.303787
Epoch 7.88: Loss = 0.391998
Epoch 7.89: Loss = 0.419373
Epoch 7.90: Loss = 0.336044
Epoch 7.91: Loss = 0.456406
Epoch 7.92: Loss = 0.451736
Epoch 7.93: Loss = 0.475342
Epoch 7.94: Loss = 0.28624
Epoch 7.95: Loss = 0.37265
Epoch 7.96: Loss = 0.437927
Epoch 7.97: Loss = 0.331726
Epoch 7.98: Loss = 0.352554
Epoch 7.99: Loss = 0.447601
Epoch 7.100: Loss = 0.519638
Epoch 7.101: Loss = 0.532242
Epoch 7.102: Loss = 0.395782
Epoch 7.103: Loss = 0.344711
Epoch 7.104: Loss = 0.317429
Epoch 7.105: Loss = 0.478577
Epoch 7.106: Loss = 0.523224
Epoch 7.107: Loss = 0.309128
Epoch 7.108: Loss = 0.443771
Epoch 7.109: Loss = 0.327438
Epoch 7.110: Loss = 0.393753
Epoch 7.111: Loss = 0.299255
Epoch 7.112: Loss = 0.330872
Epoch 7.113: Loss = 0.361221
Epoch 7.114: Loss = 0.273972
Epoch 7.115: Loss = 0.297989
Epoch 7.116: Loss = 0.366531
Epoch 7.117: Loss = 0.215256
Epoch 7.118: Loss = 0.177689
Epoch 7.119: Loss = 0.2854
Epoch 7.120: Loss = 0.286667
TRAIN LOSS = 0.386658
TRAIN ACC = 89.0472 % (53431/60000)
Loss = 0.385315
Loss = 0.448669
Loss = 0.536377
Loss = 0.519501
Loss = 0.555374
Loss = 0.370743
Loss = 0.363831
Loss = 0.582108
Loss = 0.490662
Loss = 0.452179
Loss = 0.172806
Loss = 0.288498
Loss = 0.272064
Loss = 0.329926
Loss = 0.191025
Loss = 0.309326
Loss = 0.169067
Loss = 0.0539398
Loss = 0.187439
Loss = 0.500809
TEST LOSS = 0.358983
TEST ACC = 534.309 % (9005/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.352631
Epoch 8.2: Loss = 0.486267
Epoch 8.3: Loss = 0.481766
Epoch 8.4: Loss = 0.278137
Epoch 8.5: Loss = 0.326324
Epoch 8.6: Loss = 0.339508
Epoch 8.7: Loss = 0.307312
Epoch 8.8: Loss = 0.359131
Epoch 8.9: Loss = 0.327118
Epoch 8.10: Loss = 0.351013
Epoch 8.11: Loss = 0.408325
Epoch 8.12: Loss = 0.334259
Epoch 8.13: Loss = 0.278
Epoch 8.14: Loss = 0.316574
Epoch 8.15: Loss = 0.389832
Epoch 8.16: Loss = 0.397644
Epoch 8.17: Loss = 0.447144
Epoch 8.18: Loss = 0.566483
Epoch 8.19: Loss = 0.393723
Epoch 8.20: Loss = 0.340408
Epoch 8.21: Loss = 0.291107
Epoch 8.22: Loss = 0.286392
Epoch 8.23: Loss = 0.322723
Epoch 8.24: Loss = 0.532196
Epoch 8.25: Loss = 0.399872
Epoch 8.26: Loss = 0.51889
Epoch 8.27: Loss = 0.463043
Epoch 8.28: Loss = 0.419373
Epoch 8.29: Loss = 0.516342
Epoch 8.30: Loss = 0.490372
Epoch 8.31: Loss = 0.340408
Epoch 8.32: Loss = 0.425674
Epoch 8.33: Loss = 0.350525
Epoch 8.34: Loss = 0.41301
Epoch 8.35: Loss = 0.357956
Epoch 8.36: Loss = 0.437653
Epoch 8.37: Loss = 0.264969
Epoch 8.38: Loss = 0.325897
Epoch 8.39: Loss = 0.325027
Epoch 8.40: Loss = 0.325577
Epoch 8.41: Loss = 0.383789
Epoch 8.42: Loss = 0.57077
Epoch 8.43: Loss = 0.310059
Epoch 8.44: Loss = 0.291626
Epoch 8.45: Loss = 0.397308
Epoch 8.46: Loss = 0.400742
Epoch 8.47: Loss = 0.324081
Epoch 8.48: Loss = 0.397919
Epoch 8.49: Loss = 0.381729
Epoch 8.50: Loss = 0.457932
Epoch 8.51: Loss = 0.268967
Epoch 8.52: Loss = 0.324753
Epoch 8.53: Loss = 0.372498
Epoch 8.54: Loss = 0.478683
Epoch 8.55: Loss = 0.367996
Epoch 8.56: Loss = 0.367126
Epoch 8.57: Loss = 0.372009
Epoch 8.58: Loss = 0.368423
Epoch 8.59: Loss = 0.438461
Epoch 8.60: Loss = 0.456024
Epoch 8.61: Loss = 0.398544
Epoch 8.62: Loss = 0.478851
Epoch 8.63: Loss = 0.543213
Epoch 8.64: Loss = 0.461685
Epoch 8.65: Loss = 0.512802
Epoch 8.66: Loss = 0.355972
Epoch 8.67: Loss = 0.380783
Epoch 8.68: Loss = 0.231674
Epoch 8.69: Loss = 0.292221
Epoch 8.70: Loss = 0.467133
Epoch 8.71: Loss = 0.311447
Epoch 8.72: Loss = 0.289505
Epoch 8.73: Loss = 0.347
Epoch 8.74: Loss = 0.313492
Epoch 8.75: Loss = 0.575394
Epoch 8.76: Loss = 0.411484
Epoch 8.77: Loss = 0.300903
Epoch 8.78: Loss = 0.381531
Epoch 8.79: Loss = 0.453583
Epoch 8.80: Loss = 0.401138
Epoch 8.81: Loss = 0.309052
Epoch 8.82: Loss = 0.302399
Epoch 8.83: Loss = 0.471634
Epoch 8.84: Loss = 0.386139
Epoch 8.85: Loss = 0.543625
Epoch 8.86: Loss = 0.478455
Epoch 8.87: Loss = 0.290451
Epoch 8.88: Loss = 0.388336
Epoch 8.89: Loss = 0.412201
Epoch 8.90: Loss = 0.329697
Epoch 8.91: Loss = 0.440521
Epoch 8.92: Loss = 0.423737
Epoch 8.93: Loss = 0.481247
Epoch 8.94: Loss = 0.290573
Epoch 8.95: Loss = 0.358856
Epoch 8.96: Loss = 0.419724
Epoch 8.97: Loss = 0.329086
Epoch 8.98: Loss = 0.343399
Epoch 8.99: Loss = 0.441162
Epoch 8.100: Loss = 0.537064
Epoch 8.101: Loss = 0.526627
Epoch 8.102: Loss = 0.381744
Epoch 8.103: Loss = 0.327316
Epoch 8.104: Loss = 0.328201
Epoch 8.105: Loss = 0.463379
Epoch 8.106: Loss = 0.50325
Epoch 8.107: Loss = 0.298126
Epoch 8.108: Loss = 0.428757
Epoch 8.109: Loss = 0.330124
Epoch 8.110: Loss = 0.384567
Epoch 8.111: Loss = 0.295212
Epoch 8.112: Loss = 0.325577
Epoch 8.113: Loss = 0.358536
Epoch 8.114: Loss = 0.265381
Epoch 8.115: Loss = 0.274139
Epoch 8.116: Loss = 0.357025
Epoch 8.117: Loss = 0.223755
Epoch 8.118: Loss = 0.174316
Epoch 8.119: Loss = 0.286148
Epoch 8.120: Loss = 0.299164
TRAIN LOSS = 0.381134
TRAIN ACC = 89.3524 % (53614/60000)
Loss = 0.376022
Loss = 0.437561
Loss = 0.533218
Loss = 0.516663
Loss = 0.55687
Loss = 0.35556
Loss = 0.36734
Loss = 0.570663
Loss = 0.471115
Loss = 0.443268
Loss = 0.176712
Loss = 0.264999
Loss = 0.295547
Loss = 0.319016
Loss = 0.172012
Loss = 0.298004
Loss = 0.157089
Loss = 0.0566559
Loss = 0.187759
Loss = 0.501953
TEST LOSS = 0.352901
TEST ACC = 536.139 % (9008/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.333084
Epoch 9.2: Loss = 0.468613
Epoch 9.3: Loss = 0.476486
Epoch 9.4: Loss = 0.280228
Epoch 9.5: Loss = 0.314774
Epoch 9.6: Loss = 0.335663
Epoch 9.7: Loss = 0.311844
Epoch 9.8: Loss = 0.347427
Epoch 9.9: Loss = 0.332275
Epoch 9.10: Loss = 0.326035
Epoch 9.11: Loss = 0.390945
Epoch 9.12: Loss = 0.343735
Epoch 9.13: Loss = 0.274979
Epoch 9.14: Loss = 0.321869
Epoch 9.15: Loss = 0.391953
Epoch 9.16: Loss = 0.395355
Epoch 9.17: Loss = 0.439056
Epoch 9.18: Loss = 0.565216
Epoch 9.19: Loss = 0.381454
Epoch 9.20: Loss = 0.32695
Epoch 9.21: Loss = 0.2892
Epoch 9.22: Loss = 0.28067
Epoch 9.23: Loss = 0.318527
Epoch 9.24: Loss = 0.517212
Epoch 9.25: Loss = 0.389725
Epoch 9.26: Loss = 0.506622
Epoch 9.27: Loss = 0.446167
Epoch 9.28: Loss = 0.419144
Epoch 9.29: Loss = 0.497864
Epoch 9.30: Loss = 0.492706
Epoch 9.31: Loss = 0.344299
Epoch 9.32: Loss = 0.430328
Epoch 9.33: Loss = 0.347046
Epoch 9.34: Loss = 0.416748
Epoch 9.35: Loss = 0.352463
Epoch 9.36: Loss = 0.440414
Epoch 9.37: Loss = 0.261719
Epoch 9.38: Loss = 0.325882
Epoch 9.39: Loss = 0.337906
Epoch 9.40: Loss = 0.338974
Epoch 9.41: Loss = 0.390427
Epoch 9.42: Loss = 0.578278
Epoch 9.43: Loss = 0.29837
Epoch 9.44: Loss = 0.284882
Epoch 9.45: Loss = 0.377701
Epoch 9.46: Loss = 0.393051
Epoch 9.47: Loss = 0.327209
Epoch 9.48: Loss = 0.390976
Epoch 9.49: Loss = 0.374115
Epoch 9.50: Loss = 0.468597
Epoch 9.51: Loss = 0.274399
Epoch 9.52: Loss = 0.314178
Epoch 9.53: Loss = 0.365738
Epoch 9.54: Loss = 0.47641
Epoch 9.55: Loss = 0.354401
Epoch 9.56: Loss = 0.350571
Epoch 9.57: Loss = 0.362732
Epoch 9.58: Loss = 0.360916
Epoch 9.59: Loss = 0.432663
Epoch 9.60: Loss = 0.446762
Epoch 9.61: Loss = 0.392212
Epoch 9.62: Loss = 0.465088
Epoch 9.63: Loss = 0.529907
Epoch 9.64: Loss = 0.446472
Epoch 9.65: Loss = 0.495667
Epoch 9.66: Loss = 0.353439
Epoch 9.67: Loss = 0.381668
Epoch 9.68: Loss = 0.220032
Epoch 9.69: Loss = 0.282959
Epoch 9.70: Loss = 0.458466
Epoch 9.71: Loss = 0.306915
Epoch 9.72: Loss = 0.293137
Epoch 9.73: Loss = 0.342102
Epoch 9.74: Loss = 0.29776
Epoch 9.75: Loss = 0.571472
Epoch 9.76: Loss = 0.411331
Epoch 9.77: Loss = 0.306274
Epoch 9.78: Loss = 0.370346
Epoch 9.79: Loss = 0.436844
Epoch 9.80: Loss = 0.40358
Epoch 9.81: Loss = 0.296646
Epoch 9.82: Loss = 0.287186
Epoch 9.83: Loss = 0.445236
Epoch 9.84: Loss = 0.375153
Epoch 9.85: Loss = 0.511017
Epoch 9.86: Loss = 0.467422
Epoch 9.87: Loss = 0.292191
Epoch 9.88: Loss = 0.364624
Epoch 9.89: Loss = 0.389603
Epoch 9.90: Loss = 0.309158
Epoch 9.91: Loss = 0.443069
Epoch 9.92: Loss = 0.409256
Epoch 9.93: Loss = 0.475815
Epoch 9.94: Loss = 0.278931
Epoch 9.95: Loss = 0.34996
Epoch 9.96: Loss = 0.414368
Epoch 9.97: Loss = 0.318665
Epoch 9.98: Loss = 0.341476
Epoch 9.99: Loss = 0.423553
Epoch 9.100: Loss = 0.524704
Epoch 9.101: Loss = 0.520599
Epoch 9.102: Loss = 0.371506
Epoch 9.103: Loss = 0.298431
Epoch 9.104: Loss = 0.319397
Epoch 9.105: Loss = 0.446976
Epoch 9.106: Loss = 0.50531
Epoch 9.107: Loss = 0.299713
Epoch 9.108: Loss = 0.422546
Epoch 9.109: Loss = 0.32988
Epoch 9.110: Loss = 0.376587
Epoch 9.111: Loss = 0.292557
Epoch 9.112: Loss = 0.303955
Epoch 9.113: Loss = 0.348495
Epoch 9.114: Loss = 0.27327
Epoch 9.115: Loss = 0.292831
Epoch 9.116: Loss = 0.361099
Epoch 9.117: Loss = 0.221146
Epoch 9.118: Loss = 0.172867
Epoch 9.119: Loss = 0.267044
Epoch 9.120: Loss = 0.290543
TRAIN LOSS = 0.374435
TRAIN ACC = 89.6652 % (53801/60000)
Loss = 0.37413
Loss = 0.439102
Loss = 0.546143
Loss = 0.525803
Loss = 0.556793
Loss = 0.349442
Loss = 0.351639
Loss = 0.558167
Loss = 0.463776
Loss = 0.439835
Loss = 0.149536
Loss = 0.275345
Loss = 0.303085
Loss = 0.314636
Loss = 0.159317
Loss = 0.287216
Loss = 0.159821
Loss = 0.0540619
Loss = 0.188034
Loss = 0.50116
TEST LOSS = 0.349852
TEST ACC = 538.01 % (9023/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.341003
Epoch 10.2: Loss = 0.451111
Epoch 10.3: Loss = 0.456589
Epoch 10.4: Loss = 0.270096
Epoch 10.5: Loss = 0.297668
Epoch 10.6: Loss = 0.32074
Epoch 10.7: Loss = 0.300339
Epoch 10.8: Loss = 0.347992
Epoch 10.9: Loss = 0.332779
Epoch 10.10: Loss = 0.332428
Epoch 10.11: Loss = 0.389359
Epoch 10.12: Loss = 0.339996
Epoch 10.13: Loss = 0.286072
Epoch 10.14: Loss = 0.320404
Epoch 10.15: Loss = 0.38237
Epoch 10.16: Loss = 0.363922
Epoch 10.17: Loss = 0.427536
Epoch 10.18: Loss = 0.55603
Epoch 10.19: Loss = 0.389999
Epoch 10.20: Loss = 0.304108
Epoch 10.21: Loss = 0.29007
Epoch 10.22: Loss = 0.267624
Epoch 10.23: Loss = 0.304276
Epoch 10.24: Loss = 0.481232
Epoch 10.25: Loss = 0.36058
Epoch 10.26: Loss = 0.504868
Epoch 10.27: Loss = 0.454483
Epoch 10.28: Loss = 0.410522
Epoch 10.29: Loss = 0.502701
Epoch 10.30: Loss = 0.467377
Epoch 10.31: Loss = 0.337082
Epoch 10.32: Loss = 0.41333
Epoch 10.33: Loss = 0.336349
Epoch 10.34: Loss = 0.397461
Epoch 10.35: Loss = 0.337158
Epoch 10.36: Loss = 0.436523
Epoch 10.37: Loss = 0.273758
Epoch 10.38: Loss = 0.323654
Epoch 10.39: Loss = 0.320541
Epoch 10.40: Loss = 0.335983
Epoch 10.41: Loss = 0.379822
Epoch 10.42: Loss = 0.573624
Epoch 10.43: Loss = 0.300644
Epoch 10.44: Loss = 0.294144
Epoch 10.45: Loss = 0.36203
Epoch 10.46: Loss = 0.386337
Epoch 10.47: Loss = 0.323486
Epoch 10.48: Loss = 0.383636
Epoch 10.49: Loss = 0.369644
Epoch 10.50: Loss = 0.469879
Epoch 10.51: Loss = 0.269424
Epoch 10.52: Loss = 0.309998
Epoch 10.53: Loss = 0.369858
Epoch 10.54: Loss = 0.478027
Epoch 10.55: Loss = 0.35524
Epoch 10.56: Loss = 0.360245
Epoch 10.57: Loss = 0.369583
Epoch 10.58: Loss = 0.362991
Epoch 10.59: Loss = 0.433853
Epoch 10.60: Loss = 0.437653
Epoch 10.61: Loss = 0.383423
Epoch 10.62: Loss = 0.461304
Epoch 10.63: Loss = 0.537033
Epoch 10.64: Loss = 0.453323
Epoch 10.65: Loss = 0.498795
Epoch 10.66: Loss = 0.372055
Epoch 10.67: Loss = 0.372833
Epoch 10.68: Loss = 0.219391
Epoch 10.69: Loss = 0.283905
Epoch 10.70: Loss = 0.45929
Epoch 10.71: Loss = 0.315308
Epoch 10.72: Loss = 0.286255
Epoch 10.73: Loss = 0.353302
Epoch 10.74: Loss = 0.297852
Epoch 10.75: Loss = 0.594437
Epoch 10.76: Loss = 0.403717
Epoch 10.77: Loss = 0.290817
Epoch 10.78: Loss = 0.375549
Epoch 10.79: Loss = 0.444534
Epoch 10.80: Loss = 0.391296
Epoch 10.81: Loss = 0.292435
Epoch 10.82: Loss = 0.298935
Epoch 10.83: Loss = 0.452515
Epoch 10.84: Loss = 0.380768
Epoch 10.85: Loss = 0.512192
Epoch 10.86: Loss = 0.469391
Epoch 10.87: Loss = 0.296738
Epoch 10.88: Loss = 0.361343
Epoch 10.89: Loss = 0.410187
Epoch 10.90: Loss = 0.30043
Epoch 10.91: Loss = 0.441788
Epoch 10.92: Loss = 0.422989
Epoch 10.93: Loss = 0.483063
Epoch 10.94: Loss = 0.275681
Epoch 10.95: Loss = 0.331268
Epoch 10.96: Loss = 0.419113
Epoch 10.97: Loss = 0.310013
Epoch 10.98: Loss = 0.326965
Epoch 10.99: Loss = 0.427399
Epoch 10.100: Loss = 0.523392
Epoch 10.101: Loss = 0.511551
Epoch 10.102: Loss = 0.359299
Epoch 10.103: Loss = 0.296463
Epoch 10.104: Loss = 0.314514
Epoch 10.105: Loss = 0.442581
Epoch 10.106: Loss = 0.496536
Epoch 10.107: Loss = 0.282852
Epoch 10.108: Loss = 0.430374
Epoch 10.109: Loss = 0.320541
Epoch 10.110: Loss = 0.365356
Epoch 10.111: Loss = 0.28775
Epoch 10.112: Loss = 0.299072
Epoch 10.113: Loss = 0.340271
Epoch 10.114: Loss = 0.265762
Epoch 10.115: Loss = 0.280167
Epoch 10.116: Loss = 0.340958
Epoch 10.117: Loss = 0.207489
Epoch 10.118: Loss = 0.175064
Epoch 10.119: Loss = 0.274918
Epoch 10.120: Loss = 0.305557
TRAIN LOSS = 0.370697
TRAIN ACC = 89.8239 % (53897/60000)
Loss = 0.3582
Loss = 0.428085
Loss = 0.52269
Loss = 0.524521
Loss = 0.549789
Loss = 0.340027
Loss = 0.338943
Loss = 0.548111
Loss = 0.465179
Loss = 0.441269
Loss = 0.163086
Loss = 0.27359
Loss = 0.292679
Loss = 0.326202
Loss = 0.164841
Loss = 0.275345
Loss = 0.165054
Loss = 0.0549774
Loss = 0.198624
Loss = 0.508102
TEST LOSS = 0.346966
TEST ACC = 538.969 % (9056/10000)
