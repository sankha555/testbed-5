Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.42149
Epoch 1.2: Loss = 2.34369
Epoch 1.3: Loss = 2.34296
Epoch 1.4: Loss = 2.23486
Epoch 1.5: Loss = 2.24539
Epoch 1.6: Loss = 2.17509
Epoch 1.7: Loss = 2.16522
Epoch 1.8: Loss = 2.07841
Epoch 1.9: Loss = 2.09627
Epoch 1.10: Loss = 2.02368
Epoch 1.11: Loss = 1.95317
Epoch 1.12: Loss = 1.96078
Epoch 1.13: Loss = 1.88994
Epoch 1.14: Loss = 1.85989
Epoch 1.15: Loss = 1.9375
Epoch 1.16: Loss = 1.84338
Epoch 1.17: Loss = 1.79977
Epoch 1.18: Loss = 1.77515
Epoch 1.19: Loss = 1.73032
Epoch 1.20: Loss = 1.70818
Epoch 1.21: Loss = 1.60431
Epoch 1.22: Loss = 1.61002
Epoch 1.23: Loss = 1.57564
Epoch 1.24: Loss = 1.64691
Epoch 1.25: Loss = 1.56656
Epoch 1.26: Loss = 1.57584
Epoch 1.27: Loss = 1.5464
Epoch 1.28: Loss = 1.54912
Epoch 1.29: Loss = 1.52623
Epoch 1.30: Loss = 1.59468
Epoch 1.31: Loss = 1.43462
Epoch 1.32: Loss = 1.4733
Epoch 1.33: Loss = 1.41428
Epoch 1.34: Loss = 1.41907
Epoch 1.35: Loss = 1.38747
Epoch 1.36: Loss = 1.47041
Epoch 1.37: Loss = 1.35335
Epoch 1.38: Loss = 1.27762
Epoch 1.39: Loss = 1.27628
Epoch 1.40: Loss = 1.24435
Epoch 1.41: Loss = 1.29649
Epoch 1.42: Loss = 1.25284
Epoch 1.43: Loss = 1.18294
Epoch 1.44: Loss = 1.11356
Epoch 1.45: Loss = 1.24643
Epoch 1.46: Loss = 1.18277
Epoch 1.47: Loss = 1.09636
Epoch 1.48: Loss = 1.18304
Epoch 1.49: Loss = 1.12431
Epoch 1.50: Loss = 1.20645
Epoch 1.51: Loss = 1.02425
Epoch 1.52: Loss = 1.03577
Epoch 1.53: Loss = 1.08058
Epoch 1.54: Loss = 1.09764
Epoch 1.55: Loss = 1.10574
Epoch 1.56: Loss = 1.02161
Epoch 1.57: Loss = 0.953812
Epoch 1.58: Loss = 1.02719
Epoch 1.59: Loss = 1.01971
Epoch 1.60: Loss = 1.0999
Epoch 1.61: Loss = 1.08424
Epoch 1.62: Loss = 1.05525
Epoch 1.63: Loss = 1.09509
Epoch 1.64: Loss = 1.04549
Epoch 1.65: Loss = 1.11053
Epoch 1.66: Loss = 0.938644
Epoch 1.67: Loss = 0.970062
Epoch 1.68: Loss = 0.792969
Epoch 1.69: Loss = 0.883545
Epoch 1.70: Loss = 0.970993
Epoch 1.71: Loss = 0.918655
Epoch 1.72: Loss = 0.862473
Epoch 1.73: Loss = 0.927826
Epoch 1.74: Loss = 0.778046
Epoch 1.75: Loss = 0.900925
Epoch 1.76: Loss = 0.856033
Epoch 1.77: Loss = 0.812698
Epoch 1.78: Loss = 0.805573
Epoch 1.79: Loss = 0.795105
Epoch 1.80: Loss = 0.936783
Epoch 1.81: Loss = 0.782593
Epoch 1.82: Loss = 0.740707
Epoch 1.83: Loss = 0.925659
Epoch 1.84: Loss = 0.822342
Epoch 1.85: Loss = 0.935379
Epoch 1.86: Loss = 0.808136
Epoch 1.87: Loss = 0.759995
Epoch 1.88: Loss = 0.76355
Epoch 1.89: Loss = 0.842133
Epoch 1.90: Loss = 0.739426
Epoch 1.91: Loss = 0.811676
Epoch 1.92: Loss = 0.787872
Epoch 1.93: Loss = 0.833344
Epoch 1.94: Loss = 0.6548
Epoch 1.95: Loss = 0.792267
Epoch 1.96: Loss = 0.743576
Epoch 1.97: Loss = 0.597382
Epoch 1.98: Loss = 0.695145
Epoch 1.99: Loss = 0.793472
Epoch 1.100: Loss = 0.882935
Epoch 1.101: Loss = 0.782486
Epoch 1.102: Loss = 0.719574
Epoch 1.103: Loss = 0.663254
Epoch 1.104: Loss = 0.660385
Epoch 1.105: Loss = 0.755936
Epoch 1.106: Loss = 0.729095
Epoch 1.107: Loss = 0.656342
Epoch 1.108: Loss = 0.711716
Epoch 1.109: Loss = 0.650696
Epoch 1.110: Loss = 0.674164
Epoch 1.111: Loss = 0.574997
Epoch 1.112: Loss = 0.551804
Epoch 1.113: Loss = 0.680969
Epoch 1.114: Loss = 0.594849
Epoch 1.115: Loss = 0.650452
Epoch 1.116: Loss = 0.66246
Epoch 1.117: Loss = 0.567551
Epoch 1.118: Loss = 0.487427
Epoch 1.119: Loss = 0.505005
Epoch 1.120: Loss = 0.506592
TRAIN LOSS = 1.1627
TRAIN ACC = 66.2827 % (39771/60000)
Loss = 0.676834
Loss = 0.692352
Loss = 0.816849
Loss = 0.776535
Loss = 0.820313
Loss = 0.698654
Loss = 0.642563
Loss = 0.807449
Loss = 0.768875
Loss = 0.756226
Loss = 0.377548
Loss = 0.5737
Loss = 0.412811
Loss = 0.611603
Loss = 0.489304
Loss = 0.52063
Loss = 0.475815
Loss = 0.28653
Loss = 0.474426
Loss = 0.728729
TEST LOSS = 0.620387
TEST ACC = 397.71 % (8181/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.610123
Epoch 2.2: Loss = 0.687103
Epoch 2.3: Loss = 0.736771
Epoch 2.4: Loss = 0.55397
Epoch 2.5: Loss = 0.572769
Epoch 2.6: Loss = 0.584122
Epoch 2.7: Loss = 0.639816
Epoch 2.8: Loss = 0.615616
Epoch 2.9: Loss = 0.646927
Epoch 2.10: Loss = 0.654709
Epoch 2.11: Loss = 0.580109
Epoch 2.12: Loss = 0.595963
Epoch 2.13: Loss = 0.53241
Epoch 2.14: Loss = 0.54866
Epoch 2.15: Loss = 0.7341
Epoch 2.16: Loss = 0.694016
Epoch 2.17: Loss = 0.662628
Epoch 2.18: Loss = 0.725388
Epoch 2.19: Loss = 0.605133
Epoch 2.20: Loss = 0.543777
Epoch 2.21: Loss = 0.507156
Epoch 2.22: Loss = 0.490845
Epoch 2.23: Loss = 0.522141
Epoch 2.24: Loss = 0.720535
Epoch 2.25: Loss = 0.573425
Epoch 2.26: Loss = 0.648239
Epoch 2.27: Loss = 0.670746
Epoch 2.28: Loss = 0.639191
Epoch 2.29: Loss = 0.683426
Epoch 2.30: Loss = 0.760437
Epoch 2.31: Loss = 0.534195
Epoch 2.32: Loss = 0.717117
Epoch 2.33: Loss = 0.582779
Epoch 2.34: Loss = 0.62706
Epoch 2.35: Loss = 0.636688
Epoch 2.36: Loss = 0.673782
Epoch 2.37: Loss = 0.514465
Epoch 2.38: Loss = 0.500534
Epoch 2.39: Loss = 0.560257
Epoch 2.40: Loss = 0.521759
Epoch 2.41: Loss = 0.603592
Epoch 2.42: Loss = 0.665466
Epoch 2.43: Loss = 0.50267
Epoch 2.44: Loss = 0.479233
Epoch 2.45: Loss = 0.599091
Epoch 2.46: Loss = 0.621918
Epoch 2.47: Loss = 0.499664
Epoch 2.48: Loss = 0.585602
Epoch 2.49: Loss = 0.579483
Epoch 2.50: Loss = 0.686829
Epoch 2.51: Loss = 0.522202
Epoch 2.52: Loss = 0.508514
Epoch 2.53: Loss = 0.565506
Epoch 2.54: Loss = 0.60025
Epoch 2.55: Loss = 0.57309
Epoch 2.56: Loss = 0.554581
Epoch 2.57: Loss = 0.518036
Epoch 2.58: Loss = 0.589493
Epoch 2.59: Loss = 0.575287
Epoch 2.60: Loss = 0.659409
Epoch 2.61: Loss = 0.653549
Epoch 2.62: Loss = 0.658447
Epoch 2.63: Loss = 0.695984
Epoch 2.64: Loss = 0.630569
Epoch 2.65: Loss = 0.78183
Epoch 2.66: Loss = 0.559937
Epoch 2.67: Loss = 0.61499
Epoch 2.68: Loss = 0.415726
Epoch 2.69: Loss = 0.492783
Epoch 2.70: Loss = 0.649979
Epoch 2.71: Loss = 0.504608
Epoch 2.72: Loss = 0.547653
Epoch 2.73: Loss = 0.578659
Epoch 2.74: Loss = 0.433502
Epoch 2.75: Loss = 0.667389
Epoch 2.76: Loss = 0.570541
Epoch 2.77: Loss = 0.507523
Epoch 2.78: Loss = 0.528183
Epoch 2.79: Loss = 0.555969
Epoch 2.80: Loss = 0.683929
Epoch 2.81: Loss = 0.503372
Epoch 2.82: Loss = 0.43631
Epoch 2.83: Loss = 0.655411
Epoch 2.84: Loss = 0.553452
Epoch 2.85: Loss = 0.755051
Epoch 2.86: Loss = 0.595627
Epoch 2.87: Loss = 0.495911
Epoch 2.88: Loss = 0.548325
Epoch 2.89: Loss = 0.615417
Epoch 2.90: Loss = 0.497284
Epoch 2.91: Loss = 0.582245
Epoch 2.92: Loss = 0.583054
Epoch 2.93: Loss = 0.609863
Epoch 2.94: Loss = 0.467026
Epoch 2.95: Loss = 0.5793
Epoch 2.96: Loss = 0.556946
Epoch 2.97: Loss = 0.425018
Epoch 2.98: Loss = 0.482773
Epoch 2.99: Loss = 0.580566
Epoch 2.100: Loss = 0.704422
Epoch 2.101: Loss = 0.626587
Epoch 2.102: Loss = 0.503738
Epoch 2.103: Loss = 0.523178
Epoch 2.104: Loss = 0.474594
Epoch 2.105: Loss = 0.596207
Epoch 2.106: Loss = 0.60083
Epoch 2.107: Loss = 0.454041
Epoch 2.108: Loss = 0.546692
Epoch 2.109: Loss = 0.478363
Epoch 2.110: Loss = 0.536835
Epoch 2.111: Loss = 0.429108
Epoch 2.112: Loss = 0.410767
Epoch 2.113: Loss = 0.515305
Epoch 2.114: Loss = 0.426773
Epoch 2.115: Loss = 0.484879
Epoch 2.116: Loss = 0.507278
Epoch 2.117: Loss = 0.365051
Epoch 2.118: Loss = 0.310364
Epoch 2.119: Loss = 0.379364
Epoch 2.120: Loss = 0.38623
TRAIN LOSS = 0.571793
TRAIN ACC = 82.4966 % (49501/60000)
Loss = 0.50383
Loss = 0.576782
Loss = 0.688675
Loss = 0.679504
Loss = 0.682022
Loss = 0.528793
Loss = 0.524216
Loss = 0.687134
Loss = 0.622498
Loss = 0.633087
Loss = 0.249573
Loss = 0.427475
Loss = 0.319061
Loss = 0.5047
Loss = 0.34256
Loss = 0.396378
Loss = 0.318939
Loss = 0.143784
Loss = 0.29892
Loss = 0.606934
TEST LOSS = 0.486743
TEST ACC = 495.009 % (8462/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.456085
Epoch 3.2: Loss = 0.5513
Epoch 3.3: Loss = 0.610184
Epoch 3.4: Loss = 0.40567
Epoch 3.5: Loss = 0.475433
Epoch 3.6: Loss = 0.468796
Epoch 3.7: Loss = 0.494553
Epoch 3.8: Loss = 0.474075
Epoch 3.9: Loss = 0.473907
Epoch 3.10: Loss = 0.513184
Epoch 3.11: Loss = 0.48732
Epoch 3.12: Loss = 0.485245
Epoch 3.13: Loss = 0.452225
Epoch 3.14: Loss = 0.439194
Epoch 3.15: Loss = 0.57579
Epoch 3.16: Loss = 0.564331
Epoch 3.17: Loss = 0.580429
Epoch 3.18: Loss = 0.673203
Epoch 3.19: Loss = 0.474136
Epoch 3.20: Loss = 0.479996
Epoch 3.21: Loss = 0.405869
Epoch 3.22: Loss = 0.409103
Epoch 3.23: Loss = 0.417892
Epoch 3.24: Loss = 0.656067
Epoch 3.25: Loss = 0.503891
Epoch 3.26: Loss = 0.589249
Epoch 3.27: Loss = 0.630997
Epoch 3.28: Loss = 0.568008
Epoch 3.29: Loss = 0.609665
Epoch 3.30: Loss = 0.669952
Epoch 3.31: Loss = 0.448334
Epoch 3.32: Loss = 0.619766
Epoch 3.33: Loss = 0.481018
Epoch 3.34: Loss = 0.577087
Epoch 3.35: Loss = 0.554825
Epoch 3.36: Loss = 0.590881
Epoch 3.37: Loss = 0.414963
Epoch 3.38: Loss = 0.435974
Epoch 3.39: Loss = 0.491714
Epoch 3.40: Loss = 0.448502
Epoch 3.41: Loss = 0.53389
Epoch 3.42: Loss = 0.624573
Epoch 3.43: Loss = 0.426453
Epoch 3.44: Loss = 0.418839
Epoch 3.45: Loss = 0.533676
Epoch 3.46: Loss = 0.548782
Epoch 3.47: Loss = 0.465363
Epoch 3.48: Loss = 0.550659
Epoch 3.49: Loss = 0.502121
Epoch 3.50: Loss = 0.596497
Epoch 3.51: Loss = 0.45842
Epoch 3.52: Loss = 0.406448
Epoch 3.53: Loss = 0.488892
Epoch 3.54: Loss = 0.528076
Epoch 3.55: Loss = 0.487457
Epoch 3.56: Loss = 0.499557
Epoch 3.57: Loss = 0.451141
Epoch 3.58: Loss = 0.530701
Epoch 3.59: Loss = 0.549011
Epoch 3.60: Loss = 0.578598
Epoch 3.61: Loss = 0.561401
Epoch 3.62: Loss = 0.591049
Epoch 3.63: Loss = 0.62709
Epoch 3.64: Loss = 0.587006
Epoch 3.65: Loss = 0.692215
Epoch 3.66: Loss = 0.497787
Epoch 3.67: Loss = 0.560501
Epoch 3.68: Loss = 0.328766
Epoch 3.69: Loss = 0.435791
Epoch 3.70: Loss = 0.633179
Epoch 3.71: Loss = 0.453293
Epoch 3.72: Loss = 0.462204
Epoch 3.73: Loss = 0.526108
Epoch 3.74: Loss = 0.398056
Epoch 3.75: Loss = 0.673111
Epoch 3.76: Loss = 0.516464
Epoch 3.77: Loss = 0.465393
Epoch 3.78: Loss = 0.473007
Epoch 3.79: Loss = 0.504578
Epoch 3.80: Loss = 0.62674
Epoch 3.81: Loss = 0.452103
Epoch 3.82: Loss = 0.396072
Epoch 3.83: Loss = 0.611664
Epoch 3.84: Loss = 0.524445
Epoch 3.85: Loss = 0.73764
Epoch 3.86: Loss = 0.534058
Epoch 3.87: Loss = 0.392471
Epoch 3.88: Loss = 0.47438
Epoch 3.89: Loss = 0.58313
Epoch 3.90: Loss = 0.442917
Epoch 3.91: Loss = 0.536194
Epoch 3.92: Loss = 0.526016
Epoch 3.93: Loss = 0.59021
Epoch 3.94: Loss = 0.417786
Epoch 3.95: Loss = 0.529633
Epoch 3.96: Loss = 0.57283
Epoch 3.97: Loss = 0.378738
Epoch 3.98: Loss = 0.426132
Epoch 3.99: Loss = 0.591599
Epoch 3.100: Loss = 0.628357
Epoch 3.101: Loss = 0.602295
Epoch 3.102: Loss = 0.488876
Epoch 3.103: Loss = 0.471115
Epoch 3.104: Loss = 0.430725
Epoch 3.105: Loss = 0.593842
Epoch 3.106: Loss = 0.554672
Epoch 3.107: Loss = 0.398575
Epoch 3.108: Loss = 0.508987
Epoch 3.109: Loss = 0.470383
Epoch 3.110: Loss = 0.503967
Epoch 3.111: Loss = 0.398926
Epoch 3.112: Loss = 0.408722
Epoch 3.113: Loss = 0.452423
Epoch 3.114: Loss = 0.400757
Epoch 3.115: Loss = 0.446747
Epoch 3.116: Loss = 0.452057
Epoch 3.117: Loss = 0.327316
Epoch 3.118: Loss = 0.281342
Epoch 3.119: Loss = 0.338028
Epoch 3.120: Loss = 0.418777
TRAIN LOSS = 0.506241
TRAIN ACC = 84.6954 % (50819/60000)
Loss = 0.471954
Loss = 0.587234
Loss = 0.679047
Loss = 0.652176
Loss = 0.686539
Loss = 0.487366
Loss = 0.511581
Loss = 0.695999
Loss = 0.611343
Loss = 0.599457
Loss = 0.23349
Loss = 0.37912
Loss = 0.330307
Loss = 0.524216
Loss = 0.309769
Loss = 0.398972
Loss = 0.30217
Loss = 0.115524
Loss = 0.292221
Loss = 0.60498
TEST LOSS = 0.473673
TEST ACC = 508.189 % (8524/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.4328
Epoch 4.2: Loss = 0.541946
Epoch 4.3: Loss = 0.591492
Epoch 4.4: Loss = 0.378433
Epoch 4.5: Loss = 0.428787
Epoch 4.6: Loss = 0.45665
Epoch 4.7: Loss = 0.454468
Epoch 4.8: Loss = 0.441833
Epoch 4.9: Loss = 0.432327
Epoch 4.10: Loss = 0.492416
Epoch 4.11: Loss = 0.478989
Epoch 4.12: Loss = 0.491516
Epoch 4.13: Loss = 0.456146
Epoch 4.14: Loss = 0.419708
Epoch 4.15: Loss = 0.551346
Epoch 4.16: Loss = 0.547028
Epoch 4.17: Loss = 0.576889
Epoch 4.18: Loss = 0.613129
Epoch 4.19: Loss = 0.488266
Epoch 4.20: Loss = 0.448471
Epoch 4.21: Loss = 0.381943
Epoch 4.22: Loss = 0.384995
Epoch 4.23: Loss = 0.374863
Epoch 4.24: Loss = 0.634033
Epoch 4.25: Loss = 0.48468
Epoch 4.26: Loss = 0.603958
Epoch 4.27: Loss = 0.60994
Epoch 4.28: Loss = 0.548386
Epoch 4.29: Loss = 0.591873
Epoch 4.30: Loss = 0.609909
Epoch 4.31: Loss = 0.422974
Epoch 4.32: Loss = 0.561234
Epoch 4.33: Loss = 0.461044
Epoch 4.34: Loss = 0.554611
Epoch 4.35: Loss = 0.530457
Epoch 4.36: Loss = 0.596329
Epoch 4.37: Loss = 0.381592
Epoch 4.38: Loss = 0.420013
Epoch 4.39: Loss = 0.465576
Epoch 4.40: Loss = 0.434738
Epoch 4.41: Loss = 0.48703
Epoch 4.42: Loss = 0.648346
Epoch 4.43: Loss = 0.41362
Epoch 4.44: Loss = 0.380478
Epoch 4.45: Loss = 0.494019
Epoch 4.46: Loss = 0.522186
Epoch 4.47: Loss = 0.456543
Epoch 4.48: Loss = 0.526611
Epoch 4.49: Loss = 0.486481
Epoch 4.50: Loss = 0.57605
Epoch 4.51: Loss = 0.40625
Epoch 4.52: Loss = 0.400467
Epoch 4.53: Loss = 0.485748
Epoch 4.54: Loss = 0.532257
Epoch 4.55: Loss = 0.463974
Epoch 4.56: Loss = 0.470871
Epoch 4.57: Loss = 0.464264
Epoch 4.58: Loss = 0.505081
Epoch 4.59: Loss = 0.532471
Epoch 4.60: Loss = 0.581253
Epoch 4.61: Loss = 0.516312
Epoch 4.62: Loss = 0.566116
Epoch 4.63: Loss = 0.655151
Epoch 4.64: Loss = 0.588028
Epoch 4.65: Loss = 0.679459
Epoch 4.66: Loss = 0.47554
Epoch 4.67: Loss = 0.515137
Epoch 4.68: Loss = 0.323334
Epoch 4.69: Loss = 0.419418
Epoch 4.70: Loss = 0.621964
Epoch 4.71: Loss = 0.447525
Epoch 4.72: Loss = 0.45871
Epoch 4.73: Loss = 0.497879
Epoch 4.74: Loss = 0.404724
Epoch 4.75: Loss = 0.713028
Epoch 4.76: Loss = 0.497711
Epoch 4.77: Loss = 0.462311
Epoch 4.78: Loss = 0.465134
Epoch 4.79: Loss = 0.537598
Epoch 4.80: Loss = 0.59726
Epoch 4.81: Loss = 0.431458
Epoch 4.82: Loss = 0.404999
Epoch 4.83: Loss = 0.583466
Epoch 4.84: Loss = 0.525146
Epoch 4.85: Loss = 0.724396
Epoch 4.86: Loss = 0.557297
Epoch 4.87: Loss = 0.359177
Epoch 4.88: Loss = 0.469482
Epoch 4.89: Loss = 0.55835
Epoch 4.90: Loss = 0.449829
Epoch 4.91: Loss = 0.523041
Epoch 4.92: Loss = 0.542923
Epoch 4.93: Loss = 0.601166
Epoch 4.94: Loss = 0.423859
Epoch 4.95: Loss = 0.515396
Epoch 4.96: Loss = 0.580902
Epoch 4.97: Loss = 0.40799
Epoch 4.98: Loss = 0.424179
Epoch 4.99: Loss = 0.561279
Epoch 4.100: Loss = 0.675537
Epoch 4.101: Loss = 0.603836
Epoch 4.102: Loss = 0.512344
Epoch 4.103: Loss = 0.483368
Epoch 4.104: Loss = 0.435913
Epoch 4.105: Loss = 0.588272
Epoch 4.106: Loss = 0.606216
Epoch 4.107: Loss = 0.389206
Epoch 4.108: Loss = 0.515427
Epoch 4.109: Loss = 0.473907
Epoch 4.110: Loss = 0.509949
Epoch 4.111: Loss = 0.404846
Epoch 4.112: Loss = 0.405762
Epoch 4.113: Loss = 0.448105
Epoch 4.114: Loss = 0.374146
Epoch 4.115: Loss = 0.448883
Epoch 4.116: Loss = 0.451996
Epoch 4.117: Loss = 0.285751
Epoch 4.118: Loss = 0.262115
Epoch 4.119: Loss = 0.348465
Epoch 4.120: Loss = 0.385635
TRAIN LOSS = 0.494217
TRAIN ACC = 85.2753 % (51168/60000)
Loss = 0.451187
Loss = 0.567505
Loss = 0.633362
Loss = 0.673279
Loss = 0.661987
Loss = 0.474228
Loss = 0.491913
Loss = 0.713608
Loss = 0.637299
Loss = 0.623688
Loss = 0.220123
Loss = 0.401428
Loss = 0.306946
Loss = 0.506561
Loss = 0.283676
Loss = 0.427841
Loss = 0.300537
Loss = 0.114029
Loss = 0.262253
Loss = 0.606384
TEST LOSS = 0.467892
TEST ACC = 511.679 % (8581/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.415298
Epoch 5.2: Loss = 0.53714
Epoch 5.3: Loss = 0.592484
Epoch 5.4: Loss = 0.369614
Epoch 5.5: Loss = 0.392258
Epoch 5.6: Loss = 0.451569
Epoch 5.7: Loss = 0.44339
Epoch 5.8: Loss = 0.427856
Epoch 5.9: Loss = 0.424316
Epoch 5.10: Loss = 0.47113
Epoch 5.11: Loss = 0.506287
Epoch 5.12: Loss = 0.507217
Epoch 5.13: Loss = 0.435104
Epoch 5.14: Loss = 0.407166
Epoch 5.15: Loss = 0.565216
Epoch 5.16: Loss = 0.555298
Epoch 5.17: Loss = 0.592468
Epoch 5.18: Loss = 0.604889
Epoch 5.19: Loss = 0.46669
Epoch 5.20: Loss = 0.432785
Epoch 5.21: Loss = 0.371399
Epoch 5.22: Loss = 0.386032
Epoch 5.23: Loss = 0.34436
Epoch 5.24: Loss = 0.65358
Epoch 5.25: Loss = 0.462128
Epoch 5.26: Loss = 0.619247
Epoch 5.27: Loss = 0.589661
Epoch 5.28: Loss = 0.558502
Epoch 5.29: Loss = 0.546844
Epoch 5.30: Loss = 0.636124
Epoch 5.31: Loss = 0.412125
Epoch 5.32: Loss = 0.553741
Epoch 5.33: Loss = 0.441788
Epoch 5.34: Loss = 0.545639
Epoch 5.35: Loss = 0.507813
Epoch 5.36: Loss = 0.631119
Epoch 5.37: Loss = 0.368362
Epoch 5.38: Loss = 0.393921
Epoch 5.39: Loss = 0.472214
Epoch 5.40: Loss = 0.440643
Epoch 5.41: Loss = 0.488647
Epoch 5.42: Loss = 0.671417
Epoch 5.43: Loss = 0.432388
Epoch 5.44: Loss = 0.371353
Epoch 5.45: Loss = 0.493179
Epoch 5.46: Loss = 0.500946
Epoch 5.47: Loss = 0.43454
Epoch 5.48: Loss = 0.527664
Epoch 5.49: Loss = 0.505508
Epoch 5.50: Loss = 0.568451
Epoch 5.51: Loss = 0.420288
Epoch 5.52: Loss = 0.393341
Epoch 5.53: Loss = 0.473709
Epoch 5.54: Loss = 0.563553
Epoch 5.55: Loss = 0.460236
Epoch 5.56: Loss = 0.441772
Epoch 5.57: Loss = 0.455994
Epoch 5.58: Loss = 0.514038
Epoch 5.59: Loss = 0.572983
Epoch 5.60: Loss = 0.592087
Epoch 5.61: Loss = 0.509277
Epoch 5.62: Loss = 0.602692
Epoch 5.63: Loss = 0.676178
Epoch 5.64: Loss = 0.599167
Epoch 5.65: Loss = 0.703461
Epoch 5.66: Loss = 0.468231
Epoch 5.67: Loss = 0.504776
Epoch 5.68: Loss = 0.306656
Epoch 5.69: Loss = 0.422043
Epoch 5.70: Loss = 0.663742
Epoch 5.71: Loss = 0.458984
Epoch 5.72: Loss = 0.467178
Epoch 5.73: Loss = 0.496231
Epoch 5.74: Loss = 0.399673
Epoch 5.75: Loss = 0.763596
Epoch 5.76: Loss = 0.52449
Epoch 5.77: Loss = 0.475372
Epoch 5.78: Loss = 0.482193
Epoch 5.79: Loss = 0.56221
Epoch 5.80: Loss = 0.599121
Epoch 5.81: Loss = 0.42453
Epoch 5.82: Loss = 0.402084
Epoch 5.83: Loss = 0.621384
Epoch 5.84: Loss = 0.549225
Epoch 5.85: Loss = 0.741577
Epoch 5.86: Loss = 0.593826
Epoch 5.87: Loss = 0.350769
Epoch 5.88: Loss = 0.474976
Epoch 5.89: Loss = 0.594925
Epoch 5.90: Loss = 0.45314
Epoch 5.91: Loss = 0.520386
Epoch 5.92: Loss = 0.551163
Epoch 5.93: Loss = 0.610153
Epoch 5.94: Loss = 0.437454
Epoch 5.95: Loss = 0.538284
Epoch 5.96: Loss = 0.559586
Epoch 5.97: Loss = 0.416336
Epoch 5.98: Loss = 0.42691
Epoch 5.99: Loss = 0.602936
Epoch 5.100: Loss = 0.650497
Epoch 5.101: Loss = 0.623795
Epoch 5.102: Loss = 0.49498
Epoch 5.103: Loss = 0.440231
Epoch 5.104: Loss = 0.422714
Epoch 5.105: Loss = 0.627274
Epoch 5.106: Loss = 0.613953
Epoch 5.107: Loss = 0.419113
Epoch 5.108: Loss = 0.530334
Epoch 5.109: Loss = 0.484619
Epoch 5.110: Loss = 0.515671
Epoch 5.111: Loss = 0.411835
Epoch 5.112: Loss = 0.408615
Epoch 5.113: Loss = 0.461365
Epoch 5.114: Loss = 0.399582
Epoch 5.115: Loss = 0.430328
Epoch 5.116: Loss = 0.466339
Epoch 5.117: Loss = 0.294312
Epoch 5.118: Loss = 0.251068
Epoch 5.119: Loss = 0.304657
Epoch 5.120: Loss = 0.402908
TRAIN LOSS = 0.496872
TRAIN ACC = 85.582 % (51351/60000)
Loss = 0.465759
Loss = 0.540939
Loss = 0.619812
Loss = 0.690994
Loss = 0.636353
Loss = 0.482376
Loss = 0.467682
Loss = 0.697739
Loss = 0.6418
Loss = 0.624329
Loss = 0.230362
Loss = 0.389694
Loss = 0.309784
Loss = 0.483963
Loss = 0.279266
Loss = 0.406784
Loss = 0.313538
Loss = 0.107697
Loss = 0.265076
Loss = 0.584305
TEST LOSS = 0.461912
TEST ACC = 513.509 % (8656/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.42688
Epoch 6.2: Loss = 0.548004
Epoch 6.3: Loss = 0.578537
Epoch 6.4: Loss = 0.383636
Epoch 6.5: Loss = 0.416351
Epoch 6.6: Loss = 0.468964
Epoch 6.7: Loss = 0.446426
Epoch 6.8: Loss = 0.438095
Epoch 6.9: Loss = 0.396927
Epoch 6.10: Loss = 0.473419
Epoch 6.11: Loss = 0.496429
Epoch 6.12: Loss = 0.493759
Epoch 6.13: Loss = 0.404968
Epoch 6.14: Loss = 0.389893
Epoch 6.15: Loss = 0.55423
Epoch 6.16: Loss = 0.577805
Epoch 6.17: Loss = 0.580048
Epoch 6.18: Loss = 0.635361
Epoch 6.19: Loss = 0.461838
Epoch 6.20: Loss = 0.441956
Epoch 6.21: Loss = 0.346771
Epoch 6.22: Loss = 0.388611
Epoch 6.23: Loss = 0.371597
Epoch 6.24: Loss = 0.689529
Epoch 6.25: Loss = 0.494293
Epoch 6.26: Loss = 0.593872
Epoch 6.27: Loss = 0.64061
Epoch 6.28: Loss = 0.551285
Epoch 6.29: Loss = 0.589325
Epoch 6.30: Loss = 0.655396
Epoch 6.31: Loss = 0.39003
Epoch 6.32: Loss = 0.574005
Epoch 6.33: Loss = 0.453537
Epoch 6.34: Loss = 0.547195
Epoch 6.35: Loss = 0.523788
Epoch 6.36: Loss = 0.659042
Epoch 6.37: Loss = 0.408707
Epoch 6.38: Loss = 0.423416
Epoch 6.39: Loss = 0.488007
Epoch 6.40: Loss = 0.472092
Epoch 6.41: Loss = 0.47319
Epoch 6.42: Loss = 0.702744
Epoch 6.43: Loss = 0.385696
Epoch 6.44: Loss = 0.394623
Epoch 6.45: Loss = 0.488098
Epoch 6.46: Loss = 0.537277
Epoch 6.47: Loss = 0.449417
Epoch 6.48: Loss = 0.527908
Epoch 6.49: Loss = 0.53598
Epoch 6.50: Loss = 0.597275
Epoch 6.51: Loss = 0.450912
Epoch 6.52: Loss = 0.396912
Epoch 6.53: Loss = 0.458344
Epoch 6.54: Loss = 0.57074
Epoch 6.55: Loss = 0.47728
Epoch 6.56: Loss = 0.462814
Epoch 6.57: Loss = 0.479538
Epoch 6.58: Loss = 0.531479
Epoch 6.59: Loss = 0.541443
Epoch 6.60: Loss = 0.609207
Epoch 6.61: Loss = 0.49501
Epoch 6.62: Loss = 0.59024
Epoch 6.63: Loss = 0.689941
Epoch 6.64: Loss = 0.569855
Epoch 6.65: Loss = 0.725662
Epoch 6.66: Loss = 0.43428
Epoch 6.67: Loss = 0.527084
Epoch 6.68: Loss = 0.303528
Epoch 6.69: Loss = 0.413513
Epoch 6.70: Loss = 0.641968
Epoch 6.71: Loss = 0.483398
Epoch 6.72: Loss = 0.441589
Epoch 6.73: Loss = 0.477081
Epoch 6.74: Loss = 0.386475
Epoch 6.75: Loss = 0.789902
Epoch 6.76: Loss = 0.526047
Epoch 6.77: Loss = 0.496429
Epoch 6.78: Loss = 0.52478
Epoch 6.79: Loss = 0.545563
Epoch 6.80: Loss = 0.616028
Epoch 6.81: Loss = 0.445267
Epoch 6.82: Loss = 0.37825
Epoch 6.83: Loss = 0.579117
Epoch 6.84: Loss = 0.546082
Epoch 6.85: Loss = 0.794662
Epoch 6.86: Loss = 0.588898
Epoch 6.87: Loss = 0.376602
Epoch 6.88: Loss = 0.512314
Epoch 6.89: Loss = 0.565598
Epoch 6.90: Loss = 0.4646
Epoch 6.91: Loss = 0.569626
Epoch 6.92: Loss = 0.547897
Epoch 6.93: Loss = 0.6082
Epoch 6.94: Loss = 0.471802
Epoch 6.95: Loss = 0.515472
Epoch 6.96: Loss = 0.56813
Epoch 6.97: Loss = 0.399109
Epoch 6.98: Loss = 0.463455
Epoch 6.99: Loss = 0.601837
Epoch 6.100: Loss = 0.674957
Epoch 6.101: Loss = 0.647232
Epoch 6.102: Loss = 0.507751
Epoch 6.103: Loss = 0.509293
Epoch 6.104: Loss = 0.447357
Epoch 6.105: Loss = 0.605392
Epoch 6.106: Loss = 0.592148
Epoch 6.107: Loss = 0.433868
Epoch 6.108: Loss = 0.519226
Epoch 6.109: Loss = 0.475906
Epoch 6.110: Loss = 0.507584
Epoch 6.111: Loss = 0.402969
Epoch 6.112: Loss = 0.39798
Epoch 6.113: Loss = 0.467819
Epoch 6.114: Loss = 0.417847
Epoch 6.115: Loss = 0.435669
Epoch 6.116: Loss = 0.495667
Epoch 6.117: Loss = 0.293854
Epoch 6.118: Loss = 0.247406
Epoch 6.119: Loss = 0.348755
Epoch 6.120: Loss = 0.381363
TRAIN LOSS = 0.503601
TRAIN ACC = 85.8444 % (51509/60000)
Loss = 0.459534
Loss = 0.526031
Loss = 0.631805
Loss = 0.692795
Loss = 0.6474
Loss = 0.488983
Loss = 0.485123
Loss = 0.705795
Loss = 0.63736
Loss = 0.61351
Loss = 0.236481
Loss = 0.379517
Loss = 0.34433
Loss = 0.469269
Loss = 0.299835
Loss = 0.478607
Loss = 0.337814
Loss = 0.1008
Loss = 0.242981
Loss = 0.602554
TEST LOSS = 0.469026
TEST ACC = 515.089 % (8656/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.436462
Epoch 7.2: Loss = 0.577637
Epoch 7.3: Loss = 0.693939
Epoch 7.4: Loss = 0.382462
Epoch 7.5: Loss = 0.430557
Epoch 7.6: Loss = 0.501541
Epoch 7.7: Loss = 0.451172
Epoch 7.8: Loss = 0.480347
Epoch 7.9: Loss = 0.433456
Epoch 7.10: Loss = 0.489059
Epoch 7.11: Loss = 0.557861
Epoch 7.12: Loss = 0.538055
Epoch 7.13: Loss = 0.408951
Epoch 7.14: Loss = 0.424011
Epoch 7.15: Loss = 0.571121
Epoch 7.16: Loss = 0.571793
Epoch 7.17: Loss = 0.588196
Epoch 7.18: Loss = 0.643585
Epoch 7.19: Loss = 0.490723
Epoch 7.20: Loss = 0.423691
Epoch 7.21: Loss = 0.400467
Epoch 7.22: Loss = 0.362747
Epoch 7.23: Loss = 0.386459
Epoch 7.24: Loss = 0.677872
Epoch 7.25: Loss = 0.50589
Epoch 7.26: Loss = 0.664261
Epoch 7.27: Loss = 0.670547
Epoch 7.28: Loss = 0.555069
Epoch 7.29: Loss = 0.590729
Epoch 7.30: Loss = 0.677185
Epoch 7.31: Loss = 0.372025
Epoch 7.32: Loss = 0.599701
Epoch 7.33: Loss = 0.444061
Epoch 7.34: Loss = 0.539032
Epoch 7.35: Loss = 0.53363
Epoch 7.36: Loss = 0.640106
Epoch 7.37: Loss = 0.421753
Epoch 7.38: Loss = 0.411911
Epoch 7.39: Loss = 0.475586
Epoch 7.40: Loss = 0.48114
Epoch 7.41: Loss = 0.48671
Epoch 7.42: Loss = 0.711502
Epoch 7.43: Loss = 0.423843
Epoch 7.44: Loss = 0.424179
Epoch 7.45: Loss = 0.494125
Epoch 7.46: Loss = 0.567505
Epoch 7.47: Loss = 0.446869
Epoch 7.48: Loss = 0.55925
Epoch 7.49: Loss = 0.504013
Epoch 7.50: Loss = 0.610077
Epoch 7.51: Loss = 0.464859
Epoch 7.52: Loss = 0.429642
Epoch 7.53: Loss = 0.443924
Epoch 7.54: Loss = 0.598648
Epoch 7.55: Loss = 0.479919
Epoch 7.56: Loss = 0.462753
Epoch 7.57: Loss = 0.492554
Epoch 7.58: Loss = 0.544464
Epoch 7.59: Loss = 0.581329
Epoch 7.60: Loss = 0.613571
Epoch 7.61: Loss = 0.541153
Epoch 7.62: Loss = 0.619339
Epoch 7.63: Loss = 0.69313
Epoch 7.64: Loss = 0.60379
Epoch 7.65: Loss = 0.709732
Epoch 7.66: Loss = 0.436401
Epoch 7.67: Loss = 0.515244
Epoch 7.68: Loss = 0.339691
Epoch 7.69: Loss = 0.413956
Epoch 7.70: Loss = 0.609589
Epoch 7.71: Loss = 0.47081
Epoch 7.72: Loss = 0.428574
Epoch 7.73: Loss = 0.450867
Epoch 7.74: Loss = 0.398407
Epoch 7.75: Loss = 0.755829
Epoch 7.76: Loss = 0.515594
Epoch 7.77: Loss = 0.493469
Epoch 7.78: Loss = 0.503738
Epoch 7.79: Loss = 0.539154
Epoch 7.80: Loss = 0.587875
Epoch 7.81: Loss = 0.467194
Epoch 7.82: Loss = 0.368469
Epoch 7.83: Loss = 0.635788
Epoch 7.84: Loss = 0.562363
Epoch 7.85: Loss = 0.783615
Epoch 7.86: Loss = 0.568222
Epoch 7.87: Loss = 0.397781
Epoch 7.88: Loss = 0.576691
Epoch 7.89: Loss = 0.619995
Epoch 7.90: Loss = 0.465866
Epoch 7.91: Loss = 0.58078
Epoch 7.92: Loss = 0.552246
Epoch 7.93: Loss = 0.614212
Epoch 7.94: Loss = 0.41011
Epoch 7.95: Loss = 0.511322
Epoch 7.96: Loss = 0.586304
Epoch 7.97: Loss = 0.387115
Epoch 7.98: Loss = 0.477661
Epoch 7.99: Loss = 0.63147
Epoch 7.100: Loss = 0.663071
Epoch 7.101: Loss = 0.640259
Epoch 7.102: Loss = 0.469193
Epoch 7.103: Loss = 0.48735
Epoch 7.104: Loss = 0.447098
Epoch 7.105: Loss = 0.558517
Epoch 7.106: Loss = 0.607224
Epoch 7.107: Loss = 0.424194
Epoch 7.108: Loss = 0.532425
Epoch 7.109: Loss = 0.46785
Epoch 7.110: Loss = 0.515762
Epoch 7.111: Loss = 0.400574
Epoch 7.112: Loss = 0.407104
Epoch 7.113: Loss = 0.468216
Epoch 7.114: Loss = 0.388626
Epoch 7.115: Loss = 0.406586
Epoch 7.116: Loss = 0.479279
Epoch 7.117: Loss = 0.292786
Epoch 7.118: Loss = 0.252319
Epoch 7.119: Loss = 0.377609
Epoch 7.120: Loss = 0.386124
TRAIN LOSS = 0.511185
TRAIN ACC = 85.7498 % (51453/60000)
Loss = 0.444519
Loss = 0.524872
Loss = 0.629669
Loss = 0.715744
Loss = 0.651047
Loss = 0.476166
Loss = 0.48024
Loss = 0.707336
Loss = 0.605881
Loss = 0.603821
Loss = 0.267273
Loss = 0.37587
Loss = 0.356506
Loss = 0.453751
Loss = 0.254807
Loss = 0.449356
Loss = 0.286804
Loss = 0.101608
Loss = 0.258469
Loss = 0.690445
TEST LOSS = 0.466709
TEST ACC = 514.529 % (8732/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.437958
Epoch 8.2: Loss = 0.600922
Epoch 8.3: Loss = 0.651611
Epoch 8.4: Loss = 0.372421
Epoch 8.5: Loss = 0.41011
Epoch 8.6: Loss = 0.475876
Epoch 8.7: Loss = 0.426514
Epoch 8.8: Loss = 0.44809
Epoch 8.9: Loss = 0.404709
Epoch 8.10: Loss = 0.437378
Epoch 8.11: Loss = 0.500305
Epoch 8.12: Loss = 0.508621
Epoch 8.13: Loss = 0.403061
Epoch 8.14: Loss = 0.406219
Epoch 8.15: Loss = 0.548096
Epoch 8.16: Loss = 0.564209
Epoch 8.17: Loss = 0.555618
Epoch 8.18: Loss = 0.677612
Epoch 8.19: Loss = 0.468521
Epoch 8.20: Loss = 0.458618
Epoch 8.21: Loss = 0.391602
Epoch 8.22: Loss = 0.363022
Epoch 8.23: Loss = 0.391571
Epoch 8.24: Loss = 0.673721
Epoch 8.25: Loss = 0.502991
Epoch 8.26: Loss = 0.68779
Epoch 8.27: Loss = 0.683044
Epoch 8.28: Loss = 0.561798
Epoch 8.29: Loss = 0.585464
Epoch 8.30: Loss = 0.712082
Epoch 8.31: Loss = 0.387146
Epoch 8.32: Loss = 0.59671
Epoch 8.33: Loss = 0.451996
Epoch 8.34: Loss = 0.554138
Epoch 8.35: Loss = 0.522522
Epoch 8.36: Loss = 0.634308
Epoch 8.37: Loss = 0.402039
Epoch 8.38: Loss = 0.42749
Epoch 8.39: Loss = 0.484161
Epoch 8.40: Loss = 0.520355
Epoch 8.41: Loss = 0.497177
Epoch 8.42: Loss = 0.712814
Epoch 8.43: Loss = 0.407074
Epoch 8.44: Loss = 0.428955
Epoch 8.45: Loss = 0.493103
Epoch 8.46: Loss = 0.557678
Epoch 8.47: Loss = 0.443619
Epoch 8.48: Loss = 0.579666
Epoch 8.49: Loss = 0.513138
Epoch 8.50: Loss = 0.616776
Epoch 8.51: Loss = 0.418503
Epoch 8.52: Loss = 0.414932
Epoch 8.53: Loss = 0.456375
Epoch 8.54: Loss = 0.560684
Epoch 8.55: Loss = 0.471863
Epoch 8.56: Loss = 0.485947
Epoch 8.57: Loss = 0.475723
Epoch 8.58: Loss = 0.570282
Epoch 8.59: Loss = 0.556778
Epoch 8.60: Loss = 0.603134
Epoch 8.61: Loss = 0.54509
Epoch 8.62: Loss = 0.676498
Epoch 8.63: Loss = 0.71077
Epoch 8.64: Loss = 0.621094
Epoch 8.65: Loss = 0.689377
Epoch 8.66: Loss = 0.427109
Epoch 8.67: Loss = 0.558929
Epoch 8.68: Loss = 0.339523
Epoch 8.69: Loss = 0.387894
Epoch 8.70: Loss = 0.611145
Epoch 8.71: Loss = 0.492752
Epoch 8.72: Loss = 0.408768
Epoch 8.73: Loss = 0.475983
Epoch 8.74: Loss = 0.391251
Epoch 8.75: Loss = 0.795731
Epoch 8.76: Loss = 0.556519
Epoch 8.77: Loss = 0.486816
Epoch 8.78: Loss = 0.540268
Epoch 8.79: Loss = 0.539291
Epoch 8.80: Loss = 0.578125
Epoch 8.81: Loss = 0.494171
Epoch 8.82: Loss = 0.389435
Epoch 8.83: Loss = 0.636169
Epoch 8.84: Loss = 0.579834
Epoch 8.85: Loss = 0.735428
Epoch 8.86: Loss = 0.605621
Epoch 8.87: Loss = 0.401978
Epoch 8.88: Loss = 0.554779
Epoch 8.89: Loss = 0.625031
Epoch 8.90: Loss = 0.458771
Epoch 8.91: Loss = 0.517853
Epoch 8.92: Loss = 0.574234
Epoch 8.93: Loss = 0.630066
Epoch 8.94: Loss = 0.426361
Epoch 8.95: Loss = 0.511597
Epoch 8.96: Loss = 0.546005
Epoch 8.97: Loss = 0.412598
Epoch 8.98: Loss = 0.477112
Epoch 8.99: Loss = 0.660202
Epoch 8.100: Loss = 0.704849
Epoch 8.101: Loss = 0.66507
Epoch 8.102: Loss = 0.468018
Epoch 8.103: Loss = 0.456635
Epoch 8.104: Loss = 0.471344
Epoch 8.105: Loss = 0.595459
Epoch 8.106: Loss = 0.637772
Epoch 8.107: Loss = 0.447739
Epoch 8.108: Loss = 0.57251
Epoch 8.109: Loss = 0.53299
Epoch 8.110: Loss = 0.473145
Epoch 8.111: Loss = 0.418945
Epoch 8.112: Loss = 0.423431
Epoch 8.113: Loss = 0.477295
Epoch 8.114: Loss = 0.386185
Epoch 8.115: Loss = 0.437592
Epoch 8.116: Loss = 0.504776
Epoch 8.117: Loss = 0.294952
Epoch 8.118: Loss = 0.276169
Epoch 8.119: Loss = 0.400284
Epoch 8.120: Loss = 0.407227
TRAIN LOSS = 0.513992
TRAIN ACC = 86.1389 % (51686/60000)
Loss = 0.477753
Loss = 0.548355
Loss = 0.67926
Loss = 0.747421
Loss = 0.660019
Loss = 0.505951
Loss = 0.514496
Loss = 0.770721
Loss = 0.646057
Loss = 0.60643
Loss = 0.271744
Loss = 0.41156
Loss = 0.368591
Loss = 0.459076
Loss = 0.273621
Loss = 0.37587
Loss = 0.361191
Loss = 0.0831909
Loss = 0.290909
Loss = 0.713928
TEST LOSS = 0.488307
TEST ACC = 516.859 % (8673/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.448914
Epoch 9.2: Loss = 0.599869
Epoch 9.3: Loss = 0.677948
Epoch 9.4: Loss = 0.417984
Epoch 9.5: Loss = 0.410553
Epoch 9.6: Loss = 0.476746
Epoch 9.7: Loss = 0.45488
Epoch 9.8: Loss = 0.473511
Epoch 9.9: Loss = 0.469864
Epoch 9.10: Loss = 0.463715
Epoch 9.11: Loss = 0.504745
Epoch 9.12: Loss = 0.538132
Epoch 9.13: Loss = 0.366501
Epoch 9.14: Loss = 0.477997
Epoch 9.15: Loss = 0.569168
Epoch 9.16: Loss = 0.555756
Epoch 9.17: Loss = 0.58551
Epoch 9.18: Loss = 0.737671
Epoch 9.19: Loss = 0.470093
Epoch 9.20: Loss = 0.449203
Epoch 9.21: Loss = 0.395691
Epoch 9.22: Loss = 0.391388
Epoch 9.23: Loss = 0.390244
Epoch 9.24: Loss = 0.684982
Epoch 9.25: Loss = 0.518494
Epoch 9.26: Loss = 0.741272
Epoch 9.27: Loss = 0.68483
Epoch 9.28: Loss = 0.580963
Epoch 9.29: Loss = 0.635193
Epoch 9.30: Loss = 0.731216
Epoch 9.31: Loss = 0.397156
Epoch 9.32: Loss = 0.666092
Epoch 9.33: Loss = 0.428848
Epoch 9.34: Loss = 0.531647
Epoch 9.35: Loss = 0.592758
Epoch 9.36: Loss = 0.681503
Epoch 9.37: Loss = 0.436157
Epoch 9.38: Loss = 0.451584
Epoch 9.39: Loss = 0.473648
Epoch 9.40: Loss = 0.466522
Epoch 9.41: Loss = 0.517471
Epoch 9.42: Loss = 0.702438
Epoch 9.43: Loss = 0.434555
Epoch 9.44: Loss = 0.441711
Epoch 9.45: Loss = 0.505569
Epoch 9.46: Loss = 0.604538
Epoch 9.47: Loss = 0.509827
Epoch 9.48: Loss = 0.639099
Epoch 9.49: Loss = 0.487473
Epoch 9.50: Loss = 0.562225
Epoch 9.51: Loss = 0.439728
Epoch 9.52: Loss = 0.421753
Epoch 9.53: Loss = 0.497574
Epoch 9.54: Loss = 0.556824
Epoch 9.55: Loss = 0.507034
Epoch 9.56: Loss = 0.489075
Epoch 9.57: Loss = 0.46347
Epoch 9.58: Loss = 0.589478
Epoch 9.59: Loss = 0.625412
Epoch 9.60: Loss = 0.613083
Epoch 9.61: Loss = 0.580719
Epoch 9.62: Loss = 0.668549
Epoch 9.63: Loss = 0.734604
Epoch 9.64: Loss = 0.620163
Epoch 9.65: Loss = 0.722412
Epoch 9.66: Loss = 0.450729
Epoch 9.67: Loss = 0.520523
Epoch 9.68: Loss = 0.328796
Epoch 9.69: Loss = 0.43251
Epoch 9.70: Loss = 0.598068
Epoch 9.71: Loss = 0.519363
Epoch 9.72: Loss = 0.407913
Epoch 9.73: Loss = 0.510818
Epoch 9.74: Loss = 0.41835
Epoch 9.75: Loss = 0.805054
Epoch 9.76: Loss = 0.606949
Epoch 9.77: Loss = 0.49147
Epoch 9.78: Loss = 0.508865
Epoch 9.79: Loss = 0.573074
Epoch 9.80: Loss = 0.608002
Epoch 9.81: Loss = 0.460709
Epoch 9.82: Loss = 0.445282
Epoch 9.83: Loss = 0.688705
Epoch 9.84: Loss = 0.561798
Epoch 9.85: Loss = 0.803467
Epoch 9.86: Loss = 0.586258
Epoch 9.87: Loss = 0.388397
Epoch 9.88: Loss = 0.567825
Epoch 9.89: Loss = 0.692474
Epoch 9.90: Loss = 0.447205
Epoch 9.91: Loss = 0.534729
Epoch 9.92: Loss = 0.575058
Epoch 9.93: Loss = 0.702637
Epoch 9.94: Loss = 0.443481
Epoch 9.95: Loss = 0.512985
Epoch 9.96: Loss = 0.589584
Epoch 9.97: Loss = 0.422318
Epoch 9.98: Loss = 0.473648
Epoch 9.99: Loss = 0.628662
Epoch 9.100: Loss = 0.746231
Epoch 9.101: Loss = 0.705963
Epoch 9.102: Loss = 0.511719
Epoch 9.103: Loss = 0.460571
Epoch 9.104: Loss = 0.482361
Epoch 9.105: Loss = 0.66275
Epoch 9.106: Loss = 0.64064
Epoch 9.107: Loss = 0.473297
Epoch 9.108: Loss = 0.582504
Epoch 9.109: Loss = 0.492508
Epoch 9.110: Loss = 0.506317
Epoch 9.111: Loss = 0.454346
Epoch 9.112: Loss = 0.366882
Epoch 9.113: Loss = 0.470886
Epoch 9.114: Loss = 0.370178
Epoch 9.115: Loss = 0.431213
Epoch 9.116: Loss = 0.523468
Epoch 9.117: Loss = 0.310547
Epoch 9.118: Loss = 0.293396
Epoch 9.119: Loss = 0.411179
Epoch 9.120: Loss = 0.447708
TRAIN LOSS = 0.530121
TRAIN ACC = 86.0275 % (51619/60000)
Loss = 0.478287
Loss = 0.605362
Loss = 0.684891
Loss = 0.819733
Loss = 0.705765
Loss = 0.518097
Loss = 0.526001
Loss = 0.774536
Loss = 0.662476
Loss = 0.602051
Loss = 0.239655
Loss = 0.423935
Loss = 0.367004
Loss = 0.511505
Loss = 0.274292
Loss = 0.423096
Loss = 0.420181
Loss = 0.0826416
Loss = 0.318909
Loss = 0.755859
TEST LOSS = 0.509714
TEST ACC = 516.19 % (8652/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.435547
Epoch 10.2: Loss = 0.577194
Epoch 10.3: Loss = 0.685089
Epoch 10.4: Loss = 0.39888
Epoch 10.5: Loss = 0.384628
Epoch 10.6: Loss = 0.487411
Epoch 10.7: Loss = 0.4673
Epoch 10.8: Loss = 0.503067
Epoch 10.9: Loss = 0.48439
Epoch 10.10: Loss = 0.516739
Epoch 10.11: Loss = 0.522308
Epoch 10.12: Loss = 0.577148
Epoch 10.13: Loss = 0.395935
Epoch 10.14: Loss = 0.472336
Epoch 10.15: Loss = 0.588074
Epoch 10.16: Loss = 0.556137
Epoch 10.17: Loss = 0.615143
Epoch 10.18: Loss = 0.733139
Epoch 10.19: Loss = 0.502151
Epoch 10.20: Loss = 0.429779
Epoch 10.21: Loss = 0.395111
Epoch 10.22: Loss = 0.408783
Epoch 10.23: Loss = 0.38501
Epoch 10.24: Loss = 0.619507
Epoch 10.25: Loss = 0.544525
Epoch 10.26: Loss = 0.736176
Epoch 10.27: Loss = 0.719513
Epoch 10.28: Loss = 0.580673
Epoch 10.29: Loss = 0.636917
Epoch 10.30: Loss = 0.745087
Epoch 10.31: Loss = 0.395203
Epoch 10.32: Loss = 0.627029
Epoch 10.33: Loss = 0.456207
Epoch 10.34: Loss = 0.563095
Epoch 10.35: Loss = 0.593369
Epoch 10.36: Loss = 0.673965
Epoch 10.37: Loss = 0.458984
Epoch 10.38: Loss = 0.495972
Epoch 10.39: Loss = 0.494812
Epoch 10.40: Loss = 0.480499
Epoch 10.41: Loss = 0.535904
Epoch 10.42: Loss = 0.744385
Epoch 10.43: Loss = 0.439682
Epoch 10.44: Loss = 0.460312
Epoch 10.45: Loss = 0.554031
Epoch 10.46: Loss = 0.614563
Epoch 10.47: Loss = 0.539764
Epoch 10.48: Loss = 0.632019
Epoch 10.49: Loss = 0.561096
Epoch 10.50: Loss = 0.56131
Epoch 10.51: Loss = 0.443878
Epoch 10.52: Loss = 0.431976
Epoch 10.53: Loss = 0.509827
Epoch 10.54: Loss = 0.592499
Epoch 10.55: Loss = 0.527344
Epoch 10.56: Loss = 0.539398
Epoch 10.57: Loss = 0.499649
Epoch 10.58: Loss = 0.618332
Epoch 10.59: Loss = 0.622879
Epoch 10.60: Loss = 0.623978
Epoch 10.61: Loss = 0.598343
Epoch 10.62: Loss = 0.729736
Epoch 10.63: Loss = 0.773621
Epoch 10.64: Loss = 0.670151
Epoch 10.65: Loss = 0.710526
Epoch 10.66: Loss = 0.458359
Epoch 10.67: Loss = 0.559586
Epoch 10.68: Loss = 0.376785
Epoch 10.69: Loss = 0.447113
Epoch 10.70: Loss = 0.635788
Epoch 10.71: Loss = 0.544296
Epoch 10.72: Loss = 0.460587
Epoch 10.73: Loss = 0.557373
Epoch 10.74: Loss = 0.411728
Epoch 10.75: Loss = 0.793945
Epoch 10.76: Loss = 0.606262
Epoch 10.77: Loss = 0.463211
Epoch 10.78: Loss = 0.575958
Epoch 10.79: Loss = 0.622238
Epoch 10.80: Loss = 0.607437
Epoch 10.81: Loss = 0.51387
Epoch 10.82: Loss = 0.442841
Epoch 10.83: Loss = 0.67691
Epoch 10.84: Loss = 0.567215
Epoch 10.85: Loss = 0.859787
Epoch 10.86: Loss = 0.647995
Epoch 10.87: Loss = 0.471176
Epoch 10.88: Loss = 0.601028
Epoch 10.89: Loss = 0.661972
Epoch 10.90: Loss = 0.473892
Epoch 10.91: Loss = 0.570068
Epoch 10.92: Loss = 0.632233
Epoch 10.93: Loss = 0.761963
Epoch 10.94: Loss = 0.435196
Epoch 10.95: Loss = 0.497849
Epoch 10.96: Loss = 0.592651
Epoch 10.97: Loss = 0.45488
Epoch 10.98: Loss = 0.477463
Epoch 10.99: Loss = 0.661621
Epoch 10.100: Loss = 0.696732
Epoch 10.101: Loss = 0.729721
Epoch 10.102: Loss = 0.537277
Epoch 10.103: Loss = 0.510269
Epoch 10.104: Loss = 0.491089
Epoch 10.105: Loss = 0.664459
Epoch 10.106: Loss = 0.664673
Epoch 10.107: Loss = 0.474899
Epoch 10.108: Loss = 0.545364
Epoch 10.109: Loss = 0.543869
Epoch 10.110: Loss = 0.528488
Epoch 10.111: Loss = 0.477234
Epoch 10.112: Loss = 0.402008
Epoch 10.113: Loss = 0.505753
Epoch 10.114: Loss = 0.388336
Epoch 10.115: Loss = 0.442719
Epoch 10.116: Loss = 0.533615
Epoch 10.117: Loss = 0.351624
Epoch 10.118: Loss = 0.272049
Epoch 10.119: Loss = 0.393799
Epoch 10.120: Loss = 0.458801
TRAIN LOSS = 0.546783
TRAIN ACC = 86.264 % (51761/60000)
Loss = 0.516571
Loss = 0.654556
Loss = 0.710892
Loss = 0.80365
Loss = 0.734573
Loss = 0.57547
Loss = 0.512512
Loss = 0.809952
Loss = 0.691879
Loss = 0.624466
Loss = 0.240601
Loss = 0.470856
Loss = 0.387466
Loss = 0.512695
Loss = 0.279465
Loss = 0.372986
Loss = 0.379868
Loss = 0.0946503
Loss = 0.328964
Loss = 0.791306
TEST LOSS = 0.524669
TEST ACC = 517.609 % (8671/10000)
