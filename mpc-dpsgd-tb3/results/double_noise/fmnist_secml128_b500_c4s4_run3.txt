Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.39636
Epoch 1.2: Loss = 2.31363
Epoch 1.3: Loss = 2.3241
Epoch 1.4: Loss = 2.29205
Epoch 1.5: Loss = 2.26569
Epoch 1.6: Loss = 2.22821
Epoch 1.7: Loss = 2.22876
Epoch 1.8: Loss = 2.19778
Epoch 1.9: Loss = 2.168
Epoch 1.10: Loss = 2.14726
Epoch 1.11: Loss = 2.1037
Epoch 1.12: Loss = 2.08359
Epoch 1.13: Loss = 2.05453
Epoch 1.14: Loss = 2.06313
Epoch 1.15: Loss = 2.06993
Epoch 1.16: Loss = 2.03421
Epoch 1.17: Loss = 1.96597
Epoch 1.18: Loss = 1.94418
Epoch 1.19: Loss = 1.92397
Epoch 1.20: Loss = 1.90091
Epoch 1.21: Loss = 1.84036
Epoch 1.22: Loss = 1.82445
Epoch 1.23: Loss = 1.76962
Epoch 1.24: Loss = 1.81485
Epoch 1.25: Loss = 1.73402
Epoch 1.26: Loss = 1.77028
Epoch 1.27: Loss = 1.70775
Epoch 1.28: Loss = 1.70447
Epoch 1.29: Loss = 1.68671
Epoch 1.30: Loss = 1.7175
Epoch 1.31: Loss = 1.60277
Epoch 1.32: Loss = 1.63509
Epoch 1.33: Loss = 1.55377
Epoch 1.34: Loss = 1.59807
Epoch 1.35: Loss = 1.50682
Epoch 1.36: Loss = 1.60454
Epoch 1.37: Loss = 1.46432
Epoch 1.38: Loss = 1.41682
Epoch 1.39: Loss = 1.39987
Epoch 1.40: Loss = 1.37198
Epoch 1.41: Loss = 1.39917
Epoch 1.42: Loss = 1.34378
Epoch 1.43: Loss = 1.34708
Epoch 1.44: Loss = 1.2328
Epoch 1.45: Loss = 1.34412
Epoch 1.46: Loss = 1.27209
Epoch 1.47: Loss = 1.19589
Epoch 1.48: Loss = 1.25026
Epoch 1.49: Loss = 1.1877
Epoch 1.50: Loss = 1.25327
Epoch 1.51: Loss = 1.11717
Epoch 1.52: Loss = 1.12448
Epoch 1.53: Loss = 1.15833
Epoch 1.54: Loss = 1.12358
Epoch 1.55: Loss = 1.10187
Epoch 1.56: Loss = 1.0444
Epoch 1.57: Loss = 0.999115
Epoch 1.58: Loss = 1.05919
Epoch 1.59: Loss = 1.033
Epoch 1.60: Loss = 1.14294
Epoch 1.61: Loss = 1.07405
Epoch 1.62: Loss = 1.09598
Epoch 1.63: Loss = 1.13638
Epoch 1.64: Loss = 1.09209
Epoch 1.65: Loss = 1.09776
Epoch 1.66: Loss = 0.990738
Epoch 1.67: Loss = 0.934448
Epoch 1.68: Loss = 0.80957
Epoch 1.69: Loss = 0.884903
Epoch 1.70: Loss = 0.947449
Epoch 1.71: Loss = 0.901733
Epoch 1.72: Loss = 0.866623
Epoch 1.73: Loss = 0.914459
Epoch 1.74: Loss = 0.766785
Epoch 1.75: Loss = 0.909805
Epoch 1.76: Loss = 0.864273
Epoch 1.77: Loss = 0.83548
Epoch 1.78: Loss = 0.811249
Epoch 1.79: Loss = 0.783981
Epoch 1.80: Loss = 0.915451
Epoch 1.81: Loss = 0.770508
Epoch 1.82: Loss = 0.734039
Epoch 1.83: Loss = 0.899063
Epoch 1.84: Loss = 0.830978
Epoch 1.85: Loss = 0.882141
Epoch 1.86: Loss = 0.783234
Epoch 1.87: Loss = 0.760239
Epoch 1.88: Loss = 0.780518
Epoch 1.89: Loss = 0.884811
Epoch 1.90: Loss = 0.70459
Epoch 1.91: Loss = 0.781113
Epoch 1.92: Loss = 0.76564
Epoch 1.93: Loss = 0.786591
Epoch 1.94: Loss = 0.656494
Epoch 1.95: Loss = 0.752579
Epoch 1.96: Loss = 0.755524
Epoch 1.97: Loss = 0.570496
Epoch 1.98: Loss = 0.668228
Epoch 1.99: Loss = 0.724182
Epoch 1.100: Loss = 0.884186
Epoch 1.101: Loss = 0.808411
Epoch 1.102: Loss = 0.684158
Epoch 1.103: Loss = 0.623779
Epoch 1.104: Loss = 0.604065
Epoch 1.105: Loss = 0.738968
Epoch 1.106: Loss = 0.745834
Epoch 1.107: Loss = 0.645218
Epoch 1.108: Loss = 0.672821
Epoch 1.109: Loss = 0.604782
Epoch 1.110: Loss = 0.642181
Epoch 1.111: Loss = 0.547318
Epoch 1.112: Loss = 0.574554
Epoch 1.113: Loss = 0.629211
Epoch 1.114: Loss = 0.544647
Epoch 1.115: Loss = 0.612213
Epoch 1.116: Loss = 0.604645
Epoch 1.117: Loss = 0.522919
Epoch 1.118: Loss = 0.463531
Epoch 1.119: Loss = 0.500214
Epoch 1.120: Loss = 0.466568
TRAIN LOSS = 1.21172
TRAIN ACC = 66.1255 % (39677/60000)
Loss = 0.63179
Loss = 0.686737
Loss = 0.819092
Loss = 0.709549
Loss = 0.768051
Loss = 0.644348
Loss = 0.614273
Loss = 0.79776
Loss = 0.779282
Loss = 0.707352
Loss = 0.357971
Loss = 0.579651
Loss = 0.412582
Loss = 0.582123
Loss = 0.475616
Loss = 0.508041
Loss = 0.451752
Loss = 0.23555
Loss = 0.425354
Loss = 0.717819
TEST LOSS = 0.595235
TEST ACC = 396.77 % (8210/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.591324
Epoch 2.2: Loss = 0.673416
Epoch 2.3: Loss = 0.669662
Epoch 2.4: Loss = 0.528
Epoch 2.5: Loss = 0.557846
Epoch 2.6: Loss = 0.545319
Epoch 2.7: Loss = 0.652222
Epoch 2.8: Loss = 0.576797
Epoch 2.9: Loss = 0.566956
Epoch 2.10: Loss = 0.578186
Epoch 2.11: Loss = 0.581955
Epoch 2.12: Loss = 0.547745
Epoch 2.13: Loss = 0.509735
Epoch 2.14: Loss = 0.567291
Epoch 2.15: Loss = 0.677353
Epoch 2.16: Loss = 0.638397
Epoch 2.17: Loss = 0.633514
Epoch 2.18: Loss = 0.708954
Epoch 2.19: Loss = 0.556229
Epoch 2.20: Loss = 0.50885
Epoch 2.21: Loss = 0.515427
Epoch 2.22: Loss = 0.48584
Epoch 2.23: Loss = 0.510498
Epoch 2.24: Loss = 0.731888
Epoch 2.25: Loss = 0.52951
Epoch 2.26: Loss = 0.632111
Epoch 2.27: Loss = 0.605331
Epoch 2.28: Loss = 0.604218
Epoch 2.29: Loss = 0.6763
Epoch 2.30: Loss = 0.770401
Epoch 2.31: Loss = 0.494308
Epoch 2.32: Loss = 0.660828
Epoch 2.33: Loss = 0.518738
Epoch 2.34: Loss = 0.614868
Epoch 2.35: Loss = 0.563385
Epoch 2.36: Loss = 0.686295
Epoch 2.37: Loss = 0.434387
Epoch 2.38: Loss = 0.478897
Epoch 2.39: Loss = 0.555252
Epoch 2.40: Loss = 0.504196
Epoch 2.41: Loss = 0.555923
Epoch 2.42: Loss = 0.657211
Epoch 2.43: Loss = 0.530182
Epoch 2.44: Loss = 0.443634
Epoch 2.45: Loss = 0.528717
Epoch 2.46: Loss = 0.613434
Epoch 2.47: Loss = 0.463776
Epoch 2.48: Loss = 0.542892
Epoch 2.49: Loss = 0.554977
Epoch 2.50: Loss = 0.579987
Epoch 2.51: Loss = 0.497803
Epoch 2.52: Loss = 0.467545
Epoch 2.53: Loss = 0.537872
Epoch 2.54: Loss = 0.582932
Epoch 2.55: Loss = 0.536087
Epoch 2.56: Loss = 0.48497
Epoch 2.57: Loss = 0.494003
Epoch 2.58: Loss = 0.572647
Epoch 2.59: Loss = 0.550018
Epoch 2.60: Loss = 0.658508
Epoch 2.61: Loss = 0.657379
Epoch 2.62: Loss = 0.640564
Epoch 2.63: Loss = 0.68898
Epoch 2.64: Loss = 0.601959
Epoch 2.65: Loss = 0.71524
Epoch 2.66: Loss = 0.514694
Epoch 2.67: Loss = 0.545105
Epoch 2.68: Loss = 0.365784
Epoch 2.69: Loss = 0.467667
Epoch 2.70: Loss = 0.622833
Epoch 2.71: Loss = 0.484085
Epoch 2.72: Loss = 0.47052
Epoch 2.73: Loss = 0.536865
Epoch 2.74: Loss = 0.407227
Epoch 2.75: Loss = 0.674026
Epoch 2.76: Loss = 0.559326
Epoch 2.77: Loss = 0.477203
Epoch 2.78: Loss = 0.515747
Epoch 2.79: Loss = 0.522568
Epoch 2.80: Loss = 0.641327
Epoch 2.81: Loss = 0.473267
Epoch 2.82: Loss = 0.444473
Epoch 2.83: Loss = 0.668884
Epoch 2.84: Loss = 0.542221
Epoch 2.85: Loss = 0.688919
Epoch 2.86: Loss = 0.55838
Epoch 2.87: Loss = 0.463608
Epoch 2.88: Loss = 0.46553
Epoch 2.89: Loss = 0.683258
Epoch 2.90: Loss = 0.458923
Epoch 2.91: Loss = 0.569336
Epoch 2.92: Loss = 0.586182
Epoch 2.93: Loss = 0.620377
Epoch 2.94: Loss = 0.456848
Epoch 2.95: Loss = 0.537674
Epoch 2.96: Loss = 0.620728
Epoch 2.97: Loss = 0.4272
Epoch 2.98: Loss = 0.476395
Epoch 2.99: Loss = 0.588989
Epoch 2.100: Loss = 0.7117
Epoch 2.101: Loss = 0.690704
Epoch 2.102: Loss = 0.510086
Epoch 2.103: Loss = 0.467575
Epoch 2.104: Loss = 0.463516
Epoch 2.105: Loss = 0.615311
Epoch 2.106: Loss = 0.605087
Epoch 2.107: Loss = 0.472153
Epoch 2.108: Loss = 0.529327
Epoch 2.109: Loss = 0.491074
Epoch 2.110: Loss = 0.543716
Epoch 2.111: Loss = 0.413757
Epoch 2.112: Loss = 0.435638
Epoch 2.113: Loss = 0.476578
Epoch 2.114: Loss = 0.458649
Epoch 2.115: Loss = 0.468445
Epoch 2.116: Loss = 0.49733
Epoch 2.117: Loss = 0.388611
Epoch 2.118: Loss = 0.302155
Epoch 2.119: Loss = 0.421097
Epoch 2.120: Loss = 0.389374
TRAIN LOSS = 0.550751
TRAIN ACC = 82.9956 % (49799/60000)
Loss = 0.51239
Loss = 0.646942
Loss = 0.728745
Loss = 0.627594
Loss = 0.705215
Loss = 0.5112
Loss = 0.513962
Loss = 0.725388
Loss = 0.664291
Loss = 0.615677
Loss = 0.237991
Loss = 0.479095
Loss = 0.346863
Loss = 0.507919
Loss = 0.348694
Loss = 0.437714
Loss = 0.318405
Loss = 0.136261
Loss = 0.329514
Loss = 0.702362
TEST LOSS = 0.504811
TEST ACC = 497.989 % (8414/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.516663
Epoch 3.2: Loss = 0.600021
Epoch 3.3: Loss = 0.624252
Epoch 3.4: Loss = 0.403351
Epoch 3.5: Loss = 0.427628
Epoch 3.6: Loss = 0.480347
Epoch 3.7: Loss = 0.503387
Epoch 3.8: Loss = 0.490997
Epoch 3.9: Loss = 0.470245
Epoch 3.10: Loss = 0.483688
Epoch 3.11: Loss = 0.49144
Epoch 3.12: Loss = 0.439606
Epoch 3.13: Loss = 0.425613
Epoch 3.14: Loss = 0.469513
Epoch 3.15: Loss = 0.550186
Epoch 3.16: Loss = 0.57225
Epoch 3.17: Loss = 0.585312
Epoch 3.18: Loss = 0.65007
Epoch 3.19: Loss = 0.508301
Epoch 3.20: Loss = 0.473328
Epoch 3.21: Loss = 0.440384
Epoch 3.22: Loss = 0.414551
Epoch 3.23: Loss = 0.413177
Epoch 3.24: Loss = 0.600952
Epoch 3.25: Loss = 0.479385
Epoch 3.26: Loss = 0.608856
Epoch 3.27: Loss = 0.575089
Epoch 3.28: Loss = 0.591736
Epoch 3.29: Loss = 0.647232
Epoch 3.30: Loss = 0.774536
Epoch 3.31: Loss = 0.402588
Epoch 3.32: Loss = 0.639801
Epoch 3.33: Loss = 0.456833
Epoch 3.34: Loss = 0.592712
Epoch 3.35: Loss = 0.518341
Epoch 3.36: Loss = 0.644394
Epoch 3.37: Loss = 0.416
Epoch 3.38: Loss = 0.433899
Epoch 3.39: Loss = 0.507874
Epoch 3.40: Loss = 0.456146
Epoch 3.41: Loss = 0.502548
Epoch 3.42: Loss = 0.674225
Epoch 3.43: Loss = 0.477585
Epoch 3.44: Loss = 0.4151
Epoch 3.45: Loss = 0.483459
Epoch 3.46: Loss = 0.566879
Epoch 3.47: Loss = 0.464172
Epoch 3.48: Loss = 0.530014
Epoch 3.49: Loss = 0.504547
Epoch 3.50: Loss = 0.554688
Epoch 3.51: Loss = 0.483643
Epoch 3.52: Loss = 0.450897
Epoch 3.53: Loss = 0.50563
Epoch 3.54: Loss = 0.57692
Epoch 3.55: Loss = 0.491028
Epoch 3.56: Loss = 0.485962
Epoch 3.57: Loss = 0.465958
Epoch 3.58: Loss = 0.552414
Epoch 3.59: Loss = 0.544907
Epoch 3.60: Loss = 0.644669
Epoch 3.61: Loss = 0.594589
Epoch 3.62: Loss = 0.646698
Epoch 3.63: Loss = 0.710541
Epoch 3.64: Loss = 0.614029
Epoch 3.65: Loss = 0.701355
Epoch 3.66: Loss = 0.542953
Epoch 3.67: Loss = 0.528717
Epoch 3.68: Loss = 0.360703
Epoch 3.69: Loss = 0.43959
Epoch 3.70: Loss = 0.627686
Epoch 3.71: Loss = 0.450562
Epoch 3.72: Loss = 0.445587
Epoch 3.73: Loss = 0.523285
Epoch 3.74: Loss = 0.401993
Epoch 3.75: Loss = 0.696014
Epoch 3.76: Loss = 0.521057
Epoch 3.77: Loss = 0.474274
Epoch 3.78: Loss = 0.540344
Epoch 3.79: Loss = 0.502823
Epoch 3.80: Loss = 0.595184
Epoch 3.81: Loss = 0.455673
Epoch 3.82: Loss = 0.428574
Epoch 3.83: Loss = 0.691437
Epoch 3.84: Loss = 0.538361
Epoch 3.85: Loss = 0.70813
Epoch 3.86: Loss = 0.556107
Epoch 3.87: Loss = 0.44429
Epoch 3.88: Loss = 0.452576
Epoch 3.89: Loss = 0.623322
Epoch 3.90: Loss = 0.421753
Epoch 3.91: Loss = 0.580215
Epoch 3.92: Loss = 0.540085
Epoch 3.93: Loss = 0.591187
Epoch 3.94: Loss = 0.395813
Epoch 3.95: Loss = 0.552994
Epoch 3.96: Loss = 0.647247
Epoch 3.97: Loss = 0.439438
Epoch 3.98: Loss = 0.469376
Epoch 3.99: Loss = 0.55632
Epoch 3.100: Loss = 0.719299
Epoch 3.101: Loss = 0.697433
Epoch 3.102: Loss = 0.506287
Epoch 3.103: Loss = 0.503342
Epoch 3.104: Loss = 0.465317
Epoch 3.105: Loss = 0.600708
Epoch 3.106: Loss = 0.653091
Epoch 3.107: Loss = 0.445923
Epoch 3.108: Loss = 0.516479
Epoch 3.109: Loss = 0.459
Epoch 3.110: Loss = 0.55011
Epoch 3.111: Loss = 0.391541
Epoch 3.112: Loss = 0.439911
Epoch 3.113: Loss = 0.484268
Epoch 3.114: Loss = 0.434387
Epoch 3.115: Loss = 0.457748
Epoch 3.116: Loss = 0.483841
Epoch 3.117: Loss = 0.377747
Epoch 3.118: Loss = 0.29425
Epoch 3.119: Loss = 0.401733
Epoch 3.120: Loss = 0.381348
TRAIN LOSS = 0.520264
TRAIN ACC = 84.4269 % (50659/60000)
Loss = 0.484375
Loss = 0.616135
Loss = 0.688828
Loss = 0.655579
Loss = 0.710373
Loss = 0.500534
Loss = 0.48671
Loss = 0.698563
Loss = 0.636765
Loss = 0.557175
Loss = 0.231522
Loss = 0.50087
Loss = 0.372253
Loss = 0.477844
Loss = 0.31398
Loss = 0.437454
Loss = 0.279633
Loss = 0.120621
Loss = 0.286758
Loss = 0.71579
TEST LOSS = 0.488588
TEST ACC = 506.589 % (8537/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.534393
Epoch 4.2: Loss = 0.592529
Epoch 4.3: Loss = 0.609985
Epoch 4.4: Loss = 0.39595
Epoch 4.5: Loss = 0.429794
Epoch 4.6: Loss = 0.440384
Epoch 4.7: Loss = 0.488876
Epoch 4.8: Loss = 0.434586
Epoch 4.9: Loss = 0.452576
Epoch 4.10: Loss = 0.490829
Epoch 4.11: Loss = 0.521255
Epoch 4.12: Loss = 0.442795
Epoch 4.13: Loss = 0.415909
Epoch 4.14: Loss = 0.480957
Epoch 4.15: Loss = 0.547684
Epoch 4.16: Loss = 0.554901
Epoch 4.17: Loss = 0.530136
Epoch 4.18: Loss = 0.662201
Epoch 4.19: Loss = 0.53772
Epoch 4.20: Loss = 0.452988
Epoch 4.21: Loss = 0.418533
Epoch 4.22: Loss = 0.377518
Epoch 4.23: Loss = 0.402527
Epoch 4.24: Loss = 0.616257
Epoch 4.25: Loss = 0.470596
Epoch 4.26: Loss = 0.643677
Epoch 4.27: Loss = 0.568222
Epoch 4.28: Loss = 0.601288
Epoch 4.29: Loss = 0.67453
Epoch 4.30: Loss = 0.727356
Epoch 4.31: Loss = 0.406113
Epoch 4.32: Loss = 0.631607
Epoch 4.33: Loss = 0.468903
Epoch 4.34: Loss = 0.606598
Epoch 4.35: Loss = 0.453506
Epoch 4.36: Loss = 0.667419
Epoch 4.37: Loss = 0.384079
Epoch 4.38: Loss = 0.446533
Epoch 4.39: Loss = 0.488907
Epoch 4.40: Loss = 0.462585
Epoch 4.41: Loss = 0.472717
Epoch 4.42: Loss = 0.712814
Epoch 4.43: Loss = 0.446747
Epoch 4.44: Loss = 0.407516
Epoch 4.45: Loss = 0.473206
Epoch 4.46: Loss = 0.555817
Epoch 4.47: Loss = 0.4272
Epoch 4.48: Loss = 0.531372
Epoch 4.49: Loss = 0.479401
Epoch 4.50: Loss = 0.540146
Epoch 4.51: Loss = 0.435028
Epoch 4.52: Loss = 0.425903
Epoch 4.53: Loss = 0.492584
Epoch 4.54: Loss = 0.572083
Epoch 4.55: Loss = 0.468231
Epoch 4.56: Loss = 0.506744
Epoch 4.57: Loss = 0.473953
Epoch 4.58: Loss = 0.598282
Epoch 4.59: Loss = 0.520813
Epoch 4.60: Loss = 0.631668
Epoch 4.61: Loss = 0.538544
Epoch 4.62: Loss = 0.619324
Epoch 4.63: Loss = 0.680954
Epoch 4.64: Loss = 0.606659
Epoch 4.65: Loss = 0.67778
Epoch 4.66: Loss = 0.529205
Epoch 4.67: Loss = 0.497009
Epoch 4.68: Loss = 0.346848
Epoch 4.69: Loss = 0.425232
Epoch 4.70: Loss = 0.674057
Epoch 4.71: Loss = 0.442078
Epoch 4.72: Loss = 0.427551
Epoch 4.73: Loss = 0.524399
Epoch 4.74: Loss = 0.39917
Epoch 4.75: Loss = 0.747208
Epoch 4.76: Loss = 0.507584
Epoch 4.77: Loss = 0.442764
Epoch 4.78: Loss = 0.522644
Epoch 4.79: Loss = 0.528839
Epoch 4.80: Loss = 0.565277
Epoch 4.81: Loss = 0.439255
Epoch 4.82: Loss = 0.391464
Epoch 4.83: Loss = 0.632111
Epoch 4.84: Loss = 0.582626
Epoch 4.85: Loss = 0.767838
Epoch 4.86: Loss = 0.543793
Epoch 4.87: Loss = 0.435806
Epoch 4.88: Loss = 0.482666
Epoch 4.89: Loss = 0.5952
Epoch 4.90: Loss = 0.404556
Epoch 4.91: Loss = 0.559204
Epoch 4.92: Loss = 0.557587
Epoch 4.93: Loss = 0.606644
Epoch 4.94: Loss = 0.435638
Epoch 4.95: Loss = 0.599838
Epoch 4.96: Loss = 0.644211
Epoch 4.97: Loss = 0.464417
Epoch 4.98: Loss = 0.474655
Epoch 4.99: Loss = 0.537552
Epoch 4.100: Loss = 0.721039
Epoch 4.101: Loss = 0.694092
Epoch 4.102: Loss = 0.498108
Epoch 4.103: Loss = 0.512848
Epoch 4.104: Loss = 0.460037
Epoch 4.105: Loss = 0.602173
Epoch 4.106: Loss = 0.612823
Epoch 4.107: Loss = 0.436157
Epoch 4.108: Loss = 0.476089
Epoch 4.109: Loss = 0.478287
Epoch 4.110: Loss = 0.562225
Epoch 4.111: Loss = 0.38147
Epoch 4.112: Loss = 0.472321
Epoch 4.113: Loss = 0.486038
Epoch 4.114: Loss = 0.452362
Epoch 4.115: Loss = 0.431473
Epoch 4.116: Loss = 0.491562
Epoch 4.117: Loss = 0.350708
Epoch 4.118: Loss = 0.285599
Epoch 4.119: Loss = 0.402176
Epoch 4.120: Loss = 0.396347
TRAIN LOSS = 0.51474
TRAIN ACC = 85.35 % (51212/60000)
Loss = 0.491974
Loss = 0.616989
Loss = 0.67807
Loss = 0.65625
Loss = 0.703262
Loss = 0.512299
Loss = 0.48674
Loss = 0.716782
Loss = 0.647644
Loss = 0.579559
Loss = 0.239243
Loss = 0.47049
Loss = 0.431564
Loss = 0.509552
Loss = 0.324112
Loss = 0.48349
Loss = 0.320175
Loss = 0.102921
Loss = 0.286301
Loss = 0.768997
TEST LOSS = 0.501321
TEST ACC = 512.119 % (8597/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.528656
Epoch 5.2: Loss = 0.548462
Epoch 5.3: Loss = 0.588486
Epoch 5.4: Loss = 0.392395
Epoch 5.5: Loss = 0.458023
Epoch 5.6: Loss = 0.467682
Epoch 5.7: Loss = 0.480835
Epoch 5.8: Loss = 0.454575
Epoch 5.9: Loss = 0.420975
Epoch 5.10: Loss = 0.487259
Epoch 5.11: Loss = 0.514114
Epoch 5.12: Loss = 0.431732
Epoch 5.13: Loss = 0.380249
Epoch 5.14: Loss = 0.477905
Epoch 5.15: Loss = 0.522934
Epoch 5.16: Loss = 0.591049
Epoch 5.17: Loss = 0.580856
Epoch 5.18: Loss = 0.685379
Epoch 5.19: Loss = 0.497589
Epoch 5.20: Loss = 0.440308
Epoch 5.21: Loss = 0.498734
Epoch 5.22: Loss = 0.399551
Epoch 5.23: Loss = 0.439438
Epoch 5.24: Loss = 0.666458
Epoch 5.25: Loss = 0.472931
Epoch 5.26: Loss = 0.651581
Epoch 5.27: Loss = 0.595581
Epoch 5.28: Loss = 0.595123
Epoch 5.29: Loss = 0.722794
Epoch 5.30: Loss = 0.753387
Epoch 5.31: Loss = 0.428558
Epoch 5.32: Loss = 0.591431
Epoch 5.33: Loss = 0.444519
Epoch 5.34: Loss = 0.656647
Epoch 5.35: Loss = 0.514328
Epoch 5.36: Loss = 0.647522
Epoch 5.37: Loss = 0.442505
Epoch 5.38: Loss = 0.453201
Epoch 5.39: Loss = 0.485382
Epoch 5.40: Loss = 0.422455
Epoch 5.41: Loss = 0.50386
Epoch 5.42: Loss = 0.778778
Epoch 5.43: Loss = 0.48056
Epoch 5.44: Loss = 0.464783
Epoch 5.45: Loss = 0.466431
Epoch 5.46: Loss = 0.539459
Epoch 5.47: Loss = 0.499069
Epoch 5.48: Loss = 0.599289
Epoch 5.49: Loss = 0.498672
Epoch 5.50: Loss = 0.616547
Epoch 5.51: Loss = 0.501083
Epoch 5.52: Loss = 0.451691
Epoch 5.53: Loss = 0.509064
Epoch 5.54: Loss = 0.613892
Epoch 5.55: Loss = 0.570999
Epoch 5.56: Loss = 0.491898
Epoch 5.57: Loss = 0.48172
Epoch 5.58: Loss = 0.614639
Epoch 5.59: Loss = 0.52861
Epoch 5.60: Loss = 0.650787
Epoch 5.61: Loss = 0.579514
Epoch 5.62: Loss = 0.620407
Epoch 5.63: Loss = 0.709091
Epoch 5.64: Loss = 0.670319
Epoch 5.65: Loss = 0.725723
Epoch 5.66: Loss = 0.57489
Epoch 5.67: Loss = 0.49649
Epoch 5.68: Loss = 0.374786
Epoch 5.69: Loss = 0.397095
Epoch 5.70: Loss = 0.6492
Epoch 5.71: Loss = 0.432007
Epoch 5.72: Loss = 0.452408
Epoch 5.73: Loss = 0.578064
Epoch 5.74: Loss = 0.419983
Epoch 5.75: Loss = 0.717331
Epoch 5.76: Loss = 0.527435
Epoch 5.77: Loss = 0.434326
Epoch 5.78: Loss = 0.529495
Epoch 5.79: Loss = 0.55809
Epoch 5.80: Loss = 0.565536
Epoch 5.81: Loss = 0.452103
Epoch 5.82: Loss = 0.400177
Epoch 5.83: Loss = 0.657669
Epoch 5.84: Loss = 0.552734
Epoch 5.85: Loss = 0.704056
Epoch 5.86: Loss = 0.571838
Epoch 5.87: Loss = 0.445114
Epoch 5.88: Loss = 0.448486
Epoch 5.89: Loss = 0.611816
Epoch 5.90: Loss = 0.391891
Epoch 5.91: Loss = 0.616776
Epoch 5.92: Loss = 0.603088
Epoch 5.93: Loss = 0.647522
Epoch 5.94: Loss = 0.447296
Epoch 5.95: Loss = 0.538574
Epoch 5.96: Loss = 0.605957
Epoch 5.97: Loss = 0.450714
Epoch 5.98: Loss = 0.475769
Epoch 5.99: Loss = 0.567123
Epoch 5.100: Loss = 0.692307
Epoch 5.101: Loss = 0.690414
Epoch 5.102: Loss = 0.459152
Epoch 5.103: Loss = 0.471497
Epoch 5.104: Loss = 0.462677
Epoch 5.105: Loss = 0.638947
Epoch 5.106: Loss = 0.656677
Epoch 5.107: Loss = 0.460434
Epoch 5.108: Loss = 0.544098
Epoch 5.109: Loss = 0.443741
Epoch 5.110: Loss = 0.609863
Epoch 5.111: Loss = 0.40062
Epoch 5.112: Loss = 0.477341
Epoch 5.113: Loss = 0.517944
Epoch 5.114: Loss = 0.436264
Epoch 5.115: Loss = 0.445847
Epoch 5.116: Loss = 0.494781
Epoch 5.117: Loss = 0.365326
Epoch 5.118: Loss = 0.260284
Epoch 5.119: Loss = 0.379883
Epoch 5.120: Loss = 0.381714
TRAIN LOSS = 0.52655
TRAIN ACC = 85.7498 % (51453/60000)
Loss = 0.500824
Loss = 0.626648
Loss = 0.77655
Loss = 0.67308
Loss = 0.749542
Loss = 0.553772
Loss = 0.519272
Loss = 0.746887
Loss = 0.690872
Loss = 0.600189
Loss = 0.224747
Loss = 0.529037
Loss = 0.388947
Loss = 0.492371
Loss = 0.285339
Loss = 0.462372
Loss = 0.304794
Loss = 0.0971375
Loss = 0.304932
Loss = 0.714417
TEST LOSS = 0.512086
TEST ACC = 514.529 % (8622/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.506699
Epoch 6.2: Loss = 0.589081
Epoch 6.3: Loss = 0.587341
Epoch 6.4: Loss = 0.360947
Epoch 6.5: Loss = 0.45018
Epoch 6.6: Loss = 0.456543
Epoch 6.7: Loss = 0.475052
Epoch 6.8: Loss = 0.41481
Epoch 6.9: Loss = 0.43576
Epoch 6.10: Loss = 0.489365
Epoch 6.11: Loss = 0.537933
Epoch 6.12: Loss = 0.418198
Epoch 6.13: Loss = 0.424789
Epoch 6.14: Loss = 0.496582
Epoch 6.15: Loss = 0.554657
Epoch 6.16: Loss = 0.581284
Epoch 6.17: Loss = 0.57196
Epoch 6.18: Loss = 0.720245
Epoch 6.19: Loss = 0.534119
Epoch 6.20: Loss = 0.413712
Epoch 6.21: Loss = 0.503876
Epoch 6.22: Loss = 0.429718
Epoch 6.23: Loss = 0.438538
Epoch 6.24: Loss = 0.62886
Epoch 6.25: Loss = 0.506256
Epoch 6.26: Loss = 0.704193
Epoch 6.27: Loss = 0.597427
Epoch 6.28: Loss = 0.579712
Epoch 6.29: Loss = 0.735001
Epoch 6.30: Loss = 0.685669
Epoch 6.31: Loss = 0.462891
Epoch 6.32: Loss = 0.610184
Epoch 6.33: Loss = 0.488754
Epoch 6.34: Loss = 0.585159
Epoch 6.35: Loss = 0.496796
Epoch 6.36: Loss = 0.694336
Epoch 6.37: Loss = 0.430023
Epoch 6.38: Loss = 0.449371
Epoch 6.39: Loss = 0.475403
Epoch 6.40: Loss = 0.498215
Epoch 6.41: Loss = 0.484741
Epoch 6.42: Loss = 0.80957
Epoch 6.43: Loss = 0.484085
Epoch 6.44: Loss = 0.438644
Epoch 6.45: Loss = 0.564545
Epoch 6.46: Loss = 0.55722
Epoch 6.47: Loss = 0.488174
Epoch 6.48: Loss = 0.582474
Epoch 6.49: Loss = 0.542313
Epoch 6.50: Loss = 0.602417
Epoch 6.51: Loss = 0.514313
Epoch 6.52: Loss = 0.428238
Epoch 6.53: Loss = 0.498596
Epoch 6.54: Loss = 0.670074
Epoch 6.55: Loss = 0.592346
Epoch 6.56: Loss = 0.570099
Epoch 6.57: Loss = 0.511948
Epoch 6.58: Loss = 0.616302
Epoch 6.59: Loss = 0.55014
Epoch 6.60: Loss = 0.612228
Epoch 6.61: Loss = 0.535843
Epoch 6.62: Loss = 0.622284
Epoch 6.63: Loss = 0.672699
Epoch 6.64: Loss = 0.651978
Epoch 6.65: Loss = 0.685791
Epoch 6.66: Loss = 0.566116
Epoch 6.67: Loss = 0.493637
Epoch 6.68: Loss = 0.359695
Epoch 6.69: Loss = 0.419724
Epoch 6.70: Loss = 0.697174
Epoch 6.71: Loss = 0.417496
Epoch 6.72: Loss = 0.456238
Epoch 6.73: Loss = 0.559891
Epoch 6.74: Loss = 0.375854
Epoch 6.75: Loss = 0.761398
Epoch 6.76: Loss = 0.587585
Epoch 6.77: Loss = 0.405426
Epoch 6.78: Loss = 0.519669
Epoch 6.79: Loss = 0.587677
Epoch 6.80: Loss = 0.541351
Epoch 6.81: Loss = 0.421448
Epoch 6.82: Loss = 0.400604
Epoch 6.83: Loss = 0.614487
Epoch 6.84: Loss = 0.561646
Epoch 6.85: Loss = 0.69838
Epoch 6.86: Loss = 0.592484
Epoch 6.87: Loss = 0.415985
Epoch 6.88: Loss = 0.440445
Epoch 6.89: Loss = 0.591949
Epoch 6.90: Loss = 0.412003
Epoch 6.91: Loss = 0.615799
Epoch 6.92: Loss = 0.633209
Epoch 6.93: Loss = 0.648468
Epoch 6.94: Loss = 0.437073
Epoch 6.95: Loss = 0.577789
Epoch 6.96: Loss = 0.632996
Epoch 6.97: Loss = 0.454727
Epoch 6.98: Loss = 0.515457
Epoch 6.99: Loss = 0.611679
Epoch 6.100: Loss = 0.786713
Epoch 6.101: Loss = 0.73027
Epoch 6.102: Loss = 0.477325
Epoch 6.103: Loss = 0.51561
Epoch 6.104: Loss = 0.487701
Epoch 6.105: Loss = 0.658875
Epoch 6.106: Loss = 0.672333
Epoch 6.107: Loss = 0.455963
Epoch 6.108: Loss = 0.549973
Epoch 6.109: Loss = 0.446365
Epoch 6.110: Loss = 0.626007
Epoch 6.111: Loss = 0.404099
Epoch 6.112: Loss = 0.469193
Epoch 6.113: Loss = 0.502762
Epoch 6.114: Loss = 0.411896
Epoch 6.115: Loss = 0.406494
Epoch 6.116: Loss = 0.464981
Epoch 6.117: Loss = 0.375366
Epoch 6.118: Loss = 0.24852
Epoch 6.119: Loss = 0.391174
Epoch 6.120: Loss = 0.416626
TRAIN LOSS = 0.531937
TRAIN ACC = 85.9741 % (51586/60000)
Loss = 0.5186
Loss = 0.657227
Loss = 0.776047
Loss = 0.737457
Loss = 0.80864
Loss = 0.615341
Loss = 0.563263
Loss = 0.800385
Loss = 0.714951
Loss = 0.638977
Loss = 0.177826
Loss = 0.524139
Loss = 0.359665
Loss = 0.513458
Loss = 0.285919
Loss = 0.430695
Loss = 0.311478
Loss = 0.078064
Loss = 0.336975
Loss = 0.739334
TEST LOSS = 0.529422
TEST ACC = 515.858 % (8643/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.532089
Epoch 7.2: Loss = 0.624756
Epoch 7.3: Loss = 0.627151
Epoch 7.4: Loss = 0.387146
Epoch 7.5: Loss = 0.440826
Epoch 7.6: Loss = 0.454346
Epoch 7.7: Loss = 0.483505
Epoch 7.8: Loss = 0.463654
Epoch 7.9: Loss = 0.490265
Epoch 7.10: Loss = 0.486008
Epoch 7.11: Loss = 0.544968
Epoch 7.12: Loss = 0.396973
Epoch 7.13: Loss = 0.439774
Epoch 7.14: Loss = 0.45752
Epoch 7.15: Loss = 0.555771
Epoch 7.16: Loss = 0.602493
Epoch 7.17: Loss = 0.632919
Epoch 7.18: Loss = 0.759903
Epoch 7.19: Loss = 0.581268
Epoch 7.20: Loss = 0.457153
Epoch 7.21: Loss = 0.525085
Epoch 7.22: Loss = 0.419479
Epoch 7.23: Loss = 0.444077
Epoch 7.24: Loss = 0.67215
Epoch 7.25: Loss = 0.489761
Epoch 7.26: Loss = 0.781677
Epoch 7.27: Loss = 0.62236
Epoch 7.28: Loss = 0.616684
Epoch 7.29: Loss = 0.809998
Epoch 7.30: Loss = 0.683228
Epoch 7.31: Loss = 0.484375
Epoch 7.32: Loss = 0.624725
Epoch 7.33: Loss = 0.524414
Epoch 7.34: Loss = 0.595993
Epoch 7.35: Loss = 0.541992
Epoch 7.36: Loss = 0.712204
Epoch 7.37: Loss = 0.413757
Epoch 7.38: Loss = 0.505814
Epoch 7.39: Loss = 0.471527
Epoch 7.40: Loss = 0.453308
Epoch 7.41: Loss = 0.550888
Epoch 7.42: Loss = 0.838135
Epoch 7.43: Loss = 0.460312
Epoch 7.44: Loss = 0.433258
Epoch 7.45: Loss = 0.560623
Epoch 7.46: Loss = 0.612579
Epoch 7.47: Loss = 0.465134
Epoch 7.48: Loss = 0.55072
Epoch 7.49: Loss = 0.509979
Epoch 7.50: Loss = 0.638931
Epoch 7.51: Loss = 0.493637
Epoch 7.52: Loss = 0.450119
Epoch 7.53: Loss = 0.503616
Epoch 7.54: Loss = 0.620972
Epoch 7.55: Loss = 0.598282
Epoch 7.56: Loss = 0.549469
Epoch 7.57: Loss = 0.527298
Epoch 7.58: Loss = 0.579315
Epoch 7.59: Loss = 0.560516
Epoch 7.60: Loss = 0.669449
Epoch 7.61: Loss = 0.575668
Epoch 7.62: Loss = 0.640121
Epoch 7.63: Loss = 0.727921
Epoch 7.64: Loss = 0.723785
Epoch 7.65: Loss = 0.744553
Epoch 7.66: Loss = 0.590714
Epoch 7.67: Loss = 0.544052
Epoch 7.68: Loss = 0.369659
Epoch 7.69: Loss = 0.432358
Epoch 7.70: Loss = 0.741348
Epoch 7.71: Loss = 0.421494
Epoch 7.72: Loss = 0.495544
Epoch 7.73: Loss = 0.644562
Epoch 7.74: Loss = 0.42186
Epoch 7.75: Loss = 0.789536
Epoch 7.76: Loss = 0.598312
Epoch 7.77: Loss = 0.447174
Epoch 7.78: Loss = 0.60498
Epoch 7.79: Loss = 0.646408
Epoch 7.80: Loss = 0.537003
Epoch 7.81: Loss = 0.432343
Epoch 7.82: Loss = 0.422043
Epoch 7.83: Loss = 0.625565
Epoch 7.84: Loss = 0.562897
Epoch 7.85: Loss = 0.68576
Epoch 7.86: Loss = 0.608536
Epoch 7.87: Loss = 0.418945
Epoch 7.88: Loss = 0.532501
Epoch 7.89: Loss = 0.660217
Epoch 7.90: Loss = 0.41452
Epoch 7.91: Loss = 0.654251
Epoch 7.92: Loss = 0.672562
Epoch 7.93: Loss = 0.680542
Epoch 7.94: Loss = 0.431793
Epoch 7.95: Loss = 0.600189
Epoch 7.96: Loss = 0.662888
Epoch 7.97: Loss = 0.430557
Epoch 7.98: Loss = 0.482651
Epoch 7.99: Loss = 0.641418
Epoch 7.100: Loss = 0.859528
Epoch 7.101: Loss = 0.794952
Epoch 7.102: Loss = 0.505219
Epoch 7.103: Loss = 0.562805
Epoch 7.104: Loss = 0.496048
Epoch 7.105: Loss = 0.711792
Epoch 7.106: Loss = 0.735672
Epoch 7.107: Loss = 0.461029
Epoch 7.108: Loss = 0.551346
Epoch 7.109: Loss = 0.509842
Epoch 7.110: Loss = 0.650803
Epoch 7.111: Loss = 0.435776
Epoch 7.112: Loss = 0.470016
Epoch 7.113: Loss = 0.533798
Epoch 7.114: Loss = 0.480423
Epoch 7.115: Loss = 0.425064
Epoch 7.116: Loss = 0.484528
Epoch 7.117: Loss = 0.366806
Epoch 7.118: Loss = 0.267899
Epoch 7.119: Loss = 0.424454
Epoch 7.120: Loss = 0.416641
TRAIN LOSS = 0.553741
TRAIN ACC = 86.0199 % (51614/60000)
Loss = 0.506119
Loss = 0.667419
Loss = 0.773956
Loss = 0.736755
Loss = 0.834518
Loss = 0.600876
Loss = 0.565567
Loss = 0.823471
Loss = 0.700821
Loss = 0.669769
Loss = 0.198608
Loss = 0.485291
Loss = 0.343948
Loss = 0.51239
Loss = 0.310486
Loss = 0.456009
Loss = 0.289658
Loss = 0.0850677
Loss = 0.333038
Loss = 0.817642
TEST LOSS = 0.53557
TEST ACC = 516.139 % (8623/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.612427
Epoch 8.2: Loss = 0.642303
Epoch 8.3: Loss = 0.710464
Epoch 8.4: Loss = 0.396805
Epoch 8.5: Loss = 0.453964
Epoch 8.6: Loss = 0.447235
Epoch 8.7: Loss = 0.441696
Epoch 8.8: Loss = 0.442413
Epoch 8.9: Loss = 0.513565
Epoch 8.10: Loss = 0.478424
Epoch 8.11: Loss = 0.56076
Epoch 8.12: Loss = 0.418457
Epoch 8.13: Loss = 0.439682
Epoch 8.14: Loss = 0.478256
Epoch 8.15: Loss = 0.570984
Epoch 8.16: Loss = 0.579971
Epoch 8.17: Loss = 0.620422
Epoch 8.18: Loss = 0.74234
Epoch 8.19: Loss = 0.567108
Epoch 8.20: Loss = 0.439682
Epoch 8.21: Loss = 0.513504
Epoch 8.22: Loss = 0.438644
Epoch 8.23: Loss = 0.426697
Epoch 8.24: Loss = 0.676529
Epoch 8.25: Loss = 0.540802
Epoch 8.26: Loss = 0.702042
Epoch 8.27: Loss = 0.596603
Epoch 8.28: Loss = 0.592209
Epoch 8.29: Loss = 0.772507
Epoch 8.30: Loss = 0.724304
Epoch 8.31: Loss = 0.461655
Epoch 8.32: Loss = 0.675583
Epoch 8.33: Loss = 0.547623
Epoch 8.34: Loss = 0.616898
Epoch 8.35: Loss = 0.564514
Epoch 8.36: Loss = 0.661819
Epoch 8.37: Loss = 0.396301
Epoch 8.38: Loss = 0.46019
Epoch 8.39: Loss = 0.503311
Epoch 8.40: Loss = 0.471802
Epoch 8.41: Loss = 0.5896
Epoch 8.42: Loss = 0.817963
Epoch 8.43: Loss = 0.46019
Epoch 8.44: Loss = 0.443451
Epoch 8.45: Loss = 0.554382
Epoch 8.46: Loss = 0.649353
Epoch 8.47: Loss = 0.455856
Epoch 8.48: Loss = 0.638657
Epoch 8.49: Loss = 0.515289
Epoch 8.50: Loss = 0.640015
Epoch 8.51: Loss = 0.488342
Epoch 8.52: Loss = 0.420685
Epoch 8.53: Loss = 0.502747
Epoch 8.54: Loss = 0.635696
Epoch 8.55: Loss = 0.598022
Epoch 8.56: Loss = 0.561249
Epoch 8.57: Loss = 0.581665
Epoch 8.58: Loss = 0.603088
Epoch 8.59: Loss = 0.594315
Epoch 8.60: Loss = 0.630615
Epoch 8.61: Loss = 0.552841
Epoch 8.62: Loss = 0.661804
Epoch 8.63: Loss = 0.730713
Epoch 8.64: Loss = 0.754227
Epoch 8.65: Loss = 0.689331
Epoch 8.66: Loss = 0.536621
Epoch 8.67: Loss = 0.595093
Epoch 8.68: Loss = 0.405899
Epoch 8.69: Loss = 0.437225
Epoch 8.70: Loss = 0.753159
Epoch 8.71: Loss = 0.414459
Epoch 8.72: Loss = 0.48233
Epoch 8.73: Loss = 0.601334
Epoch 8.74: Loss = 0.421646
Epoch 8.75: Loss = 0.772858
Epoch 8.76: Loss = 0.632797
Epoch 8.77: Loss = 0.494385
Epoch 8.78: Loss = 0.602219
Epoch 8.79: Loss = 0.657822
Epoch 8.80: Loss = 0.551498
Epoch 8.81: Loss = 0.46875
Epoch 8.82: Loss = 0.403702
Epoch 8.83: Loss = 0.709152
Epoch 8.84: Loss = 0.5504
Epoch 8.85: Loss = 0.734711
Epoch 8.86: Loss = 0.639664
Epoch 8.87: Loss = 0.424774
Epoch 8.88: Loss = 0.537064
Epoch 8.89: Loss = 0.633698
Epoch 8.90: Loss = 0.431992
Epoch 8.91: Loss = 0.708084
Epoch 8.92: Loss = 0.696884
Epoch 8.93: Loss = 0.696304
Epoch 8.94: Loss = 0.479279
Epoch 8.95: Loss = 0.682678
Epoch 8.96: Loss = 0.751068
Epoch 8.97: Loss = 0.439697
Epoch 8.98: Loss = 0.587173
Epoch 8.99: Loss = 0.621216
Epoch 8.100: Loss = 0.82959
Epoch 8.101: Loss = 0.852905
Epoch 8.102: Loss = 0.578445
Epoch 8.103: Loss = 0.561203
Epoch 8.104: Loss = 0.521744
Epoch 8.105: Loss = 0.777786
Epoch 8.106: Loss = 0.83046
Epoch 8.107: Loss = 0.462891
Epoch 8.108: Loss = 0.620911
Epoch 8.109: Loss = 0.512238
Epoch 8.110: Loss = 0.707596
Epoch 8.111: Loss = 0.454758
Epoch 8.112: Loss = 0.501572
Epoch 8.113: Loss = 0.538696
Epoch 8.114: Loss = 0.469574
Epoch 8.115: Loss = 0.521561
Epoch 8.116: Loss = 0.56488
Epoch 8.117: Loss = 0.399002
Epoch 8.118: Loss = 0.292236
Epoch 8.119: Loss = 0.317825
Epoch 8.120: Loss = 0.42923
TRAIN LOSS = 0.566437
TRAIN ACC = 85.9268 % (51559/60000)
Loss = 0.538391
Loss = 0.637039
Loss = 0.833893
Loss = 0.755478
Loss = 0.824097
Loss = 0.600418
Loss = 0.515228
Loss = 0.862274
Loss = 0.69725
Loss = 0.739868
Loss = 0.227234
Loss = 0.507172
Loss = 0.369644
Loss = 0.545837
Loss = 0.329727
Loss = 0.496445
Loss = 0.354813
Loss = 0.0581665
Loss = 0.352966
Loss = 0.875626
TEST LOSS = 0.556078
TEST ACC = 515.588 % (8640/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.608032
Epoch 9.2: Loss = 0.686371
Epoch 9.3: Loss = 0.712952
Epoch 9.4: Loss = 0.408737
Epoch 9.5: Loss = 0.467377
Epoch 9.6: Loss = 0.533264
Epoch 9.7: Loss = 0.483276
Epoch 9.8: Loss = 0.445023
Epoch 9.9: Loss = 0.568268
Epoch 9.10: Loss = 0.528381
Epoch 9.11: Loss = 0.601059
Epoch 9.12: Loss = 0.458771
Epoch 9.13: Loss = 0.48497
Epoch 9.14: Loss = 0.497955
Epoch 9.15: Loss = 0.658691
Epoch 9.16: Loss = 0.66153
Epoch 9.17: Loss = 0.695129
Epoch 9.18: Loss = 0.784576
Epoch 9.19: Loss = 0.651535
Epoch 9.20: Loss = 0.46109
Epoch 9.21: Loss = 0.563141
Epoch 9.22: Loss = 0.494125
Epoch 9.23: Loss = 0.476044
Epoch 9.24: Loss = 0.828064
Epoch 9.25: Loss = 0.617584
Epoch 9.26: Loss = 0.782303
Epoch 9.27: Loss = 0.609314
Epoch 9.28: Loss = 0.662552
Epoch 9.29: Loss = 0.868958
Epoch 9.30: Loss = 0.720978
Epoch 9.31: Loss = 0.539719
Epoch 9.32: Loss = 0.757324
Epoch 9.33: Loss = 0.553116
Epoch 9.34: Loss = 0.691406
Epoch 9.35: Loss = 0.644257
Epoch 9.36: Loss = 0.812241
Epoch 9.37: Loss = 0.438431
Epoch 9.38: Loss = 0.505844
Epoch 9.39: Loss = 0.540802
Epoch 9.40: Loss = 0.493622
Epoch 9.41: Loss = 0.610901
Epoch 9.42: Loss = 0.846329
Epoch 9.43: Loss = 0.497192
Epoch 9.44: Loss = 0.526215
Epoch 9.45: Loss = 0.621887
Epoch 9.46: Loss = 0.6866
Epoch 9.47: Loss = 0.564621
Epoch 9.48: Loss = 0.637909
Epoch 9.49: Loss = 0.589783
Epoch 9.50: Loss = 0.665955
Epoch 9.51: Loss = 0.553757
Epoch 9.52: Loss = 0.497192
Epoch 9.53: Loss = 0.592377
Epoch 9.54: Loss = 0.778641
Epoch 9.55: Loss = 0.625793
Epoch 9.56: Loss = 0.674698
Epoch 9.57: Loss = 0.626129
Epoch 9.58: Loss = 0.633057
Epoch 9.59: Loss = 0.594894
Epoch 9.60: Loss = 0.672119
Epoch 9.61: Loss = 0.580383
Epoch 9.62: Loss = 0.711731
Epoch 9.63: Loss = 0.806854
Epoch 9.64: Loss = 0.77243
Epoch 9.65: Loss = 0.776749
Epoch 9.66: Loss = 0.534683
Epoch 9.67: Loss = 0.650894
Epoch 9.68: Loss = 0.431137
Epoch 9.69: Loss = 0.467972
Epoch 9.70: Loss = 0.77356
Epoch 9.71: Loss = 0.477859
Epoch 9.72: Loss = 0.476974
Epoch 9.73: Loss = 0.655716
Epoch 9.74: Loss = 0.418457
Epoch 9.75: Loss = 0.820831
Epoch 9.76: Loss = 0.646042
Epoch 9.77: Loss = 0.521362
Epoch 9.78: Loss = 0.608292
Epoch 9.79: Loss = 0.741852
Epoch 9.80: Loss = 0.543198
Epoch 9.81: Loss = 0.556946
Epoch 9.82: Loss = 0.414917
Epoch 9.83: Loss = 0.746216
Epoch 9.84: Loss = 0.593887
Epoch 9.85: Loss = 0.761887
Epoch 9.86: Loss = 0.706116
Epoch 9.87: Loss = 0.403046
Epoch 9.88: Loss = 0.574234
Epoch 9.89: Loss = 0.694244
Epoch 9.90: Loss = 0.44957
Epoch 9.91: Loss = 0.781479
Epoch 9.92: Loss = 0.70845
Epoch 9.93: Loss = 0.808228
Epoch 9.94: Loss = 0.524689
Epoch 9.95: Loss = 0.679901
Epoch 9.96: Loss = 0.790848
Epoch 9.97: Loss = 0.442825
Epoch 9.98: Loss = 0.565582
Epoch 9.99: Loss = 0.696335
Epoch 9.100: Loss = 0.87886
Epoch 9.101: Loss = 0.850479
Epoch 9.102: Loss = 0.582367
Epoch 9.103: Loss = 0.587158
Epoch 9.104: Loss = 0.502228
Epoch 9.105: Loss = 0.79332
Epoch 9.106: Loss = 0.905273
Epoch 9.107: Loss = 0.488007
Epoch 9.108: Loss = 0.618637
Epoch 9.109: Loss = 0.527374
Epoch 9.110: Loss = 0.734909
Epoch 9.111: Loss = 0.480545
Epoch 9.112: Loss = 0.529053
Epoch 9.113: Loss = 0.596008
Epoch 9.114: Loss = 0.477417
Epoch 9.115: Loss = 0.5495
Epoch 9.116: Loss = 0.54364
Epoch 9.117: Loss = 0.380554
Epoch 9.118: Loss = 0.296234
Epoch 9.119: Loss = 0.306107
Epoch 9.120: Loss = 0.437302
TRAIN LOSS = 0.6073
TRAIN ACC = 85.6415 % (51387/60000)
Loss = 0.587677
Loss = 0.64621
Loss = 0.872574
Loss = 0.804108
Loss = 0.86116
Loss = 0.59053
Loss = 0.497589
Loss = 0.928085
Loss = 0.835922
Loss = 0.736237
Loss = 0.201645
Loss = 0.531631
Loss = 0.423859
Loss = 0.555573
Loss = 0.328232
Loss = 0.419769
Loss = 0.354324
Loss = 0.0670471
Loss = 0.321472
Loss = 0.937241
TEST LOSS = 0.575044
TEST ACC = 513.869 % (8643/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.626617
Epoch 10.2: Loss = 0.759628
Epoch 10.3: Loss = 0.72345
Epoch 10.4: Loss = 0.470276
Epoch 10.5: Loss = 0.47467
Epoch 10.6: Loss = 0.541107
Epoch 10.7: Loss = 0.525803
Epoch 10.8: Loss = 0.482346
Epoch 10.9: Loss = 0.538773
Epoch 10.10: Loss = 0.579559
Epoch 10.11: Loss = 0.575653
Epoch 10.12: Loss = 0.459351
Epoch 10.13: Loss = 0.455841
Epoch 10.14: Loss = 0.519012
Epoch 10.15: Loss = 0.685547
Epoch 10.16: Loss = 0.609711
Epoch 10.17: Loss = 0.673874
Epoch 10.18: Loss = 0.806
Epoch 10.19: Loss = 0.695282
Epoch 10.20: Loss = 0.486969
Epoch 10.21: Loss = 0.528336
Epoch 10.22: Loss = 0.459656
Epoch 10.23: Loss = 0.450089
Epoch 10.24: Loss = 0.723557
Epoch 10.25: Loss = 0.638992
Epoch 10.26: Loss = 0.756393
Epoch 10.27: Loss = 0.618332
Epoch 10.28: Loss = 0.637817
Epoch 10.29: Loss = 0.818497
Epoch 10.30: Loss = 0.752808
Epoch 10.31: Loss = 0.534241
Epoch 10.32: Loss = 0.660965
Epoch 10.33: Loss = 0.60788
Epoch 10.34: Loss = 0.698135
Epoch 10.35: Loss = 0.661026
Epoch 10.36: Loss = 0.799576
Epoch 10.37: Loss = 0.429962
Epoch 10.38: Loss = 0.486511
Epoch 10.39: Loss = 0.545181
Epoch 10.40: Loss = 0.437943
Epoch 10.41: Loss = 0.597931
Epoch 10.42: Loss = 0.894653
Epoch 10.43: Loss = 0.501785
Epoch 10.44: Loss = 0.51622
Epoch 10.45: Loss = 0.628464
Epoch 10.46: Loss = 0.638016
Epoch 10.47: Loss = 0.532288
Epoch 10.48: Loss = 0.730728
Epoch 10.49: Loss = 0.579239
Epoch 10.50: Loss = 0.75145
Epoch 10.51: Loss = 0.538696
Epoch 10.52: Loss = 0.497238
Epoch 10.53: Loss = 0.627274
Epoch 10.54: Loss = 0.816666
Epoch 10.55: Loss = 0.679642
Epoch 10.56: Loss = 0.583481
Epoch 10.57: Loss = 0.538025
Epoch 10.58: Loss = 0.644485
Epoch 10.59: Loss = 0.63945
Epoch 10.60: Loss = 0.719269
Epoch 10.61: Loss = 0.548508
Epoch 10.62: Loss = 0.662003
Epoch 10.63: Loss = 0.842133
Epoch 10.64: Loss = 0.712479
Epoch 10.65: Loss = 0.85997
Epoch 10.66: Loss = 0.589584
Epoch 10.67: Loss = 0.611588
Epoch 10.68: Loss = 0.403259
Epoch 10.69: Loss = 0.47374
Epoch 10.70: Loss = 0.779709
Epoch 10.71: Loss = 0.499283
Epoch 10.72: Loss = 0.498917
Epoch 10.73: Loss = 0.652756
Epoch 10.74: Loss = 0.457214
Epoch 10.75: Loss = 0.944931
Epoch 10.76: Loss = 0.706573
Epoch 10.77: Loss = 0.533508
Epoch 10.78: Loss = 0.661392
Epoch 10.79: Loss = 0.729599
Epoch 10.80: Loss = 0.54335
Epoch 10.81: Loss = 0.64534
Epoch 10.82: Loss = 0.495575
Epoch 10.83: Loss = 0.686142
Epoch 10.84: Loss = 0.628571
Epoch 10.85: Loss = 0.835724
Epoch 10.86: Loss = 0.735886
Epoch 10.87: Loss = 0.388412
Epoch 10.88: Loss = 0.613312
Epoch 10.89: Loss = 0.697571
Epoch 10.90: Loss = 0.426529
Epoch 10.91: Loss = 0.76593
Epoch 10.92: Loss = 0.647705
Epoch 10.93: Loss = 0.787964
Epoch 10.94: Loss = 0.530151
Epoch 10.95: Loss = 0.670807
Epoch 10.96: Loss = 0.766342
Epoch 10.97: Loss = 0.437454
Epoch 10.98: Loss = 0.565857
Epoch 10.99: Loss = 0.751419
Epoch 10.100: Loss = 0.91832
Epoch 10.101: Loss = 0.863739
Epoch 10.102: Loss = 0.581726
Epoch 10.103: Loss = 0.576599
Epoch 10.104: Loss = 0.521591
Epoch 10.105: Loss = 0.799393
Epoch 10.106: Loss = 0.982376
Epoch 10.107: Loss = 0.492737
Epoch 10.108: Loss = 0.621399
Epoch 10.109: Loss = 0.485306
Epoch 10.110: Loss = 0.777542
Epoch 10.111: Loss = 0.49057
Epoch 10.112: Loss = 0.527939
Epoch 10.113: Loss = 0.632339
Epoch 10.114: Loss = 0.443909
Epoch 10.115: Loss = 0.504944
Epoch 10.116: Loss = 0.584152
Epoch 10.117: Loss = 0.41011
Epoch 10.118: Loss = 0.366745
Epoch 10.119: Loss = 0.393082
Epoch 10.120: Loss = 0.475311
TRAIN LOSS = 0.61525
TRAIN ACC = 85.6415 % (51387/60000)
Loss = 0.58287
Loss = 0.738586
Loss = 0.855682
Loss = 0.804291
Loss = 0.880905
Loss = 0.609131
Loss = 0.577332
Loss = 0.952682
Loss = 0.829865
Loss = 0.722916
Loss = 0.259613
Loss = 0.501068
Loss = 0.492752
Loss = 0.612915
Loss = 0.330109
Loss = 0.51767
Loss = 0.351105
Loss = 0.076828
Loss = 0.366287
Loss = 0.937851
TEST LOSS = 0.600023
TEST ACC = 513.869 % (8636/10000)
