Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.44478
Epoch 1.2: Loss = 2.33488
Epoch 1.3: Loss = 2.26134
Epoch 1.4: Loss = 2.16565
Epoch 1.5: Loss = 2.09938
Epoch 1.6: Loss = 2.04073
Epoch 1.7: Loss = 1.98769
Epoch 1.8: Loss = 1.93723
Epoch 1.9: Loss = 1.87924
Epoch 1.10: Loss = 1.80855
Epoch 1.11: Loss = 1.82675
Epoch 1.12: Loss = 1.75589
Epoch 1.13: Loss = 1.74304
Epoch 1.14: Loss = 1.66687
Epoch 1.15: Loss = 1.64484
Epoch 1.16: Loss = 1.63129
Epoch 1.17: Loss = 1.57957
Epoch 1.18: Loss = 1.55649
Epoch 1.19: Loss = 1.49849
Epoch 1.20: Loss = 1.50838
Epoch 1.21: Loss = 1.4393
Epoch 1.22: Loss = 1.3894
Epoch 1.23: Loss = 1.41896
Epoch 1.24: Loss = 1.46333
Epoch 1.25: Loss = 1.39156
Epoch 1.26: Loss = 1.33978
Epoch 1.27: Loss = 1.2941
Epoch 1.28: Loss = 1.29303
Epoch 1.29: Loss = 1.27939
Epoch 1.30: Loss = 1.2554
Epoch 1.31: Loss = 1.24384
Epoch 1.32: Loss = 1.24139
Epoch 1.33: Loss = 1.1636
Epoch 1.34: Loss = 1.23683
Epoch 1.35: Loss = 1.25427
Epoch 1.36: Loss = 1.22632
Epoch 1.37: Loss = 1.19418
Epoch 1.38: Loss = 1.14613
Epoch 1.39: Loss = 1.13531
Epoch 1.40: Loss = 1.14363
Epoch 1.41: Loss = 1.18199
Epoch 1.42: Loss = 1.11343
Epoch 1.43: Loss = 1.07927
Epoch 1.44: Loss = 1.03156
Epoch 1.45: Loss = 1.06364
Epoch 1.46: Loss = 1.09995
Epoch 1.47: Loss = 1.04643
Epoch 1.48: Loss = 1.02122
Epoch 1.49: Loss = 1.05051
Epoch 1.50: Loss = 1.01312
Epoch 1.51: Loss = 0.961502
Epoch 1.52: Loss = 1.04282
Epoch 1.53: Loss = 1.06184
Epoch 1.54: Loss = 0.937469
Epoch 1.55: Loss = 0.983231
Epoch 1.56: Loss = 1.0209
Epoch 1.57: Loss = 1.00146
Epoch 1.58: Loss = 0.951172
Epoch 1.59: Loss = 0.98378
Epoch 1.60: Loss = 1.00606
Epoch 1.61: Loss = 0.908386
Epoch 1.62: Loss = 0.986206
Epoch 1.63: Loss = 0.865479
Epoch 1.64: Loss = 0.878418
Epoch 1.65: Loss = 0.918747
Epoch 1.66: Loss = 0.923645
Epoch 1.67: Loss = 0.858307
Epoch 1.68: Loss = 0.976242
Epoch 1.69: Loss = 0.920746
Epoch 1.70: Loss = 0.908417
Epoch 1.71: Loss = 0.847183
Epoch 1.72: Loss = 0.84436
Epoch 1.73: Loss = 0.942917
Epoch 1.74: Loss = 0.940308
Epoch 1.75: Loss = 0.858994
Epoch 1.76: Loss = 0.881363
Epoch 1.77: Loss = 0.874542
Epoch 1.78: Loss = 0.85614
Epoch 1.79: Loss = 0.808029
Epoch 1.80: Loss = 0.851334
Epoch 1.81: Loss = 0.815674
Epoch 1.82: Loss = 0.866333
Epoch 1.83: Loss = 0.863647
Epoch 1.84: Loss = 0.866989
Epoch 1.85: Loss = 0.805237
Epoch 1.86: Loss = 0.899704
Epoch 1.87: Loss = 0.891464
Epoch 1.88: Loss = 0.763519
Epoch 1.89: Loss = 0.881134
Epoch 1.90: Loss = 0.837372
Epoch 1.91: Loss = 0.851639
Epoch 1.92: Loss = 0.862778
Epoch 1.93: Loss = 0.832886
Epoch 1.94: Loss = 0.807968
Epoch 1.95: Loss = 0.864487
Epoch 1.96: Loss = 0.801758
Epoch 1.97: Loss = 0.677628
Epoch 1.98: Loss = 0.812439
Epoch 1.99: Loss = 0.810822
Epoch 1.100: Loss = 0.790115
Epoch 1.101: Loss = 0.833344
Epoch 1.102: Loss = 0.834824
Epoch 1.103: Loss = 0.846985
Epoch 1.104: Loss = 0.773773
Epoch 1.105: Loss = 0.75795
Epoch 1.106: Loss = 0.886627
Epoch 1.107: Loss = 0.809433
Epoch 1.108: Loss = 0.792419
Epoch 1.109: Loss = 0.77887
Epoch 1.110: Loss = 0.809982
Epoch 1.111: Loss = 0.740463
Epoch 1.112: Loss = 0.717087
Epoch 1.113: Loss = 0.78476
Epoch 1.114: Loss = 0.779068
Epoch 1.115: Loss = 0.791153
Epoch 1.116: Loss = 0.71936
Epoch 1.117: Loss = 0.841873
Epoch 1.118: Loss = 0.728729
Epoch 1.119: Loss = 0.782486
Epoch 1.120: Loss = 0.76268
TRAIN LOSS = 1.11499
TRAIN ACC = 62.2833 % (37372/60000)
Loss = 0.727173
Loss = 0.817612
Loss = 0.852051
Loss = 0.752747
Loss = 0.716736
Loss = 0.877701
Loss = 0.882599
Loss = 0.83432
Loss = 0.771088
Loss = 0.719574
Loss = 0.848145
Loss = 0.804672
Loss = 0.798599
Loss = 0.808029
Loss = 0.786133
Loss = 0.848083
Loss = 0.753693
Loss = 0.794113
Loss = 0.839142
Loss = 0.78598
TEST LOSS = 0.800909
TEST ACC = 373.72 % (7175/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.814499
Epoch 2.2: Loss = 0.748093
Epoch 2.3: Loss = 0.829681
Epoch 2.4: Loss = 0.69754
Epoch 2.5: Loss = 0.763794
Epoch 2.6: Loss = 0.823639
Epoch 2.7: Loss = 0.766312
Epoch 2.8: Loss = 0.830215
Epoch 2.9: Loss = 0.706741
Epoch 2.10: Loss = 0.64769
Epoch 2.11: Loss = 0.819168
Epoch 2.12: Loss = 0.754562
Epoch 2.13: Loss = 0.770309
Epoch 2.14: Loss = 0.766876
Epoch 2.15: Loss = 0.758514
Epoch 2.16: Loss = 0.809708
Epoch 2.17: Loss = 0.722839
Epoch 2.18: Loss = 0.760468
Epoch 2.19: Loss = 0.730148
Epoch 2.20: Loss = 0.817566
Epoch 2.21: Loss = 0.696228
Epoch 2.22: Loss = 0.66098
Epoch 2.23: Loss = 0.764359
Epoch 2.24: Loss = 0.847031
Epoch 2.25: Loss = 0.743698
Epoch 2.26: Loss = 0.705856
Epoch 2.27: Loss = 0.728775
Epoch 2.28: Loss = 0.722122
Epoch 2.29: Loss = 0.749832
Epoch 2.30: Loss = 0.714401
Epoch 2.31: Loss = 0.777649
Epoch 2.32: Loss = 0.728134
Epoch 2.33: Loss = 0.6642
Epoch 2.34: Loss = 0.799927
Epoch 2.35: Loss = 0.780548
Epoch 2.36: Loss = 0.781052
Epoch 2.37: Loss = 0.773407
Epoch 2.38: Loss = 0.707626
Epoch 2.39: Loss = 0.783142
Epoch 2.40: Loss = 0.734344
Epoch 2.41: Loss = 0.773407
Epoch 2.42: Loss = 0.733383
Epoch 2.43: Loss = 0.737274
Epoch 2.44: Loss = 0.652084
