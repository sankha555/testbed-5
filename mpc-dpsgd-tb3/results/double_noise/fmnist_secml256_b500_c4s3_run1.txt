Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 256]) => Dense([60000, 1, 256]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.29291
Epoch 1.2: Loss = 2.19588
Epoch 1.3: Loss = 2.14645
Epoch 1.4: Loss = 2.08258
Epoch 1.5: Loss = 2.03171
Epoch 1.6: Loss = 2.01024
Epoch 1.7: Loss = 1.94456
Epoch 1.8: Loss = 1.89136
Epoch 1.9: Loss = 1.83871
Epoch 1.10: Loss = 1.75714
Epoch 1.11: Loss = 1.80202
Epoch 1.12: Loss = 1.71571
Epoch 1.13: Loss = 1.6647
Epoch 1.14: Loss = 1.62988
Epoch 1.15: Loss = 1.58664
Epoch 1.16: Loss = 1.55058
Epoch 1.17: Loss = 1.51871
Epoch 1.18: Loss = 1.48645
Epoch 1.19: Loss = 1.42763
Epoch 1.20: Loss = 1.43456
Epoch 1.21: Loss = 1.38599
Epoch 1.22: Loss = 1.37674
Epoch 1.23: Loss = 1.33833
Epoch 1.24: Loss = 1.38301
Epoch 1.25: Loss = 1.30646
Epoch 1.26: Loss = 1.25284
Epoch 1.27: Loss = 1.22758
Epoch 1.28: Loss = 1.24286
Epoch 1.29: Loss = 1.19722
Epoch 1.30: Loss = 1.18607
Epoch 1.31: Loss = 1.19249
Epoch 1.32: Loss = 1.16986
Epoch 1.33: Loss = 1.06348
Epoch 1.34: Loss = 1.15352
Epoch 1.35: Loss = 1.17389
Epoch 1.36: Loss = 1.13557
Epoch 1.37: Loss = 1.11414
Epoch 1.38: Loss = 1.08249
Epoch 1.39: Loss = 1.04466
Epoch 1.40: Loss = 1.05746
Epoch 1.41: Loss = 1.09404
Epoch 1.42: Loss = 1.01373
Epoch 1.43: Loss = 1.02188
Epoch 1.44: Loss = 0.960419
Epoch 1.45: Loss = 0.988708
Epoch 1.46: Loss = 1.01074
Epoch 1.47: Loss = 0.955475
Epoch 1.48: Loss = 0.959747
Epoch 1.49: Loss = 0.997223
Epoch 1.50: Loss = 0.964127
Epoch 1.51: Loss = 0.906433
Epoch 1.52: Loss = 0.997971
Epoch 1.53: Loss = 0.986771
Epoch 1.54: Loss = 0.848572
Epoch 1.55: Loss = 0.942612
Epoch 1.56: Loss = 0.945175
Epoch 1.57: Loss = 0.975403
Epoch 1.58: Loss = 0.927811
Epoch 1.59: Loss = 0.92218
Epoch 1.60: Loss = 0.960358
Epoch 1.61: Loss = 0.847061
Epoch 1.62: Loss = 0.93515
Epoch 1.63: Loss = 0.807327
Epoch 1.64: Loss = 0.841385
Epoch 1.65: Loss = 0.896271
Epoch 1.66: Loss = 0.864304
Epoch 1.67: Loss = 0.810883
Epoch 1.68: Loss = 0.926819
Epoch 1.69: Loss = 0.883499
Epoch 1.70: Loss = 0.846588
Epoch 1.71: Loss = 0.800018
Epoch 1.72: Loss = 0.826736
Epoch 1.73: Loss = 0.906631
Epoch 1.74: Loss = 0.898239
Epoch 1.75: Loss = 0.811996
Epoch 1.76: Loss = 0.843109
Epoch 1.77: Loss = 0.79837
Epoch 1.78: Loss = 0.840408
Epoch 1.79: Loss = 0.760254
Epoch 1.80: Loss = 0.819702
Epoch 1.81: Loss = 0.797211
Epoch 1.82: Loss = 0.829269
Epoch 1.83: Loss = 0.851807
Epoch 1.84: Loss = 0.812759
Epoch 1.85: Loss = 0.78598
Epoch 1.86: Loss = 0.866241
Epoch 1.87: Loss = 0.876923
Epoch 1.88: Loss = 0.73877
Epoch 1.89: Loss = 0.864944
Epoch 1.90: Loss = 0.791229
Epoch 1.91: Loss = 0.846878
Epoch 1.92: Loss = 0.805374
Epoch 1.93: Loss = 0.80954
Epoch 1.94: Loss = 0.786301
Epoch 1.95: Loss = 0.802017
Epoch 1.96: Loss = 0.783371
Epoch 1.97: Loss = 0.670105
Epoch 1.98: Loss = 0.786728
Epoch 1.99: Loss = 0.818085
Epoch 1.100: Loss = 0.749802
Epoch 1.101: Loss = 0.842514
Epoch 1.102: Loss = 0.800354
Epoch 1.103: Loss = 0.811157
Epoch 1.104: Loss = 0.776489
Epoch 1.105: Loss = 0.727524
Epoch 1.106: Loss = 0.858612
Epoch 1.107: Loss = 0.761139
Epoch 1.108: Loss = 0.795013
Epoch 1.109: Loss = 0.772415
Epoch 1.110: Loss = 0.797073
Epoch 1.111: Loss = 0.706253
Epoch 1.112: Loss = 0.714813
Epoch 1.113: Loss = 0.783676
Epoch 1.114: Loss = 0.790924
Epoch 1.115: Loss = 0.779144
Epoch 1.116: Loss = 0.68985
Epoch 1.117: Loss = 0.841034
Epoch 1.118: Loss = 0.692642
Epoch 1.119: Loss = 0.72998
Epoch 1.120: Loss = 0.704071
TRAIN LOSS = 1.06572
TRAIN ACC = 63.9648 % (38381/60000)
Loss = 0.693283
Loss = 0.802963
Loss = 0.792892
Loss = 0.691635
Loss = 0.693588
Loss = 0.854401
Loss = 0.873215
Loss = 0.834991
Loss = 0.742004
Loss = 0.684464
Loss = 0.813934
Loss = 0.781555
Loss = 0.77475
Loss = 0.799179
Loss = 0.751862
Loss = 0.816788
Loss = 0.71344
Loss = 0.794006
Loss = 0.834183
Loss = 0.776001
TEST LOSS = 0.775957
TEST ACC = 383.809 % (7295/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.759506
Epoch 2.2: Loss = 0.706619
Epoch 2.3: Loss = 0.821411
Epoch 2.4: Loss = 0.661194
Epoch 2.5: Loss = 0.72052
Epoch 2.6: Loss = 0.837799
Epoch 2.7: Loss = 0.767563
Epoch 2.8: Loss = 0.811142
Epoch 2.9: Loss = 0.681015
Epoch 2.10: Loss = 0.591812
Epoch 2.11: Loss = 0.793411
Epoch 2.12: Loss = 0.732361
Epoch 2.13: Loss = 0.742081
Epoch 2.14: Loss = 0.760666
Epoch 2.15: Loss = 0.753311
Epoch 2.16: Loss = 0.786072
Epoch 2.17: Loss = 0.711121
Epoch 2.18: Loss = 0.772003
Epoch 2.19: Loss = 0.703888
Epoch 2.20: Loss = 0.836929
Epoch 2.21: Loss = 0.70372
Epoch 2.22: Loss = 0.642014
Epoch 2.23: Loss = 0.787201
Epoch 2.24: Loss = 0.809814
Epoch 2.25: Loss = 0.702866
Epoch 2.26: Loss = 0.660309
Epoch 2.27: Loss = 0.714172
Epoch 2.28: Loss = 0.729675
Epoch 2.29: Loss = 0.74437
Epoch 2.30: Loss = 0.704575
Epoch 2.31: Loss = 0.793762
Epoch 2.32: Loss = 0.719788
Epoch 2.33: Loss = 0.626114
Epoch 2.34: Loss = 0.782745
Epoch 2.35: Loss = 0.780289
Epoch 2.36: Loss = 0.761566
Epoch 2.37: Loss = 0.774261
Epoch 2.38: Loss = 0.724167
Epoch 2.39: Loss = 0.757156
Epoch 2.40: Loss = 0.727722
Epoch 2.41: Loss = 0.756454
Epoch 2.42: Loss = 0.726715
Epoch 2.43: Loss = 0.728134
Epoch 2.44: Loss = 0.63797
Epoch 2.45: Loss = 0.740036
Epoch 2.46: Loss = 0.7845
Epoch 2.47: Loss = 0.668915
Epoch 2.48: Loss = 0.706223
Epoch 2.49: Loss = 0.767288
Epoch 2.50: Loss = 0.731903
Epoch 2.51: Loss = 0.616653
Epoch 2.52: Loss = 0.754837
Epoch 2.53: Loss = 0.806702
Epoch 2.54: Loss = 0.584808
Epoch 2.55: Loss = 0.71965
Epoch 2.56: Loss = 0.72438
Epoch 2.57: Loss = 0.782074
Epoch 2.58: Loss = 0.760986
Epoch 2.59: Loss = 0.743225
Epoch 2.60: Loss = 0.763733
Epoch 2.61: Loss = 0.662521
Epoch 2.62: Loss = 0.767776
Epoch 2.63: Loss = 0.592361
Epoch 2.64: Loss = 0.625641
Epoch 2.65: Loss = 0.697723
Epoch 2.66: Loss = 0.683334
Epoch 2.67: Loss = 0.647278
Epoch 2.68: Loss = 0.800095
Epoch 2.69: Loss = 0.721008
Epoch 2.70: Loss = 0.710373
Epoch 2.71: Loss = 0.613876
Epoch 2.72: Loss = 0.692429
Epoch 2.73: Loss = 0.801971
Epoch 2.74: Loss = 0.776505
Epoch 2.75: Loss = 0.659637
Epoch 2.76: Loss = 0.68071
Epoch 2.77: Loss = 0.645721
Epoch 2.78: Loss = 0.714218
Epoch 2.79: Loss = 0.649384
Epoch 2.80: Loss = 0.675156
Epoch 2.81: Loss = 0.670868
Epoch 2.82: Loss = 0.708099
Epoch 2.83: Loss = 0.750748
Epoch 2.84: Loss = 0.675339
Epoch 2.85: Loss = 0.665497
Epoch 2.86: Loss = 0.751083
Epoch 2.87: Loss = 0.732224
Epoch 2.88: Loss = 0.642014
Epoch 2.89: Loss = 0.79628
Epoch 2.90: Loss = 0.704895
Epoch 2.91: Loss = 0.783218
Epoch 2.92: Loss = 0.722
Epoch 2.93: Loss = 0.774002
Epoch 2.94: Loss = 0.677795
Epoch 2.95: Loss = 0.698486
Epoch 2.96: Loss = 0.726654
Epoch 2.97: Loss = 0.594635
Epoch 2.98: Loss = 0.668625
Epoch 2.99: Loss = 0.729004
Epoch 2.100: Loss = 0.635956
Epoch 2.101: Loss = 0.774368
Epoch 2.102: Loss = 0.711426
Epoch 2.103: Loss = 0.686661
Epoch 2.104: Loss = 0.656082
Epoch 2.105: Loss = 0.649658
Epoch 2.106: Loss = 0.789871
Epoch 2.107: Loss = 0.735519
Epoch 2.108: Loss = 0.735565
Epoch 2.109: Loss = 0.70163
Epoch 2.110: Loss = 0.735138
Epoch 2.111: Loss = 0.642532
Epoch 2.112: Loss = 0.634796
Epoch 2.113: Loss = 0.682861
Epoch 2.114: Loss = 0.69989
Epoch 2.115: Loss = 0.680908
Epoch 2.116: Loss = 0.663513
Epoch 2.117: Loss = 0.768021
Epoch 2.118: Loss = 0.63736
Epoch 2.119: Loss = 0.713852
Epoch 2.120: Loss = 0.647491
TRAIN LOSS = 0.716461
TRAIN ACC = 75.7248 % (45437/60000)
Loss = 0.65213
Loss = 0.736984
Loss = 0.741165
Loss = 0.638092
Loss = 0.627151
Loss = 0.832138
Loss = 0.835739
Loss = 0.83049
Loss = 0.708923
Loss = 0.641327
Loss = 0.820511
Loss = 0.742966
Loss = 0.728531
Loss = 0.761414
Loss = 0.711655
Loss = 0.757278
Loss = 0.653549
Loss = 0.758499
Loss = 0.799789
Loss = 0.725464
TEST LOSS = 0.73519
TEST ACC = 454.369 % (7562/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.706116
Epoch 3.2: Loss = 0.676254
Epoch 3.3: Loss = 0.740677
Epoch 3.4: Loss = 0.611023
Epoch 3.5: Loss = 0.692963
Epoch 3.6: Loss = 0.815918
Epoch 3.7: Loss = 0.725342
Epoch 3.8: Loss = 0.788956
Epoch 3.9: Loss = 0.602051
Epoch 3.10: Loss = 0.544418
Epoch 3.11: Loss = 0.786057
Epoch 3.12: Loss = 0.74353
Epoch 3.13: Loss = 0.758316
Epoch 3.14: Loss = 0.700256
Epoch 3.15: Loss = 0.702805
Epoch 3.16: Loss = 0.765747
Epoch 3.17: Loss = 0.649673
Epoch 3.18: Loss = 0.712753
Epoch 3.19: Loss = 0.676102
Epoch 3.20: Loss = 0.804947
Epoch 3.21: Loss = 0.621521
Epoch 3.22: Loss = 0.544403
Epoch 3.23: Loss = 0.751938
Epoch 3.24: Loss = 0.789993
Epoch 3.25: Loss = 0.64267
Epoch 3.26: Loss = 0.606537
Epoch 3.27: Loss = 0.655533
Epoch 3.28: Loss = 0.740524
Epoch 3.29: Loss = 0.714081
Epoch 3.30: Loss = 0.694992
Epoch 3.31: Loss = 0.799103
Epoch 3.32: Loss = 0.647171
Epoch 3.33: Loss = 0.573334
Epoch 3.34: Loss = 0.774445
Epoch 3.35: Loss = 0.717087
Epoch 3.36: Loss = 0.753189
Epoch 3.37: Loss = 0.74028
Epoch 3.38: Loss = 0.688431
Epoch 3.39: Loss = 0.736557
Epoch 3.40: Loss = 0.665146
Epoch 3.41: Loss = 0.723953
Epoch 3.42: Loss = 0.686234
Epoch 3.43: Loss = 0.72905
Epoch 3.44: Loss = 0.599442
Epoch 3.45: Loss = 0.724442
Epoch 3.46: Loss = 0.81485
Epoch 3.47: Loss = 0.64389
Epoch 3.48: Loss = 0.657288
Epoch 3.49: Loss = 0.737274
Epoch 3.50: Loss = 0.719116
Epoch 3.51: Loss = 0.579575
Epoch 3.52: Loss = 0.758133
Epoch 3.53: Loss = 0.789322
Epoch 3.54: Loss = 0.553452
Epoch 3.55: Loss = 0.699417
Epoch 3.56: Loss = 0.729263
Epoch 3.57: Loss = 0.798752
Epoch 3.58: Loss = 0.712891
Epoch 3.59: Loss = 0.753281
Epoch 3.60: Loss = 0.689117
Epoch 3.61: Loss = 0.658691
Epoch 3.62: Loss = 0.732635
Epoch 3.63: Loss = 0.567551
Epoch 3.64: Loss = 0.585419
Epoch 3.65: Loss = 0.687347
Epoch 3.66: Loss = 0.632629
Epoch 3.67: Loss = 0.64119
Epoch 3.68: Loss = 0.79715
Epoch 3.69: Loss = 0.696991
Epoch 3.70: Loss = 0.72374
Epoch 3.71: Loss = 0.602814
Epoch 3.72: Loss = 0.661484
Epoch 3.73: Loss = 0.77211
Epoch 3.74: Loss = 0.70639
Epoch 3.75: Loss = 0.61116
Epoch 3.76: Loss = 0.651108
Epoch 3.77: Loss = 0.640656
Epoch 3.78: Loss = 0.701019
Epoch 3.79: Loss = 0.633469
Epoch 3.80: Loss = 0.646988
Epoch 3.81: Loss = 0.625031
Epoch 3.82: Loss = 0.676025
Epoch 3.83: Loss = 0.710464
Epoch 3.84: Loss = 0.625305
Epoch 3.85: Loss = 0.628128
Epoch 3.86: Loss = 0.733932
Epoch 3.87: Loss = 0.701569
Epoch 3.88: Loss = 0.597916
Epoch 3.89: Loss = 0.814789
Epoch 3.90: Loss = 0.67717
Epoch 3.91: Loss = 0.74649
Epoch 3.92: Loss = 0.654922
Epoch 3.93: Loss = 0.738403
Epoch 3.94: Loss = 0.673477
Epoch 3.95: Loss = 0.668427
Epoch 3.96: Loss = 0.697479
Epoch 3.97: Loss = 0.574387
Epoch 3.98: Loss = 0.656219
Epoch 3.99: Loss = 0.663483
Epoch 3.100: Loss = 0.610779
Epoch 3.101: Loss = 0.739899
Epoch 3.102: Loss = 0.728134
Epoch 3.103: Loss = 0.687637
Epoch 3.104: Loss = 0.618652
Epoch 3.105: Loss = 0.63028
Epoch 3.106: Loss = 0.749252
Epoch 3.107: Loss = 0.746307
Epoch 3.108: Loss = 0.760498
Epoch 3.109: Loss = 0.696579
Epoch 3.110: Loss = 0.711258
Epoch 3.111: Loss = 0.629776
Epoch 3.112: Loss = 0.648926
Epoch 3.113: Loss = 0.642868
Epoch 3.114: Loss = 0.67395
Epoch 3.115: Loss = 0.647324
Epoch 3.116: Loss = 0.60968
Epoch 3.117: Loss = 0.720337
Epoch 3.118: Loss = 0.604599
Epoch 3.119: Loss = 0.65773
Epoch 3.120: Loss = 0.618195
TRAIN LOSS = 0.687561
TRAIN ACC = 78.0334 % (46822/60000)
Loss = 0.633728
Loss = 0.749069
Loss = 0.712006
Loss = 0.621872
Loss = 0.651276
Loss = 0.813919
Loss = 0.853973
Loss = 0.837311
Loss = 0.71312
Loss = 0.616196
Loss = 0.827301
Loss = 0.793427
Loss = 0.759598
Loss = 0.752029
Loss = 0.718185
Loss = 0.755203
Loss = 0.656113
Loss = 0.765793
Loss = 0.802826
Loss = 0.711182
TEST LOSS = 0.737206
TEST ACC = 468.219 % (7696/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.672592
Epoch 4.2: Loss = 0.662323
Epoch 4.3: Loss = 0.708511
Epoch 4.4: Loss = 0.592453
Epoch 4.5: Loss = 0.670944
Epoch 4.6: Loss = 0.798996
Epoch 4.7: Loss = 0.701859
Epoch 4.8: Loss = 0.779984
Epoch 4.9: Loss = 0.603928
Epoch 4.10: Loss = 0.498306
Epoch 4.11: Loss = 0.759766
Epoch 4.12: Loss = 0.666656
Epoch 4.13: Loss = 0.735489
Epoch 4.14: Loss = 0.693634
Epoch 4.15: Loss = 0.67514
Epoch 4.16: Loss = 0.722473
Epoch 4.17: Loss = 0.595764
Epoch 4.18: Loss = 0.716248
Epoch 4.19: Loss = 0.664352
Epoch 4.20: Loss = 0.800552
Epoch 4.21: Loss = 0.618607
Epoch 4.22: Loss = 0.515106
Epoch 4.23: Loss = 0.740295
Epoch 4.24: Loss = 0.797836
Epoch 4.25: Loss = 0.631119
Epoch 4.26: Loss = 0.545517
Epoch 4.27: Loss = 0.665482
Epoch 4.28: Loss = 0.692261
Epoch 4.29: Loss = 0.704453
Epoch 4.30: Loss = 0.667694
Epoch 4.31: Loss = 0.818542
Epoch 4.32: Loss = 0.64711
Epoch 4.33: Loss = 0.580048
Epoch 4.34: Loss = 0.754181
Epoch 4.35: Loss = 0.684341
Epoch 4.36: Loss = 0.729248
Epoch 4.37: Loss = 0.736084
Epoch 4.38: Loss = 0.648361
Epoch 4.39: Loss = 0.714355
Epoch 4.40: Loss = 0.660385
Epoch 4.41: Loss = 0.73349
Epoch 4.42: Loss = 0.646942
Epoch 4.43: Loss = 0.676758
Epoch 4.44: Loss = 0.604462
Epoch 4.45: Loss = 0.685425
Epoch 4.46: Loss = 0.806137
Epoch 4.47: Loss = 0.611725
Epoch 4.48: Loss = 0.60318
Epoch 4.49: Loss = 0.720886
Epoch 4.50: Loss = 0.704895
Epoch 4.51: Loss = 0.557541
Epoch 4.52: Loss = 0.738708
Epoch 4.53: Loss = 0.76532
Epoch 4.54: Loss = 0.545929
Epoch 4.55: Loss = 0.721939
Epoch 4.56: Loss = 0.734848
Epoch 4.57: Loss = 0.809769
Epoch 4.58: Loss = 0.725372
Epoch 4.59: Loss = 0.768845
Epoch 4.60: Loss = 0.678955
Epoch 4.61: Loss = 0.61496
Epoch 4.62: Loss = 0.742218
Epoch 4.63: Loss = 0.547028
Epoch 4.64: Loss = 0.578217
Epoch 4.65: Loss = 0.697647
Epoch 4.66: Loss = 0.608032
Epoch 4.67: Loss = 0.658554
Epoch 4.68: Loss = 0.840363
Epoch 4.69: Loss = 0.672104
Epoch 4.70: Loss = 0.699051
Epoch 4.71: Loss = 0.599197
Epoch 4.72: Loss = 0.684036
Epoch 4.73: Loss = 0.790939
Epoch 4.74: Loss = 0.730713
Epoch 4.75: Loss = 0.633408
Epoch 4.76: Loss = 0.642242
Epoch 4.77: Loss = 0.622375
Epoch 4.78: Loss = 0.691895
Epoch 4.79: Loss = 0.619797
Epoch 4.80: Loss = 0.650711
Epoch 4.81: Loss = 0.663742
Epoch 4.82: Loss = 0.670959
Epoch 4.83: Loss = 0.712296
Epoch 4.84: Loss = 0.631119
Epoch 4.85: Loss = 0.656998
Epoch 4.86: Loss = 0.732697
Epoch 4.87: Loss = 0.686066
Epoch 4.88: Loss = 0.63118
Epoch 4.89: Loss = 0.828629
Epoch 4.90: Loss = 0.705032
Epoch 4.91: Loss = 0.816299
Epoch 4.92: Loss = 0.682648
Epoch 4.93: Loss = 0.759796
Epoch 4.94: Loss = 0.681854
Epoch 4.95: Loss = 0.714844
Epoch 4.96: Loss = 0.714691
Epoch 4.97: Loss = 0.593796
Epoch 4.98: Loss = 0.679123
Epoch 4.99: Loss = 0.651871
Epoch 4.100: Loss = 0.638489
Epoch 4.101: Loss = 0.716385
Epoch 4.102: Loss = 0.726974
Epoch 4.103: Loss = 0.653625
Epoch 4.104: Loss = 0.641052
Epoch 4.105: Loss = 0.637787
Epoch 4.106: Loss = 0.783371
Epoch 4.107: Loss = 0.738892
Epoch 4.108: Loss = 0.791473
Epoch 4.109: Loss = 0.734177
Epoch 4.110: Loss = 0.760361
Epoch 4.111: Loss = 0.656616
Epoch 4.112: Loss = 0.654114
Epoch 4.113: Loss = 0.662872
Epoch 4.114: Loss = 0.719452
Epoch 4.115: Loss = 0.670471
Epoch 4.116: Loss = 0.633545
Epoch 4.117: Loss = 0.756866
Epoch 4.118: Loss = 0.600952
Epoch 4.119: Loss = 0.684402
Epoch 4.120: Loss = 0.608093
TRAIN LOSS = 0.684341
TRAIN ACC = 79.2389 % (47546/60000)
Loss = 0.610428
Loss = 0.768661
Loss = 0.720322
Loss = 0.593628
Loss = 0.650406
Loss = 0.794601
Loss = 0.903442
Loss = 0.843063
Loss = 0.706192
Loss = 0.656311
Loss = 0.86438
Loss = 0.804489
Loss = 0.722244
Loss = 0.73291
Loss = 0.714081
Loss = 0.748779
Loss = 0.665268
Loss = 0.776688
Loss = 0.792465
Loss = 0.739731
TEST LOSS = 0.740404
TEST ACC = 475.459 % (7833/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.695526
Epoch 5.2: Loss = 0.697098
Epoch 5.3: Loss = 0.703644
Epoch 5.4: Loss = 0.603271
Epoch 5.5: Loss = 0.685257
Epoch 5.6: Loss = 0.842056
Epoch 5.7: Loss = 0.736328
Epoch 5.8: Loss = 0.793121
Epoch 5.9: Loss = 0.599213
Epoch 5.10: Loss = 0.503479
Epoch 5.11: Loss = 0.805618
Epoch 5.12: Loss = 0.694763
Epoch 5.13: Loss = 0.73941
Epoch 5.14: Loss = 0.70784
Epoch 5.15: Loss = 0.697601
Epoch 5.16: Loss = 0.781952
Epoch 5.17: Loss = 0.630646
Epoch 5.18: Loss = 0.723312
Epoch 5.19: Loss = 0.678818
Epoch 5.20: Loss = 0.827209
Epoch 5.21: Loss = 0.590927
Epoch 5.22: Loss = 0.549347
Epoch 5.23: Loss = 0.737061
Epoch 5.24: Loss = 0.83345
Epoch 5.25: Loss = 0.632843
Epoch 5.26: Loss = 0.547562
Epoch 5.27: Loss = 0.681458
Epoch 5.28: Loss = 0.706604
Epoch 5.29: Loss = 0.713638
Epoch 5.30: Loss = 0.724609
Epoch 5.31: Loss = 0.829453
Epoch 5.32: Loss = 0.700165
Epoch 5.33: Loss = 0.574402
Epoch 5.34: Loss = 0.798889
Epoch 5.35: Loss = 0.754303
Epoch 5.36: Loss = 0.740829
Epoch 5.37: Loss = 0.780243
Epoch 5.38: Loss = 0.68782
Epoch 5.39: Loss = 0.737381
Epoch 5.40: Loss = 0.714081
Epoch 5.41: Loss = 0.74324
Epoch 5.42: Loss = 0.669037
Epoch 5.43: Loss = 0.702026
Epoch 5.44: Loss = 0.606461
Epoch 5.45: Loss = 0.732956
Epoch 5.46: Loss = 0.826767
Epoch 5.47: Loss = 0.666245
Epoch 5.48: Loss = 0.605743
Epoch 5.49: Loss = 0.716904
Epoch 5.50: Loss = 0.74707
Epoch 5.51: Loss = 0.540939
Epoch 5.52: Loss = 0.817413
Epoch 5.53: Loss = 0.792557
Epoch 5.54: Loss = 0.546234
Epoch 5.55: Loss = 0.726578
Epoch 5.56: Loss = 0.761841
Epoch 5.57: Loss = 0.795532
Epoch 5.58: Loss = 0.740143
Epoch 5.59: Loss = 0.807816
Epoch 5.60: Loss = 0.723587
Epoch 5.61: Loss = 0.659744
Epoch 5.62: Loss = 0.773499
Epoch 5.63: Loss = 0.583481
Epoch 5.64: Loss = 0.596832
Epoch 5.65: Loss = 0.792435
Epoch 5.66: Loss = 0.658951
Epoch 5.67: Loss = 0.696228
Epoch 5.68: Loss = 0.871201
Epoch 5.69: Loss = 0.697815
Epoch 5.70: Loss = 0.772003
Epoch 5.71: Loss = 0.616989
Epoch 5.72: Loss = 0.705688
Epoch 5.73: Loss = 0.85939
Epoch 5.74: Loss = 0.777527
Epoch 5.75: Loss = 0.669281
Epoch 5.76: Loss = 0.695465
Epoch 5.77: Loss = 0.655396
Epoch 5.78: Loss = 0.720169
Epoch 5.79: Loss = 0.655579
Epoch 5.80: Loss = 0.690308
Epoch 5.81: Loss = 0.709671
Epoch 5.82: Loss = 0.719177
Epoch 5.83: Loss = 0.77417
Epoch 5.84: Loss = 0.646378
Epoch 5.85: Loss = 0.670029
Epoch 5.86: Loss = 0.779266
Epoch 5.87: Loss = 0.719803
Epoch 5.88: Loss = 0.673889
Epoch 5.89: Loss = 0.864441
Epoch 5.90: Loss = 0.750168
Epoch 5.91: Loss = 0.800247
Epoch 5.92: Loss = 0.698837
Epoch 5.93: Loss = 0.776062
Epoch 5.94: Loss = 0.738693
Epoch 5.95: Loss = 0.813431
Epoch 5.96: Loss = 0.733887
Epoch 5.97: Loss = 0.619156
Epoch 5.98: Loss = 0.683807
Epoch 5.99: Loss = 0.712234
Epoch 5.100: Loss = 0.654602
Epoch 5.101: Loss = 0.765533
Epoch 5.102: Loss = 0.821075
Epoch 5.103: Loss = 0.682007
Epoch 5.104: Loss = 0.632889
Epoch 5.105: Loss = 0.634048
Epoch 5.106: Loss = 0.832458
Epoch 5.107: Loss = 0.79303
Epoch 5.108: Loss = 0.855484
Epoch 5.109: Loss = 0.74472
Epoch 5.110: Loss = 0.753555
Epoch 5.111: Loss = 0.698639
Epoch 5.112: Loss = 0.669144
Epoch 5.113: Loss = 0.744492
Epoch 5.114: Loss = 0.71698
Epoch 5.115: Loss = 0.664963
Epoch 5.116: Loss = 0.656036
Epoch 5.117: Loss = 0.779694
Epoch 5.118: Loss = 0.644653
Epoch 5.119: Loss = 0.764023
Epoch 5.120: Loss = 0.64328
TRAIN LOSS = 0.713623
TRAIN ACC = 79.5181 % (47713/60000)
Loss = 0.642532
Loss = 0.773697
Loss = 0.70517
Loss = 0.615601
Loss = 0.692276
Loss = 0.794418
Loss = 0.953705
Loss = 0.849335
Loss = 0.755402
Loss = 0.670319
Loss = 0.878448
Loss = 0.829208
Loss = 0.763672
Loss = 0.74115
Loss = 0.725861
Loss = 0.801514
Loss = 0.649796
Loss = 0.787247
Loss = 0.793732
Loss = 0.774094
TEST LOSS = 0.759858
TEST ACC = 477.129 % (7924/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.716599
Epoch 6.2: Loss = 0.719498
Epoch 6.3: Loss = 0.744873
Epoch 6.4: Loss = 0.591965
Epoch 6.5: Loss = 0.750443
Epoch 6.6: Loss = 0.869827
Epoch 6.7: Loss = 0.784134
Epoch 6.8: Loss = 0.839569
Epoch 6.9: Loss = 0.599533
Epoch 6.10: Loss = 0.514618
Epoch 6.11: Loss = 0.793991
Epoch 6.12: Loss = 0.719833
Epoch 6.13: Loss = 0.787155
Epoch 6.14: Loss = 0.723145
Epoch 6.15: Loss = 0.765991
Epoch 6.16: Loss = 0.830673
Epoch 6.17: Loss = 0.656662
Epoch 6.18: Loss = 0.74794
Epoch 6.19: Loss = 0.682266
Epoch 6.20: Loss = 0.854752
Epoch 6.21: Loss = 0.637985
Epoch 6.22: Loss = 0.575668
Epoch 6.23: Loss = 0.815689
Epoch 6.24: Loss = 0.867752
Epoch 6.25: Loss = 0.681137
Epoch 6.26: Loss = 0.534088
Epoch 6.27: Loss = 0.746994
Epoch 6.28: Loss = 0.719116
Epoch 6.29: Loss = 0.746185
Epoch 6.30: Loss = 0.765579
Epoch 6.31: Loss = 0.856018
Epoch 6.32: Loss = 0.769318
Epoch 6.33: Loss = 0.635086
Epoch 6.34: Loss = 0.818909
Epoch 6.35: Loss = 0.809189
Epoch 6.36: Loss = 0.777771
Epoch 6.37: Loss = 0.791962
Epoch 6.38: Loss = 0.695282
Epoch 6.39: Loss = 0.764023
Epoch 6.40: Loss = 0.745392
Epoch 6.41: Loss = 0.759979
Epoch 6.42: Loss = 0.733414
Epoch 6.43: Loss = 0.742294
Epoch 6.44: Loss = 0.616547
Epoch 6.45: Loss = 0.77533
Epoch 6.46: Loss = 0.844772
Epoch 6.47: Loss = 0.71991
Epoch 6.48: Loss = 0.62326
Epoch 6.49: Loss = 0.742722
Epoch 6.50: Loss = 0.773712
Epoch 6.51: Loss = 0.577179
Epoch 6.52: Loss = 0.817383
Epoch 6.53: Loss = 0.817505
Epoch 6.54: Loss = 0.561295
Epoch 6.55: Loss = 0.745071
Epoch 6.56: Loss = 0.760925
Epoch 6.57: Loss = 0.796097
Epoch 6.58: Loss = 0.711472
Epoch 6.59: Loss = 0.843597
Epoch 6.60: Loss = 0.753265
Epoch 6.61: Loss = 0.69931
Epoch 6.62: Loss = 0.754944
Epoch 6.63: Loss = 0.613434
Epoch 6.64: Loss = 0.608093
Epoch 6.65: Loss = 0.754715
Epoch 6.66: Loss = 0.637573
Epoch 6.67: Loss = 0.652008
Epoch 6.68: Loss = 0.922424
Epoch 6.69: Loss = 0.693909
Epoch 6.70: Loss = 0.753769
Epoch 6.71: Loss = 0.625732
Epoch 6.72: Loss = 0.73465
Epoch 6.73: Loss = 0.820358
Epoch 6.74: Loss = 0.808441
Epoch 6.75: Loss = 0.681137
Epoch 6.76: Loss = 0.700089
Epoch 6.77: Loss = 0.653763
Epoch 6.78: Loss = 0.794144
Epoch 6.79: Loss = 0.684692
Epoch 6.80: Loss = 0.697662
Epoch 6.81: Loss = 0.714218
Epoch 6.82: Loss = 0.715347
Epoch 6.83: Loss = 0.792694
Epoch 6.84: Loss = 0.652664
Epoch 6.85: Loss = 0.688034
Epoch 6.86: Loss = 0.784348
Epoch 6.87: Loss = 0.773224
Epoch 6.88: Loss = 0.672836
Epoch 6.89: Loss = 0.871414
Epoch 6.90: Loss = 0.743744
Epoch 6.91: Loss = 0.79332
Epoch 6.92: Loss = 0.7043
Epoch 6.93: Loss = 0.768692
Epoch 6.94: Loss = 0.758926
Epoch 6.95: Loss = 0.766846
Epoch 6.96: Loss = 0.725586
Epoch 6.97: Loss = 0.594452
Epoch 6.98: Loss = 0.739456
Epoch 6.99: Loss = 0.726349
Epoch 6.100: Loss = 0.692123
Epoch 6.101: Loss = 0.77916
Epoch 6.102: Loss = 0.853546
Epoch 6.103: Loss = 0.696854
Epoch 6.104: Loss = 0.596558
Epoch 6.105: Loss = 0.675171
Epoch 6.106: Loss = 0.847946
Epoch 6.107: Loss = 0.755524
Epoch 6.108: Loss = 0.904297
Epoch 6.109: Loss = 0.723755
Epoch 6.110: Loss = 0.758835
Epoch 6.111: Loss = 0.719131
Epoch 6.112: Loss = 0.690887
Epoch 6.113: Loss = 0.7677
Epoch 6.114: Loss = 0.729401
Epoch 6.115: Loss = 0.674271
Epoch 6.116: Loss = 0.654373
Epoch 6.117: Loss = 0.82399
Epoch 6.118: Loss = 0.626907
Epoch 6.119: Loss = 0.809387
Epoch 6.120: Loss = 0.67894
TRAIN LOSS = 0.732513
TRAIN ACC = 79.7913 % (47877/60000)
Loss = 0.660019
Loss = 0.818893
Loss = 0.727112
Loss = 0.673233
Loss = 0.748322
Loss = 0.826767
Loss = 1.00662
Loss = 0.864532
Loss = 0.755219
Loss = 0.72818
Loss = 0.943817
Loss = 0.880676
Loss = 0.852798
Loss = 0.777298
Loss = 0.795288
Loss = 0.828873
Loss = 0.672638
Loss = 0.87735
Loss = 0.844025
Loss = 0.782837
TEST LOSS = 0.803225
TEST ACC = 478.769 % (7915/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.746552
Epoch 7.2: Loss = 0.721481
Epoch 7.3: Loss = 0.707321
Epoch 7.4: Loss = 0.619202
Epoch 7.5: Loss = 0.744553
Epoch 7.6: Loss = 0.900497
Epoch 7.7: Loss = 0.809341
Epoch 7.8: Loss = 0.856445
Epoch 7.9: Loss = 0.624557
Epoch 7.10: Loss = 0.51088
Epoch 7.11: Loss = 0.831635
Epoch 7.12: Loss = 0.708618
Epoch 7.13: Loss = 0.823441
Epoch 7.14: Loss = 0.737854
Epoch 7.15: Loss = 0.751358
Epoch 7.16: Loss = 0.804077
Epoch 7.17: Loss = 0.650833
Epoch 7.18: Loss = 0.769073
Epoch 7.19: Loss = 0.697647
Epoch 7.20: Loss = 0.847839
Epoch 7.21: Loss = 0.629272
Epoch 7.22: Loss = 0.572189
Epoch 7.23: Loss = 0.783905
Epoch 7.24: Loss = 0.893906
Epoch 7.25: Loss = 0.667587
Epoch 7.26: Loss = 0.537949
Epoch 7.27: Loss = 0.742722
Epoch 7.28: Loss = 0.751312
Epoch 7.29: Loss = 0.711472
Epoch 7.30: Loss = 0.798859
Epoch 7.31: Loss = 0.841797
Epoch 7.32: Loss = 0.741302
Epoch 7.33: Loss = 0.621399
Epoch 7.34: Loss = 0.844711
Epoch 7.35: Loss = 0.770691
Epoch 7.36: Loss = 0.787766
Epoch 7.37: Loss = 0.817001
Epoch 7.38: Loss = 0.774323
Epoch 7.39: Loss = 0.79538
Epoch 7.40: Loss = 0.778671
Epoch 7.41: Loss = 0.78157
Epoch 7.42: Loss = 0.734512
Epoch 7.43: Loss = 0.756943
Epoch 7.44: Loss = 0.60936
Epoch 7.45: Loss = 0.810318
Epoch 7.46: Loss = 0.869858
Epoch 7.47: Loss = 0.732437
Epoch 7.48: Loss = 0.683914
Epoch 7.49: Loss = 0.730591
Epoch 7.50: Loss = 0.741425
Epoch 7.51: Loss = 0.581711
Epoch 7.52: Loss = 0.854706
Epoch 7.53: Loss = 0.836212
Epoch 7.54: Loss = 0.58165
Epoch 7.55: Loss = 0.741608
Epoch 7.56: Loss = 0.792191
Epoch 7.57: Loss = 0.791672
Epoch 7.58: Loss = 0.75882
Epoch 7.59: Loss = 0.876251
Epoch 7.60: Loss = 0.772263
Epoch 7.61: Loss = 0.718704
Epoch 7.62: Loss = 0.795837
Epoch 7.63: Loss = 0.648315
Epoch 7.64: Loss = 0.606995
Epoch 7.65: Loss = 0.885452
Epoch 7.66: Loss = 0.634659
Epoch 7.67: Loss = 0.699661
Epoch 7.68: Loss = 0.927505
Epoch 7.69: Loss = 0.727997
Epoch 7.70: Loss = 0.749237
Epoch 7.71: Loss = 0.673996
Epoch 7.72: Loss = 0.746094
Epoch 7.73: Loss = 0.880432
Epoch 7.74: Loss = 0.804077
Epoch 7.75: Loss = 0.67128
Epoch 7.76: Loss = 0.75145
Epoch 7.77: Loss = 0.661224
Epoch 7.78: Loss = 0.80661
Epoch 7.79: Loss = 0.667496
Epoch 7.80: Loss = 0.685242
Epoch 7.81: Loss = 0.751831
Epoch 7.82: Loss = 0.763794
Epoch 7.83: Loss = 0.772614
Epoch 7.84: Loss = 0.63707
Epoch 7.85: Loss = 0.881622
Epoch 7.86: Loss = 0.776413
Epoch 7.87: Loss = 0.732559
Epoch 7.88: Loss = 0.692337
Epoch 7.89: Loss = 0.905899
Epoch 7.90: Loss = 0.758057
Epoch 7.91: Loss = 0.804626
Epoch 7.92: Loss = 0.74675
Epoch 7.93: Loss = 0.791412
Epoch 7.94: Loss = 0.776474
Epoch 7.95: Loss = 0.769821
Epoch 7.96: Loss = 0.803345
Epoch 7.97: Loss = 0.617722
Epoch 7.98: Loss = 0.694946
Epoch 7.99: Loss = 0.706451
Epoch 7.100: Loss = 0.710693
Epoch 7.101: Loss = 0.751266
Epoch 7.102: Loss = 0.840546
Epoch 7.103: Loss = 0.673538
Epoch 7.104: Loss = 0.835861
Epoch 7.105: Loss = 0.665421
Epoch 7.106: Loss = 0.856766
Epoch 7.107: Loss = 0.789352
Epoch 7.108: Loss = 0.958008
Epoch 7.109: Loss = 0.766113
Epoch 7.110: Loss = 0.791428
Epoch 7.111: Loss = 0.700714
Epoch 7.112: Loss = 0.733215
Epoch 7.113: Loss = 0.803253
Epoch 7.114: Loss = 0.725128
Epoch 7.115: Loss = 0.68689
Epoch 7.116: Loss = 0.681915
Epoch 7.117: Loss = 0.793015
Epoch 7.118: Loss = 0.675262
Epoch 7.119: Loss = 0.805054
Epoch 7.120: Loss = 0.723694
TRAIN LOSS = 0.748825
TRAIN ACC = 80.0186 % (48013/60000)
Loss = 0.675644
Loss = 0.806839
Loss = 0.740982
Loss = 0.697876
Loss = 0.764587
Loss = 0.873032
Loss = 0.994675
Loss = 0.867493
Loss = 0.722824
Loss = 0.718491
Loss = 0.918015
Loss = 0.887604
Loss = 0.859177
Loss = 0.782104
Loss = 0.769058
Loss = 0.798889
Loss = 0.705933
Loss = 0.873077
Loss = 0.857071
Loss = 0.789612
TEST LOSS = 0.805149
TEST ACC = 480.128 % (7931/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.718674
Epoch 8.2: Loss = 0.71611
Epoch 8.3: Loss = 0.722427
Epoch 8.4: Loss = 0.628113
Epoch 8.5: Loss = 0.743668
Epoch 8.6: Loss = 0.850983
Epoch 8.7: Loss = 0.766434
Epoch 8.8: Loss = 0.907593
Epoch 8.9: Loss = 0.640884
Epoch 8.10: Loss = 0.521301
Epoch 8.11: Loss = 0.839661
Epoch 8.12: Loss = 0.741165
Epoch 8.13: Loss = 0.835449
Epoch 8.14: Loss = 0.755814
Epoch 8.15: Loss = 0.785919
Epoch 8.16: Loss = 0.813675
Epoch 8.17: Loss = 0.639282
Epoch 8.18: Loss = 0.791794
Epoch 8.19: Loss = 0.652985
Epoch 8.20: Loss = 0.81929
Epoch 8.21: Loss = 0.656601
Epoch 8.22: Loss = 0.578659
Epoch 8.23: Loss = 0.849884
Epoch 8.24: Loss = 0.915894
Epoch 8.25: Loss = 0.706375
Epoch 8.26: Loss = 0.601379
Epoch 8.27: Loss = 0.717087
Epoch 8.28: Loss = 0.822372
Epoch 8.29: Loss = 0.704269
Epoch 8.30: Loss = 0.764481
Epoch 8.31: Loss = 0.925476
Epoch 8.32: Loss = 0.752792
Epoch 8.33: Loss = 0.682846
Epoch 8.34: Loss = 0.835464
Epoch 8.35: Loss = 0.796829
Epoch 8.36: Loss = 0.829895
Epoch 8.37: Loss = 0.80925
Epoch 8.38: Loss = 0.76297
Epoch 8.39: Loss = 0.794357
Epoch 8.40: Loss = 0.888016
Epoch 8.41: Loss = 0.766632
Epoch 8.42: Loss = 0.761917
Epoch 8.43: Loss = 0.805832
Epoch 8.44: Loss = 0.616852
Epoch 8.45: Loss = 0.830719
Epoch 8.46: Loss = 0.89476
Epoch 8.47: Loss = 0.719513
Epoch 8.48: Loss = 0.639145
Epoch 8.49: Loss = 0.814362
Epoch 8.50: Loss = 0.735992
Epoch 8.51: Loss = 0.58461
Epoch 8.52: Loss = 0.846512
Epoch 8.53: Loss = 0.866119
Epoch 8.54: Loss = 0.614487
Epoch 8.55: Loss = 0.733109
Epoch 8.56: Loss = 0.837799
Epoch 8.57: Loss = 0.824249
Epoch 8.58: Loss = 0.757278
Epoch 8.59: Loss = 0.952225
Epoch 8.60: Loss = 0.830887
Epoch 8.61: Loss = 0.75705
Epoch 8.62: Loss = 0.829285
Epoch 8.63: Loss = 0.656601
Epoch 8.64: Loss = 0.640839
Epoch 8.65: Loss = 0.827881
Epoch 8.66: Loss = 0.633575
Epoch 8.67: Loss = 0.702621
Epoch 8.68: Loss = 1.05176
Epoch 8.69: Loss = 0.747849
Epoch 8.70: Loss = 0.768784
Epoch 8.71: Loss = 0.645782
Epoch 8.72: Loss = 0.817169
Epoch 8.73: Loss = 0.876633
Epoch 8.74: Loss = 0.802078
Epoch 8.75: Loss = 0.68338
Epoch 8.76: Loss = 0.808823
Epoch 8.77: Loss = 0.669403
Epoch 8.78: Loss = 0.80661
Epoch 8.79: Loss = 10240.9
Epoch 8.80: Loss = -1237.16
Epoch 8.81: Loss = -982.748
Epoch 8.82: Loss = 963.437
Epoch 8.83: Loss = -1543.02
Epoch 8.84: Loss = -885.221
Epoch 8.85: Loss = 2952.47
Epoch 8.86: Loss = -210.996
Epoch 8.87: Loss = -1076.49
Epoch 8.88: Loss = -1669.13
Epoch 8.89: Loss = -605.88
Epoch 8.90: Loss = -526.378
Epoch 8.91: Loss = 1116.18
Epoch 8.92: Loss = 458.337
Epoch 8.93: Loss = -1367.88
Epoch 8.94: Loss = -2011.63
Epoch 8.95: Loss = 774.833
Epoch 8.96: Loss = -1125.74
Epoch 8.97: Loss = 402.39
Epoch 8.98: Loss = -1311.73
Epoch 8.99: Loss = -1285.44
Epoch 8.100: Loss = 1311.15
Epoch 8.101: Loss = -2302.81
Epoch 8.102: Loss = 466.846
Epoch 8.103: Loss = 961.958
Epoch 8.104: Loss = -191.759
Epoch 8.105: Loss = 182.512
Epoch 8.106: Loss = 1222.69
Epoch 8.107: Loss = -1596.27
Epoch 8.108: Loss = 292.959
Epoch 8.109: Loss = 419.759
Epoch 8.110: Loss = -414.762
Epoch 8.111: Loss = -1985.23
Epoch 8.112: Loss = 469.705
Epoch 8.113: Loss = -1985.27
Epoch 8.114: Loss = 2098.04
Epoch 8.115: Loss = 1148.17
Epoch 8.116: Loss = 195.088
Epoch 8.117: Loss = -14.3958
Epoch 8.118: Loss = 1055.51
Epoch 8.119: Loss = 1947.97
Epoch 8.120: Loss = -1044.37
TRAIN LOSS = 28.0509
TRAIN ACC = 55.6137 % (33370/60000)
Loss = 1237.33
Loss = 80.8434
Loss = -787.312
Loss = -146.318
Loss = -75.2179
Loss = -33.9944
Loss = 186.07
Loss = 798.039
Loss = -790.945
Loss = -887.212
Loss = 48.5892
Loss = 2382.84
Loss = 197.61
Loss = -562.542
Loss = 1329.46
Loss = 426.872
Loss = -446.997
Loss = 212.298
Loss = -635.583
Loss = 162.904
TEST LOSS = 3.76453
TEST ACC = 333.699 % (1045/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 866.463
Epoch 9.2: Loss = -590.089
Epoch 9.3: Loss = 2313.7
Epoch 9.4: Loss = 1910.56
Epoch 9.5: Loss = -1427.6
Epoch 9.6: Loss = -1546.89
Epoch 9.7: Loss = 245.332
Epoch 9.8: Loss = -3291.25
Epoch 9.9: Loss = 503.734
Epoch 9.10: Loss = -122.358
Epoch 9.11: Loss = -1568.54
Epoch 9.12: Loss = -508.241
Epoch 9.13: Loss = -743.981
Epoch 9.14: Loss = 1305.96
Epoch 9.15: Loss = -1067.69
Epoch 9.16: Loss = -2531.67
Epoch 9.17: Loss = -1312.55
Epoch 9.18: Loss = 1459.83
Epoch 9.19: Loss = 1135.07
Epoch 9.20: Loss = -325.755
Epoch 9.21: Loss = 1995.32
Epoch 9.22: Loss = 549.434
Epoch 9.23: Loss = 1647.78
Epoch 9.24: Loss = -1058.14
Epoch 9.25: Loss = 960.149
Epoch 9.26: Loss = 1766.44
Epoch 9.27: Loss = 884.97
Epoch 9.28: Loss = 178.134
Epoch 9.29: Loss = 1272.66
Epoch 9.30: Loss = 1031.11
Epoch 9.31: Loss = -987.013
Epoch 9.32: Loss = -1794.76
Epoch 9.33: Loss = -1230.02
Epoch 9.34: Loss = -59.4153
Epoch 9.35: Loss = -139.79
Epoch 9.36: Loss = 1442.59
Epoch 9.37: Loss = 740.103
Epoch 9.38: Loss = -304.214
Epoch 9.39: Loss = -303.517
Epoch 9.40: Loss = -837.056
Epoch 9.41: Loss = -669.164
Epoch 9.42: Loss = -1297.73
Epoch 9.43: Loss = 1756.78
Epoch 9.44: Loss = -3025.97
Epoch 9.45: Loss = -828.236
Epoch 9.46: Loss = -881.239
Epoch 9.47: Loss = 37.3319
Epoch 9.48: Loss = -1978.5
Epoch 9.49: Loss = 1032.25
Epoch 9.50: Loss = 281.766
Epoch 9.51: Loss = -2179.52
Epoch 9.52: Loss = 1393.45
Epoch 9.53: Loss = -1886.11
Epoch 9.54: Loss = 318.44
Epoch 9.55: Loss = -2373.7
Epoch 9.56: Loss = 2195.23
Epoch 9.57: Loss = 1749.45
Epoch 9.58: Loss = 237.115
Epoch 9.59: Loss = 673.85
Epoch 9.60: Loss = -686.757
Epoch 9.61: Loss = -929.021
Epoch 9.62: Loss = 779.693
Epoch 9.63: Loss = 65.2224
Epoch 9.64: Loss = 1146.36
Epoch 9.65: Loss = 2535.76
Epoch 9.66: Loss = -365.626
Epoch 9.67: Loss = 1094.59
Epoch 9.68: Loss = 1748.72
Epoch 9.69: Loss = 118.447
Epoch 9.70: Loss = -310.237
Epoch 9.71: Loss = 1289.32
Epoch 9.72: Loss = 1286.87
Epoch 9.73: Loss = -1260.88
Epoch 9.74: Loss = -1290.31
Epoch 9.75: Loss = -1117.61
Epoch 9.76: Loss = -525.643
Epoch 9.77: Loss = 3012.97
Epoch 9.78: Loss = 2902.54
Epoch 9.79: Loss = -1539.21
Epoch 9.80: Loss = -251.815
Epoch 9.81: Loss = -490.607
Epoch 9.82: Loss = 2795.29
Epoch 9.83: Loss = -558.896
Epoch 9.84: Loss = -970.082
Epoch 9.85: Loss = 483.065
Epoch 9.86: Loss = -1101.9
Epoch 9.87: Loss = -1696.06
Epoch 9.88: Loss = -311.012
Epoch 9.89: Loss = 46.2645
Epoch 9.90: Loss = -19.8698
Epoch 9.91: Loss = -1074.23
Epoch 9.92: Loss = -1825.38
Epoch 9.93: Loss = 1652.15
Epoch 9.94: Loss = -1042.99
Epoch 9.95: Loss = 413.69
Epoch 9.96: Loss = -1064.54
Epoch 9.97: Loss = -1241.04
Epoch 9.98: Loss = 1783.06
Epoch 9.99: Loss = 1161.82
Epoch 9.100: Loss = -2568.12
Epoch 9.101: Loss = 1815.44
Epoch 9.102: Loss = 524.994
Epoch 9.103: Loss = -425.535
Epoch 9.104: Loss = 707.931
Epoch 9.105: Loss = 932.112
Epoch 9.106: Loss = -1313.01
Epoch 9.107: Loss = 519.468
Epoch 9.108: Loss = 609.894
Epoch 9.109: Loss = 1154.13
Epoch 9.110: Loss = 100.347
Epoch 9.111: Loss = -1326.01
Epoch 9.112: Loss = -537.431
Epoch 9.113: Loss = -390.61
Epoch 9.114: Loss = -204.905
Epoch 9.115: Loss = 1060.39
Epoch 9.116: Loss = 434.425
Epoch 9.117: Loss = 666.887
Epoch 9.118: Loss = -898.087
Epoch 9.119: Loss = -1507.8
Epoch 9.120: Loss = -1988.92
TRAIN LOSS = -24.8171
TRAIN ACC = 10.0189 % (6011/60000)
Loss = 1361.29
Loss = -80.4126
Loss = 392.382
Loss = -3064.18
Loss = -3061.9
Loss = -393.328
Loss = 230.418
Loss = -363.878
Loss = 367.172
Loss = 966.427
Loss = 256.432
Loss = -365.804
Loss = -1324.35
Loss = -1668.37
Loss = -645.355
Loss = -2954.91
Loss = 2673.89
Loss = -751.195
Loss = 1161.81
Loss = 2586.65
TEST LOSS = -0.390164
TEST ACC = 60.1105 % (1030/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 1489.28
Epoch 10.2: Loss = 1121.5
Epoch 10.3: Loss = 1182.94
Epoch 10.4: Loss = -13.2194
Epoch 10.5: Loss = 1425.5
Epoch 10.6: Loss = -2439.26
Epoch 10.7: Loss = -205.447
Epoch 10.8: Loss = 1245.13
Epoch 10.9: Loss = -1502.32
Epoch 10.10: Loss = -233.983
Epoch 10.11: Loss = -1939.93
Epoch 10.12: Loss = 518.899
Epoch 10.13: Loss = -188.567
Epoch 10.14: Loss = 1243.6
Epoch 10.15: Loss = -327.998
Epoch 10.16: Loss = 1644.74
Epoch 10.17: Loss = -756.89
Epoch 10.18: Loss = 936.339
Epoch 10.19: Loss = -537.538
Epoch 10.20: Loss = 536.366
Epoch 10.21: Loss = 1337.29
Epoch 10.22: Loss = 1272.2
Epoch 10.23: Loss = 1004.48
Epoch 10.24: Loss = -180.765
Epoch 10.25: Loss = -1499.84
Epoch 10.26: Loss = -1717.15
Epoch 10.27: Loss = 374.241
Epoch 10.28: Loss = -880.991
Epoch 10.29: Loss = 363.983
Epoch 10.30: Loss = -994.597
Epoch 10.31: Loss = -1442.02
Epoch 10.32: Loss = -1333.14
Epoch 10.33: Loss = 489.628
Epoch 10.34: Loss = -648.919
Epoch 10.35: Loss = 163.343
Epoch 10.36: Loss = -46.4215
Epoch 10.37: Loss = 650.413
Epoch 10.38: Loss = -1877.14
Epoch 10.39: Loss = 1729.69
Epoch 10.40: Loss = 1114.18
Epoch 10.41: Loss = 2324.51
Epoch 10.42: Loss = 1317.23
Epoch 10.43: Loss = -1547.19
Epoch 10.44: Loss = 720.999
Epoch 10.45: Loss = 373.884
Epoch 10.46: Loss = -345.059
Epoch 10.47: Loss = 1892.4
Epoch 10.48: Loss = 1966.4
Epoch 10.49: Loss = -617.068
Epoch 10.50: Loss = -723.571
Epoch 10.51: Loss = 1588.99
Epoch 10.52: Loss = -50.915
Epoch 10.53: Loss = 946.553
Epoch 10.54: Loss = 1474.77
Epoch 10.55: Loss = -1710.81
Epoch 10.56: Loss = -1.36333
Epoch 10.57: Loss = 779.165
Epoch 10.58: Loss = 239.298
Epoch 10.59: Loss = 1926.04
Epoch 10.60: Loss = 735.198
Epoch 10.61: Loss = 164.797
Epoch 10.62: Loss = -1626.45
Epoch 10.63: Loss = -521.589
Epoch 10.64: Loss = 799.325
Epoch 10.65: Loss = 1060.21
Epoch 10.66: Loss = 768.664
Epoch 10.67: Loss = -376.03
Epoch 10.68: Loss = 589.702
Epoch 10.69: Loss = -787.955
Epoch 10.70: Loss = -406.17
Epoch 10.71: Loss = 220.647
Epoch 10.72: Loss = 742.751
Epoch 10.73: Loss = -75.8891
Epoch 10.74: Loss = -159.797
Epoch 10.75: Loss = 1208.42
Epoch 10.76: Loss = -1105.09
Epoch 10.77: Loss = 32.0208
Epoch 10.78: Loss = 557.916
Epoch 10.79: Loss = -975.969
Epoch 10.80: Loss = 1263.63
Epoch 10.81: Loss = -736.867
Epoch 10.82: Loss = 1587.77
Epoch 10.83: Loss = 839.241
Epoch 10.84: Loss = 677.223
Epoch 10.85: Loss = 534.904
Epoch 10.86: Loss = 161.006
Epoch 10.87: Loss = 91.6859
Epoch 10.88: Loss = -2240.16
Epoch 10.89: Loss = 1627.15
Epoch 10.90: Loss = 80.8675
Epoch 10.91: Loss = 791.463
Epoch 10.92: Loss = 2310.32
Epoch 10.93: Loss = 511.9
Epoch 10.94: Loss = -1116.43
Epoch 10.95: Loss = 412.93
Epoch 10.96: Loss = -1541.36
Epoch 10.97: Loss = 280.49
Epoch 10.98: Loss = -805.342
Epoch 10.99: Loss = -80.0591
Epoch 10.100: Loss = 660.128
Epoch 10.101: Loss = -164.914
Epoch 10.102: Loss = 1906.83
Epoch 10.103: Loss = 205.03
Epoch 10.104: Loss = -318.079
Epoch 10.105: Loss = 2302.43
Epoch 10.106: Loss = -1125.64
Epoch 10.107: Loss = 505.221
Epoch 10.108: Loss = -600.826
Epoch 10.109: Loss = 241.874
Epoch 10.110: Loss = 1360.22
Epoch 10.111: Loss = -13.7484
Epoch 10.112: Loss = 655.637
Epoch 10.113: Loss = 1016.57
Epoch 10.114: Loss = -873.217
Epoch 10.115: Loss = 400.566
Epoch 10.116: Loss = 374.364
Epoch 10.117: Loss = 342.113
Epoch 10.118: Loss = -544.987
Epoch 10.119: Loss = 2066.61
Epoch 10.120: Loss = 389.827
TRAIN LOSS = 215.945
TRAIN ACC = 10.1563 % (6094/60000)
Loss = 1882.17
Loss = -354.096
Loss = -33.7192
Loss = -28.4985
Loss = -1840.22
Loss = 950.928
Loss = 145.384
Loss = 471.602
Loss = 1254.85
Loss = -451.893
Loss = 395.897
Loss = 51.6261
Loss = 1351.41
Loss = 380.83
Loss = 402.575
Loss = 195.944
Loss = -1805.24
Loss = -763.175
Loss = 73.2319
Loss = 554.769
TEST LOSS = 0.0912798
TEST ACC = 60.9406 % (1012/10000)
