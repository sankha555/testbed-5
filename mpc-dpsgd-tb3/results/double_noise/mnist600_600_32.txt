Using statistical security parameter 40
Trying to run 64-bit computation
Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 600]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 600
Num Epochs: 20
Learning Rate: 0.1 to 0.05 over 10 epochs
Clipping Factor: 4
Sigma: 8
***********************************************************
Epoch 1.1: Loss = 2.40154
Epoch 1.2: Loss = 2.32687
Epoch 1.3: Loss = 2.26762
Epoch 1.4: Loss = 2.24168
Epoch 1.5: Loss = 2.16708
Epoch 1.6: Loss = 2.14011
Epoch 1.7: Loss = 2.0737
Epoch 1.8: Loss = 2.0526
Epoch 1.9: Loss = 2.0247
Epoch 1.10: Loss = 1.94139
Epoch 1.11: Loss = 1.90176
Epoch 1.12: Loss = 1.88954
Epoch 1.13: Loss = 1.83188
Epoch 1.14: Loss = 1.79132
Epoch 1.15: Loss = 1.75114
Epoch 1.16: Loss = 1.7845
Epoch 1.17: Loss = 1.70009
Epoch 1.18: Loss = 1.66963
Epoch 1.19: Loss = 1.65703
Epoch 1.20: Loss = 1.5899
Epoch 1.21: Loss = 1.57475
Epoch 1.22: Loss = 1.50458
Epoch 1.23: Loss = 1.49228
Epoch 1.24: Loss = 1.48517
Epoch 1.25: Loss = 1.43982
Epoch 1.26: Loss = 1.44414
Epoch 1.27: Loss = 1.41722
Epoch 1.28: Loss = 1.40089
Epoch 1.29: Loss = 1.40382
Epoch 1.30: Loss = 1.35867
Epoch 1.31: Loss = 1.28456
Epoch 1.32: Loss = 1.30196
Epoch 1.33: Loss = 1.31895
Epoch 1.34: Loss = 1.22925
Epoch 1.35: Loss = 1.20259
Epoch 1.36: Loss = 1.20497
Epoch 1.37: Loss = 1.15919
Epoch 1.38: Loss = 1.20258
Epoch 1.39: Loss = 1.16394
Epoch 1.40: Loss = 1.1503
Epoch 1.41: Loss = 1.18622
Epoch 1.42: Loss = 1.13934
Epoch 1.43: Loss = 1.08015
Epoch 1.44: Loss = 1.10249
Epoch 1.45: Loss = 1.00734
Epoch 1.46: Loss = 0.99704
Epoch 1.47: Loss = 1.06999
Epoch 1.48: Loss = 1.03625
Epoch 1.49: Loss = 0.926483
Epoch 1.50: Loss = 0.978165
Epoch 1.51: Loss = 0.982559
Epoch 1.52: Loss = 0.929962
Epoch 1.53: Loss = 0.997391
Epoch 1.54: Loss = 1.02791
Epoch 1.55: Loss = 0.947464
Epoch 1.56: Loss = 0.945633
Epoch 1.57: Loss = 0.887543
Epoch 1.58: Loss = 0.905045
Epoch 1.59: Loss = 0.936081
Epoch 1.60: Loss = 0.93634
Epoch 1.61: Loss = 0.83165
Epoch 1.62: Loss = 0.927109
Epoch 1.63: Loss = 0.837982
Epoch 1.64: Loss = 0.826706
Epoch 1.65: Loss = 0.798462
Epoch 1.66: Loss = 0.856796
Epoch 1.67: Loss = 0.804062
Epoch 1.68: Loss = 0.813278
Epoch 1.69: Loss = 0.787384
Epoch 1.70: Loss = 0.850021
Epoch 1.71: Loss = 0.843277
Epoch 1.72: Loss = 0.831055
Epoch 1.73: Loss = 0.759338
Epoch 1.74: Loss = 0.770279
Epoch 1.75: Loss = 0.839294
Epoch 1.76: Loss = 0.838242
Epoch 1.77: Loss = 0.746613
Epoch 1.78: Loss = 0.780884
Epoch 1.79: Loss = 0.803955
Epoch 1.80: Loss = 0.804871
Epoch 1.81: Loss = 0.803757
Epoch 1.82: Loss = 0.741089
Epoch 1.83: Loss = 0.805008
Epoch 1.84: Loss = 0.770065
Epoch 1.85: Loss = 0.672775
Epoch 1.86: Loss = 0.721039
Epoch 1.87: Loss = 0.739868
Epoch 1.88: Loss = 0.675003
Epoch 1.89: Loss = 0.74617
Epoch 1.90: Loss = 0.761215
Epoch 1.91: Loss = 0.719894
Epoch 1.92: Loss = 0.647858
Epoch 1.93: Loss = 0.697525
Epoch 1.94: Loss = 0.742447
Epoch 1.95: Loss = 0.762741
Epoch 1.96: Loss = 0.7043
Epoch 1.97: Loss = 0.694046
Epoch 1.98: Loss = 0.685532
Epoch 1.99: Loss = 0.695694
Epoch 1.100: Loss = 0.703415
TRAIN LOSS = 1.16808
TRAIN ACC = 65.2679 % (39162/60000)
Loss = 0.737381
Loss = 0.782837
Loss = 0.902435
Loss = 0.801361
Loss = 0.734329
Loss = 0.744827
Loss = 0.846344
Loss = 0.826324
Loss = 0.573883
Loss = 0.554642
Loss = 0.444763
Loss = 0.594376
Loss = 0.534576
Loss = 0.546417
Loss = 0.321747
Loss = 0.542206
Loss = 0.863022
TEST LOSS = 0.663828
TEST ACC = 391.62 % (7927/10000)
Reducing learning rate to 0.0944519
Epoch 2.1: Loss = 0.713196
Epoch 2.2: Loss = 0.604187
Epoch 2.3: Loss = 0.660416
Epoch 2.4: Loss = 0.652328
Epoch 2.5: Loss = 0.728409
Epoch 2.6: Loss = 0.63707
Epoch 2.7: Loss = 0.675507
Epoch 2.8: Loss = 0.615021
Epoch 2.9: Loss = 0.671631
Epoch 2.10: Loss = 0.72522
Epoch 2.11: Loss = 0.616074
Epoch 2.12: Loss = 0.639877
Epoch 2.13: Loss = 0.674026
Epoch 2.14: Loss = 0.609314
Epoch 2.15: Loss = 0.659546
Epoch 2.16: Loss = 0.693573
Epoch 2.17: Loss = 0.641983
Epoch 2.18: Loss = 0.739166
Epoch 2.19: Loss = 0.665207
Epoch 2.20: Loss = 0.664108
Epoch 2.21: Loss = 0.717957
Epoch 2.22: Loss = 0.725723
Epoch 2.23: Loss = 0.672821
Epoch 2.24: Loss = 0.662064
Epoch 2.25: Loss = 0.755905
Epoch 2.26: Loss = 0.632431
Epoch 2.27: Loss = 0.67363
Epoch 2.28: Loss = 0.622818
Epoch 2.29: Loss = 0.590027
Epoch 2.30: Loss = 0.619446
Epoch 2.31: Loss = 0.601288
Epoch 2.32: Loss = 0.611023
Epoch 2.33: Loss = 0.593887
Epoch 2.34: Loss = 0.605194
Epoch 2.35: Loss = 0.640686
Epoch 2.36: Loss = 0.643616
Epoch 2.37: Loss = 0.658096
Epoch 2.38: Loss = 0.633408
Epoch 2.39: Loss = 0.614944
Epoch 2.40: Loss = 0.638718
Epoch 2.41: Loss = 0.634506
Epoch 2.42: Loss = 0.623032
Epoch 2.43: Loss = 0.629013
Epoch 2.44: Loss = 0.608719
Epoch 2.45: Loss = 0.624222
Epoch 2.46: Loss = 0.696014
Epoch 2.47: Loss = 0.568024
Epoch 2.48: Loss = 0.602859
Epoch 2.49: Loss = 0.706467
Epoch 2.50: Loss = 0.639191
Epoch 2.51: Loss = 0.584
Epoch 2.52: Loss = 0.659378
Epoch 2.53: Loss = 0.624603
Epoch 2.54: Loss = 0.644974
Epoch 2.55: Loss = 0.622986
Epoch 2.56: Loss = 0.55069
Epoch 2.57: Loss = 0.606155
Epoch 2.58: Loss = 0.629105
Epoch 2.59: Loss = 0.626877
Epoch 2.60: Loss = 0.587433
Epoch 2.61: Loss = 0.660599
Epoch 2.62: Loss = 0.640366
Epoch 2.63: Loss = 0.584702
Epoch 2.64: Loss = 0.592285
Epoch 2.65: Loss = 0.579865
Epoch 2.66: Loss = 0.616943
Epoch 2.67: Loss = 0.593552
Epoch 2.68: Loss = 0.62291
Epoch 2.69: Loss = 0.641266
Epoch 2.70: Loss = 0.557983
Epoch 2.71: Loss = 0.556244
Epoch 2.72: Loss = 0.644928
Epoch 2.73: Loss = 0.598114
Epoch 2.74: Loss = 0.662704
Epoch 2.75: Loss = 0.628799
Epoch 2.76: Loss = 0.64856
Epoch 2.77: Loss = 0.738312
Epoch 2.78: Loss = 0.560883
Epoch 2.79: Loss = 0.543137
Epoch 2.80: Loss = 0.556702
Epoch 2.81: Loss = 0.61261
Epoch 2.82: Loss = 0.639603
Epoch 2.83: Loss = 0.598877
Epoch 2.84: Loss = 0.571289
Epoch 2.85: Loss = 0.536469
Epoch 2.86: Loss = 0.570267
Epoch 2.87: Loss = 0.642014
Epoch 2.88: Loss = 0.687271
Epoch 2.89: Loss = 0.596909
Epoch 2.90: Loss = 0.723419
Epoch 2.91: Loss = 0.625671
Epoch 2.92: Loss = 0.650818
Epoch 2.93: Loss = 0.687225
Epoch 2.94: Loss = 0.644882
Epoch 2.95: Loss = 0.585876
Epoch 2.96: Loss = 0.622757
Epoch 2.97: Loss = 0.56311
Epoch 2.98: Loss = 0.51683
Epoch 2.99: Loss = 0.5448
Epoch 2.100: Loss = 0.659744
TRAIN LOSS = 0.631775
TRAIN ACC = 79.9576 % (47977/60000)
Loss = 0.61557
Loss = 0.672653
Loss = 0.827179
Loss = 0.772614
Loss = 0.618866
Loss = 0.651855
Loss = 0.778778
Loss = 0.695923
Loss = 0.467728
Loss = 0.46759
Loss = 0.425186
Loss = 0.444778
Loss = 0.382385
Loss = 0.406372
Loss = 0.179108
Loss = 0.423721
Loss = 0.758224
TEST LOSS = 0.560147
TEST ACC = 479.77 % (8234/10000)
Reducing learning rate to 0.0888977
Epoch 3.1: Loss = 0.629791
Epoch 3.2: Loss = 0.618073
Epoch 3.3: Loss = 0.605072
Epoch 3.4: Loss = 0.584839
Epoch 3.5: Loss = 0.612
Epoch 3.6: Loss = 0.606949
Epoch 3.7: Loss = 0.578125
Epoch 3.8: Loss = 0.50592
Epoch 3.9: Loss = 0.517761
Epoch 3.10: Loss = 0.597397
Epoch 3.11: Loss = 0.602936
Epoch 3.12: Loss = 0.52034
Epoch 3.13: Loss = 0.587173
Epoch 3.14: Loss = 0.636459
Epoch 3.15: Loss = 0.621201
Epoch 3.16: Loss = 0.573776
Epoch 3.17: Loss = 0.536438
Epoch 3.18: Loss = 0.531601
Epoch 3.19: Loss = 0.639755
Epoch 3.20: Loss = 0.609756
Epoch 3.21: Loss = 0.656784
Epoch 3.22: Loss = 0.542908
Epoch 3.23: Loss = 0.565811
Epoch 3.24: Loss = 0.534027
Epoch 3.25: Loss = 0.538254
Epoch 3.26: Loss = 0.522202
Epoch 3.27: Loss = 0.554749
Epoch 3.28: Loss = 0.553772
Epoch 3.29: Loss = 0.590897
Epoch 3.30: Loss = 0.486847
Epoch 3.31: Loss = 0.534836
Epoch 3.32: Loss = 0.576141
Epoch 3.33: Loss = 0.578323
Epoch 3.34: Loss = 0.618637
Epoch 3.35: Loss = 0.596527
Epoch 3.36: Loss = 0.585403
Epoch 3.37: Loss = 0.567764
Epoch 3.38: Loss = 0.5401
Epoch 3.39: Loss = 0.589844
Epoch 3.40: Loss = 0.597015
Epoch 3.41: Loss = 0.67717
Epoch 3.42: Loss = 0.539139
Epoch 3.43: Loss = 0.609055
Epoch 3.44: Loss = 0.547836
Epoch 3.45: Loss = 0.544449
Epoch 3.46: Loss = 0.578308
Epoch 3.47: Loss = 0.546265
Epoch 3.48: Loss = 0.584961
Epoch 3.49: Loss = 0.662003
Epoch 3.50: Loss = 0.612427
Epoch 3.51: Loss = 0.570633
Epoch 3.52: Loss = 0.568237
Epoch 3.53: Loss = 0.61174
Epoch 3.54: Loss = 0.612915
Epoch 3.55: Loss = 0.703232
Epoch 3.56: Loss = 0.578354
Epoch 3.57: Loss = 0.628738
Epoch 3.58: Loss = 0.58287
Epoch 3.59: Loss = 0.647964
Epoch 3.60: Loss = 0.533661
Epoch 3.61: Loss = 0.568558
Epoch 3.62: Loss = 0.543976
Epoch 3.63: Loss = 0.576736
Epoch 3.64: Loss = 0.669495
Epoch 3.65: Loss = 0.596252
Epoch 3.66: Loss = 0.64325
Epoch 3.67: Loss = 0.587326
Epoch 3.68: Loss = 0.597122
Epoch 3.69: Loss = 0.510422
Epoch 3.70: Loss = 0.607086
Epoch 3.71: Loss = 0.658981
Epoch 3.72: Loss = 0.568954
Epoch 3.73: Loss = 0.600555
Epoch 3.74: Loss = 0.602325
Epoch 3.75: Loss = 0.594406
Epoch 3.76: Loss = 0.563324
Epoch 3.77: Loss = 0.632751
Epoch 3.78: Loss = 0.687393
Epoch 3.79: Loss = 0.527374
Epoch 3.80: Loss = 0.636475
Epoch 3.81: Loss = 0.590164
Epoch 3.82: Loss = 0.671585
Epoch 3.83: Loss = 0.660507
Epoch 3.84: Loss = 0.690582
Epoch 3.85: Loss = 0.693222
Epoch 3.86: Loss = 0.615448
Epoch 3.87: Loss = 0.647537
Epoch 3.88: Loss = 0.65741
Epoch 3.89: Loss = 0.663254
Epoch 3.90: Loss = 0.654053
Epoch 3.91: Loss = 0.584717
Epoch 3.92: Loss = 0.624512
Epoch 3.93: Loss = 0.541245
Epoch 3.94: Loss = 0.535416
Epoch 3.95: Loss = 0.607498
Epoch 3.96: Loss = 0.648605
Epoch 3.97: Loss = 0.641205
Epoch 3.98: Loss = 0.675171
Epoch 3.99: Loss = 0.580154
Epoch 3.100: Loss = 0.676758
TRAIN LOSS = 0.596191
TRAIN ACC = 81.752 % (49053/60000)
Loss = 0.683975
Loss = 0.673843
Loss = 0.813034
Loss = 0.823853
Loss = 0.641113
Loss = 0.681915
Loss = 0.779129
Loss = 0.749969
Loss = 0.53627
Loss = 0.453491
Loss = 0.493042
Loss = 0.462448
Loss = 0.391403
Loss = 0.503891
Loss = 0.183655
Loss = 0.437866
Loss = 0.896072
TEST LOSS = 0.594377
TEST ACC = 490.529 % (8267/10000)
Reducing learning rate to 0.0833435
Epoch 4.1: Loss = 0.739578
Epoch 4.2: Loss = 0.598297
Epoch 4.3: Loss = 0.53624
Epoch 4.4: Loss = 0.627838
Epoch 4.5: Loss = 0.541794
Epoch 4.6: Loss = 0.621628
Epoch 4.7: Loss = 0.673248
Epoch 4.8: Loss = 0.667679
Epoch 4.9: Loss = 0.549057
Epoch 4.10: Loss = 0.702866
Epoch 4.11: Loss = 0.608994
Epoch 4.12: Loss = 0.666443
Epoch 4.13: Loss = 0.543365
Epoch 4.14: Loss = 0.571396
Epoch 4.15: Loss = 0.603592
Epoch 4.16: Loss = 0.486664
Epoch 4.17: Loss = 0.673904
Epoch 4.18: Loss = 0.680237
Epoch 4.19: Loss = 0.564987
Epoch 4.20: Loss = 0.630234
Epoch 4.21: Loss = 0.636139
Epoch 4.22: Loss = 0.617142
Epoch 4.23: Loss = 0.514526
Epoch 4.24: Loss = 0.605438
Epoch 4.25: Loss = 0.622375
Epoch 4.26: Loss = 0.649918
Epoch 4.27: Loss = 0.639984
Epoch 4.28: Loss = 0.720627
Epoch 4.29: Loss = 0.615829
Epoch 4.30: Loss = 0.601593
Epoch 4.31: Loss = 0.608704
Epoch 4.32: Loss = 0.566696
Epoch 4.33: Loss = 0.659561
Epoch 4.34: Loss = 0.525177
Epoch 4.35: Loss = 0.57341
Epoch 4.36: Loss = 0.626999
Epoch 4.37: Loss = 0.743805
Epoch 4.38: Loss = 0.59137
Epoch 4.39: Loss = 0.550781
Epoch 4.40: Loss = 0.651108
Epoch 4.41: Loss = 0.593475
Epoch 4.42: Loss = 0.552521
Epoch 4.43: Loss = 0.67244
Epoch 4.44: Loss = 0.573471
Epoch 4.45: Loss = 0.644577
Epoch 4.46: Loss = 0.630707
Epoch 4.47: Loss = 0.622864
Epoch 4.48: Loss = 0.55159
Epoch 4.49: Loss = 0.637299
Epoch 4.50: Loss = 0.57843
Epoch 4.51: Loss = 0.598618
Epoch 4.52: Loss = 0.524918
Epoch 4.53: Loss = 0.545853
Epoch 4.54: Loss = 0.654297
Epoch 4.55: Loss = 0.569733
Epoch 4.56: Loss = 0.558716
Epoch 4.57: Loss = 0.590759
Epoch 4.58: Loss = 0.654236
Epoch 4.59: Loss = 0.556885
Epoch 4.60: Loss = 0.641251
Epoch 4.61: Loss = 0.764252
Epoch 4.62: Loss = 0.695206
Epoch 4.63: Loss = 0.579376
Epoch 4.64: Loss = 0.659271
Epoch 4.65: Loss = 0.667709
Epoch 4.66: Loss = 0.734802
Epoch 4.67: Loss = 0.610641
Epoch 4.68: Loss = 0.595566
Epoch 4.69: Loss = 0.516159
Epoch 4.70: Loss = 0.627182
Epoch 4.71: Loss = 0.606186
Epoch 4.72: Loss = 0.654678
Epoch 4.73: Loss = 0.6465
Epoch 4.74: Loss = 0.649292
Epoch 4.75: Loss = 0.698273
Epoch 4.76: Loss = 0.63797
Epoch 4.77: Loss = 0.649094
Epoch 4.78: Loss = 0.560287
Epoch 4.79: Loss = 0.633636
Epoch 4.80: Loss = 0.587189
Epoch 4.81: Loss = 0.675491
Epoch 4.82: Loss = 0.637604
Epoch 4.83: Loss = 0.506744
Epoch 4.84: Loss = 0.56897
Epoch 4.85: Loss = 0.539047
Epoch 4.86: Loss = 0.659271
Epoch 4.87: Loss = 0.466248
Epoch 4.88: Loss = 0.588654
Epoch 4.89: Loss = 0.635864
Epoch 4.90: Loss = 0.647324
Epoch 4.91: Loss = 0.713135
Epoch 4.92: Loss = 0.583664
Epoch 4.93: Loss = 0.652466
Epoch 4.94: Loss = 0.639618
Epoch 4.95: Loss = 0.678268
Epoch 4.96: Loss = 0.51825
Epoch 4.97: Loss = 0.558853
Epoch 4.98: Loss = 0.626038
Epoch 4.99: Loss = 0.534561
Epoch 4.100: Loss = 0.642258
TRAIN LOSS = 0.614075
TRAIN ACC = 82.2128 % (49330/60000)
Loss = 0.660995
Loss = 0.717606
Loss = 0.879272
Loss = 0.867264
Loss = 0.640808
Loss = 0.681915
Loss = 0.852509
Loss = 0.77594
Loss = 0.530075
Loss = 0.461075
Loss = 0.487167
Loss = 0.435974
Loss = 0.380447
Loss = 0.521698
Loss = 0.140656
Loss = 0.43045
Loss = 0.923721
TEST LOSS = 0.60478
TEST ACC = 493.3 % (8330/10000)
Reducing learning rate to 0.0777893
Epoch 5.1: Loss = 0.621994
Epoch 5.2: Loss = 0.64151
Epoch 5.3: Loss = 0.613663
Epoch 5.4: Loss = 0.653091
Epoch 5.5: Loss = 0.641937
Epoch 5.6: Loss = 0.592285
Epoch 5.7: Loss = 0.58223
Epoch 5.8: Loss = 0.750839
Epoch 5.9: Loss = 0.564178
Epoch 5.10: Loss = 0.637497
Epoch 5.11: Loss = 0.589951
Epoch 5.12: Loss = 0.636688
