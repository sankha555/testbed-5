Setting up connection 0
***********************************************************
Training FMNIST
Model: Dense([60000, 1, 128]) => Dense([60000, 1, 128]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 2
***********************************************************
Epoch 1.1: Loss = 2.31134
Epoch 1.2: Loss = 2.25929
Epoch 1.3: Loss = 2.26273
Epoch 1.4: Loss = 2.23509
Epoch 1.5: Loss = 2.21527
Epoch 1.6: Loss = 2.19206
Epoch 1.7: Loss = 2.17641
Epoch 1.8: Loss = 2.15556
Epoch 1.9: Loss = 2.09669
Epoch 1.10: Loss = 2.092
Epoch 1.11: Loss = 2.02866
Epoch 1.12: Loss = 2.05556
Epoch 1.13: Loss = 1.99445
Epoch 1.14: Loss = 2.01045
Epoch 1.15: Loss = 2.03259
Epoch 1.16: Loss = 1.96037
Epoch 1.17: Loss = 1.91635
Epoch 1.18: Loss = 1.88971
Epoch 1.19: Loss = 1.85211
Epoch 1.20: Loss = 1.82367
Epoch 1.21: Loss = 1.76237
Epoch 1.22: Loss = 1.74911
Epoch 1.23: Loss = 1.70401
Epoch 1.24: Loss = 1.74425
Epoch 1.25: Loss = 1.69203
Epoch 1.26: Loss = 1.69518
Epoch 1.27: Loss = 1.63116
Epoch 1.28: Loss = 1.61868
Epoch 1.29: Loss = 1.62447
Epoch 1.30: Loss = 1.67233
Epoch 1.31: Loss = 1.54608
Epoch 1.32: Loss = 1.53401
Epoch 1.33: Loss = 1.48631
Epoch 1.34: Loss = 1.49767
Epoch 1.35: Loss = 1.38113
Epoch 1.36: Loss = 1.51668
Epoch 1.37: Loss = 1.42165
Epoch 1.38: Loss = 1.31908
Epoch 1.39: Loss = 1.32806
Epoch 1.40: Loss = 1.26875
Epoch 1.41: Loss = 1.30258
Epoch 1.42: Loss = 1.26823
Epoch 1.43: Loss = 1.242
Epoch 1.44: Loss = 1.14229
Epoch 1.45: Loss = 1.25613
Epoch 1.46: Loss = 1.17627
Epoch 1.47: Loss = 1.12038
Epoch 1.48: Loss = 1.19052
Epoch 1.49: Loss = 1.12
Epoch 1.50: Loss = 1.19522
Epoch 1.51: Loss = 1.04521
Epoch 1.52: Loss = 1.03136
Epoch 1.53: Loss = 1.08904
Epoch 1.54: Loss = 1.11221
Epoch 1.55: Loss = 1.09186
Epoch 1.56: Loss = 0.985779
Epoch 1.57: Loss = 0.931839
Epoch 1.58: Loss = 0.990768
Epoch 1.59: Loss = 0.980392
Epoch 1.60: Loss = 1.11433
Epoch 1.61: Loss = 1.03961
Epoch 1.62: Loss = 1.0303
Epoch 1.63: Loss = 1.02907
Epoch 1.64: Loss = 1.03554
Epoch 1.65: Loss = 1.08385
Epoch 1.66: Loss = 0.956451
Epoch 1.67: Loss = 0.93129
Epoch 1.68: Loss = 0.768982
Epoch 1.69: Loss = 0.833862
Epoch 1.70: Loss = 0.926941
Epoch 1.71: Loss = 0.853409
Epoch 1.72: Loss = 0.83963
Epoch 1.73: Loss = 0.845352
Epoch 1.74: Loss = 0.726883
Epoch 1.75: Loss = 0.852722
Epoch 1.76: Loss = 0.821442
Epoch 1.77: Loss = 0.792221
Epoch 1.78: Loss = 0.759109
Epoch 1.79: Loss = 0.794571
Epoch 1.80: Loss = 0.879898
Epoch 1.81: Loss = 0.765823
Epoch 1.82: Loss = 0.722275
Epoch 1.83: Loss = 0.889114
Epoch 1.84: Loss = 0.785004
Epoch 1.85: Loss = 0.870865
Epoch 1.86: Loss = 0.762329
Epoch 1.87: Loss = 0.690002
Epoch 1.88: Loss = 0.733093
Epoch 1.89: Loss = 0.769135
Epoch 1.90: Loss = 0.684753
Epoch 1.91: Loss = 0.762558
Epoch 1.92: Loss = 0.722565
Epoch 1.93: Loss = 0.752075
Epoch 1.94: Loss = 0.591827
Epoch 1.95: Loss = 0.732529
Epoch 1.96: Loss = 0.687195
Epoch 1.97: Loss = 0.537003
Epoch 1.98: Loss = 0.670212
Epoch 1.99: Loss = 0.777512
Epoch 1.100: Loss = 0.834976
Epoch 1.101: Loss = 0.709091
Epoch 1.102: Loss = 0.679245
Epoch 1.103: Loss = 0.627548
Epoch 1.104: Loss = 0.576004
Epoch 1.105: Loss = 0.668976
Epoch 1.106: Loss = 0.706848
Epoch 1.107: Loss = 0.603287
Epoch 1.108: Loss = 0.620392
Epoch 1.109: Loss = 0.615326
Epoch 1.110: Loss = 0.644592
Epoch 1.111: Loss = 0.544266
Epoch 1.112: Loss = 0.504562
Epoch 1.113: Loss = 0.618622
Epoch 1.114: Loss = 0.515976
Epoch 1.115: Loss = 0.585846
Epoch 1.116: Loss = 0.594513
Epoch 1.117: Loss = 0.474106
Epoch 1.118: Loss = 0.445114
Epoch 1.119: Loss = 0.461807
Epoch 1.120: Loss = 0.465271
TRAIN LOSS = 1.16121
TRAIN ACC = 66.6611 % (39998/60000)
Loss = 0.636017
Loss = 0.682098
Loss = 0.777542
Loss = 0.738358
Loss = 0.736649
Loss = 0.650742
Loss = 0.59967
Loss = 0.75206
Loss = 0.744186
Loss = 0.679825
Loss = 0.347046
Loss = 0.530685
Loss = 0.408508
Loss = 0.559433
Loss = 0.445374
Loss = 0.431122
Loss = 0.421234
Loss = 0.23764
Loss = 0.460327
Loss = 0.675095
TEST LOSS = 0.57568
TEST ACC = 399.979 % (8261/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.550278
Epoch 2.2: Loss = 0.674042
Epoch 2.3: Loss = 0.662247
Epoch 2.4: Loss = 0.521912
Epoch 2.5: Loss = 0.541672
Epoch 2.6: Loss = 0.537872
Epoch 2.7: Loss = 0.597366
Epoch 2.8: Loss = 0.555695
Epoch 2.9: Loss = 0.555511
Epoch 2.10: Loss = 0.556976
Epoch 2.11: Loss = 0.536728
Epoch 2.12: Loss = 0.54747
Epoch 2.13: Loss = 0.467667
Epoch 2.14: Loss = 0.535629
Epoch 2.15: Loss = 0.637131
Epoch 2.16: Loss = 0.604813
Epoch 2.17: Loss = 0.599991
Epoch 2.18: Loss = 0.70314
Epoch 2.19: Loss = 0.51445
Epoch 2.20: Loss = 0.488571
Epoch 2.21: Loss = 0.459885
Epoch 2.22: Loss = 0.473053
Epoch 2.23: Loss = 0.468628
Epoch 2.24: Loss = 0.667328
Epoch 2.25: Loss = 0.562347
Epoch 2.26: Loss = 0.66481
Epoch 2.27: Loss = 0.614304
Epoch 2.28: Loss = 0.588043
Epoch 2.29: Loss = 0.661789
Epoch 2.30: Loss = 0.718552
Epoch 2.31: Loss = 0.501938
Epoch 2.32: Loss = 0.604248
Epoch 2.33: Loss = 0.51297
Epoch 2.34: Loss = 0.588135
Epoch 2.35: Loss = 0.539841
Epoch 2.36: Loss = 0.621185
Epoch 2.37: Loss = 0.462418
Epoch 2.38: Loss = 0.45195
Epoch 2.39: Loss = 0.486557
Epoch 2.40: Loss = 0.466461
Epoch 2.41: Loss = 0.534088
Epoch 2.42: Loss = 0.583282
Epoch 2.43: Loss = 0.462219
Epoch 2.44: Loss = 0.413651
Epoch 2.45: Loss = 0.513351
Epoch 2.46: Loss = 0.525391
Epoch 2.47: Loss = 0.454025
Epoch 2.48: Loss = 0.543671
Epoch 2.49: Loss = 0.506363
Epoch 2.50: Loss = 0.607971
Epoch 2.51: Loss = 0.4552
Epoch 2.52: Loss = 0.427917
Epoch 2.53: Loss = 0.489502
Epoch 2.54: Loss = 0.575302
Epoch 2.55: Loss = 0.508102
Epoch 2.56: Loss = 0.467117
Epoch 2.57: Loss = 0.451233
Epoch 2.58: Loss = 0.508957
Epoch 2.59: Loss = 0.553406
Epoch 2.60: Loss = 0.629944
Epoch 2.61: Loss = 0.575806
Epoch 2.62: Loss = 0.576096
Epoch 2.63: Loss = 0.621796
Epoch 2.64: Loss = 0.60379
Epoch 2.65: Loss = 0.692688
Epoch 2.66: Loss = 0.540421
Epoch 2.67: Loss = 0.540405
Epoch 2.68: Loss = 0.36174
Epoch 2.69: Loss = 0.41626
Epoch 2.70: Loss = 0.580002
Epoch 2.71: Loss = 0.458084
Epoch 2.72: Loss = 0.457184
Epoch 2.73: Loss = 0.486389
Epoch 2.74: Loss = 0.365891
Epoch 2.75: Loss = 0.599045
Epoch 2.76: Loss = 0.510941
Epoch 2.77: Loss = 0.450897
Epoch 2.78: Loss = 0.454758
Epoch 2.79: Loss = 0.521729
Epoch 2.80: Loss = 0.556656
Epoch 2.81: Loss = 0.471237
Epoch 2.82: Loss = 0.398758
Epoch 2.83: Loss = 0.575287
Epoch 2.84: Loss = 0.469421
Epoch 2.85: Loss = 0.633591
Epoch 2.86: Loss = 0.522476
Epoch 2.87: Loss = 0.395538
Epoch 2.88: Loss = 0.454361
Epoch 2.89: Loss = 0.518799
Epoch 2.90: Loss = 0.410507
Epoch 2.91: Loss = 0.522461
Epoch 2.92: Loss = 0.510757
Epoch 2.93: Loss = 0.543549
Epoch 2.94: Loss = 0.352478
Epoch 2.95: Loss = 0.475266
Epoch 2.96: Loss = 0.523926
Epoch 2.97: Loss = 0.373901
Epoch 2.98: Loss = 0.446625
Epoch 2.99: Loss = 0.578476
Epoch 2.100: Loss = 0.621887
Epoch 2.101: Loss = 0.562622
Epoch 2.102: Loss = 0.468079
Epoch 2.103: Loss = 0.422165
Epoch 2.104: Loss = 0.377777
Epoch 2.105: Loss = 0.502121
Epoch 2.106: Loss = 0.557617
Epoch 2.107: Loss = 0.418137
Epoch 2.108: Loss = 0.457718
Epoch 2.109: Loss = 0.435272
Epoch 2.110: Loss = 0.464752
Epoch 2.111: Loss = 0.373749
Epoch 2.112: Loss = 0.356598
Epoch 2.113: Loss = 0.440903
Epoch 2.114: Loss = 0.367264
Epoch 2.115: Loss = 0.38414
Epoch 2.116: Loss = 0.445221
Epoch 2.117: Loss = 0.312195
Epoch 2.118: Loss = 0.263229
Epoch 2.119: Loss = 0.313553
Epoch 2.120: Loss = 0.313889
TRAIN LOSS = 0.509354
TRAIN ACC = 84.4086 % (50648/60000)
Loss = 0.476715
Loss = 0.549194
Loss = 0.626602
Loss = 0.618103
Loss = 0.628616
Loss = 0.474579
Loss = 0.446274
Loss = 0.653748
Loss = 0.60968
Loss = 0.550262
Loss = 0.21553
Loss = 0.375076
Loss = 0.316864
Loss = 0.422943
Loss = 0.288254
Loss = 0.319656
Loss = 0.272995
Loss = 0.106506
Loss = 0.299637
Loss = 0.553192
TEST LOSS = 0.440221
TEST ACC = 506.479 % (8685/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.429062
Epoch 3.2: Loss = 0.527832
Epoch 3.3: Loss = 0.531204
Epoch 3.4: Loss = 0.364212
Epoch 3.5: Loss = 0.423264
Epoch 3.6: Loss = 0.398834
Epoch 3.7: Loss = 0.427689
Epoch 3.8: Loss = 0.424866
Epoch 3.9: Loss = 0.422241
Epoch 3.10: Loss = 0.447693
Epoch 3.11: Loss = 0.428055
Epoch 3.12: Loss = 0.409241
Epoch 3.13: Loss = 0.360306
Epoch 3.14: Loss = 0.403397
Epoch 3.15: Loss = 0.477386
Epoch 3.16: Loss = 0.480392
Epoch 3.17: Loss = 0.492142
Epoch 3.18: Loss = 0.638519
Epoch 3.19: Loss = 0.432449
Epoch 3.20: Loss = 0.401886
Epoch 3.21: Loss = 0.367416
Epoch 3.22: Loss = 0.345856
Epoch 3.23: Loss = 0.374725
Epoch 3.24: Loss = 0.557144
Epoch 3.25: Loss = 0.446045
Epoch 3.26: Loss = 0.582626
Epoch 3.27: Loss = 0.509766
Epoch 3.28: Loss = 0.500397
Epoch 3.29: Loss = 0.575531
Epoch 3.30: Loss = 0.644196
Epoch 3.31: Loss = 0.387268
Epoch 3.32: Loss = 0.506439
Epoch 3.33: Loss = 0.393219
Epoch 3.34: Loss = 0.521744
Epoch 3.35: Loss = 0.440338
Epoch 3.36: Loss = 0.527527
Epoch 3.37: Loss = 0.342606
Epoch 3.38: Loss = 0.404175
Epoch 3.39: Loss = 0.397736
Epoch 3.40: Loss = 0.3983
Epoch 3.41: Loss = 0.44252
Epoch 3.42: Loss = 0.554688
Epoch 3.43: Loss = 0.356033
Epoch 3.44: Loss = 0.338608
Epoch 3.45: Loss = 0.442719
Epoch 3.46: Loss = 0.463501
Epoch 3.47: Loss = 0.39621
Epoch 3.48: Loss = 0.472519
Epoch 3.49: Loss = 0.433121
Epoch 3.50: Loss = 0.53093
Epoch 3.51: Loss = 0.380295
Epoch 3.52: Loss = 0.353714
Epoch 3.53: Loss = 0.417572
Epoch 3.54: Loss = 0.523514
Epoch 3.55: Loss = 0.415436
Epoch 3.56: Loss = 0.407486
Epoch 3.57: Loss = 0.382233
Epoch 3.58: Loss = 0.451767
Epoch 3.59: Loss = 0.511185
Epoch 3.60: Loss = 0.550934
Epoch 3.61: Loss = 0.489456
Epoch 3.62: Loss = 0.522827
Epoch 3.63: Loss = 0.580048
Epoch 3.64: Loss = 0.561661
Epoch 3.65: Loss = 0.632645
Epoch 3.66: Loss = 0.46994
Epoch 3.67: Loss = 0.477875
Epoch 3.68: Loss = 0.294571
Epoch 3.69: Loss = 0.35083
Epoch 3.70: Loss = 0.527191
Epoch 3.71: Loss = 0.386185
Epoch 3.72: Loss = 0.376343
Epoch 3.73: Loss = 0.426529
Epoch 3.74: Loss = 0.324173
Epoch 3.75: Loss = 0.588165
Epoch 3.76: Loss = 0.456909
Epoch 3.77: Loss = 0.383102
Epoch 3.78: Loss = 0.41629
Epoch 3.79: Loss = 0.493088
Epoch 3.80: Loss = 0.500443
Epoch 3.81: Loss = 0.402588
Epoch 3.82: Loss = 0.334229
Epoch 3.83: Loss = 0.523788
Epoch 3.84: Loss = 0.423447
Epoch 3.85: Loss = 0.609634
Epoch 3.86: Loss = 0.517303
Epoch 3.87: Loss = 0.33577
Epoch 3.88: Loss = 0.408386
Epoch 3.89: Loss = 0.486938
Epoch 3.90: Loss = 0.370117
Epoch 3.91: Loss = 0.498108
Epoch 3.92: Loss = 0.471558
Epoch 3.93: Loss = 0.497437
Epoch 3.94: Loss = 0.313751
Epoch 3.95: Loss = 0.430313
Epoch 3.96: Loss = 0.497253
Epoch 3.97: Loss = 0.354782
Epoch 3.98: Loss = 0.412094
Epoch 3.99: Loss = 0.541397
Epoch 3.100: Loss = 0.574341
Epoch 3.101: Loss = 0.547516
Epoch 3.102: Loss = 0.43161
Epoch 3.103: Loss = 0.386108
Epoch 3.104: Loss = 0.336441
Epoch 3.105: Loss = 0.483795
Epoch 3.106: Loss = 0.54483
Epoch 3.107: Loss = 0.370697
Epoch 3.108: Loss = 0.437469
Epoch 3.109: Loss = 0.388321
Epoch 3.110: Loss = 0.442383
Epoch 3.111: Loss = 0.333588
Epoch 3.112: Loss = 0.342255
Epoch 3.113: Loss = 0.396408
Epoch 3.114: Loss = 0.334518
Epoch 3.115: Loss = 0.336395
Epoch 3.116: Loss = 0.402893
Epoch 3.117: Loss = 0.262314
Epoch 3.118: Loss = 0.236069
Epoch 3.119: Loss = 0.29248
Epoch 3.120: Loss = 0.31485
TRAIN LOSS = 0.44043
TRAIN ACC = 86.9232 % (52156/60000)
Loss = 0.421768
Loss = 0.516327
Loss = 0.580933
Loss = 0.589523
Loss = 0.622498
Loss = 0.428253
Loss = 0.379684
Loss = 0.626282
Loss = 0.569916
Loss = 0.506348
Loss = 0.178467
Loss = 0.328903
Loss = 0.298859
Loss = 0.407562
Loss = 0.262131
Loss = 0.305283
Loss = 0.248474
Loss = 0.065567
Loss = 0.254135
Loss = 0.549713
TEST LOSS = 0.407031
TEST ACC = 521.559 % (8845/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.399185
Epoch 4.2: Loss = 0.486099
Epoch 4.3: Loss = 0.483795
Epoch 4.4: Loss = 0.33873
Epoch 4.5: Loss = 0.389542
Epoch 4.6: Loss = 0.368881
Epoch 4.7: Loss = 0.374664
Epoch 4.8: Loss = 0.388596
Epoch 4.9: Loss = 0.377884
Epoch 4.10: Loss = 0.417206
Epoch 4.11: Loss = 0.410431
Epoch 4.12: Loss = 0.384979
Epoch 4.13: Loss = 0.319397
Epoch 4.14: Loss = 0.387955
Epoch 4.15: Loss = 0.443802
Epoch 4.16: Loss = 0.442169
Epoch 4.17: Loss = 0.471115
Epoch 4.18: Loss = 0.627121
Epoch 4.19: Loss = 0.427124
Epoch 4.20: Loss = 0.371735
Epoch 4.21: Loss = 0.351837
Epoch 4.22: Loss = 0.309906
Epoch 4.23: Loss = 0.3302
Epoch 4.24: Loss = 0.532669
Epoch 4.25: Loss = 0.435028
Epoch 4.26: Loss = 0.575653
Epoch 4.27: Loss = 0.50647
Epoch 4.28: Loss = 0.458527
Epoch 4.29: Loss = 0.565262
Epoch 4.30: Loss = 0.612854
Epoch 4.31: Loss = 0.360397
Epoch 4.32: Loss = 0.475861
Epoch 4.33: Loss = 0.372131
Epoch 4.34: Loss = 0.495743
Epoch 4.35: Loss = 0.415619
Epoch 4.36: Loss = 0.505066
Epoch 4.37: Loss = 0.31778
Epoch 4.38: Loss = 0.384155
Epoch 4.39: Loss = 0.365829
Epoch 4.40: Loss = 0.36351
Epoch 4.41: Loss = 0.436798
Epoch 4.42: Loss = 0.584747
Epoch 4.43: Loss = 0.345016
Epoch 4.44: Loss = 0.326736
Epoch 4.45: Loss = 0.426712
Epoch 4.46: Loss = 0.438354
Epoch 4.47: Loss = 0.382431
Epoch 4.48: Loss = 0.449768
Epoch 4.49: Loss = 0.412659
Epoch 4.50: Loss = 0.526764
Epoch 4.51: Loss = 0.362198
Epoch 4.52: Loss = 0.332855
Epoch 4.53: Loss = 0.38858
Epoch 4.54: Loss = 0.526154
Epoch 4.55: Loss = 0.384323
Epoch 4.56: Loss = 0.400589
Epoch 4.57: Loss = 0.355011
Epoch 4.58: Loss = 0.437225
Epoch 4.59: Loss = 0.483353
Epoch 4.60: Loss = 0.546005
Epoch 4.61: Loss = 0.456924
Epoch 4.62: Loss = 0.496536
Epoch 4.63: Loss = 0.574875
Epoch 4.64: Loss = 0.537704
Epoch 4.65: Loss = 0.622101
Epoch 4.66: Loss = 0.442017
Epoch 4.67: Loss = 0.429825
Epoch 4.68: Loss = 0.277023
Epoch 4.69: Loss = 0.336182
Epoch 4.70: Loss = 0.51503
Epoch 4.71: Loss = 0.364182
Epoch 4.72: Loss = 0.331421
Epoch 4.73: Loss = 0.408646
Epoch 4.74: Loss = 0.318954
Epoch 4.75: Loss = 0.613281
Epoch 4.76: Loss = 0.43631
Epoch 4.77: Loss = 0.34021
Epoch 4.78: Loss = 0.410675
Epoch 4.79: Loss = 0.501953
Epoch 4.80: Loss = 0.475708
Epoch 4.81: Loss = 0.390503
Epoch 4.82: Loss = 0.313583
Epoch 4.83: Loss = 0.512421
Epoch 4.84: Loss = 0.405975
Epoch 4.85: Loss = 0.610214
Epoch 4.86: Loss = 0.512817
Epoch 4.87: Loss = 0.315964
Epoch 4.88: Loss = 0.404037
Epoch 4.89: Loss = 0.480057
Epoch 4.90: Loss = 0.348526
Epoch 4.91: Loss = 0.498795
Epoch 4.92: Loss = 0.48056
Epoch 4.93: Loss = 0.509232
Epoch 4.94: Loss = 0.304443
Epoch 4.95: Loss = 0.42569
Epoch 4.96: Loss = 0.479568
Epoch 4.97: Loss = 0.336212
Epoch 4.98: Loss = 0.392563
Epoch 4.99: Loss = 0.528702
Epoch 4.100: Loss = 0.557678
Epoch 4.101: Loss = 0.548645
Epoch 4.102: Loss = 0.422043
Epoch 4.103: Loss = 0.372681
Epoch 4.104: Loss = 0.337585
Epoch 4.105: Loss = 0.487274
Epoch 4.106: Loss = 0.550598
Epoch 4.107: Loss = 0.355438
Epoch 4.108: Loss = 0.445679
Epoch 4.109: Loss = 0.38945
Epoch 4.110: Loss = 0.435608
Epoch 4.111: Loss = 0.333771
Epoch 4.112: Loss = 0.343719
Epoch 4.113: Loss = 0.388687
Epoch 4.114: Loss = 0.319519
Epoch 4.115: Loss = 0.316452
Epoch 4.116: Loss = 0.390121
Epoch 4.117: Loss = 0.244385
Epoch 4.118: Loss = 0.202545
Epoch 4.119: Loss = 0.294296
Epoch 4.120: Loss = 0.309433
TRAIN LOSS = 0.423141
TRAIN ACC = 87.9745 % (52787/60000)
Loss = 0.40004
Loss = 0.513
Loss = 0.571426
Loss = 0.578445
Loss = 0.624939
Loss = 0.418365
Loss = 0.365524
Loss = 0.647873
Loss = 0.567215
Loss = 0.522064
Loss = 0.166504
Loss = 0.333405
Loss = 0.279968
Loss = 0.399551
Loss = 0.226578
Loss = 0.335999
Loss = 0.221146
Loss = 0.0593262
Loss = 0.245712
Loss = 0.534332
TEST LOSS = 0.400571
TEST ACC = 527.869 % (8891/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.386597
Epoch 5.2: Loss = 0.499283
Epoch 5.3: Loss = 0.494034
Epoch 5.4: Loss = 0.330841
Epoch 5.5: Loss = 0.393173
Epoch 5.6: Loss = 0.356522
Epoch 5.7: Loss = 0.3517
Epoch 5.8: Loss = 0.381378
Epoch 5.9: Loss = 0.364212
Epoch 5.10: Loss = 0.403061
Epoch 5.11: Loss = 0.41217
Epoch 5.12: Loss = 0.374725
Epoch 5.13: Loss = 0.298645
Epoch 5.14: Loss = 0.3797
Epoch 5.15: Loss = 0.427078
Epoch 5.16: Loss = 0.446548
Epoch 5.17: Loss = 0.465073
Epoch 5.18: Loss = 0.629684
Epoch 5.19: Loss = 0.425125
Epoch 5.20: Loss = 0.357483
Epoch 5.21: Loss = 0.348404
Epoch 5.22: Loss = 0.298004
Epoch 5.23: Loss = 0.3181
Epoch 5.24: Loss = 0.537842
Epoch 5.25: Loss = 0.432602
Epoch 5.26: Loss = 0.592865
Epoch 5.27: Loss = 0.51236
Epoch 5.28: Loss = 0.462173
Epoch 5.29: Loss = 0.581329
Epoch 5.30: Loss = 0.60582
Epoch 5.31: Loss = 0.357681
Epoch 5.32: Loss = 0.475891
Epoch 5.33: Loss = 0.364014
Epoch 5.34: Loss = 0.489548
Epoch 5.35: Loss = 0.407379
Epoch 5.36: Loss = 0.48999
Epoch 5.37: Loss = 0.28891
Epoch 5.38: Loss = 0.381577
Epoch 5.39: Loss = 0.333298
Epoch 5.40: Loss = 0.359512
Epoch 5.41: Loss = 0.40976
Epoch 5.42: Loss = 0.603165
Epoch 5.43: Loss = 0.334641
Epoch 5.44: Loss = 0.310928
Epoch 5.45: Loss = 0.411453
Epoch 5.46: Loss = 0.432526
Epoch 5.47: Loss = 0.370087
Epoch 5.48: Loss = 0.452621
Epoch 5.49: Loss = 0.400284
Epoch 5.50: Loss = 0.516632
Epoch 5.51: Loss = 0.349411
Epoch 5.52: Loss = 0.325043
Epoch 5.53: Loss = 0.361954
Epoch 5.54: Loss = 0.510452
Epoch 5.55: Loss = 0.394211
Epoch 5.56: Loss = 0.376617
Epoch 5.57: Loss = 0.350632
Epoch 5.58: Loss = 0.429016
Epoch 5.59: Loss = 0.488739
Epoch 5.60: Loss = 0.531372
Epoch 5.61: Loss = 0.450165
Epoch 5.62: Loss = 0.495392
Epoch 5.63: Loss = 0.584732
Epoch 5.64: Loss = 0.509354
Epoch 5.65: Loss = 0.606903
Epoch 5.66: Loss = 0.403015
Epoch 5.67: Loss = 0.427216
Epoch 5.68: Loss = 0.264557
Epoch 5.69: Loss = 0.329025
Epoch 5.70: Loss = 0.498383
Epoch 5.71: Loss = 0.361725
Epoch 5.72: Loss = 0.309387
Epoch 5.73: Loss = 0.408417
Epoch 5.74: Loss = 0.317993
Epoch 5.75: Loss = 0.60643
Epoch 5.76: Loss = 0.432877
Epoch 5.77: Loss = 0.334045
Epoch 5.78: Loss = 0.426834
Epoch 5.79: Loss = 0.493866
Epoch 5.80: Loss = 0.469162
Epoch 5.81: Loss = 0.381836
Epoch 5.82: Loss = 0.300461
Epoch 5.83: Loss = 0.495468
Epoch 5.84: Loss = 0.390717
Epoch 5.85: Loss = 0.601181
Epoch 5.86: Loss = 0.512848
Epoch 5.87: Loss = 0.309891
Epoch 5.88: Loss = 0.398376
Epoch 5.89: Loss = 0.463684
Epoch 5.90: Loss = 0.329514
Epoch 5.91: Loss = 0.487671
Epoch 5.92: Loss = 0.491852
Epoch 5.93: Loss = 0.534637
Epoch 5.94: Loss = 0.308441
Epoch 5.95: Loss = 0.402084
Epoch 5.96: Loss = 0.476181
Epoch 5.97: Loss = 0.351044
Epoch 5.98: Loss = 0.385712
Epoch 5.99: Loss = 0.510605
Epoch 5.100: Loss = 0.561356
Epoch 5.101: Loss = 0.554871
Epoch 5.102: Loss = 0.417419
Epoch 5.103: Loss = 0.359375
Epoch 5.104: Loss = 0.331772
Epoch 5.105: Loss = 0.479309
Epoch 5.106: Loss = 0.529602
Epoch 5.107: Loss = 0.333344
Epoch 5.108: Loss = 0.447205
Epoch 5.109: Loss = 0.364456
Epoch 5.110: Loss = 0.419189
Epoch 5.111: Loss = 0.312714
Epoch 5.112: Loss = 0.334671
Epoch 5.113: Loss = 0.38913
Epoch 5.114: Loss = 0.305939
Epoch 5.115: Loss = 0.293564
Epoch 5.116: Loss = 0.384995
Epoch 5.117: Loss = 0.230545
Epoch 5.118: Loss = 0.195801
Epoch 5.119: Loss = 0.300659
Epoch 5.120: Loss = 0.325027
TRAIN LOSS = 0.415894
TRAIN ACC = 88.591 % (53157/60000)
Loss = 0.396591
Loss = 0.501236
Loss = 0.573425
Loss = 0.57341
Loss = 0.628128
Loss = 0.402267
Loss = 0.372467
Loss = 0.627747
Loss = 0.553009
Loss = 0.497833
Loss = 0.167694
Loss = 0.31427
Loss = 0.293274
Loss = 0.394104
Loss = 0.226959
Loss = 0.314972
Loss = 0.209396
Loss = 0.0475922
Loss = 0.227005
Loss = 0.508606
TEST LOSS = 0.391499
TEST ACC = 531.569 % (8954/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.349426
Epoch 6.2: Loss = 0.491653
Epoch 6.3: Loss = 0.491043
Epoch 6.4: Loss = 0.302322
Epoch 6.5: Loss = 0.400467
Epoch 6.6: Loss = 0.341354
Epoch 6.7: Loss = 0.348648
Epoch 6.8: Loss = 0.37149
Epoch 6.9: Loss = 0.345657
Epoch 6.10: Loss = 0.387787
Epoch 6.11: Loss = 0.406952
Epoch 6.12: Loss = 0.368134
Epoch 6.13: Loss = 0.317795
Epoch 6.14: Loss = 0.381454
Epoch 6.15: Loss = 0.407288
Epoch 6.16: Loss = 0.438431
Epoch 6.17: Loss = 0.449066
Epoch 6.18: Loss = 0.648224
Epoch 6.19: Loss = 0.429596
Epoch 6.20: Loss = 0.334503
Epoch 6.21: Loss = 0.362564
Epoch 6.22: Loss = 0.284363
Epoch 6.23: Loss = 0.300888
Epoch 6.24: Loss = 0.528488
Epoch 6.25: Loss = 0.405548
Epoch 6.26: Loss = 0.593002
Epoch 6.27: Loss = 0.489609
Epoch 6.28: Loss = 0.447067
Epoch 6.29: Loss = 0.57074
Epoch 6.30: Loss = 0.585205
Epoch 6.31: Loss = 0.352966
Epoch 6.32: Loss = 0.450241
Epoch 6.33: Loss = 0.35672
Epoch 6.34: Loss = 0.484573
Epoch 6.35: Loss = 0.380676
Epoch 6.36: Loss = 0.487869
Epoch 6.37: Loss = 0.277222
Epoch 6.38: Loss = 0.376999
Epoch 6.39: Loss = 0.328445
Epoch 6.40: Loss = 0.365509
Epoch 6.41: Loss = 0.391357
Epoch 6.42: Loss = 0.618759
Epoch 6.43: Loss = 0.329361
Epoch 6.44: Loss = 0.299576
Epoch 6.45: Loss = 0.398529
Epoch 6.46: Loss = 0.416946
Epoch 6.47: Loss = 0.3638
Epoch 6.48: Loss = 0.450043
Epoch 6.49: Loss = 0.384659
Epoch 6.50: Loss = 0.508835
Epoch 6.51: Loss = 0.326965
Epoch 6.52: Loss = 0.327103
Epoch 6.53: Loss = 0.355194
Epoch 6.54: Loss = 0.511322
Epoch 6.55: Loss = 0.369095
Epoch 6.56: Loss = 0.373566
Epoch 6.57: Loss = 0.363892
Epoch 6.58: Loss = 0.414825
Epoch 6.59: Loss = 0.457123
Epoch 6.60: Loss = 0.526855
Epoch 6.61: Loss = 0.433136
Epoch 6.62: Loss = 0.474243
Epoch 6.63: Loss = 0.590363
Epoch 6.64: Loss = 0.487823
Epoch 6.65: Loss = 0.597061
Epoch 6.66: Loss = 0.404724
Epoch 6.67: Loss = 0.413116
Epoch 6.68: Loss = 0.27095
Epoch 6.69: Loss = 0.331772
Epoch 6.70: Loss = 0.465515
Epoch 6.71: Loss = 0.356094
Epoch 6.72: Loss = 0.299332
Epoch 6.73: Loss = 0.400253
Epoch 6.74: Loss = 0.320023
Epoch 6.75: Loss = 0.61058
Epoch 6.76: Loss = 0.449829
Epoch 6.77: Loss = 0.334015
Epoch 6.78: Loss = 0.411072
Epoch 6.79: Loss = 0.495071
Epoch 6.80: Loss = 0.441376
Epoch 6.81: Loss = 0.366852
Epoch 6.82: Loss = 0.302292
Epoch 6.83: Loss = 0.505539
Epoch 6.84: Loss = 0.383072
Epoch 6.85: Loss = 0.58197
Epoch 6.86: Loss = 0.521652
Epoch 6.87: Loss = 0.305649
Epoch 6.88: Loss = 0.407104
Epoch 6.89: Loss = 0.465698
Epoch 6.90: Loss = 0.332703
Epoch 6.91: Loss = 0.500183
Epoch 6.92: Loss = 0.506317
Epoch 6.93: Loss = 0.51239
Epoch 6.94: Loss = 0.305939
Epoch 6.95: Loss = 0.402634
Epoch 6.96: Loss = 0.470566
Epoch 6.97: Loss = 0.339615
Epoch 6.98: Loss = 0.375
Epoch 6.99: Loss = 0.510468
Epoch 6.100: Loss = 0.595139
Epoch 6.101: Loss = 0.573257
Epoch 6.102: Loss = 0.39682
Epoch 6.103: Loss = 0.361801
Epoch 6.104: Loss = 0.348404
Epoch 6.105: Loss = 0.487213
Epoch 6.106: Loss = 0.559708
Epoch 6.107: Loss = 0.331161
Epoch 6.108: Loss = 0.474197
Epoch 6.109: Loss = 0.336838
Epoch 6.110: Loss = 0.419525
Epoch 6.111: Loss = 0.300934
Epoch 6.112: Loss = 0.343567
Epoch 6.113: Loss = 0.374298
Epoch 6.114: Loss = 0.316727
Epoch 6.115: Loss = 0.289429
Epoch 6.116: Loss = 0.391968
Epoch 6.117: Loss = 0.228241
Epoch 6.118: Loss = 0.203415
Epoch 6.119: Loss = 0.301941
Epoch 6.120: Loss = 0.319336
TRAIN LOSS = 0.410538
TRAIN ACC = 88.974 % (53387/60000)
Loss = 0.401443
Loss = 0.501846
Loss = 0.573166
Loss = 0.586685
Loss = 0.610977
Loss = 0.410553
Loss = 0.353012
Loss = 0.650467
Loss = 0.564667
Loss = 0.498886
Loss = 0.155029
Loss = 0.333878
Loss = 0.303696
Loss = 0.392532
Loss = 0.227859
Loss = 0.331497
Loss = 0.196014
Loss = 0.0510406
Loss = 0.227951
Loss = 0.499985
TEST LOSS = 0.393559
TEST ACC = 533.868 % (8983/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.362518
Epoch 7.2: Loss = 0.47731
Epoch 7.3: Loss = 0.488998
Epoch 7.4: Loss = 0.285721
Epoch 7.5: Loss = 0.378693
Epoch 7.6: Loss = 0.334946
Epoch 7.7: Loss = 0.32637
Epoch 7.8: Loss = 0.377243
Epoch 7.9: Loss = 0.357285
Epoch 7.10: Loss = 0.388306
Epoch 7.11: Loss = 0.425629
Epoch 7.12: Loss = 0.371094
Epoch 7.13: Loss = 0.307068
Epoch 7.14: Loss = 0.383575
Epoch 7.15: Loss = 0.401733
Epoch 7.16: Loss = 0.432037
Epoch 7.17: Loss = 0.443787
Epoch 7.18: Loss = 0.659439
Epoch 7.19: Loss = 0.443192
Epoch 7.20: Loss = 0.323914
Epoch 7.21: Loss = 0.354935
Epoch 7.22: Loss = 0.269623
Epoch 7.23: Loss = 0.30069
Epoch 7.24: Loss = 0.520752
Epoch 7.25: Loss = 0.397522
Epoch 7.26: Loss = 0.636169
Epoch 7.27: Loss = 0.499954
Epoch 7.28: Loss = 0.460403
Epoch 7.29: Loss = 0.579773
Epoch 7.30: Loss = 0.587662
Epoch 7.31: Loss = 0.34433
Epoch 7.32: Loss = 0.467438
Epoch 7.33: Loss = 0.353378
Epoch 7.34: Loss = 0.478287
Epoch 7.35: Loss = 0.394012
Epoch 7.36: Loss = 0.501556
Epoch 7.37: Loss = 0.27507
Epoch 7.38: Loss = 0.396652
Epoch 7.39: Loss = 0.328568
Epoch 7.40: Loss = 0.363724
Epoch 7.41: Loss = 0.391586
Epoch 7.42: Loss = 0.651001
Epoch 7.43: Loss = 0.323395
Epoch 7.44: Loss = 0.320862
Epoch 7.45: Loss = 0.39061
Epoch 7.46: Loss = 0.407333
Epoch 7.47: Loss = 0.382401
Epoch 7.48: Loss = 0.473633
Epoch 7.49: Loss = 0.379929
Epoch 7.50: Loss = 0.503693
Epoch 7.51: Loss = 0.330795
Epoch 7.52: Loss = 0.321686
Epoch 7.53: Loss = 0.350006
Epoch 7.54: Loss = 0.51709
Epoch 7.55: Loss = 0.372574
Epoch 7.56: Loss = 0.367035
Epoch 7.57: Loss = 0.371216
Epoch 7.58: Loss = 0.404251
Epoch 7.59: Loss = 0.460373
Epoch 7.60: Loss = 0.530884
Epoch 7.61: Loss = 0.420288
Epoch 7.62: Loss = 0.465057
Epoch 7.63: Loss = 0.588791
Epoch 7.64: Loss = 0.47554
Epoch 7.65: Loss = 0.620789
Epoch 7.66: Loss = 0.392838
Epoch 7.67: Loss = 0.411316
Epoch 7.68: Loss = 0.273071
Epoch 7.69: Loss = 0.319534
Epoch 7.70: Loss = 0.480576
Epoch 7.71: Loss = 0.367081
Epoch 7.72: Loss = 0.285858
Epoch 7.73: Loss = 0.393005
Epoch 7.74: Loss = 0.313416
Epoch 7.75: Loss = 0.651428
Epoch 7.76: Loss = 0.450287
Epoch 7.77: Loss = 0.358505
Epoch 7.78: Loss = 0.413254
Epoch 7.79: Loss = 0.496124
Epoch 7.80: Loss = 0.452377
Epoch 7.81: Loss = 0.36058
Epoch 7.82: Loss = 0.312912
Epoch 7.83: Loss = 0.499329
Epoch 7.84: Loss = 0.39325
Epoch 7.85: Loss = 0.587601
Epoch 7.86: Loss = 0.523575
Epoch 7.87: Loss = 0.292664
Epoch 7.88: Loss = 0.381912
Epoch 7.89: Loss = 0.459259
Epoch 7.90: Loss = 0.305878
Epoch 7.91: Loss = 0.525513
Epoch 7.92: Loss = 0.500504
Epoch 7.93: Loss = 0.54068
Epoch 7.94: Loss = 0.296371
Epoch 7.95: Loss = 0.404282
Epoch 7.96: Loss = 0.482956
Epoch 7.97: Loss = 0.320419
Epoch 7.98: Loss = 0.370819
Epoch 7.99: Loss = 0.487488
Epoch 7.100: Loss = 0.588608
Epoch 7.101: Loss = 0.561523
Epoch 7.102: Loss = 0.391174
Epoch 7.103: Loss = 0.345551
Epoch 7.104: Loss = 0.346832
Epoch 7.105: Loss = 0.492203
Epoch 7.106: Loss = 0.595657
Epoch 7.107: Loss = 0.331879
Epoch 7.108: Loss = 0.456879
Epoch 7.109: Loss = 0.327606
Epoch 7.110: Loss = 0.425262
Epoch 7.111: Loss = 0.317505
Epoch 7.112: Loss = 0.321655
Epoch 7.113: Loss = 0.364685
Epoch 7.114: Loss = 0.316803
Epoch 7.115: Loss = 0.278809
Epoch 7.116: Loss = 0.398026
Epoch 7.117: Loss = 0.216278
Epoch 7.118: Loss = 0.19899
Epoch 7.119: Loss = 0.292496
Epoch 7.120: Loss = 0.318649
TRAIN LOSS = 0.410767
TRAIN ACC = 89.3295 % (53600/60000)
Loss = 0.369522
Loss = 0.501221
Loss = 0.561005
Loss = 0.59079
Loss = 0.59201
Loss = 0.384766
Loss = 0.321686
Loss = 0.656372
Loss = 0.552185
Loss = 0.489349
Loss = 0.168793
Loss = 0.325134
Loss = 0.308151
Loss = 0.40242
Loss = 0.227066
Loss = 0.316925
Loss = 0.213013
Loss = 0.0536957
Loss = 0.221863
Loss = 0.512299
TEST LOSS = 0.388413
TEST ACC = 535.999 % (9027/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.353531
Epoch 8.2: Loss = 0.47908
Epoch 8.3: Loss = 0.461456
Epoch 8.4: Loss = 0.299088
Epoch 8.5: Loss = 0.369843
Epoch 8.6: Loss = 0.349991
Epoch 8.7: Loss = 0.331528
Epoch 8.8: Loss = 0.3638
Epoch 8.9: Loss = 0.370651
Epoch 8.10: Loss = 0.367737
Epoch 8.11: Loss = 0.416306
Epoch 8.12: Loss = 0.348022
Epoch 8.13: Loss = 0.297379
Epoch 8.14: Loss = 0.367798
Epoch 8.15: Loss = 0.402328
Epoch 8.16: Loss = 0.421448
Epoch 8.17: Loss = 0.444443
Epoch 8.18: Loss = 0.681625
Epoch 8.19: Loss = 0.449783
Epoch 8.20: Loss = 0.320679
Epoch 8.21: Loss = 0.327515
Epoch 8.22: Loss = 0.259018
Epoch 8.23: Loss = 0.302124
Epoch 8.24: Loss = 0.524292
Epoch 8.25: Loss = 0.427551
Epoch 8.26: Loss = 0.636612
Epoch 8.27: Loss = 0.514008
Epoch 8.28: Loss = 0.449036
Epoch 8.29: Loss = 0.585037
Epoch 8.30: Loss = 0.561646
Epoch 8.31: Loss = 0.337234
Epoch 8.32: Loss = 0.437683
Epoch 8.33: Loss = 0.339783
Epoch 8.34: Loss = 0.475677
Epoch 8.35: Loss = 0.398926
Epoch 8.36: Loss = 0.482758
Epoch 8.37: Loss = 0.265259
Epoch 8.38: Loss = 0.384064
Epoch 8.39: Loss = 0.322266
Epoch 8.40: Loss = 0.361252
Epoch 8.41: Loss = 0.370605
Epoch 8.42: Loss = 0.633911
Epoch 8.43: Loss = 0.333374
Epoch 8.44: Loss = 0.325104
Epoch 8.45: Loss = 0.38089
Epoch 8.46: Loss = 0.401398
Epoch 8.47: Loss = 0.382477
Epoch 8.48: Loss = 0.447067
Epoch 8.49: Loss = 0.367477
Epoch 8.50: Loss = 0.500687
Epoch 8.51: Loss = 0.332428
Epoch 8.52: Loss = 0.333557
Epoch 8.53: Loss = 0.346313
Epoch 8.54: Loss = 0.508652
Epoch 8.55: Loss = 0.375397
Epoch 8.56: Loss = 0.359406
Epoch 8.57: Loss = 0.359924
Epoch 8.58: Loss = 0.401077
Epoch 8.59: Loss = 0.467896
Epoch 8.60: Loss = 0.530334
Epoch 8.61: Loss = 0.404221
Epoch 8.62: Loss = 0.459839
Epoch 8.63: Loss = 0.610458
Epoch 8.64: Loss = 0.495285
Epoch 8.65: Loss = 0.642151
Epoch 8.66: Loss = 0.394638
Epoch 8.67: Loss = 0.419128
Epoch 8.68: Loss = 0.273544
Epoch 8.69: Loss = 0.332733
Epoch 8.70: Loss = 0.472275
Epoch 8.71: Loss = 0.364609
Epoch 8.72: Loss = 0.277008
Epoch 8.73: Loss = 0.393066
Epoch 8.74: Loss = 0.310562
Epoch 8.75: Loss = 0.646851
Epoch 8.76: Loss = 0.451843
Epoch 8.77: Loss = 0.347748
Epoch 8.78: Loss = 0.415192
Epoch 8.79: Loss = 0.506561
Epoch 8.80: Loss = 0.422714
Epoch 8.81: Loss = 0.360229
Epoch 8.82: Loss = 0.306396
Epoch 8.83: Loss = 0.48732
Epoch 8.84: Loss = 0.387238
Epoch 8.85: Loss = 0.545197
Epoch 8.86: Loss = 0.542725
Epoch 8.87: Loss = 0.287628
Epoch 8.88: Loss = 0.39267
Epoch 8.89: Loss = 0.443649
Epoch 8.90: Loss = 0.32132
Epoch 8.91: Loss = 0.522995
Epoch 8.92: Loss = 0.502869
Epoch 8.93: Loss = 0.545868
Epoch 8.94: Loss = 0.287109
Epoch 8.95: Loss = 0.375519
Epoch 8.96: Loss = 0.488983
Epoch 8.97: Loss = 0.340118
Epoch 8.98: Loss = 0.35701
Epoch 8.99: Loss = 0.470078
Epoch 8.100: Loss = 0.584442
Epoch 8.101: Loss = 0.563812
Epoch 8.102: Loss = 0.390244
Epoch 8.103: Loss = 0.343994
Epoch 8.104: Loss = 0.351074
Epoch 8.105: Loss = 0.48851
Epoch 8.106: Loss = 0.608582
Epoch 8.107: Loss = 0.322418
Epoch 8.108: Loss = 0.46907
Epoch 8.109: Loss = 0.330292
Epoch 8.110: Loss = 0.410416
Epoch 8.111: Loss = 0.303101
Epoch 8.112: Loss = 0.315765
Epoch 8.113: Loss = 0.360367
Epoch 8.114: Loss = 0.283356
Epoch 8.115: Loss = 0.270767
Epoch 8.116: Loss = 0.417313
Epoch 8.117: Loss = 0.217911
Epoch 8.118: Loss = 0.207809
Epoch 8.119: Loss = 0.269043
Epoch 8.120: Loss = 0.309372
TRAIN LOSS = 0.407257
TRAIN ACC = 89.6591 % (53798/60000)
Loss = 0.362411
Loss = 0.50444
Loss = 0.566223
Loss = 0.60228
Loss = 0.604065
Loss = 0.40332
Loss = 0.330414
Loss = 0.644745
Loss = 0.557022
Loss = 0.499924
Loss = 0.186127
Loss = 0.32782
Loss = 0.305557
Loss = 0.405746
Loss = 0.215912
Loss = 0.290741
Loss = 0.202957
Loss = 0.051651
Loss = 0.229584
Loss = 0.527802
TEST LOSS = 0.390937
TEST ACC = 537.979 % (9037/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.344131
Epoch 9.2: Loss = 0.484329
Epoch 9.3: Loss = 0.464218
Epoch 9.4: Loss = 0.302826
Epoch 9.5: Loss = 0.384583
Epoch 9.6: Loss = 0.343338
Epoch 9.7: Loss = 0.347458
Epoch 9.8: Loss = 0.379761
Epoch 9.9: Loss = 0.358475
Epoch 9.10: Loss = 0.361542
Epoch 9.11: Loss = 0.401779
Epoch 9.12: Loss = 0.355438
Epoch 9.13: Loss = 0.288086
Epoch 9.14: Loss = 0.373398
Epoch 9.15: Loss = 0.405869
Epoch 9.16: Loss = 0.420471
Epoch 9.17: Loss = 0.445618
Epoch 9.18: Loss = 0.678787
Epoch 9.19: Loss = 0.443832
Epoch 9.20: Loss = 0.323517
Epoch 9.21: Loss = 0.347168
Epoch 9.22: Loss = 0.273621
Epoch 9.23: Loss = 0.303665
Epoch 9.24: Loss = 0.538559
Epoch 9.25: Loss = 0.42067
Epoch 9.26: Loss = 0.623199
Epoch 9.27: Loss = 0.482635
Epoch 9.28: Loss = 0.423965
Epoch 9.29: Loss = 0.554306
Epoch 9.30: Loss = 0.564575
Epoch 9.31: Loss = 0.332993
Epoch 9.32: Loss = 0.450836
Epoch 9.33: Loss = 0.330887
Epoch 9.34: Loss = 0.46315
Epoch 9.35: Loss = 0.382507
Epoch 9.36: Loss = 0.480637
Epoch 9.37: Loss = 0.256668
Epoch 9.38: Loss = 0.391327
Epoch 9.39: Loss = 0.302979
Epoch 9.40: Loss = 0.391953
Epoch 9.41: Loss = 0.357605
Epoch 9.42: Loss = 0.63765
Epoch 9.43: Loss = 0.329056
Epoch 9.44: Loss = 0.334808
Epoch 9.45: Loss = 0.398636
Epoch 9.46: Loss = 0.405823
Epoch 9.47: Loss = 0.388443
Epoch 9.48: Loss = 0.444611
Epoch 9.49: Loss = 0.349869
Epoch 9.50: Loss = 0.497833
Epoch 9.51: Loss = 0.311813
Epoch 9.52: Loss = 0.341156
Epoch 9.53: Loss = 0.347076
Epoch 9.54: Loss = 0.52182
Epoch 9.55: Loss = 0.390411
Epoch 9.56: Loss = 0.370758
Epoch 9.57: Loss = 0.361084
Epoch 9.58: Loss = 0.378601
Epoch 9.59: Loss = 0.444977
Epoch 9.60: Loss = 0.517654
Epoch 9.61: Loss = 0.387283
Epoch 9.62: Loss = 0.454605
Epoch 9.63: Loss = 0.602798
Epoch 9.64: Loss = 0.4711
Epoch 9.65: Loss = 0.621719
Epoch 9.66: Loss = 0.398102
Epoch 9.67: Loss = 0.386887
Epoch 9.68: Loss = 0.265366
Epoch 9.69: Loss = 0.306229
Epoch 9.70: Loss = 0.466095
Epoch 9.71: Loss = 0.35881
Epoch 9.72: Loss = 0.262939
Epoch 9.73: Loss = 0.39418
Epoch 9.74: Loss = 0.29776
Epoch 9.75: Loss = 0.675858
Epoch 9.76: Loss = 0.444778
Epoch 9.77: Loss = 0.334793
Epoch 9.78: Loss = 0.408783
Epoch 9.79: Loss = 0.481949
Epoch 9.80: Loss = 0.417786
Epoch 9.81: Loss = 0.357849
Epoch 9.82: Loss = 0.311707
Epoch 9.83: Loss = 0.497223
Epoch 9.84: Loss = 0.377426
Epoch 9.85: Loss = 0.562012
Epoch 9.86: Loss = 0.548203
Epoch 9.87: Loss = 0.286331
Epoch 9.88: Loss = 0.37941
Epoch 9.89: Loss = 0.454788
Epoch 9.90: Loss = 0.327911
Epoch 9.91: Loss = 0.501236
Epoch 9.92: Loss = 0.504257
Epoch 9.93: Loss = 0.534805
Epoch 9.94: Loss = 0.268082
Epoch 9.95: Loss = 0.380966
Epoch 9.96: Loss = 0.480621
Epoch 9.97: Loss = 0.342255
Epoch 9.98: Loss = 0.338791
Epoch 9.99: Loss = 0.453903
Epoch 9.100: Loss = 0.569412
Epoch 9.101: Loss = 0.570221
Epoch 9.102: Loss = 0.379425
Epoch 9.103: Loss = 0.361267
Epoch 9.104: Loss = 0.35051
Epoch 9.105: Loss = 0.473083
Epoch 9.106: Loss = 0.600494
Epoch 9.107: Loss = 0.310745
Epoch 9.108: Loss = 0.446472
Epoch 9.109: Loss = 0.319183
Epoch 9.110: Loss = 0.40097
Epoch 9.111: Loss = 0.299118
Epoch 9.112: Loss = 0.306137
Epoch 9.113: Loss = 0.350113
Epoch 9.114: Loss = 0.25708
Epoch 9.115: Loss = 0.25856
Epoch 9.116: Loss = 0.407028
Epoch 9.117: Loss = 0.218781
Epoch 9.118: Loss = 0.196869
Epoch 9.119: Loss = 0.274139
Epoch 9.120: Loss = 0.298325
TRAIN LOSS = 0.402878
TRAIN ACC = 89.7491 % (53852/60000)
Loss = 0.359268
Loss = 0.4814
Loss = 0.544632
Loss = 0.592484
Loss = 0.59671
Loss = 0.399567
Loss = 0.316772
Loss = 0.660919
Loss = 0.530869
Loss = 0.4711
Loss = 0.202103
Loss = 0.299881
Loss = 0.297394
Loss = 0.399979
Loss = 0.192062
Loss = 0.281311
Loss = 0.179291
Loss = 0.0440063
Loss = 0.220337
Loss = 0.519806
TEST LOSS = 0.379494
TEST ACC = 538.519 % (9040/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.325714
Epoch 10.2: Loss = 0.47522
Epoch 10.3: Loss = 0.471237
Epoch 10.4: Loss = 0.300537
Epoch 10.5: Loss = 0.389542
Epoch 10.6: Loss = 0.330902
Epoch 10.7: Loss = 0.337738
Epoch 10.8: Loss = 0.355911
Epoch 10.9: Loss = 0.35463
Epoch 10.10: Loss = 0.360992
Epoch 10.11: Loss = 0.418198
Epoch 10.12: Loss = 0.346375
Epoch 10.13: Loss = 0.293671
Epoch 10.14: Loss = 0.37381
Epoch 10.15: Loss = 0.395187
Epoch 10.16: Loss = 0.425568
Epoch 10.17: Loss = 0.448868
Epoch 10.18: Loss = 0.670486
Epoch 10.19: Loss = 0.440445
Epoch 10.20: Loss = 0.314331
Epoch 10.21: Loss = 0.332062
Epoch 10.22: Loss = 0.271301
Epoch 10.23: Loss = 0.317017
Epoch 10.24: Loss = 0.564133
Epoch 10.25: Loss = 0.419037
Epoch 10.26: Loss = 0.650833
Epoch 10.27: Loss = 0.495941
Epoch 10.28: Loss = 0.427719
Epoch 10.29: Loss = 0.546066
Epoch 10.30: Loss = 0.5401
Epoch 10.31: Loss = 0.341583
Epoch 10.32: Loss = 0.452942
Epoch 10.33: Loss = 0.34671
Epoch 10.34: Loss = 0.470291
Epoch 10.35: Loss = 0.39418
Epoch 10.36: Loss = 0.476227
Epoch 10.37: Loss = 0.268524
Epoch 10.38: Loss = 0.369888
Epoch 10.39: Loss = 0.301331
Epoch 10.40: Loss = 0.385178
Epoch 10.41: Loss = 0.368469
Epoch 10.42: Loss = 0.647797
Epoch 10.43: Loss = 0.310242
Epoch 10.44: Loss = 0.338074
Epoch 10.45: Loss = 0.389374
Epoch 10.46: Loss = 0.412643
Epoch 10.47: Loss = 0.365524
Epoch 10.48: Loss = 0.452805
Epoch 10.49: Loss = 0.358612
Epoch 10.50: Loss = 0.497711
Epoch 10.51: Loss = 0.305557
Epoch 10.52: Loss = 0.322388
Epoch 10.53: Loss = 0.345551
Epoch 10.54: Loss = 0.522522
Epoch 10.55: Loss = 0.387421
Epoch 10.56: Loss = 0.385468
Epoch 10.57: Loss = 0.370499
Epoch 10.58: Loss = 0.39212
Epoch 10.59: Loss = 0.44577
Epoch 10.60: Loss = 0.510437
Epoch 10.61: Loss = 0.393661
Epoch 10.62: Loss = 0.451752
Epoch 10.63: Loss = 0.612411
Epoch 10.64: Loss = 0.496872
Epoch 10.65: Loss = 0.660782
Epoch 10.66: Loss = 0.390472
Epoch 10.67: Loss = 0.371277
Epoch 10.68: Loss = 0.270096
Epoch 10.69: Loss = 0.300735
Epoch 10.70: Loss = 0.472717
Epoch 10.71: Loss = 0.373566
Epoch 10.72: Loss = 0.259232
Epoch 10.73: Loss = 0.387741
Epoch 10.74: Loss = 0.298782
Epoch 10.75: Loss = 0.674988
Epoch 10.76: Loss = 0.432434
Epoch 10.77: Loss = 0.317123
Epoch 10.78: Loss = 0.400314
Epoch 10.79: Loss = 0.49173
Epoch 10.80: Loss = 0.419479
Epoch 10.81: Loss = 0.364639
Epoch 10.82: Loss = 0.304108
Epoch 10.83: Loss = 0.498001
Epoch 10.84: Loss = 0.387726
Epoch 10.85: Loss = 0.558548
Epoch 10.86: Loss = 0.534943
Epoch 10.87: Loss = 0.287186
Epoch 10.88: Loss = 0.389603
Epoch 10.89: Loss = 0.46019
Epoch 10.90: Loss = 0.318344
Epoch 10.91: Loss = 0.488388
Epoch 10.92: Loss = 0.502335
Epoch 10.93: Loss = 0.5681
Epoch 10.94: Loss = 0.275131
Epoch 10.95: Loss = 0.388657
Epoch 10.96: Loss = 0.458969
Epoch 10.97: Loss = 0.352051
Epoch 10.98: Loss = 0.351379
Epoch 10.99: Loss = 0.458023
Epoch 10.100: Loss = 0.595078
Epoch 10.101: Loss = 0.584991
Epoch 10.102: Loss = 0.375992
Epoch 10.103: Loss = 0.376343
Epoch 10.104: Loss = 0.354904
Epoch 10.105: Loss = 0.489059
Epoch 10.106: Loss = 0.576935
Epoch 10.107: Loss = 0.304352
Epoch 10.108: Loss = 0.456741
Epoch 10.109: Loss = 0.319489
Epoch 10.110: Loss = 0.412003
Epoch 10.111: Loss = 0.299072
Epoch 10.112: Loss = 0.322891
Epoch 10.113: Loss = 0.351929
Epoch 10.114: Loss = 0.26857
Epoch 10.115: Loss = 0.271484
Epoch 10.116: Loss = 0.410553
Epoch 10.117: Loss = 0.229294
Epoch 10.118: Loss = 0.182922
Epoch 10.119: Loss = 0.272064
Epoch 10.120: Loss = 0.317703
TRAIN LOSS = 0.404358
TRAIN ACC = 89.8483 % (53911/60000)
Loss = 0.363419
Loss = 0.484116
Loss = 0.576385
Loss = 0.600403
Loss = 0.606995
Loss = 0.394257
Loss = 0.308823
Loss = 0.674606
Loss = 0.558136
Loss = 0.475677
Loss = 0.188995
Loss = 0.318283
Loss = 0.325821
Loss = 0.405319
Loss = 0.201248
Loss = 0.283783
Loss = 0.196808
Loss = 0.051239
Loss = 0.246567
Loss = 0.56041
TEST LOSS = 0.391064
TEST ACC = 539.11 % (9035/10000)
