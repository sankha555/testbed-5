Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 4
***********************************************************
Epoch 1.1: Loss = 2.35521
Epoch 1.2: Loss = 2.31282
Epoch 1.3: Loss = 2.27296
Epoch 1.4: Loss = 2.23071
Epoch 1.5: Loss = 2.17534
Epoch 1.6: Loss = 2.1628
Epoch 1.7: Loss = 2.16046
Epoch 1.8: Loss = 2.0842
Epoch 1.9: Loss = 2.02541
Epoch 1.10: Loss = 1.97171
Epoch 1.11: Loss = 1.94421
Epoch 1.12: Loss = 1.92633
Epoch 1.13: Loss = 1.86751
Epoch 1.14: Loss = 1.88078
Epoch 1.15: Loss = 1.91132
Epoch 1.16: Loss = 1.83562
Epoch 1.17: Loss = 1.78746
Epoch 1.18: Loss = 1.78726
Epoch 1.19: Loss = 1.71312
Epoch 1.20: Loss = 1.68602
Epoch 1.21: Loss = 1.62201
Epoch 1.22: Loss = 1.64592
Epoch 1.23: Loss = 1.5583
Epoch 1.24: Loss = 1.60706
Epoch 1.25: Loss = 1.54677
Epoch 1.26: Loss = 1.54619
Epoch 1.27: Loss = 1.49124
Epoch 1.28: Loss = 1.46301
Epoch 1.29: Loss = 1.4827
Epoch 1.30: Loss = 1.52565
Epoch 1.31: Loss = 1.3985
Epoch 1.32: Loss = 1.43535
Epoch 1.33: Loss = 1.35422
Epoch 1.34: Loss = 1.4193
Epoch 1.35: Loss = 1.34668
Epoch 1.36: Loss = 1.41077
Epoch 1.37: Loss = 1.3045
Epoch 1.38: Loss = 1.27646
Epoch 1.39: Loss = 1.25505
Epoch 1.40: Loss = 1.15164
Epoch 1.41: Loss = 1.2083
Epoch 1.42: Loss = 1.20427
Epoch 1.43: Loss = 1.1207
Epoch 1.44: Loss = 1.07582
Epoch 1.45: Loss = 1.16599
Epoch 1.46: Loss = 1.15421
Epoch 1.47: Loss = 1.0519
Epoch 1.48: Loss = 1.1149
Epoch 1.49: Loss = 1.05702
Epoch 1.50: Loss = 1.15224
Epoch 1.51: Loss = 0.985321
Epoch 1.52: Loss = 0.976654
Epoch 1.53: Loss = 1.02309
Epoch 1.54: Loss = 1.05988
Epoch 1.55: Loss = 1.0229
Epoch 1.56: Loss = 0.955353
Epoch 1.57: Loss = 0.904526
Epoch 1.58: Loss = 0.920273
Epoch 1.59: Loss = 0.959259
Epoch 1.60: Loss = 1.04425
Epoch 1.61: Loss = 0.985519
Epoch 1.62: Loss = 1.00615
Epoch 1.63: Loss = 1.01878
Epoch 1.64: Loss = 0.990509
Epoch 1.65: Loss = 1.02591
Epoch 1.66: Loss = 0.901321
Epoch 1.67: Loss = 0.947021
Epoch 1.68: Loss = 0.775177
Epoch 1.69: Loss = 0.836914
Epoch 1.70: Loss = 0.926926
Epoch 1.71: Loss = 0.869827
Epoch 1.72: Loss = 0.833176
Epoch 1.73: Loss = 0.845016
Epoch 1.74: Loss = 0.737991
Epoch 1.75: Loss = 0.863968
Epoch 1.76: Loss = 0.853394
Epoch 1.77: Loss = 0.798935
Epoch 1.78: Loss = 0.749237
Epoch 1.79: Loss = 0.806641
Epoch 1.80: Loss = 0.871872
Epoch 1.81: Loss = 0.725372
Epoch 1.82: Loss = 0.7155
Epoch 1.83: Loss = 0.87706
Epoch 1.84: Loss = 0.815125
Epoch 1.85: Loss = 0.876511
Epoch 1.86: Loss = 0.750778
Epoch 1.87: Loss = 0.687866
Epoch 1.88: Loss = 0.731705
Epoch 1.89: Loss = 0.847107
Epoch 1.90: Loss = 0.646683
Epoch 1.91: Loss = 0.752121
Epoch 1.92: Loss = 0.763428
Epoch 1.93: Loss = 0.795609
Epoch 1.94: Loss = 0.626572
Epoch 1.95: Loss = 0.747635
Epoch 1.96: Loss = 0.690262
Epoch 1.97: Loss = 0.58606
Epoch 1.98: Loss = 0.668869
Epoch 1.99: Loss = 0.782547
Epoch 1.100: Loss = 0.818192
Epoch 1.101: Loss = 0.728683
Epoch 1.102: Loss = 0.68515
Epoch 1.103: Loss = 0.611633
Epoch 1.104: Loss = 0.593002
Epoch 1.105: Loss = 0.739761
Epoch 1.106: Loss = 0.6996
Epoch 1.107: Loss = 0.601974
Epoch 1.108: Loss = 0.678604
Epoch 1.109: Loss = 0.642288
Epoch 1.110: Loss = 0.666077
Epoch 1.111: Loss = 0.55867
Epoch 1.112: Loss = 0.578369
Epoch 1.113: Loss = 0.638733
Epoch 1.114: Loss = 0.560257
Epoch 1.115: Loss = 0.61647
Epoch 1.116: Loss = 0.624191
Epoch 1.117: Loss = 0.524521
Epoch 1.118: Loss = 0.451294
Epoch 1.119: Loss = 0.454651
Epoch 1.120: Loss = 0.535095
TRAIN LOSS = 1.12358
TRAIN ACC = 68.2831 % (40972/60000)
Loss = 0.658325
Loss = 0.656967
Loss = 0.810287
Loss = 0.736069
Loss = 0.78093
Loss = 0.649216
Loss = 0.637054
Loss = 0.778107
Loss = 0.755585
Loss = 0.7146
Loss = 0.395691
Loss = 0.552094
Loss = 0.391983
Loss = 0.638809
Loss = 0.518478
Loss = 0.485382
Loss = 0.48201
Loss = 0.27803
Loss = 0.492569
Loss = 0.733383
TEST LOSS = 0.607278
TEST ACC = 409.72 % (8207/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.595886
Epoch 2.2: Loss = 0.707336
Epoch 2.3: Loss = 0.672318
Epoch 2.4: Loss = 0.554092
Epoch 2.5: Loss = 0.56929
Epoch 2.6: Loss = 0.559967
Epoch 2.7: Loss = 0.612091
Epoch 2.8: Loss = 0.596237
Epoch 2.9: Loss = 0.583328
Epoch 2.10: Loss = 0.586182
Epoch 2.11: Loss = 0.57402
Epoch 2.12: Loss = 0.57666
Epoch 2.13: Loss = 0.539825
Epoch 2.14: Loss = 0.576248
Epoch 2.15: Loss = 0.712067
Epoch 2.16: Loss = 0.671005
Epoch 2.17: Loss = 0.650864
Epoch 2.18: Loss = 0.694244
Epoch 2.19: Loss = 0.536819
Epoch 2.20: Loss = 0.538498
Epoch 2.21: Loss = 0.480743
Epoch 2.22: Loss = 0.506393
Epoch 2.23: Loss = 0.517227
Epoch 2.24: Loss = 0.660751
Epoch 2.25: Loss = 0.581711
Epoch 2.26: Loss = 0.649048
Epoch 2.27: Loss = 0.662918
Epoch 2.28: Loss = 0.632538
Epoch 2.29: Loss = 0.67247
Epoch 2.30: Loss = 0.722504
Epoch 2.31: Loss = 0.496704
Epoch 2.32: Loss = 0.690338
Epoch 2.33: Loss = 0.566162
Epoch 2.34: Loss = 0.644684
Epoch 2.35: Loss = 0.628021
Epoch 2.36: Loss = 0.680237
Epoch 2.37: Loss = 0.483292
Epoch 2.38: Loss = 0.481598
Epoch 2.39: Loss = 0.542694
Epoch 2.40: Loss = 0.482178
Epoch 2.41: Loss = 0.582916
Epoch 2.42: Loss = 0.630035
Epoch 2.43: Loss = 0.501648
Epoch 2.44: Loss = 0.442383
Epoch 2.45: Loss = 0.56218
Epoch 2.46: Loss = 0.629852
Epoch 2.47: Loss = 0.510849
Epoch 2.48: Loss = 0.602539
Epoch 2.49: Loss = 0.541138
Epoch 2.50: Loss = 0.634521
Epoch 2.51: Loss = 0.507339
Epoch 2.52: Loss = 0.448853
Epoch 2.53: Loss = 0.516083
Epoch 2.54: Loss = 0.623581
Epoch 2.55: Loss = 0.522797
Epoch 2.56: Loss = 0.517792
Epoch 2.57: Loss = 0.483185
Epoch 2.58: Loss = 0.531082
Epoch 2.59: Loss = 0.556763
Epoch 2.60: Loss = 0.624222
Epoch 2.61: Loss = 0.636719
Epoch 2.62: Loss = 0.650894
Epoch 2.63: Loss = 0.668472
Epoch 2.64: Loss = 0.623093
Epoch 2.65: Loss = 0.695496
Epoch 2.66: Loss = 0.528595
Epoch 2.67: Loss = 0.58522
Epoch 2.68: Loss = 0.381897
Epoch 2.69: Loss = 0.478729
Epoch 2.70: Loss = 0.650406
Epoch 2.71: Loss = 0.497055
Epoch 2.72: Loss = 0.501221
Epoch 2.73: Loss = 0.526367
Epoch 2.74: Loss = 0.425323
Epoch 2.75: Loss = 0.620209
Epoch 2.76: Loss = 0.567688
Epoch 2.77: Loss = 0.473938
Epoch 2.78: Loss = 0.471756
Epoch 2.79: Loss = 0.562805
Epoch 2.80: Loss = 0.627045
Epoch 2.81: Loss = 0.483597
Epoch 2.82: Loss = 0.442963
Epoch 2.83: Loss = 0.633575
Epoch 2.84: Loss = 0.563614
Epoch 2.85: Loss = 0.67955
Epoch 2.86: Loss = 0.547989
Epoch 2.87: Loss = 0.450729
Epoch 2.88: Loss = 0.503754
Epoch 2.89: Loss = 0.668381
Epoch 2.90: Loss = 0.429871
Epoch 2.91: Loss = 0.544952
Epoch 2.92: Loss = 0.577408
Epoch 2.93: Loss = 0.613251
Epoch 2.94: Loss = 0.4366
Epoch 2.95: Loss = 0.54834
Epoch 2.96: Loss = 0.552155
Epoch 2.97: Loss = 0.384384
Epoch 2.98: Loss = 0.473801
Epoch 2.99: Loss = 0.590561
Epoch 2.100: Loss = 0.648529
Epoch 2.101: Loss = 0.595657
Epoch 2.102: Loss = 0.508301
Epoch 2.103: Loss = 0.466599
Epoch 2.104: Loss = 0.41951
Epoch 2.105: Loss = 0.598236
Epoch 2.106: Loss = 0.555466
Epoch 2.107: Loss = 0.444138
Epoch 2.108: Loss = 0.53389
Epoch 2.109: Loss = 0.481308
Epoch 2.110: Loss = 0.528412
Epoch 2.111: Loss = 0.403046
Epoch 2.112: Loss = 0.423599
Epoch 2.113: Loss = 0.500229
Epoch 2.114: Loss = 0.421524
Epoch 2.115: Loss = 0.464447
Epoch 2.116: Loss = 0.474792
Epoch 2.117: Loss = 0.371399
Epoch 2.118: Loss = 0.298676
Epoch 2.119: Loss = 0.372955
Epoch 2.120: Loss = 0.40744
TRAIN LOSS = 0.550644
TRAIN ACC = 83.3878 % (50035/60000)
Loss = 0.510468
Loss = 0.553497
Loss = 0.64183
Loss = 0.624405
Loss = 0.658707
Loss = 0.50914
Loss = 0.481003
Loss = 0.649002
Loss = 0.6203
Loss = 0.597519
Loss = 0.273468
Loss = 0.447571
Loss = 0.315155
Loss = 0.49498
Loss = 0.37912
Loss = 0.393631
Loss = 0.366043
Loss = 0.158447
Loss = 0.346298
Loss = 0.66304
TEST LOSS = 0.484181
TEST ACC = 500.349 % (8497/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.469818
Epoch 3.2: Loss = 0.584427
Epoch 3.3: Loss = 0.547516
Epoch 3.4: Loss = 0.414383
Epoch 3.5: Loss = 0.446167
Epoch 3.6: Loss = 0.455765
Epoch 3.7: Loss = 0.469269
Epoch 3.8: Loss = 0.473511
Epoch 3.9: Loss = 0.443237
Epoch 3.10: Loss = 0.499298
Epoch 3.11: Loss = 0.477188
Epoch 3.12: Loss = 0.470947
Epoch 3.13: Loss = 0.440094
Epoch 3.14: Loss = 0.454941
Epoch 3.15: Loss = 0.59433
Epoch 3.16: Loss = 0.564713
Epoch 3.17: Loss = 0.563431
Epoch 3.18: Loss = 0.644897
Epoch 3.19: Loss = 0.444855
Epoch 3.20: Loss = 0.4422
Epoch 3.21: Loss = 0.398087
Epoch 3.22: Loss = 0.387238
Epoch 3.23: Loss = 0.442093
Epoch 3.24: Loss = 0.596359
Epoch 3.25: Loss = 0.496445
Epoch 3.26: Loss = 0.581879
Epoch 3.27: Loss = 0.580841
Epoch 3.28: Loss = 0.539764
Epoch 3.29: Loss = 0.616776
Epoch 3.30: Loss = 0.616928
Epoch 3.31: Loss = 0.396042
Epoch 3.32: Loss = 0.617584
Epoch 3.33: Loss = 0.471741
Epoch 3.34: Loss = 0.556946
Epoch 3.35: Loss = 0.563583
Epoch 3.36: Loss = 0.560043
Epoch 3.37: Loss = 0.374878
Epoch 3.38: Loss = 0.399734
Epoch 3.39: Loss = 0.485535
Epoch 3.40: Loss = 0.463318
Epoch 3.41: Loss = 0.487762
Epoch 3.42: Loss = 0.583084
Epoch 3.43: Loss = 0.437378
Epoch 3.44: Loss = 0.358932
Epoch 3.45: Loss = 0.504532
Epoch 3.46: Loss = 0.563492
Epoch 3.47: Loss = 0.487411
Epoch 3.48: Loss = 0.48793
Epoch 3.49: Loss = 0.501404
Epoch 3.50: Loss = 0.582581
Epoch 3.51: Loss = 0.449402
Epoch 3.52: Loss = 0.366882
Epoch 3.53: Loss = 0.447159
Epoch 3.54: Loss = 0.561493
Epoch 3.55: Loss = 0.470581
Epoch 3.56: Loss = 0.464203
Epoch 3.57: Loss = 0.452591
Epoch 3.58: Loss = 0.469269
Epoch 3.59: Loss = 0.535065
Epoch 3.60: Loss = 0.556244
Epoch 3.61: Loss = 0.581787
Epoch 3.62: Loss = 0.584488
Epoch 3.63: Loss = 0.622589
Epoch 3.64: Loss = 0.569672
Epoch 3.65: Loss = 0.649887
Epoch 3.66: Loss = 0.463608
Epoch 3.67: Loss = 0.512833
Epoch 3.68: Loss = 0.321014
Epoch 3.69: Loss = 0.424316
Epoch 3.70: Loss = 0.624344
Epoch 3.71: Loss = 0.445099
Epoch 3.72: Loss = 0.445145
Epoch 3.73: Loss = 0.49942
Epoch 3.74: Loss = 0.374817
Epoch 3.75: Loss = 0.647446
Epoch 3.76: Loss = 0.528625
Epoch 3.77: Loss = 0.418655
Epoch 3.78: Loss = 0.436066
Epoch 3.79: Loss = 0.540726
Epoch 3.80: Loss = 0.573624
Epoch 3.81: Loss = 0.398972
Epoch 3.82: Loss = 0.387543
Epoch 3.83: Loss = 0.584503
Epoch 3.84: Loss = 0.522064
Epoch 3.85: Loss = 0.65538
Epoch 3.86: Loss = 0.520844
Epoch 3.87: Loss = 0.390366
Epoch 3.88: Loss = 0.461227
Epoch 3.89: Loss = 0.602493
Epoch 3.90: Loss = 0.394241
Epoch 3.91: Loss = 0.50975
Epoch 3.92: Loss = 0.541321
Epoch 3.93: Loss = 0.567291
Epoch 3.94: Loss = 0.385452
Epoch 3.95: Loss = 0.484787
Epoch 3.96: Loss = 0.537064
Epoch 3.97: Loss = 0.370575
Epoch 3.98: Loss = 0.432526
Epoch 3.99: Loss = 0.53157
Epoch 3.100: Loss = 0.625626
Epoch 3.101: Loss = 0.5858
Epoch 3.102: Loss = 0.447723
Epoch 3.103: Loss = 0.408478
Epoch 3.104: Loss = 0.385834
Epoch 3.105: Loss = 0.584534
Epoch 3.106: Loss = 0.532288
Epoch 3.107: Loss = 0.397095
Epoch 3.108: Loss = 0.500549
Epoch 3.109: Loss = 0.444992
Epoch 3.110: Loss = 0.481125
Epoch 3.111: Loss = 0.370193
Epoch 3.112: Loss = 0.367676
Epoch 3.113: Loss = 0.467514
Epoch 3.114: Loss = 0.39241
Epoch 3.115: Loss = 0.399597
Epoch 3.116: Loss = 0.448242
Epoch 3.117: Loss = 0.301437
Epoch 3.118: Loss = 0.25325
Epoch 3.119: Loss = 0.344528
Epoch 3.120: Loss = 0.378754
TRAIN LOSS = 0.487289
TRAIN ACC = 85.1898 % (51116/60000)
Loss = 0.473938
Loss = 0.518692
Loss = 0.602386
Loss = 0.625946
Loss = 0.62648
Loss = 0.483307
Loss = 0.447266
Loss = 0.624405
Loss = 0.568649
Loss = 0.547302
Loss = 0.234222
Loss = 0.400589
Loss = 0.291351
Loss = 0.440811
Loss = 0.333984
Loss = 0.328766
Loss = 0.313843
Loss = 0.10994
Loss = 0.29512
Loss = 0.601196
TEST LOSS = 0.44341
TEST ACC = 511.159 % (8668/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.432709
Epoch 4.2: Loss = 0.554398
Epoch 4.3: Loss = 0.541824
Epoch 4.4: Loss = 0.400131
Epoch 4.5: Loss = 0.428879
Epoch 4.6: Loss = 0.428574
Epoch 4.7: Loss = 0.444519
Epoch 4.8: Loss = 0.452942
Epoch 4.9: Loss = 0.399506
Epoch 4.10: Loss = 0.460617
Epoch 4.11: Loss = 0.456299
Epoch 4.12: Loss = 0.445282
Epoch 4.13: Loss = 0.411652
Epoch 4.14: Loss = 0.414337
Epoch 4.15: Loss = 0.510742
Epoch 4.16: Loss = 0.534668
Epoch 4.17: Loss = 0.519592
Epoch 4.18: Loss = 0.642944
Epoch 4.19: Loss = 0.429825
Epoch 4.20: Loss = 0.402924
Epoch 4.21: Loss = 0.37854
Epoch 4.22: Loss = 0.351379
Epoch 4.23: Loss = 0.398148
Epoch 4.24: Loss = 0.569214
Epoch 4.25: Loss = 0.460266
Epoch 4.26: Loss = 0.581787
Epoch 4.27: Loss = 0.539673
Epoch 4.28: Loss = 0.524353
Epoch 4.29: Loss = 0.629761
Epoch 4.30: Loss = 0.623764
Epoch 4.31: Loss = 0.375458
Epoch 4.32: Loss = 0.567627
Epoch 4.33: Loss = 0.437195
Epoch 4.34: Loss = 0.555054
Epoch 4.35: Loss = 0.541153
Epoch 4.36: Loss = 0.533524
Epoch 4.37: Loss = 0.346313
Epoch 4.38: Loss = 0.38623
Epoch 4.39: Loss = 0.455902
Epoch 4.40: Loss = 0.421738
Epoch 4.41: Loss = 0.447815
Epoch 4.42: Loss = 0.588913
Epoch 4.43: Loss = 0.414413
Epoch 4.44: Loss = 0.340881
Epoch 4.45: Loss = 0.484161
Epoch 4.46: Loss = 0.518265
Epoch 4.47: Loss = 0.481857
Epoch 4.48: Loss = 0.435623
Epoch 4.49: Loss = 0.447052
Epoch 4.50: Loss = 0.541122
Epoch 4.51: Loss = 0.410172
Epoch 4.52: Loss = 0.365829
Epoch 4.53: Loss = 0.426559
Epoch 4.54: Loss = 0.543381
Epoch 4.55: Loss = 0.434341
Epoch 4.56: Loss = 0.433929
Epoch 4.57: Loss = 0.432083
Epoch 4.58: Loss = 0.435822
Epoch 4.59: Loss = 0.531876
Epoch 4.60: Loss = 0.52327
Epoch 4.61: Loss = 0.538559
Epoch 4.62: Loss = 0.574951
Epoch 4.63: Loss = 0.602829
Epoch 4.64: Loss = 0.552643
Epoch 4.65: Loss = 0.625824
Epoch 4.66: Loss = 0.451111
Epoch 4.67: Loss = 0.485794
Epoch 4.68: Loss = 0.28775
Epoch 4.69: Loss = 0.39711
Epoch 4.70: Loss = 0.565308
Epoch 4.71: Loss = 0.39447
Epoch 4.72: Loss = 0.410858
Epoch 4.73: Loss = 0.468216
Epoch 4.74: Loss = 0.350983
Epoch 4.75: Loss = 0.617783
Epoch 4.76: Loss = 0.487106
Epoch 4.77: Loss = 0.401703
Epoch 4.78: Loss = 0.418015
Epoch 4.79: Loss = 0.518921
Epoch 4.80: Loss = 0.540558
Epoch 4.81: Loss = 0.379074
Epoch 4.82: Loss = 0.383545
Epoch 4.83: Loss = 0.557663
Epoch 4.84: Loss = 0.499527
Epoch 4.85: Loss = 0.611908
Epoch 4.86: Loss = 0.515167
Epoch 4.87: Loss = 0.376266
Epoch 4.88: Loss = 0.459824
Epoch 4.89: Loss = 0.559204
Epoch 4.90: Loss = 0.380676
Epoch 4.91: Loss = 0.506226
Epoch 4.92: Loss = 0.509201
Epoch 4.93: Loss = 0.580917
Epoch 4.94: Loss = 0.348129
Epoch 4.95: Loss = 0.459412
Epoch 4.96: Loss = 0.513062
Epoch 4.97: Loss = 0.354721
Epoch 4.98: Loss = 0.410095
Epoch 4.99: Loss = 0.496185
Epoch 4.100: Loss = 0.653305
Epoch 4.101: Loss = 0.621155
Epoch 4.102: Loss = 0.409714
Epoch 4.103: Loss = 0.378128
Epoch 4.104: Loss = 0.391342
Epoch 4.105: Loss = 0.577179
Epoch 4.106: Loss = 0.515335
Epoch 4.107: Loss = 0.36734
Epoch 4.108: Loss = 0.480576
Epoch 4.109: Loss = 0.421616
Epoch 4.110: Loss = 0.468567
Epoch 4.111: Loss = 0.369736
Epoch 4.112: Loss = 0.340073
Epoch 4.113: Loss = 0.422531
Epoch 4.114: Loss = 0.369644
Epoch 4.115: Loss = 0.368362
Epoch 4.116: Loss = 0.428925
Epoch 4.117: Loss = 0.296982
Epoch 4.118: Loss = 0.231064
Epoch 4.119: Loss = 0.324112
Epoch 4.120: Loss = 0.372757
TRAIN LOSS = 0.463547
TRAIN ACC = 86.1191 % (51674/60000)
Loss = 0.450897
Loss = 0.51564
Loss = 0.602539
Loss = 0.607666
Loss = 0.616974
Loss = 0.472275
Loss = 0.432983
Loss = 0.638596
Loss = 0.553024
Loss = 0.505112
Loss = 0.212677
Loss = 0.389191
Loss = 0.286362
Loss = 0.426315
Loss = 0.29924
Loss = 0.305466
Loss = 0.251709
Loss = 0.0900879
Loss = 0.260254
Loss = 0.607147
TEST LOSS = 0.426208
TEST ACC = 516.739 % (8733/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.423203
Epoch 5.2: Loss = 0.51062
Epoch 5.3: Loss = 0.508331
Epoch 5.4: Loss = 0.37085
Epoch 5.5: Loss = 0.401077
Epoch 5.6: Loss = 0.402435
Epoch 5.7: Loss = 0.401794
Epoch 5.8: Loss = 0.44342
Epoch 5.9: Loss = 0.39798
Epoch 5.10: Loss = 0.446091
Epoch 5.11: Loss = 0.455994
Epoch 5.12: Loss = 0.436752
Epoch 5.13: Loss = 0.35672
Epoch 5.14: Loss = 0.388885
Epoch 5.15: Loss = 0.491821
Epoch 5.16: Loss = 0.502548
Epoch 5.17: Loss = 0.5354
Epoch 5.18: Loss = 0.658264
Epoch 5.19: Loss = 0.462936
Epoch 5.20: Loss = 0.389099
Epoch 5.21: Loss = 0.350769
Epoch 5.22: Loss = 0.322159
Epoch 5.23: Loss = 0.377258
Epoch 5.24: Loss = 0.506607
Epoch 5.25: Loss = 0.464752
Epoch 5.26: Loss = 0.577103
Epoch 5.27: Loss = 0.506866
Epoch 5.28: Loss = 0.500046
Epoch 5.29: Loss = 0.598831
Epoch 5.30: Loss = 0.622192
Epoch 5.31: Loss = 0.37706
Epoch 5.32: Loss = 0.552353
Epoch 5.33: Loss = 0.394608
Epoch 5.34: Loss = 0.51828
Epoch 5.35: Loss = 0.501419
Epoch 5.36: Loss = 0.476257
Epoch 5.37: Loss = 0.331039
Epoch 5.38: Loss = 0.362274
Epoch 5.39: Loss = 0.437408
Epoch 5.40: Loss = 0.388031
Epoch 5.41: Loss = 0.42337
Epoch 5.42: Loss = 0.535645
Epoch 5.43: Loss = 0.398041
Epoch 5.44: Loss = 0.326904
Epoch 5.45: Loss = 0.437073
Epoch 5.46: Loss = 0.48172
Epoch 5.47: Loss = 0.460464
Epoch 5.48: Loss = 0.44809
Epoch 5.49: Loss = 0.435944
Epoch 5.50: Loss = 0.552475
Epoch 5.51: Loss = 0.404129
Epoch 5.52: Loss = 0.362442
Epoch 5.53: Loss = 0.406479
Epoch 5.54: Loss = 0.545227
Epoch 5.55: Loss = 0.433105
Epoch 5.56: Loss = 0.409988
Epoch 5.57: Loss = 0.407486
Epoch 5.58: Loss = 0.407166
Epoch 5.59: Loss = 0.507355
Epoch 5.60: Loss = 0.519135
Epoch 5.61: Loss = 0.512344
Epoch 5.62: Loss = 0.561218
Epoch 5.63: Loss = 0.616638
Epoch 5.64: Loss = 0.505463
Epoch 5.65: Loss = 0.616486
Epoch 5.66: Loss = 0.424454
Epoch 5.67: Loss = 0.4711
Epoch 5.68: Loss = 0.288193
Epoch 5.69: Loss = 0.37529
Epoch 5.70: Loss = 0.574081
Epoch 5.71: Loss = 0.381866
Epoch 5.72: Loss = 0.403534
Epoch 5.73: Loss = 0.455338
Epoch 5.74: Loss = 0.363724
Epoch 5.75: Loss = 0.599854
Epoch 5.76: Loss = 0.494019
Epoch 5.77: Loss = 0.379669
Epoch 5.78: Loss = 0.412445
Epoch 5.79: Loss = 0.522308
Epoch 5.80: Loss = 0.528168
Epoch 5.81: Loss = 0.360458
Epoch 5.82: Loss = 0.355942
Epoch 5.83: Loss = 0.549652
Epoch 5.84: Loss = 0.470154
Epoch 5.85: Loss = 0.601471
Epoch 5.86: Loss = 0.509094
Epoch 5.87: Loss = 0.334442
Epoch 5.88: Loss = 0.401352
Epoch 5.89: Loss = 0.53244
Epoch 5.90: Loss = 0.383026
Epoch 5.91: Loss = 0.498199
Epoch 5.92: Loss = 0.501816
Epoch 5.93: Loss = 0.576447
Epoch 5.94: Loss = 0.32872
Epoch 5.95: Loss = 0.419067
Epoch 5.96: Loss = 0.479675
Epoch 5.97: Loss = 0.331192
Epoch 5.98: Loss = 0.395096
Epoch 5.99: Loss = 0.467941
Epoch 5.100: Loss = 0.61116
Epoch 5.101: Loss = 0.596359
Epoch 5.102: Loss = 0.398697
Epoch 5.103: Loss = 0.373703
Epoch 5.104: Loss = 0.393661
Epoch 5.105: Loss = 0.552643
Epoch 5.106: Loss = 0.52182
Epoch 5.107: Loss = 0.340485
Epoch 5.108: Loss = 0.455002
Epoch 5.109: Loss = 0.39949
Epoch 5.110: Loss = 0.43956
Epoch 5.111: Loss = 0.347305
Epoch 5.112: Loss = 0.338394
Epoch 5.113: Loss = 0.394882
Epoch 5.114: Loss = 0.358154
Epoch 5.115: Loss = 0.335052
Epoch 5.116: Loss = 0.41217
Epoch 5.117: Loss = 0.271759
Epoch 5.118: Loss = 0.222092
Epoch 5.119: Loss = 0.332123
Epoch 5.120: Loss = 0.358047
TRAIN LOSS = 0.445724
TRAIN ACC = 87.146 % (52290/60000)
Loss = 0.442505
Loss = 0.522263
Loss = 0.586655
Loss = 0.597351
Loss = 0.597275
Loss = 0.438065
Loss = 0.418732
Loss = 0.629059
Loss = 0.585358
Loss = 0.512909
Loss = 0.191666
Loss = 0.350891
Loss = 0.290833
Loss = 0.394424
Loss = 0.274582
Loss = 0.285858
Loss = 0.241043
Loss = 0.0726166
Loss = 0.224457
Loss = 0.585556
TEST LOSS = 0.412105
TEST ACC = 522.899 % (8786/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.39537
Epoch 6.2: Loss = 0.493652
Epoch 6.3: Loss = 0.468475
Epoch 6.4: Loss = 0.367676
Epoch 6.5: Loss = 0.360077
Epoch 6.6: Loss = 0.398483
Epoch 6.7: Loss = 0.415497
Epoch 6.8: Loss = 0.436707
Epoch 6.9: Loss = 0.391174
Epoch 6.10: Loss = 0.439545
Epoch 6.11: Loss = 0.450455
Epoch 6.12: Loss = 0.426895
Epoch 6.13: Loss = 0.319931
Epoch 6.14: Loss = 0.351944
Epoch 6.15: Loss = 0.463043
Epoch 6.16: Loss = 0.483246
Epoch 6.17: Loss = 0.511658
Epoch 6.18: Loss = 0.672852
Epoch 6.19: Loss = 0.46579
Epoch 6.20: Loss = 0.369705
Epoch 6.21: Loss = 0.368164
Epoch 6.22: Loss = 0.307709
Epoch 6.23: Loss = 0.367126
Epoch 6.24: Loss = 0.489487
Epoch 6.25: Loss = 0.497757
Epoch 6.26: Loss = 0.562744
Epoch 6.27: Loss = 0.518768
Epoch 6.28: Loss = 0.489578
Epoch 6.29: Loss = 0.588974
Epoch 6.30: Loss = 0.622147
Epoch 6.31: Loss = 0.371887
Epoch 6.32: Loss = 0.538712
Epoch 6.33: Loss = 0.384933
Epoch 6.34: Loss = 0.488495
Epoch 6.35: Loss = 0.480423
Epoch 6.36: Loss = 0.485565
Epoch 6.37: Loss = 0.317535
Epoch 6.38: Loss = 0.348862
Epoch 6.39: Loss = 0.419022
Epoch 6.40: Loss = 0.389725
Epoch 6.41: Loss = 0.402267
Epoch 6.42: Loss = 0.600998
Epoch 6.43: Loss = 0.371643
Epoch 6.44: Loss = 0.31601
Epoch 6.45: Loss = 0.453094
Epoch 6.46: Loss = 0.48288
Epoch 6.47: Loss = 0.44046
Epoch 6.48: Loss = 0.433136
Epoch 6.49: Loss = 0.402817
Epoch 6.50: Loss = 0.530807
Epoch 6.51: Loss = 0.358505
Epoch 6.52: Loss = 0.343216
Epoch 6.53: Loss = 0.382553
Epoch 6.54: Loss = 0.542297
Epoch 6.55: Loss = 0.424423
Epoch 6.56: Loss = 0.402008
Epoch 6.57: Loss = 0.414398
Epoch 6.58: Loss = 0.411774
Epoch 6.59: Loss = 0.520432
Epoch 6.60: Loss = 0.490051
Epoch 6.61: Loss = 0.50853
Epoch 6.62: Loss = 0.549988
Epoch 6.63: Loss = 0.604904
Epoch 6.64: Loss = 0.507339
Epoch 6.65: Loss = 0.62294
Epoch 6.66: Loss = 0.435013
Epoch 6.67: Loss = 0.446609
Epoch 6.68: Loss = 0.284058
Epoch 6.69: Loss = 0.379257
Epoch 6.70: Loss = 0.554626
Epoch 6.71: Loss = 0.360031
Epoch 6.72: Loss = 0.392563
Epoch 6.73: Loss = 0.448669
Epoch 6.74: Loss = 0.347595
Epoch 6.75: Loss = 0.612701
Epoch 6.76: Loss = 0.477814
Epoch 6.77: Loss = 0.357086
Epoch 6.78: Loss = 0.387589
Epoch 6.79: Loss = 0.513626
Epoch 6.80: Loss = 0.508713
Epoch 6.81: Loss = 0.351181
Epoch 6.82: Loss = 0.337158
Epoch 6.83: Loss = 0.548264
Epoch 6.84: Loss = 0.443939
Epoch 6.85: Loss = 0.603302
Epoch 6.86: Loss = 0.514465
Epoch 6.87: Loss = 0.338806
Epoch 6.88: Loss = 0.414368
Epoch 6.89: Loss = 0.542953
Epoch 6.90: Loss = 0.373779
Epoch 6.91: Loss = 0.505295
Epoch 6.92: Loss = 0.498123
Epoch 6.93: Loss = 0.567749
Epoch 6.94: Loss = 0.346375
Epoch 6.95: Loss = 0.435349
Epoch 6.96: Loss = 0.439362
Epoch 6.97: Loss = 0.34404
Epoch 6.98: Loss = 0.386566
Epoch 6.99: Loss = 0.486816
Epoch 6.100: Loss = 0.613358
Epoch 6.101: Loss = 0.590683
Epoch 6.102: Loss = 0.42099
Epoch 6.103: Loss = 0.40358
Epoch 6.104: Loss = 0.398209
Epoch 6.105: Loss = 0.548355
Epoch 6.106: Loss = 0.550537
Epoch 6.107: Loss = 0.330841
Epoch 6.108: Loss = 0.458847
Epoch 6.109: Loss = 0.411804
Epoch 6.110: Loss = 0.490891
Epoch 6.111: Loss = 0.347961
Epoch 6.112: Loss = 0.338715
Epoch 6.113: Loss = 0.398209
Epoch 6.114: Loss = 0.348785
Epoch 6.115: Loss = 0.330475
Epoch 6.116: Loss = 0.441406
Epoch 6.117: Loss = 0.260803
Epoch 6.118: Loss = 0.202911
Epoch 6.119: Loss = 0.310379
Epoch 6.120: Loss = 0.339478
TRAIN LOSS = 0.439651
TRAIN ACC = 87.6022 % (52564/60000)
Loss = 0.436157
Loss = 0.525299
Loss = 0.611191
Loss = 0.614609
Loss = 0.603928
Loss = 0.462753
Loss = 0.426498
Loss = 0.645828
Loss = 0.602753
Loss = 0.530548
Loss = 0.201859
Loss = 0.40329
Loss = 0.271225
Loss = 0.37619
Loss = 0.251663
Loss = 0.277039
Loss = 0.195206
Loss = 0.0628967
Loss = 0.233368
Loss = 0.570572
TEST LOSS = 0.415143
TEST ACC = 525.639 % (8797/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.383102
Epoch 7.2: Loss = 0.488266
Epoch 7.3: Loss = 0.493393
Epoch 7.4: Loss = 0.370972
Epoch 7.5: Loss = 0.39064
Epoch 7.6: Loss = 0.355927
Epoch 7.7: Loss = 0.38295
Epoch 7.8: Loss = 0.456802
Epoch 7.9: Loss = 0.375046
Epoch 7.10: Loss = 0.469849
Epoch 7.11: Loss = 0.450516
Epoch 7.12: Loss = 0.428314
Epoch 7.13: Loss = 0.32077
Epoch 7.14: Loss = 0.374252
Epoch 7.15: Loss = 0.478867
Epoch 7.16: Loss = 0.473923
Epoch 7.17: Loss = 0.498657
Epoch 7.18: Loss = 0.69455
Epoch 7.19: Loss = 0.456314
Epoch 7.20: Loss = 0.358521
Epoch 7.21: Loss = 0.375092
Epoch 7.22: Loss = 0.315491
Epoch 7.23: Loss = 0.352692
Epoch 7.24: Loss = 0.503159
Epoch 7.25: Loss = 0.461685
Epoch 7.26: Loss = 0.62056
Epoch 7.27: Loss = 0.543091
Epoch 7.28: Loss = 0.482635
Epoch 7.29: Loss = 0.617767
Epoch 7.30: Loss = 0.613327
Epoch 7.31: Loss = 0.348267
Epoch 7.32: Loss = 0.538467
Epoch 7.33: Loss = 0.390671
Epoch 7.34: Loss = 0.522629
Epoch 7.35: Loss = 0.470139
Epoch 7.36: Loss = 0.484573
Epoch 7.37: Loss = 0.311279
Epoch 7.38: Loss = 0.371124
Epoch 7.39: Loss = 0.404388
Epoch 7.40: Loss = 0.414749
Epoch 7.41: Loss = 0.393066
Epoch 7.42: Loss = 0.60173
Epoch 7.43: Loss = 0.35994
Epoch 7.44: Loss = 0.326309
Epoch 7.45: Loss = 0.461914
Epoch 7.46: Loss = 0.479111
Epoch 7.47: Loss = 0.488586
Epoch 7.48: Loss = 0.439667
Epoch 7.49: Loss = 0.406967
Epoch 7.50: Loss = 0.515732
Epoch 7.51: Loss = 0.370743
Epoch 7.52: Loss = 0.355759
Epoch 7.53: Loss = 0.397827
Epoch 7.54: Loss = 0.559204
Epoch 7.55: Loss = 0.46434
Epoch 7.56: Loss = 0.43808
Epoch 7.57: Loss = 0.417816
Epoch 7.58: Loss = 0.421677
Epoch 7.59: Loss = 0.508667
Epoch 7.60: Loss = 0.486267
Epoch 7.61: Loss = 0.507706
Epoch 7.62: Loss = 0.568054
Epoch 7.63: Loss = 0.596725
Epoch 7.64: Loss = 0.530212
Epoch 7.65: Loss = 0.661179
Epoch 7.66: Loss = 0.446838
Epoch 7.67: Loss = 0.454788
Epoch 7.68: Loss = 0.317337
Epoch 7.69: Loss = 0.397659
Epoch 7.70: Loss = 0.574432
Epoch 7.71: Loss = 0.377609
Epoch 7.72: Loss = 0.383163
Epoch 7.73: Loss = 0.463913
Epoch 7.74: Loss = 0.363007
Epoch 7.75: Loss = 0.597275
Epoch 7.76: Loss = 0.517609
Epoch 7.77: Loss = 0.367798
Epoch 7.78: Loss = 0.399277
Epoch 7.79: Loss = 0.551773
Epoch 7.80: Loss = 0.527725
Epoch 7.81: Loss = 0.362656
Epoch 7.82: Loss = 0.355286
Epoch 7.83: Loss = 0.537888
Epoch 7.84: Loss = 0.450699
Epoch 7.85: Loss = 0.597183
Epoch 7.86: Loss = 0.565475
Epoch 7.87: Loss = 0.341263
Epoch 7.88: Loss = 0.450928
Epoch 7.89: Loss = 0.549759
Epoch 7.90: Loss = 0.417221
Epoch 7.91: Loss = 0.51561
Epoch 7.92: Loss = 0.519012
Epoch 7.93: Loss = 0.595917
Epoch 7.94: Loss = 0.33522
Epoch 7.95: Loss = 0.442856
Epoch 7.96: Loss = 0.443909
Epoch 7.97: Loss = 0.350632
Epoch 7.98: Loss = 0.388351
Epoch 7.99: Loss = 0.490723
Epoch 7.100: Loss = 0.633835
Epoch 7.101: Loss = 0.589996
Epoch 7.102: Loss = 0.420425
Epoch 7.103: Loss = 0.40332
Epoch 7.104: Loss = 0.41153
Epoch 7.105: Loss = 0.544708
Epoch 7.106: Loss = 0.540451
Epoch 7.107: Loss = 0.301117
Epoch 7.108: Loss = 0.451767
Epoch 7.109: Loss = 0.425858
Epoch 7.110: Loss = 0.490875
Epoch 7.111: Loss = 0.3396
Epoch 7.112: Loss = 0.351105
Epoch 7.113: Loss = 0.422943
Epoch 7.114: Loss = 0.377808
Epoch 7.115: Loss = 0.325928
Epoch 7.116: Loss = 0.440567
Epoch 7.117: Loss = 0.300201
Epoch 7.118: Loss = 0.211823
Epoch 7.119: Loss = 0.312042
Epoch 7.120: Loss = 0.338013
TRAIN LOSS = 0.447372
TRAIN ACC = 87.6251 % (52578/60000)
Loss = 0.453644
Loss = 0.531433
Loss = 0.598694
Loss = 0.635986
Loss = 0.615616
Loss = 0.426926
Loss = 0.425644
Loss = 0.624329
Loss = 0.639862
Loss = 0.550751
Loss = 0.217072
Loss = 0.378311
Loss = 0.27623
Loss = 0.400299
Loss = 0.254837
Loss = 0.30397
Loss = 0.227722
Loss = 0.0533447
Loss = 0.244217
Loss = 0.593948
TEST LOSS = 0.422642
TEST ACC = 525.78 % (8819/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.400131
Epoch 8.2: Loss = 0.537308
Epoch 8.3: Loss = 0.516953
Epoch 8.4: Loss = 0.388458
Epoch 8.5: Loss = 0.390762
Epoch 8.6: Loss = 0.365768
Epoch 8.7: Loss = 0.382446
Epoch 8.8: Loss = 0.445938
Epoch 8.9: Loss = 0.404449
Epoch 8.10: Loss = 0.482193
Epoch 8.11: Loss = 0.444534
Epoch 8.12: Loss = 0.445618
Epoch 8.13: Loss = 0.343765
Epoch 8.14: Loss = 0.391632
Epoch 8.15: Loss = 0.458023
Epoch 8.16: Loss = 0.466721
Epoch 8.17: Loss = 0.50029
Epoch 8.18: Loss = 0.725342
Epoch 8.19: Loss = 0.458847
Epoch 8.20: Loss = 0.358978
Epoch 8.21: Loss = 0.397476
Epoch 8.22: Loss = 0.316406
Epoch 8.23: Loss = 0.347778
Epoch 8.24: Loss = 0.505432
Epoch 8.25: Loss = 0.485306
Epoch 8.26: Loss = 0.556274
Epoch 8.27: Loss = 0.553726
Epoch 8.28: Loss = 0.48703
Epoch 8.29: Loss = 0.624207
Epoch 8.30: Loss = 0.632401
Epoch 8.31: Loss = 0.356323
Epoch 8.32: Loss = 0.569962
Epoch 8.33: Loss = 0.419205
Epoch 8.34: Loss = 0.543686
Epoch 8.35: Loss = 0.481171
Epoch 8.36: Loss = 0.511383
Epoch 8.37: Loss = 0.297729
Epoch 8.38: Loss = 0.365509
Epoch 8.39: Loss = 0.401535
Epoch 8.40: Loss = 0.418884
Epoch 8.41: Loss = 0.392303
Epoch 8.42: Loss = 0.668167
Epoch 8.43: Loss = 0.366928
Epoch 8.44: Loss = 0.317032
Epoch 8.45: Loss = 0.456406
Epoch 8.46: Loss = 0.498337
Epoch 8.47: Loss = 0.491226
Epoch 8.48: Loss = 0.457001
Epoch 8.49: Loss = 0.395477
Epoch 8.50: Loss = 0.514374
Epoch 8.51: Loss = 0.369431
Epoch 8.52: Loss = 0.378479
Epoch 8.53: Loss = 0.387268
Epoch 8.54: Loss = 0.550995
Epoch 8.55: Loss = 0.453644
Epoch 8.56: Loss = 0.46611
Epoch 8.57: Loss = 0.43161
Epoch 8.58: Loss = 0.4086
Epoch 8.59: Loss = 0.511749
Epoch 8.60: Loss = 0.518265
Epoch 8.61: Loss = 0.513062
Epoch 8.62: Loss = 0.582428
Epoch 8.63: Loss = 0.602463
Epoch 8.64: Loss = 0.487473
Epoch 8.65: Loss = 0.645782
Epoch 8.66: Loss = 0.420029
Epoch 8.67: Loss = 0.432144
Epoch 8.68: Loss = 0.333374
Epoch 8.69: Loss = 0.386322
Epoch 8.70: Loss = 0.550613
Epoch 8.71: Loss = 0.408386
Epoch 8.72: Loss = 0.395477
Epoch 8.73: Loss = 0.45253
Epoch 8.74: Loss = 0.366943
Epoch 8.75: Loss = 0.599411
Epoch 8.76: Loss = 0.52919
Epoch 8.77: Loss = 0.338333
Epoch 8.78: Loss = 0.398819
Epoch 8.79: Loss = 0.570999
Epoch 8.80: Loss = 0.526672
Epoch 8.81: Loss = 0.361069
Epoch 8.82: Loss = 0.344193
Epoch 8.83: Loss = 0.555786
Epoch 8.84: Loss = 0.446289
Epoch 8.85: Loss = 0.592682
Epoch 8.86: Loss = 0.564926
Epoch 8.87: Loss = 0.335983
Epoch 8.88: Loss = 0.443878
Epoch 8.89: Loss = 0.542236
Epoch 8.90: Loss = 0.377731
Epoch 8.91: Loss = 0.525681
Epoch 8.92: Loss = 0.516754
Epoch 8.93: Loss = 0.612839
Epoch 8.94: Loss = 0.325272
Epoch 8.95: Loss = 0.458725
Epoch 8.96: Loss = 0.457581
Epoch 8.97: Loss = 0.362747
Epoch 8.98: Loss = 0.413834
Epoch 8.99: Loss = 0.50116
Epoch 8.100: Loss = 0.619415
Epoch 8.101: Loss = 0.59877
Epoch 8.102: Loss = 0.369949
Epoch 8.103: Loss = 0.396454
Epoch 8.104: Loss = 0.4207
Epoch 8.105: Loss = 0.522507
Epoch 8.106: Loss = 0.546753
Epoch 8.107: Loss = 0.298599
Epoch 8.108: Loss = 0.480728
Epoch 8.109: Loss = 0.426865
Epoch 8.110: Loss = 0.493759
Epoch 8.111: Loss = 0.339142
Epoch 8.112: Loss = 0.309967
Epoch 8.113: Loss = 0.421585
Epoch 8.114: Loss = 0.363754
Epoch 8.115: Loss = 0.352798
Epoch 8.116: Loss = 0.436401
Epoch 8.117: Loss = 0.282379
Epoch 8.118: Loss = 0.225815
Epoch 8.119: Loss = 0.29982
Epoch 8.120: Loss = 0.353592
TRAIN LOSS = 0.450439
TRAIN ACC = 87.7777 % (52669/60000)
Loss = 0.442139
Loss = 0.537766
Loss = 0.599625
Loss = 0.615433
Loss = 0.612625
Loss = 0.442749
Loss = 0.387375
Loss = 0.62674
Loss = 0.640442
Loss = 0.56575
Loss = 0.192917
Loss = 0.381241
Loss = 0.298431
Loss = 0.411789
Loss = 0.229645
Loss = 0.328186
Loss = 0.216644
Loss = 0.0505371
Loss = 0.261078
Loss = 0.628403
TEST LOSS = 0.423476
TEST ACC = 526.689 % (8818/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.395859
Epoch 9.2: Loss = 0.552933
Epoch 9.3: Loss = 0.508774
Epoch 9.4: Loss = 0.379639
Epoch 9.5: Loss = 0.394226
Epoch 9.6: Loss = 0.354782
Epoch 9.7: Loss = 0.377579
Epoch 9.8: Loss = 0.446396
Epoch 9.9: Loss = 0.397858
Epoch 9.10: Loss = 0.465179
Epoch 9.11: Loss = 0.446808
Epoch 9.12: Loss = 0.443146
Epoch 9.13: Loss = 0.327637
Epoch 9.14: Loss = 0.395767
Epoch 9.15: Loss = 0.451401
Epoch 9.16: Loss = 0.453857
Epoch 9.17: Loss = 0.534866
Epoch 9.18: Loss = 0.683578
Epoch 9.19: Loss = 0.458923
Epoch 9.20: Loss = 0.33725
Epoch 9.21: Loss = 0.378555
Epoch 9.22: Loss = 0.339691
Epoch 9.23: Loss = 0.334
Epoch 9.24: Loss = 0.481186
Epoch 9.25: Loss = 0.476578
Epoch 9.26: Loss = 0.57048
Epoch 9.27: Loss = 0.56369
Epoch 9.28: Loss = 0.47966
Epoch 9.29: Loss = 0.623245
Epoch 9.30: Loss = 0.574326
Epoch 9.31: Loss = 0.369446
Epoch 9.32: Loss = 0.573776
Epoch 9.33: Loss = 0.397446
Epoch 9.34: Loss = 0.522079
Epoch 9.35: Loss = 0.485519
Epoch 9.36: Loss = 0.468521
Epoch 9.37: Loss = 0.316055
Epoch 9.38: Loss = 0.343872
Epoch 9.39: Loss = 0.38652
Epoch 9.40: Loss = 0.3992
Epoch 9.41: Loss = 0.373932
Epoch 9.42: Loss = 0.692917
Epoch 9.43: Loss = 0.362244
Epoch 9.44: Loss = 0.3396
Epoch 9.45: Loss = 0.463943
Epoch 9.46: Loss = 0.469589
Epoch 9.47: Loss = 0.469513
Epoch 9.48: Loss = 0.444244
Epoch 9.49: Loss = 0.399979
Epoch 9.50: Loss = 0.538513
Epoch 9.51: Loss = 0.344116
Epoch 9.52: Loss = 0.382645
Epoch 9.53: Loss = 0.383667
Epoch 9.54: Loss = 0.564285
Epoch 9.55: Loss = 0.466537
Epoch 9.56: Loss = 0.415314
Epoch 9.57: Loss = 0.430344
Epoch 9.58: Loss = 0.411041
Epoch 9.59: Loss = 0.458191
Epoch 9.60: Loss = 0.539841
Epoch 9.61: Loss = 0.509583
Epoch 9.62: Loss = 0.605881
Epoch 9.63: Loss = 0.594177
Epoch 9.64: Loss = 0.523621
Epoch 9.65: Loss = 0.67514
Epoch 9.66: Loss = 0.4254
Epoch 9.67: Loss = 0.446503
Epoch 9.68: Loss = 0.308243
Epoch 9.69: Loss = 0.408844
Epoch 9.70: Loss = 0.531143
Epoch 9.71: Loss = 0.421722
Epoch 9.72: Loss = 0.397232
Epoch 9.73: Loss = 0.460907
Epoch 9.74: Loss = 0.354813
Epoch 9.75: Loss = 0.649551
Epoch 9.76: Loss = 0.506546
Epoch 9.77: Loss = 0.341599
Epoch 9.78: Loss = 0.407532
Epoch 9.79: Loss = 0.576004
Epoch 9.80: Loss = 0.493469
Epoch 9.81: Loss = 0.400711
Epoch 9.82: Loss = 0.371521
Epoch 9.83: Loss = 0.572662
Epoch 9.84: Loss = 0.437378
Epoch 9.85: Loss = 0.63414
Epoch 9.86: Loss = 0.569168
Epoch 9.87: Loss = 0.35025
Epoch 9.88: Loss = 0.446045
Epoch 9.89: Loss = 0.539429
Epoch 9.90: Loss = 0.370804
Epoch 9.91: Loss = 0.562149
Epoch 9.92: Loss = 0.524811
Epoch 9.93: Loss = 0.578995
Epoch 9.94: Loss = 0.33548
Epoch 9.95: Loss = 0.472748
Epoch 9.96: Loss = 0.469421
Epoch 9.97: Loss = 0.377151
Epoch 9.98: Loss = 0.428848
Epoch 9.99: Loss = 0.484695
Epoch 9.100: Loss = 0.664963
Epoch 9.101: Loss = 0.665802
Epoch 9.102: Loss = 0.433914
Epoch 9.103: Loss = 0.414505
Epoch 9.104: Loss = 0.431229
Epoch 9.105: Loss = 0.537445
Epoch 9.106: Loss = 0.560471
Epoch 9.107: Loss = 0.295761
Epoch 9.108: Loss = 0.503281
Epoch 9.109: Loss = 0.428787
Epoch 9.110: Loss = 0.505081
Epoch 9.111: Loss = 0.351761
Epoch 9.112: Loss = 0.323074
Epoch 9.113: Loss = 0.425018
Epoch 9.114: Loss = 0.366348
Epoch 9.115: Loss = 0.372574
Epoch 9.116: Loss = 0.466644
Epoch 9.117: Loss = 0.294373
Epoch 9.118: Loss = 0.224319
Epoch 9.119: Loss = 0.343964
Epoch 9.120: Loss = 0.351807
TRAIN LOSS = 0.453049
TRAIN ACC = 88.063 % (52840/60000)
Loss = 0.430847
Loss = 0.526291
Loss = 0.586395
Loss = 0.627197
Loss = 0.623047
Loss = 0.434097
Loss = 0.36937
Loss = 0.636108
Loss = 0.635544
Loss = 0.574127
Loss = 0.208023
Loss = 0.361115
Loss = 0.315491
Loss = 0.41391
Loss = 0.211624
Loss = 0.352875
Loss = 0.224762
Loss = 0.0615082
Loss = 0.242371
Loss = 0.634125
TEST LOSS = 0.423441
TEST ACC = 528.4 % (8857/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.406876
Epoch 10.2: Loss = 0.556717
Epoch 10.3: Loss = 0.549866
Epoch 10.4: Loss = 0.371399
Epoch 10.5: Loss = 0.380707
Epoch 10.6: Loss = 0.367188
Epoch 10.7: Loss = 0.390076
Epoch 10.8: Loss = 0.45665
Epoch 10.9: Loss = 0.433929
Epoch 10.10: Loss = 0.457062
Epoch 10.11: Loss = 0.451477
Epoch 10.12: Loss = 0.468277
Epoch 10.13: Loss = 0.333481
Epoch 10.14: Loss = 0.400482
Epoch 10.15: Loss = 0.44397
Epoch 10.16: Loss = 0.483856
Epoch 10.17: Loss = 0.528763
Epoch 10.18: Loss = 0.741989
Epoch 10.19: Loss = 0.494339
Epoch 10.20: Loss = 0.365707
Epoch 10.21: Loss = 0.37439
Epoch 10.22: Loss = 0.330734
Epoch 10.23: Loss = 0.352066
Epoch 10.24: Loss = 0.516922
Epoch 10.25: Loss = 0.493622
Epoch 10.26: Loss = 0.576035
Epoch 10.27: Loss = 0.569763
Epoch 10.28: Loss = 0.504044
Epoch 10.29: Loss = 0.614273
Epoch 10.30: Loss = 0.621506
Epoch 10.31: Loss = 0.380341
Epoch 10.32: Loss = 0.58046
Epoch 10.33: Loss = 0.379059
Epoch 10.34: Loss = 0.505173
Epoch 10.35: Loss = 0.485764
Epoch 10.36: Loss = 0.472153
Epoch 10.37: Loss = 0.300812
Epoch 10.38: Loss = 0.341507
Epoch 10.39: Loss = 0.409622
Epoch 10.40: Loss = 0.415573
Epoch 10.41: Loss = 0.405411
Epoch 10.42: Loss = 0.714615
Epoch 10.43: Loss = 0.366348
Epoch 10.44: Loss = 0.322098
Epoch 10.45: Loss = 0.491333
Epoch 10.46: Loss = 0.504745
Epoch 10.47: Loss = 0.494293
Epoch 10.48: Loss = 0.473038
Epoch 10.49: Loss = 0.406967
Epoch 10.50: Loss = 0.547211
Epoch 10.51: Loss = 0.347336
Epoch 10.52: Loss = 0.376175
Epoch 10.53: Loss = 0.410965
Epoch 10.54: Loss = 0.591095
Epoch 10.55: Loss = 0.441284
Epoch 10.56: Loss = 0.446609
Epoch 10.57: Loss = 0.492813
Epoch 10.58: Loss = 0.432739
Epoch 10.59: Loss = 0.468964
Epoch 10.60: Loss = 0.533417
Epoch 10.61: Loss = 0.524323
Epoch 10.62: Loss = 0.602051
Epoch 10.63: Loss = 0.626846
Epoch 10.64: Loss = 0.530289
Epoch 10.65: Loss = 0.652908
Epoch 10.66: Loss = 0.434982
Epoch 10.67: Loss = 0.481277
Epoch 10.68: Loss = 0.303268
Epoch 10.69: Loss = 0.404587
Epoch 10.70: Loss = 0.563766
Epoch 10.71: Loss = 0.409866
Epoch 10.72: Loss = 0.417404
Epoch 10.73: Loss = 0.489746
Epoch 10.74: Loss = 0.348511
Epoch 10.75: Loss = 0.640182
Epoch 10.76: Loss = 0.503204
Epoch 10.77: Loss = 0.325607
Epoch 10.78: Loss = 0.428436
Epoch 10.79: Loss = 0.544067
Epoch 10.80: Loss = 0.507126
Epoch 10.81: Loss = 0.398972
Epoch 10.82: Loss = 0.378296
Epoch 10.83: Loss = 0.582565
Epoch 10.84: Loss = 0.47612
Epoch 10.85: Loss = 0.626129
Epoch 10.86: Loss = 0.56604
Epoch 10.87: Loss = 0.337448
Epoch 10.88: Loss = 0.459808
Epoch 10.89: Loss = 0.5746
Epoch 10.90: Loss = 0.378204
Epoch 10.91: Loss = 0.523682
Epoch 10.92: Loss = 0.531754
Epoch 10.93: Loss = 0.578583
Epoch 10.94: Loss = 0.334946
Epoch 10.95: Loss = 0.50174
Epoch 10.96: Loss = 0.484909
Epoch 10.97: Loss = 0.394791
Epoch 10.98: Loss = 0.456223
Epoch 10.99: Loss = 0.528915
Epoch 10.100: Loss = 0.730484
Epoch 10.101: Loss = 0.64621
Epoch 10.102: Loss = 0.417679
Epoch 10.103: Loss = 0.430847
Epoch 10.104: Loss = 0.444641
Epoch 10.105: Loss = 0.560074
Epoch 10.106: Loss = 0.559937
Epoch 10.107: Loss = 0.308121
Epoch 10.108: Loss = 0.525803
Epoch 10.109: Loss = 0.440018
Epoch 10.110: Loss = 0.515106
Epoch 10.111: Loss = 0.34906
Epoch 10.112: Loss = 0.312927
Epoch 10.113: Loss = 0.419769
Epoch 10.114: Loss = 0.395752
Epoch 10.115: Loss = 0.381439
Epoch 10.116: Loss = 0.483444
Epoch 10.117: Loss = 0.290619
Epoch 10.118: Loss = 0.232376
Epoch 10.119: Loss = 0.376068
Epoch 10.120: Loss = 0.377563
TRAIN LOSS = 0.463531
TRAIN ACC = 88.0463 % (52830/60000)
Loss = 0.442108
Loss = 0.559387
Loss = 0.592606
Loss = 0.622543
Loss = 0.623383
Loss = 0.454376
Loss = 0.375717
Loss = 0.608154
Loss = 0.632706
Loss = 0.560776
Loss = 0.229004
Loss = 0.394852
Loss = 0.319809
Loss = 0.412781
Loss = 0.220886
Loss = 0.402466
Loss = 0.212799
Loss = 0.0630493
Loss = 0.244095
Loss = 0.667969
TEST LOSS = 0.431973
TEST ACC = 528.299 % (8870/10000)
