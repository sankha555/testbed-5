Setting up connection 0
***********************************************************
Training MNIST
Model: Dense([60000, 1, 100]) => Dense([60000, 1, 10]) => MultiOutput([60000, 10]) => 
Train Examples: 60000
Batch Size: 500
Num Epochs: 10
Learning Rate: 0.1 to 0.1 over 10 epochs
Clipping Factor: 4
Sigma: 1
***********************************************************
Epoch 1.1: Loss = 2.49229
Epoch 1.2: Loss = 2.31007
Epoch 1.3: Loss = 2.22142
Epoch 1.4: Loss = 2.09715
Epoch 1.5: Loss = 2.08485
Epoch 1.6: Loss = 2.02542
Epoch 1.7: Loss = 1.96005
Epoch 1.8: Loss = 1.89526
Epoch 1.9: Loss = 1.85439
Epoch 1.10: Loss = 1.79959
Epoch 1.11: Loss = 1.81314
Epoch 1.12: Loss = 1.76964
Epoch 1.13: Loss = 1.72748
Epoch 1.14: Loss = 1.67368
Epoch 1.15: Loss = 1.63078
Epoch 1.16: Loss = 1.62311
Epoch 1.17: Loss = 1.58984
Epoch 1.18: Loss = 1.54503
Epoch 1.19: Loss = 1.50514
Epoch 1.20: Loss = 1.55055
Epoch 1.21: Loss = 1.47269
Epoch 1.22: Loss = 1.44576
Epoch 1.23: Loss = 1.40079
Epoch 1.24: Loss = 1.47302
Epoch 1.25: Loss = 1.40379
Epoch 1.26: Loss = 1.35922
Epoch 1.27: Loss = 1.31271
Epoch 1.28: Loss = 1.311
Epoch 1.29: Loss = 1.30171
Epoch 1.30: Loss = 1.28348
Epoch 1.31: Loss = 1.31198
Epoch 1.32: Loss = 1.27011
Epoch 1.33: Loss = 1.1832
Epoch 1.34: Loss = 1.23395
Epoch 1.35: Loss = 1.30011
Epoch 1.36: Loss = 1.26323
Epoch 1.37: Loss = 1.23163
Epoch 1.38: Loss = 1.19528
Epoch 1.39: Loss = 1.14366
Epoch 1.40: Loss = 1.17441
Epoch 1.41: Loss = 1.19794
Epoch 1.42: Loss = 1.13467
Epoch 1.43: Loss = 1.09351
Epoch 1.44: Loss = 1.06061
Epoch 1.45: Loss = 1.09824
Epoch 1.46: Loss = 1.09944
Epoch 1.47: Loss = 1.0892
Epoch 1.48: Loss = 1.03726
Epoch 1.49: Loss = 1.07063
Epoch 1.50: Loss = 1.03427
Epoch 1.51: Loss = 1.02431
Epoch 1.52: Loss = 1.07394
Epoch 1.53: Loss = 1.04146
Epoch 1.54: Loss = 0.941132
Epoch 1.55: Loss = 1.01111
Epoch 1.56: Loss = 1.03612
Epoch 1.57: Loss = 1.04948
Epoch 1.58: Loss = 1.00237
Epoch 1.59: Loss = 0.987961
Epoch 1.60: Loss = 1.00784
Epoch 1.61: Loss = 0.916763
Epoch 1.62: Loss = 1.03397
Epoch 1.63: Loss = 0.878357
Epoch 1.64: Loss = 0.927414
Epoch 1.65: Loss = 0.926041
Epoch 1.66: Loss = 0.935318
Epoch 1.67: Loss = 0.865158
Epoch 1.68: Loss = 0.973984
Epoch 1.69: Loss = 0.930481
Epoch 1.70: Loss = 0.907867
Epoch 1.71: Loss = 0.83548
Epoch 1.72: Loss = 0.864105
Epoch 1.73: Loss = 0.939362
Epoch 1.74: Loss = 0.948624
Epoch 1.75: Loss = 0.885986
Epoch 1.76: Loss = 0.88504
Epoch 1.77: Loss = 0.857101
Epoch 1.78: Loss = 0.891022
Epoch 1.79: Loss = 0.798203
Epoch 1.80: Loss = 0.852783
Epoch 1.81: Loss = 0.82663
Epoch 1.82: Loss = 0.856033
Epoch 1.83: Loss = 0.893631
Epoch 1.84: Loss = 0.853485
Epoch 1.85: Loss = 0.830231
Epoch 1.86: Loss = 0.888474
Epoch 1.87: Loss = 0.893463
Epoch 1.88: Loss = 0.782425
Epoch 1.89: Loss = 0.885422
Epoch 1.90: Loss = 0.812134
Epoch 1.91: Loss = 0.874557
Epoch 1.92: Loss = 0.837006
Epoch 1.93: Loss = 0.863998
Epoch 1.94: Loss = 0.82692
Epoch 1.95: Loss = 0.873413
Epoch 1.96: Loss = 0.80043
Epoch 1.97: Loss = 0.719864
Epoch 1.98: Loss = 0.814819
Epoch 1.99: Loss = 0.837769
Epoch 1.100: Loss = 0.803207
Epoch 1.101: Loss = 0.844727
Epoch 1.102: Loss = 0.84642
Epoch 1.103: Loss = 0.837997
Epoch 1.104: Loss = 0.800156
Epoch 1.105: Loss = 0.761627
Epoch 1.106: Loss = 0.924667
Epoch 1.107: Loss = 0.814804
Epoch 1.108: Loss = 0.814835
Epoch 1.109: Loss = 0.804672
Epoch 1.110: Loss = 0.784775
Epoch 1.111: Loss = 0.750748
Epoch 1.112: Loss = 0.710632
Epoch 1.113: Loss = 0.795914
Epoch 1.114: Loss = 0.787308
Epoch 1.115: Loss = 0.79924
Epoch 1.116: Loss = 0.702332
Epoch 1.117: Loss = 0.850708
Epoch 1.118: Loss = 0.716293
Epoch 1.119: Loss = 0.752853
Epoch 1.120: Loss = 0.753647
TRAIN LOSS = 1.12512
TRAIN ACC = 61.0886 % (36655/60000)
Loss = 0.716003
Loss = 0.824112
Loss = 0.816193
Loss = 0.745773
Loss = 0.736557
Loss = 0.869308
Loss = 0.892456
Loss = 0.838654
Loss = 0.784256
Loss = 0.738052
Loss = 0.83696
Loss = 0.810822
Loss = 0.8004
Loss = 0.819122
Loss = 0.774109
Loss = 0.840347
Loss = 0.762695
Loss = 0.792435
Loss = 0.851654
Loss = 0.766495
TEST LOSS = 0.80082
TEST ACC = 366.55 % (7109/10000)
Reducing learning rate to 0.100006
Epoch 2.1: Loss = 0.798111
Epoch 2.2: Loss = 0.753525
Epoch 2.3: Loss = 0.829803
Epoch 2.4: Loss = 0.708603
Epoch 2.5: Loss = 0.772324
Epoch 2.6: Loss = 0.827347
Epoch 2.7: Loss = 0.757919
Epoch 2.8: Loss = 0.819122
Epoch 2.9: Loss = 0.694992
Epoch 2.10: Loss = 0.63533
Epoch 2.11: Loss = 0.814438
Epoch 2.12: Loss = 0.759857
Epoch 2.13: Loss = 0.759979
Epoch 2.14: Loss = 0.762939
Epoch 2.15: Loss = 0.754181
Epoch 2.16: Loss = 0.805008
Epoch 2.17: Loss = 0.727997
Epoch 2.18: Loss = 0.764984
Epoch 2.19: Loss = 0.737259
Epoch 2.20: Loss = 0.817093
Epoch 2.21: Loss = 0.707184
Epoch 2.22: Loss = 0.672516
Epoch 2.23: Loss = 0.736633
Epoch 2.24: Loss = 0.844345
Epoch 2.25: Loss = 0.750153
Epoch 2.26: Loss = 0.680084
Epoch 2.27: Loss = 0.719284
Epoch 2.28: Loss = 0.720032
Epoch 2.29: Loss = 0.744293
Epoch 2.30: Loss = 0.713806
Epoch 2.31: Loss = 0.787994
Epoch 2.32: Loss = 0.703995
Epoch 2.33: Loss = 0.656219
Epoch 2.34: Loss = 0.779404
Epoch 2.35: Loss = 0.788345
Epoch 2.36: Loss = 0.794189
Epoch 2.37: Loss = 0.776443
Epoch 2.38: Loss = 0.725159
Epoch 2.39: Loss = 0.771652
Epoch 2.40: Loss = 0.741333
Epoch 2.41: Loss = 0.76825
Epoch 2.42: Loss = 0.744354
Epoch 2.43: Loss = 0.702942
Epoch 2.44: Loss = 0.657501
Epoch 2.45: Loss = 0.724579
Epoch 2.46: Loss = 0.795898
Epoch 2.47: Loss = 0.693665
Epoch 2.48: Loss = 0.670944
Epoch 2.49: Loss = 0.773453
Epoch 2.50: Loss = 0.720306
Epoch 2.51: Loss = 0.643188
Epoch 2.52: Loss = 0.755173
Epoch 2.53: Loss = 0.779648
Epoch 2.54: Loss = 0.612839
Epoch 2.55: Loss = 0.724426
Epoch 2.56: Loss = 0.739883
Epoch 2.57: Loss = 0.770844
Epoch 2.58: Loss = 0.730942
Epoch 2.59: Loss = 0.73558
Epoch 2.60: Loss = 0.73259
Epoch 2.61: Loss = 0.663162
Epoch 2.62: Loss = 0.776932
Epoch 2.63: Loss = 0.619553
Epoch 2.64: Loss = 0.649063
Epoch 2.65: Loss = 0.688721
Epoch 2.66: Loss = 0.67775
Epoch 2.67: Loss = 0.656357
Epoch 2.68: Loss = 0.783096
Epoch 2.69: Loss = 0.717163
Epoch 2.70: Loss = 0.723663
Epoch 2.71: Loss = 0.610016
Epoch 2.72: Loss = 0.682053
Epoch 2.73: Loss = 0.781174
Epoch 2.74: Loss = 0.753632
Epoch 2.75: Loss = 0.670761
Epoch 2.76: Loss = 0.688385
Epoch 2.77: Loss = 0.671051
Epoch 2.78: Loss = 0.721252
Epoch 2.79: Loss = 0.654373
Epoch 2.80: Loss = 0.648926
Epoch 2.81: Loss = 0.660614
Epoch 2.82: Loss = 0.642746
Epoch 2.83: Loss = 0.74765
Epoch 2.84: Loss = 0.657303
Epoch 2.85: Loss = 0.673691
Epoch 2.86: Loss = 0.733063
Epoch 2.87: Loss = 0.721939
Epoch 2.88: Loss = 0.628616
Epoch 2.89: Loss = 0.759048
Epoch 2.90: Loss = 0.675446
Epoch 2.91: Loss = 0.750504
Epoch 2.92: Loss = 0.693863
Epoch 2.93: Loss = 0.741699
Epoch 2.94: Loss = 0.665695
Epoch 2.95: Loss = 0.71846
Epoch 2.96: Loss = 0.653381
Epoch 2.97: Loss = 0.590805
Epoch 2.98: Loss = 0.662003
Epoch 2.99: Loss = 0.693878
Epoch 2.100: Loss = 0.674973
Epoch 2.101: Loss = 0.717484
Epoch 2.102: Loss = 0.714951
Epoch 2.103: Loss = 0.688583
Epoch 2.104: Loss = 0.642456
Epoch 2.105: Loss = 0.626221
Epoch 2.106: Loss = 0.798843
Epoch 2.107: Loss = 0.730453
Epoch 2.108: Loss = 0.717667
Epoch 2.109: Loss = 0.714249
Epoch 2.110: Loss = 0.672485
Epoch 2.111: Loss = 0.632126
Epoch 2.112: Loss = 0.614914
Epoch 2.113: Loss = 0.669128
Epoch 2.114: Loss = 0.675079
Epoch 2.115: Loss = 0.685272
Epoch 2.116: Loss = 0.61235
Epoch 2.117: Loss = 0.742096
Epoch 2.118: Loss = 0.607071
Epoch 2.119: Loss = 0.660126
Epoch 2.120: Loss = 0.651398
TRAIN LOSS = 0.714554
TRAIN ACC = 75.1099 % (45068/60000)
Loss = 0.620834
Loss = 0.743546
Loss = 0.701599
Loss = 0.6259
Loss = 0.631165
Loss = 0.786102
Loss = 0.813736
Loss = 0.759766
Loss = 0.700439
Loss = 0.631241
Loss = 0.766327
Loss = 0.741425
Loss = 0.701187
Loss = 0.717468
Loss = 0.685959
Loss = 0.751923
Loss = 0.656204
Loss = 0.719055
Loss = 0.76712
Loss = 0.687225
TEST LOSS = 0.710411
TEST ACC = 450.679 % (7550/10000)
Reducing learning rate to 0.100006
Epoch 3.1: Loss = 0.699539
Epoch 3.2: Loss = 0.670441
Epoch 3.3: Loss = 0.726685
Epoch 3.4: Loss = 0.608215
Epoch 3.5: Loss = 0.67215
Epoch 3.6: Loss = 0.746689
Epoch 3.7: Loss = 0.672531
Epoch 3.8: Loss = 0.766022
Epoch 3.9: Loss = 0.586075
Epoch 3.10: Loss = 0.53302
Epoch 3.11: Loss = 0.745438
Epoch 3.12: Loss = 0.669342
Epoch 3.13: Loss = 0.694855
Epoch 3.14: Loss = 0.672623
Epoch 3.15: Loss = 0.681122
Epoch 3.16: Loss = 0.722916
Epoch 3.17: Loss = 0.633591
Epoch 3.18: Loss = 0.679749
Epoch 3.19: Loss = 0.657547
Epoch 3.20: Loss = 0.742966
Epoch 3.21: Loss = 0.608353
Epoch 3.22: Loss = 0.565948
Epoch 3.23: Loss = 0.661011
Epoch 3.24: Loss = 0.766464
Epoch 3.25: Loss = 0.659317
Epoch 3.26: Loss = 0.585693
Epoch 3.27: Loss = 0.653397
Epoch 3.28: Loss = 0.65007
Epoch 3.29: Loss = 0.66864
Epoch 3.30: Loss = 0.651443
Epoch 3.31: Loss = 0.722504
Epoch 3.32: Loss = 0.632492
Epoch 3.33: Loss = 0.583649
Epoch 3.34: Loss = 0.724579
Epoch 3.35: Loss = 0.6996
Epoch 3.36: Loss = 0.729965
Epoch 3.37: Loss = 0.708725
Epoch 3.38: Loss = 0.662598
Epoch 3.39: Loss = 0.716309
Epoch 3.40: Loss = 0.666122
Epoch 3.41: Loss = 0.700287
Epoch 3.42: Loss = 0.67897
Epoch 3.43: Loss = 0.643417
Epoch 3.44: Loss = 0.591858
Epoch 3.45: Loss = 0.670578
Epoch 3.46: Loss = 0.746384
Epoch 3.47: Loss = 0.636642
Epoch 3.48: Loss = 0.60968
Epoch 3.49: Loss = 0.711273
Epoch 3.50: Loss = 0.666275
Epoch 3.51: Loss = 0.572098
Epoch 3.52: Loss = 0.697144
Epoch 3.53: Loss = 0.731552
Epoch 3.54: Loss = 0.55069
Epoch 3.55: Loss = 0.673691
Epoch 3.56: Loss = 0.687897
Epoch 3.57: Loss = 0.720642
Epoch 3.58: Loss = 0.670563
Epoch 3.59: Loss = 0.701309
Epoch 3.60: Loss = 0.662018
Epoch 3.61: Loss = 0.612762
Epoch 3.62: Loss = 0.719055
Epoch 3.63: Loss = 0.565765
Epoch 3.64: Loss = 0.582474
Epoch 3.65: Loss = 0.642532
Epoch 3.66: Loss = 0.606689
Epoch 3.67: Loss = 0.618164
Epoch 3.68: Loss = 0.748856
Epoch 3.69: Loss = 0.66774
Epoch 3.70: Loss = 0.687729
Epoch 3.71: Loss = 0.560455
Epoch 3.72: Loss = 0.644775
Epoch 3.73: Loss = 0.736389
Epoch 3.74: Loss = 0.701202
Epoch 3.75: Loss = 0.621246
Epoch 3.76: Loss = 0.648163
Epoch 3.77: Loss = 0.629578
Epoch 3.78: Loss = 0.677551
Epoch 3.79: Loss = 0.625961
Epoch 3.80: Loss = 0.590225
Epoch 3.81: Loss = 0.628387
Epoch 3.82: Loss = 0.588882
Epoch 3.83: Loss = 0.70697
Epoch 3.84: Loss = 0.603043
Epoch 3.85: Loss = 0.638321
Epoch 3.86: Loss = 0.696793
Epoch 3.87: Loss = 0.672333
Epoch 3.88: Loss = 0.585403
Epoch 3.89: Loss = 0.730835
Epoch 3.90: Loss = 0.644135
Epoch 3.91: Loss = 0.716415
Epoch 3.92: Loss = 0.654861
Epoch 3.93: Loss = 0.704926
Epoch 3.94: Loss = 0.626495
Epoch 3.95: Loss = 0.675339
Epoch 3.96: Loss = 0.620712
Epoch 3.97: Loss = 0.552841
Epoch 3.98: Loss = 0.621094
Epoch 3.99: Loss = 0.648834
Epoch 3.100: Loss = 0.643494
Epoch 3.101: Loss = 0.680756
Epoch 3.102: Loss = 0.674881
Epoch 3.103: Loss = 0.645126
Epoch 3.104: Loss = 0.592072
Epoch 3.105: Loss = 0.585403
Epoch 3.106: Loss = 0.757385
Epoch 3.107: Loss = 0.703735
Epoch 3.108: Loss = 0.704605
Epoch 3.109: Loss = 0.691437
Epoch 3.110: Loss = 0.643463
Epoch 3.111: Loss = 0.603546
Epoch 3.112: Loss = 0.58931
Epoch 3.113: Loss = 0.625107
Epoch 3.114: Loss = 0.644379
Epoch 3.115: Loss = 0.649323
Epoch 3.116: Loss = 0.585876
Epoch 3.117: Loss = 0.705093
Epoch 3.118: Loss = 0.576477
Epoch 3.119: Loss = 0.637604
Epoch 3.120: Loss = 0.613022
TRAIN LOSS = 0.658173
TRAIN ACC = 77.9037 % (46744/60000)
Loss = 0.584595
Loss = 0.715866
Loss = 0.655487
Loss = 0.584244
Loss = 0.602875
Loss = 0.761551
Loss = 0.790375
Loss = 0.729431
Loss = 0.669373
Loss = 0.599472
Loss = 0.753571
Loss = 0.72757
Loss = 0.67131
Loss = 0.687256
Loss = 0.658401
Loss = 0.725433
Loss = 0.626465
Loss = 0.696091
Loss = 0.742218
Loss = 0.657898
TEST LOSS = 0.681974
TEST ACC = 467.439 % (7701/10000)
Reducing learning rate to 0.100006
Epoch 4.1: Loss = 0.664627
Epoch 4.2: Loss = 0.643951
Epoch 4.3: Loss = 0.690079
Epoch 4.4: Loss = 0.573807
Epoch 4.5: Loss = 0.639938
Epoch 4.6: Loss = 0.716461
Epoch 4.7: Loss = 0.647095
Epoch 4.8: Loss = 0.74585
Epoch 4.9: Loss = 0.547745
Epoch 4.10: Loss = 0.491989
Epoch 4.11: Loss = 0.720886
Epoch 4.12: Loss = 0.635117
Epoch 4.13: Loss = 0.67926
Epoch 4.14: Loss = 0.636749
Epoch 4.15: Loss = 0.647018
Epoch 4.16: Loss = 0.694717
Epoch 4.17: Loss = 0.591492
Epoch 4.18: Loss = 0.651215
Epoch 4.19: Loss = 0.624496
Epoch 4.20: Loss = 0.717102
Epoch 4.21: Loss = 0.570526
Epoch 4.22: Loss = 0.518616
Epoch 4.23: Loss = 0.638931
Epoch 4.24: Loss = 0.744263
Epoch 4.25: Loss = 0.621109
Epoch 4.26: Loss = 0.547394
Epoch 4.27: Loss = 0.624435
Epoch 4.28: Loss = 0.627655
Epoch 4.29: Loss = 0.646133
Epoch 4.30: Loss = 0.630692
Epoch 4.31: Loss = 0.694046
Epoch 4.32: Loss = 0.597946
Epoch 4.33: Loss = 0.557388
Epoch 4.34: Loss = 0.700729
Epoch 4.35: Loss = 0.669632
Epoch 4.36: Loss = 0.697281
Epoch 4.37: Loss = 0.686722
Epoch 4.38: Loss = 0.641357
Epoch 4.39: Loss = 0.697449
Epoch 4.40: Loss = 0.63414
Epoch 4.41: Loss = 0.685928
Epoch 4.42: Loss = 0.645584
Epoch 4.43: Loss = 0.62262
Epoch 4.44: Loss = 0.568161
Epoch 4.45: Loss = 0.644653
Epoch 4.46: Loss = 0.736069
Epoch 4.47: Loss = 0.617111
Epoch 4.48: Loss = 0.583008
Epoch 4.49: Loss = 0.676346
Epoch 4.50: Loss = 0.644455
Epoch 4.51: Loss = 0.538437
Epoch 4.52: Loss = 0.67572
Epoch 4.53: Loss = 0.710236
Epoch 4.54: Loss = 0.517441
Epoch 4.55: Loss = 0.647018
Epoch 4.56: Loss = 0.656708
Epoch 4.57: Loss = 0.702484
Epoch 4.58: Loss = 0.637634
Epoch 4.59: Loss = 0.693497
Epoch 4.60: Loss = 0.637482
Epoch 4.61: Loss = 0.594513
Epoch 4.62: Loss = 0.698425
Epoch 4.63: Loss = 0.545776
Epoch 4.64: Loss = 0.55014
Epoch 4.65: Loss = 0.632492
Epoch 4.66: Loss = 0.570938
Epoch 4.67: Loss = 0.606155
Epoch 4.68: Loss = 0.751923
Epoch 4.69: Loss = 0.645996
Epoch 4.70: Loss = 0.668091
Epoch 4.71: Loss = 0.536362
Epoch 4.72: Loss = 0.631638
Epoch 4.73: Loss = 0.711655
Epoch 4.74: Loss = 0.673599
Epoch 4.75: Loss = 0.602188
Epoch 4.76: Loss = 0.633636
Epoch 4.77: Loss = 0.611542
Epoch 4.78: Loss = 0.656601
Epoch 4.79: Loss = 0.613358
Epoch 4.80: Loss = 0.569183
Epoch 4.81: Loss = 0.612488
Epoch 4.82: Loss = 0.562729
Epoch 4.83: Loss = 0.686584
Epoch 4.84: Loss = 0.5737
Epoch 4.85: Loss = 0.623703
Epoch 4.86: Loss = 0.68251
Epoch 4.87: Loss = 0.641663
Epoch 4.88: Loss = 0.56778
Epoch 4.89: Loss = 0.719955
Epoch 4.90: Loss = 0.636124
Epoch 4.91: Loss = 0.693893
Epoch 4.92: Loss = 0.634323
Epoch 4.93: Loss = 0.678207
Epoch 4.94: Loss = 0.605682
Epoch 4.95: Loss = 0.648834
Epoch 4.96: Loss = 0.597366
Epoch 4.97: Loss = 0.536697
Epoch 4.98: Loss = 0.601746
Epoch 4.99: Loss = 0.627701
Epoch 4.100: Loss = 0.627701
Epoch 4.101: Loss = 0.665985
Epoch 4.102: Loss = 0.665039
Epoch 4.103: Loss = 0.627701
Epoch 4.104: Loss = 0.567947
Epoch 4.105: Loss = 0.563766
Epoch 4.106: Loss = 0.738602
Epoch 4.107: Loss = 0.684509
Epoch 4.108: Loss = 0.708969
Epoch 4.109: Loss = 0.681091
Epoch 4.110: Loss = 0.630402
Epoch 4.111: Loss = 0.589218
Epoch 4.112: Loss = 0.579956
Epoch 4.113: Loss = 0.602753
Epoch 4.114: Loss = 0.630173
Epoch 4.115: Loss = 0.623672
Epoch 4.116: Loss = 0.563644
Epoch 4.117: Loss = 0.681824
Epoch 4.118: Loss = 0.553085
Epoch 4.119: Loss = 0.622955
Epoch 4.120: Loss = 0.591904
TRAIN LOSS = 0.634872
TRAIN ACC = 79.1306 % (47480/60000)
Loss = 0.557785
Loss = 0.694687
Loss = 0.624695
Loss = 0.555496
Loss = 0.583755
Loss = 0.730637
Loss = 0.777328
Loss = 0.706009
Loss = 0.654083
Loss = 0.577972
Loss = 0.744568
Loss = 0.727188
Loss = 0.649765
Loss = 0.660904
Loss = 0.635284
Loss = 0.69664
Loss = 0.60817
Loss = 0.675171
Loss = 0.720047
Loss = 0.636932
TEST LOSS = 0.660856
TEST ACC = 474.799 % (7842/10000)
Reducing learning rate to 0.100006
Epoch 5.1: Loss = 0.637344
Epoch 5.2: Loss = 0.628098
Epoch 5.3: Loss = 0.665924
Epoch 5.4: Loss = 0.556564
Epoch 5.5: Loss = 0.62883
Epoch 5.6: Loss = 0.699127
Epoch 5.7: Loss = 0.631561
Epoch 5.8: Loss = 0.735168
Epoch 5.9: Loss = 0.525833
Epoch 5.10: Loss = 0.468597
Epoch 5.11: Loss = 0.712006
Epoch 5.12: Loss = 0.621414
Epoch 5.13: Loss = 0.658554
Epoch 5.14: Loss = 0.622299
Epoch 5.15: Loss = 0.636795
Epoch 5.16: Loss = 0.67421
Epoch 5.17: Loss = 0.570145
Epoch 5.18: Loss = 0.639053
Epoch 5.19: Loss = 0.603531
Epoch 5.20: Loss = 0.707672
Epoch 5.21: Loss = 0.545776
Epoch 5.22: Loss = 0.488831
Epoch 5.23: Loss = 0.620087
Epoch 5.24: Loss = 0.725647
Epoch 5.25: Loss = 0.604019
Epoch 5.26: Loss = 0.528961
Epoch 5.27: Loss = 0.604446
Epoch 5.28: Loss = 0.616531
Epoch 5.29: Loss = 0.627441
Epoch 5.30: Loss = 0.61824
Epoch 5.31: Loss = 0.681946
Epoch 5.32: Loss = 0.581772
Epoch 5.33: Loss = 0.541138
Epoch 5.34: Loss = 0.684006
Epoch 5.35: Loss = 0.651733
Epoch 5.36: Loss = 0.686569
Epoch 5.37: Loss = 0.674896
Epoch 5.38: Loss = 0.627762
Epoch 5.39: Loss = 0.679398
Epoch 5.40: Loss = 0.609482
Epoch 5.41: Loss = 0.667603
Epoch 5.42: Loss = 0.631119
Epoch 5.43: Loss = 0.608734
Epoch 5.44: Loss = 0.547592
Epoch 5.45: Loss = 0.63176
Epoch 5.46: Loss = 0.718796
Epoch 5.47: Loss = 0.600952
Epoch 5.48: Loss = 0.56192
Epoch 5.49: Loss = 0.657761
Epoch 5.50: Loss = 0.629791
Epoch 5.51: Loss = 0.515533
Epoch 5.52: Loss = 0.658066
Epoch 5.53: Loss = 0.685684
Epoch 5.54: Loss = 0.497101
Epoch 5.55: Loss = 0.624481
Epoch 5.56: Loss = 0.635315
Epoch 5.57: Loss = 0.690414
Epoch 5.58: Loss = 0.611557
Epoch 5.59: Loss = 0.689056
Epoch 5.60: Loss = 0.628113
Epoch 5.61: Loss = 0.579514
Epoch 5.62: Loss = 0.673187
Epoch 5.63: Loss = 0.531387
Epoch 5.64: Loss = 0.52626
Epoch 5.65: Loss = 0.61145
Epoch 5.66: Loss = 0.545578
Epoch 5.67: Loss = 0.589401
Epoch 5.68: Loss = 0.747437
Epoch 5.69: Loss = 0.631836
Epoch 5.70: Loss = 0.640991
Epoch 5.71: Loss = 0.518219
Epoch 5.72: Loss = 0.619492
Epoch 5.73: Loss = 0.690552
Epoch 5.74: Loss = 0.654648
Epoch 5.75: Loss = 0.580063
Epoch 5.76: Loss = 0.614029
Epoch 5.77: Loss = 0.601349
Epoch 5.78: Loss = 0.639328
Epoch 5.79: Loss = 0.604782
Epoch 5.80: Loss = 0.554169
Epoch 5.81: Loss = 0.596863
Epoch 5.82: Loss = 0.54982
Epoch 5.83: Loss = 0.673584
Epoch 5.84: Loss = 0.554245
Epoch 5.85: Loss = 0.605515
Epoch 5.86: Loss = 0.668777
Epoch 5.87: Loss = 0.615662
Epoch 5.88: Loss = 0.546143
Epoch 5.89: Loss = 0.704315
Epoch 5.90: Loss = 0.618958
Epoch 5.91: Loss = 0.675064
Epoch 5.92: Loss = 0.621429
Epoch 5.93: Loss = 0.648346
Epoch 5.94: Loss = 0.58432
Epoch 5.95: Loss = 0.632675
Epoch 5.96: Loss = 0.581192
Epoch 5.97: Loss = 0.51973
Epoch 5.98: Loss = 0.590363
Epoch 5.99: Loss = 0.606705
Epoch 5.100: Loss = 0.614227
Epoch 5.101: Loss = 0.645889
Epoch 5.102: Loss = 0.649673
Epoch 5.103: Loss = 0.609085
Epoch 5.104: Loss = 0.54567
Epoch 5.105: Loss = 0.55069
Epoch 5.106: Loss = 0.714127
Epoch 5.107: Loss = 0.666702
Epoch 5.108: Loss = 0.700119
Epoch 5.109: Loss = 0.663483
Epoch 5.110: Loss = 0.606949
Epoch 5.111: Loss = 0.575043
Epoch 5.112: Loss = 0.573685
Epoch 5.113: Loss = 0.586823
Epoch 5.114: Loss = 0.620834
Epoch 5.115: Loss = 0.610611
Epoch 5.116: Loss = 0.553116
Epoch 5.117: Loss = 0.667511
Epoch 5.118: Loss = 0.544785
Epoch 5.119: Loss = 0.605759
Epoch 5.120: Loss = 0.576889
TRAIN LOSS = 0.617859
TRAIN ACC = 79.9576 % (47976/60000)
Loss = 0.544922
Loss = 0.689896
Loss = 0.610901
Loss = 0.538788
Loss = 0.574982
Loss = 0.712494
Loss = 0.773819
Loss = 0.689926
Loss = 0.643829
Loss = 0.573196
Loss = 0.737274
Loss = 0.725296
Loss = 0.643051
Loss = 0.653824
Loss = 0.622849
Loss = 0.67485
Loss = 0.605835
Loss = 0.665909
Loss = 0.703918
Loss = 0.625549
TEST LOSS = 0.650555
TEST ACC = 479.759 % (7908/10000)
Reducing learning rate to 0.100006
Epoch 6.1: Loss = 0.612885
Epoch 6.2: Loss = 0.616348
Epoch 6.3: Loss = 0.646652
Epoch 6.4: Loss = 0.543091
Epoch 6.5: Loss = 0.615753
Epoch 6.6: Loss = 0.682297
Epoch 6.7: Loss = 0.619995
Epoch 6.8: Loss = 0.718491
Epoch 6.9: Loss = 0.509338
Epoch 6.10: Loss = 0.453644
Epoch 6.11: Loss = 0.691544
Epoch 6.12: Loss = 0.605057
Epoch 6.13: Loss = 0.640366
Epoch 6.14: Loss = 0.605286
Epoch 6.15: Loss = 0.627823
Epoch 6.16: Loss = 0.657791
Epoch 6.17: Loss = 0.552155
Epoch 6.18: Loss = 0.628632
Epoch 6.19: Loss = 0.585281
Epoch 6.20: Loss = 0.691681
Epoch 6.21: Loss = 0.532242
Epoch 6.22: Loss = 0.466019
Epoch 6.23: Loss = 0.612167
Epoch 6.24: Loss = 0.713364
Epoch 6.25: Loss = 0.589569
Epoch 6.26: Loss = 0.511246
Epoch 6.27: Loss = 0.587952
Epoch 6.28: Loss = 0.60556
Epoch 6.29: Loss = 0.608902
Epoch 6.30: Loss = 0.607422
Epoch 6.31: Loss = 0.670593
Epoch 6.32: Loss = 0.564835
Epoch 6.33: Loss = 0.532166
Epoch 6.34: Loss = 0.665192
Epoch 6.35: Loss = 0.640701
Epoch 6.36: Loss = 0.671982
Epoch 6.37: Loss = 0.664536
Epoch 6.38: Loss = 0.60817
Epoch 6.39: Loss = 0.667114
Epoch 6.40: Loss = 0.600632
Epoch 6.41: Loss = 0.659683
Epoch 6.42: Loss = 0.615326
Epoch 6.43: Loss = 0.601501
Epoch 6.44: Loss = 0.5354
Epoch 6.45: Loss = 0.631821
Epoch 6.46: Loss = 0.704498
Epoch 6.47: Loss = 0.59404
Epoch 6.48: Loss = 0.545074
Epoch 6.49: Loss = 0.640762
Epoch 6.50: Loss = 0.617676
Epoch 6.51: Loss = 0.500061
Epoch 6.52: Loss = 0.652542
Epoch 6.53: Loss = 0.675735
Epoch 6.54: Loss = 0.492523
Epoch 6.55: Loss = 0.614487
Epoch 6.56: Loss = 0.625275
Epoch 6.57: Loss = 0.685394
Epoch 6.58: Loss = 0.597214
Epoch 6.59: Loss = 0.693375
Epoch 6.60: Loss = 0.622925
Epoch 6.61: Loss = 0.578033
Epoch 6.62: Loss = 0.661819
Epoch 6.63: Loss = 0.522034
Epoch 6.64: Loss = 0.504211
Epoch 6.65: Loss = 0.600998
Epoch 6.66: Loss = 0.530533
Epoch 6.67: Loss = 0.574509
Epoch 6.68: Loss = 0.747101
Epoch 6.69: Loss = 0.61763
Epoch 6.70: Loss = 0.622391
Epoch 6.71: Loss = 0.513626
Epoch 6.72: Loss = 0.610947
Epoch 6.73: Loss = 0.684799
Epoch 6.74: Loss = 0.644226
Epoch 6.75: Loss = 0.565781
Epoch 6.76: Loss = 0.606766
Epoch 6.77: Loss = 0.588654
Epoch 6.78: Loss = 0.630173
Epoch 6.79: Loss = 0.592377
Epoch 6.80: Loss = 0.540817
Epoch 6.81: Loss = 0.580917
Epoch 6.82: Loss = 0.539505
Epoch 6.83: Loss = 0.664398
Epoch 6.84: Loss = 0.545731
Epoch 6.85: Loss = 0.593277
Epoch 6.86: Loss = 0.661331
Epoch 6.87: Loss = 0.603973
Epoch 6.88: Loss = 0.534836
Epoch 6.89: Loss = 0.698486
Epoch 6.90: Loss = 0.604568
Epoch 6.91: Loss = 0.66362
Epoch 6.92: Loss = 0.608521
Epoch 6.93: Loss = 0.639145
Epoch 6.94: Loss = 0.577728
Epoch 6.95: Loss = 0.625534
Epoch 6.96: Loss = 0.57254
Epoch 6.97: Loss = 0.511963
Epoch 6.98: Loss = 0.582001
Epoch 6.99: Loss = 0.597107
Epoch 6.100: Loss = 0.607956
Epoch 6.101: Loss = 0.631241
Epoch 6.102: Loss = 0.64473
Epoch 6.103: Loss = 0.602966
Epoch 6.104: Loss = 0.534912
Epoch 6.105: Loss = 0.542068
Epoch 6.106: Loss = 0.704483
Epoch 6.107: Loss = 0.655563
Epoch 6.108: Loss = 0.707306
Epoch 6.109: Loss = 0.654694
Epoch 6.110: Loss = 0.590271
Epoch 6.111: Loss = 0.569
Epoch 6.112: Loss = 0.566315
Epoch 6.113: Loss = 0.583084
Epoch 6.114: Loss = 0.616837
Epoch 6.115: Loss = 0.600555
Epoch 6.116: Loss = 0.54808
Epoch 6.117: Loss = 0.653732
Epoch 6.118: Loss = 0.536362
Epoch 6.119: Loss = 0.596771
Epoch 6.120: Loss = 0.567596
TRAIN LOSS = 0.606583
TRAIN ACC = 80.5878 % (48355/60000)
Loss = 0.52803
Loss = 0.674561
Loss = 0.592682
Loss = 0.523941
Loss = 0.570526
Loss = 0.699402
Loss = 0.766266
Loss = 0.675613
Loss = 0.633713
Loss = 0.564117
Loss = 0.732758
Loss = 0.720428
Loss = 0.630951
Loss = 0.641129
Loss = 0.616806
Loss = 0.663559
Loss = 0.595337
Loss = 0.653259
Loss = 0.693878
Loss = 0.613876
TEST LOSS = 0.639541
TEST ACC = 483.549 % (7964/10000)
Reducing learning rate to 0.100006
Epoch 7.1: Loss = 0.601074
Epoch 7.2: Loss = 0.615707
Epoch 7.3: Loss = 0.636963
Epoch 7.4: Loss = 0.535461
Epoch 7.5: Loss = 0.61116
Epoch 7.6: Loss = 0.685425
Epoch 7.7: Loss = 0.608414
Epoch 7.8: Loss = 0.713364
Epoch 7.9: Loss = 0.496552
Epoch 7.10: Loss = 0.442795
Epoch 7.11: Loss = 0.679672
Epoch 7.12: Loss = 0.594055
Epoch 7.13: Loss = 0.628311
Epoch 7.14: Loss = 0.599503
Epoch 7.15: Loss = 0.623596
Epoch 7.16: Loss = 0.653122
Epoch 7.17: Loss = 0.535797
Epoch 7.18: Loss = 0.624817
Epoch 7.19: Loss = 0.575745
Epoch 7.20: Loss = 0.686371
Epoch 7.21: Loss = 0.520966
Epoch 7.22: Loss = 0.456635
Epoch 7.23: Loss = 0.599426
Epoch 7.24: Loss = 0.700317
Epoch 7.25: Loss = 0.581802
Epoch 7.26: Loss = 0.494415
Epoch 7.27: Loss = 0.576035
Epoch 7.28: Loss = 0.599258
Epoch 7.29: Loss = 0.599365
Epoch 7.30: Loss = 0.601013
Epoch 7.31: Loss = 0.661133
Epoch 7.32: Loss = 0.552322
Epoch 7.33: Loss = 0.52121
Epoch 7.34: Loss = 0.652481
Epoch 7.35: Loss = 0.635406
Epoch 7.36: Loss = 0.65976
Epoch 7.37: Loss = 0.661957
Epoch 7.38: Loss = 0.595322
Epoch 7.39: Loss = 0.657455
Epoch 7.40: Loss = 0.591354
Epoch 7.41: Loss = 0.642944
Epoch 7.42: Loss = 0.609528
Epoch 7.43: Loss = 0.589203
Epoch 7.44: Loss = 0.523621
Epoch 7.45: Loss = 0.619873
Epoch 7.46: Loss = 0.696808
Epoch 7.47: Loss = 0.589813
Epoch 7.48: Loss = 0.531708
Epoch 7.49: Loss = 0.634308
Epoch 7.50: Loss = 0.609802
Epoch 7.51: Loss = 0.482529
Epoch 7.52: Loss = 0.63768
Epoch 7.53: Loss = 0.662323
Epoch 7.54: Loss = 0.479416
Epoch 7.55: Loss = 0.601105
Epoch 7.56: Loss = 0.610107
Epoch 7.57: Loss = 0.673187
Epoch 7.58: Loss = 0.577301
Epoch 7.59: Loss = 0.685852
Epoch 7.60: Loss = 0.609665
Epoch 7.61: Loss = 0.568253
Epoch 7.62: Loss = 0.648239
Epoch 7.63: Loss = 0.507767
Epoch 7.64: Loss = 0.493973
Epoch 7.65: Loss = 0.591171
Epoch 7.66: Loss = 0.511429
Epoch 7.67: Loss = 0.555695
Epoch 7.68: Loss = 0.749069
Epoch 7.69: Loss = 0.596008
Epoch 7.70: Loss = 0.609451
Epoch 7.71: Loss = 0.501755
Epoch 7.72: Loss = 0.605026
Epoch 7.73: Loss = 0.669739
Epoch 7.74: Loss = 0.637741
Epoch 7.75: Loss = 0.558136
Epoch 7.76: Loss = 0.591873
Epoch 7.77: Loss = 0.577606
Epoch 7.78: Loss = 0.614258
Epoch 7.79: Loss = 0.585724
Epoch 7.80: Loss = 0.531326
Epoch 7.81: Loss = 0.576019
Epoch 7.82: Loss = 0.537659
Epoch 7.83: Loss = 0.654236
Epoch 7.84: Loss = 0.532654
Epoch 7.85: Loss = 0.584335
Epoch 7.86: Loss = 0.652649
Epoch 7.87: Loss = 0.599792
Epoch 7.88: Loss = 0.526367
Epoch 7.89: Loss = 0.690079
Epoch 7.90: Loss = 0.603302
Epoch 7.91: Loss = 0.648438
Epoch 7.92: Loss = 0.600113
Epoch 7.93: Loss = 0.619476
Epoch 7.94: Loss = 0.574356
Epoch 7.95: Loss = 0.620758
Epoch 7.96: Loss = 0.561737
Epoch 7.97: Loss = 0.502335
Epoch 7.98: Loss = 0.576706
Epoch 7.99: Loss = 0.588135
Epoch 7.100: Loss = 0.603806
Epoch 7.101: Loss = 0.619324
Epoch 7.102: Loss = 0.633362
Epoch 7.103: Loss = 0.594086
Epoch 7.104: Loss = 0.522461
Epoch 7.105: Loss = 0.535309
Epoch 7.106: Loss = 0.693253
Epoch 7.107: Loss = 0.641159
Epoch 7.108: Loss = 0.708191
Epoch 7.109: Loss = 0.640854
Epoch 7.110: Loss = 0.583359
Epoch 7.111: Loss = 0.560074
Epoch 7.112: Loss = 0.563553
Epoch 7.113: Loss = 0.576187
Epoch 7.114: Loss = 0.608948
Epoch 7.115: Loss = 0.587402
Epoch 7.116: Loss = 0.535126
Epoch 7.117: Loss = 0.64357
Epoch 7.118: Loss = 0.529999
Epoch 7.119: Loss = 0.589294
Epoch 7.120: Loss = 0.558731
TRAIN LOSS = 0.596802
TRAIN ACC = 81.0608 % (48638/60000)
Loss = 0.516876
Loss = 0.658997
Loss = 0.582367
Loss = 0.512558
Loss = 0.558594
Loss = 0.678085
Loss = 0.757141
Loss = 0.668472
Loss = 0.619019
Loss = 0.55188
Loss = 0.732788
Loss = 0.713821
Loss = 0.621017
Loss = 0.628662
Loss = 0.602036
Loss = 0.647842
Loss = 0.586365
Loss = 0.644867
Loss = 0.680344
Loss = 0.602768
TEST LOSS = 0.628225
TEST ACC = 486.378 % (8003/10000)
Reducing learning rate to 0.100006
Epoch 8.1: Loss = 0.579544
Epoch 8.2: Loss = 0.609589
Epoch 8.3: Loss = 0.618225
Epoch 8.4: Loss = 0.524002
Epoch 8.5: Loss = 0.60498
Epoch 8.6: Loss = 0.678284
Epoch 8.7: Loss = 0.593643
Epoch 8.8: Loss = 0.69545
Epoch 8.9: Loss = 0.482758
Epoch 8.10: Loss = 0.42894
Epoch 8.11: Loss = 0.663895
Epoch 8.12: Loss = 0.575333
Epoch 8.13: Loss = 0.614151
Epoch 8.14: Loss = 0.591843
Epoch 8.15: Loss = 0.618805
Epoch 8.16: Loss = 0.643341
Epoch 8.17: Loss = 0.529678
Epoch 8.18: Loss = 0.619263
Epoch 8.19: Loss = 0.563705
Epoch 8.20: Loss = 0.674561
Epoch 8.21: Loss = 0.505295
Epoch 8.22: Loss = 0.442535
Epoch 8.23: Loss = 0.579575
Epoch 8.24: Loss = 0.687927
Epoch 8.25: Loss = 0.583817
Epoch 8.26: Loss = 0.483597
Epoch 8.27: Loss = 0.564621
Epoch 8.28: Loss = 0.591217
Epoch 8.29: Loss = 0.585724
Epoch 8.30: Loss = 0.591293
Epoch 8.31: Loss = 0.647171
Epoch 8.32: Loss = 0.545471
Epoch 8.33: Loss = 0.512222
Epoch 8.34: Loss = 0.640717
Epoch 8.35: Loss = 0.629929
Epoch 8.36: Loss = 0.65213
Epoch 8.37: Loss = 0.654938
Epoch 8.38: Loss = 0.585419
Epoch 8.39: Loss = 0.648041
Epoch 8.40: Loss = 0.57811
Epoch 8.41: Loss = 0.627121
Epoch 8.42: Loss = 0.595627
Epoch 8.43: Loss = 0.587372
Epoch 8.44: Loss = 0.509369
Epoch 8.45: Loss = 0.608017
Epoch 8.46: Loss = 0.68219
Epoch 8.47: Loss = 0.582764
Epoch 8.48: Loss = 0.521271
Epoch 8.49: Loss = 0.618912
Epoch 8.50: Loss = 0.605835
Epoch 8.51: Loss = 0.476395
Epoch 8.52: Loss = 0.627808
Epoch 8.53: Loss = 0.656006
Epoch 8.54: Loss = 0.473007
Epoch 8.55: Loss = 0.592896
Epoch 8.56: Loss = 0.599182
Epoch 8.57: Loss = 0.654846
Epoch 8.58: Loss = 0.56723
Epoch 8.59: Loss = 0.685242
Epoch 8.60: Loss = 0.611389
Epoch 8.61: Loss = 0.55629
Epoch 8.62: Loss = 0.627991
Epoch 8.63: Loss = 0.50351
Epoch 8.64: Loss = 0.481567
Epoch 8.65: Loss = 0.583282
Epoch 8.66: Loss = 0.502182
Epoch 8.67: Loss = 0.541458
Epoch 8.68: Loss = 0.74826
Epoch 8.69: Loss = 0.585999
Epoch 8.70: Loss = 0.603668
Epoch 8.71: Loss = 0.502319
Epoch 8.72: Loss = 0.599854
Epoch 8.73: Loss = 0.668701
Epoch 8.74: Loss = 0.629807
Epoch 8.75: Loss = 0.551956
Epoch 8.76: Loss = 0.589188
Epoch 8.77: Loss = 0.574783
Epoch 8.78: Loss = 0.616302
Epoch 8.79: Loss = 0.574844
Epoch 8.80: Loss = 0.529297
Epoch 8.81: Loss = 0.569092
Epoch 8.82: Loss = 0.530045
Epoch 8.83: Loss = 0.647568
Epoch 8.84: Loss = 0.532959
Epoch 8.85: Loss = 0.57695
Epoch 8.86: Loss = 0.644135
Epoch 8.87: Loss = 0.593185
Epoch 8.88: Loss = 0.526443
Epoch 8.89: Loss = 0.681915
Epoch 8.90: Loss = 0.60498
Epoch 8.91: Loss = 0.644806
Epoch 8.92: Loss = 0.595123
Epoch 8.93: Loss = 0.612762
Epoch 8.94: Loss = 0.573517
Epoch 8.95: Loss = 0.612549
Epoch 8.96: Loss = 0.557404
Epoch 8.97: Loss = 0.49205
Epoch 8.98: Loss = 0.568375
Epoch 8.99: Loss = 0.575119
Epoch 8.100: Loss = 0.598557
Epoch 8.101: Loss = 0.610001
Epoch 8.102: Loss = 0.630646
Epoch 8.103: Loss = 0.582657
Epoch 8.104: Loss = 0.513763
Epoch 8.105: Loss = 0.5327
Epoch 8.106: Loss = 0.678299
Epoch 8.107: Loss = 0.632919
Epoch 8.108: Loss = 0.710022
Epoch 8.109: Loss = 0.63147
Epoch 8.110: Loss = 0.581497
Epoch 8.111: Loss = 0.551712
Epoch 8.112: Loss = 0.561203
Epoch 8.113: Loss = 0.568527
Epoch 8.114: Loss = 0.607559
Epoch 8.115: Loss = 0.58107
Epoch 8.116: Loss = 0.535019
Epoch 8.117: Loss = 0.636307
Epoch 8.118: Loss = 0.525681
Epoch 8.119: Loss = 0.584686
Epoch 8.120: Loss = 0.552643
TRAIN LOSS = 0.588501
TRAIN ACC = 81.4209 % (48855/60000)
Loss = 0.508392
Loss = 0.653214
Loss = 0.582596
Loss = 0.507309
Loss = 0.558212
Loss = 0.673096
Loss = 0.753403
Loss = 0.670364
Loss = 0.61911
Loss = 0.550156
Loss = 0.731201
Loss = 0.710388
Loss = 0.620285
Loss = 0.629791
Loss = 0.604874
Loss = 0.641449
Loss = 0.587219
Loss = 0.647095
Loss = 0.669708
Loss = 0.603043
TEST LOSS = 0.626045
TEST ACC = 488.55 % (8026/10000)
Reducing learning rate to 0.100006
Epoch 9.1: Loss = 0.571411
Epoch 9.2: Loss = 0.601639
Epoch 9.3: Loss = 0.618713
Epoch 9.4: Loss = 0.516754
Epoch 9.5: Loss = 0.603256
Epoch 9.6: Loss = 0.675659
Epoch 9.7: Loss = 0.577835
Epoch 9.8: Loss = 0.689316
Epoch 9.9: Loss = 0.475876
Epoch 9.10: Loss = 0.419662
Epoch 9.11: Loss = 0.656921
Epoch 9.12: Loss = 0.572479
Epoch 9.13: Loss = 0.608948
Epoch 9.14: Loss = 0.582184
Epoch 9.15: Loss = 0.609741
Epoch 9.16: Loss = 0.63269
Epoch 9.17: Loss = 0.513214
Epoch 9.18: Loss = 0.611298
Epoch 9.19: Loss = 0.556137
Epoch 9.20: Loss = 0.664368
Epoch 9.21: Loss = 0.497971
Epoch 9.22: Loss = 0.436569
Epoch 9.23: Loss = 0.580811
Epoch 9.24: Loss = 0.676773
Epoch 9.25: Loss = 0.574493
Epoch 9.26: Loss = 0.472015
Epoch 9.27: Loss = 0.556183
Epoch 9.28: Loss = 0.582611
Epoch 9.29: Loss = 0.5811
Epoch 9.30: Loss = 0.582535
Epoch 9.31: Loss = 0.642426
Epoch 9.32: Loss = 0.542282
Epoch 9.33: Loss = 0.515656
Epoch 9.34: Loss = 0.633789
Epoch 9.35: Loss = 0.623596
Epoch 9.36: Loss = 0.647354
Epoch 9.37: Loss = 0.649689
Epoch 9.38: Loss = 0.576889
Epoch 9.39: Loss = 0.641266
Epoch 9.40: Loss = 0.567047
Epoch 9.41: Loss = 0.61937
Epoch 9.42: Loss = 0.588181
Epoch 9.43: Loss = 0.581619
Epoch 9.44: Loss = 0.510666
Epoch 9.45: Loss = 0.59761
Epoch 9.46: Loss = 0.684265
Epoch 9.47: Loss = 0.579666
Epoch 9.48: Loss = 0.507721
Epoch 9.49: Loss = 0.613327
Epoch 9.50: Loss = 0.607224
Epoch 9.51: Loss = 0.468933
Epoch 9.52: Loss = 0.63446
Epoch 9.53: Loss = 0.641647
Epoch 9.54: Loss = 0.469879
Epoch 9.55: Loss = 0.596146
Epoch 9.56: Loss = 0.597504
Epoch 9.57: Loss = 0.652161
Epoch 9.58: Loss = 0.55751
Epoch 9.59: Loss = 0.682999
Epoch 9.60: Loss = 0.600113
Epoch 9.61: Loss = 0.545883
Epoch 9.62: Loss = 0.627136
Epoch 9.63: Loss = 0.49646
Epoch 9.64: Loss = 0.479141
Epoch 9.65: Loss = 0.584946
Epoch 9.66: Loss = 0.499939
Epoch 9.67: Loss = 0.535278
Epoch 9.68: Loss = 0.751678
Epoch 9.69: Loss = 0.567719
Epoch 9.70: Loss = 0.592941
Epoch 9.71: Loss = 0.496292
Epoch 9.72: Loss = 0.596634
Epoch 9.73: Loss = 0.661163
Epoch 9.74: Loss = 0.621735
Epoch 9.75: Loss = 0.544281
Epoch 9.76: Loss = 0.57489
Epoch 9.77: Loss = 0.569412
Epoch 9.78: Loss = 0.598206
Epoch 9.79: Loss = 0.570374
Epoch 9.80: Loss = 0.523727
Epoch 9.81: Loss = 0.566177
Epoch 9.82: Loss = 0.528137
Epoch 9.83: Loss = 0.63858
Epoch 9.84: Loss = 0.524475
Epoch 9.85: Loss = 0.569412
Epoch 9.86: Loss = 0.637299
Epoch 9.87: Loss = 0.588593
Epoch 9.88: Loss = 0.522827
Epoch 9.89: Loss = 0.673523
Epoch 9.90: Loss = 0.601883
Epoch 9.91: Loss = 0.63562
Epoch 9.92: Loss = 0.585434
Epoch 9.93: Loss = 0.600769
Epoch 9.94: Loss = 0.569748
Epoch 9.95: Loss = 0.607559
Epoch 9.96: Loss = 0.554214
Epoch 9.97: Loss = 0.488022
Epoch 9.98: Loss = 0.570068
Epoch 9.99: Loss = 0.573166
Epoch 9.100: Loss = 0.586853
Epoch 9.101: Loss = 0.604385
Epoch 9.102: Loss = 0.626434
Epoch 9.103: Loss = 0.57666
Epoch 9.104: Loss = 0.510345
Epoch 9.105: Loss = 0.524216
Epoch 9.106: Loss = 0.670883
Epoch 9.107: Loss = 0.629745
Epoch 9.108: Loss = 0.714432
Epoch 9.109: Loss = 0.626328
Epoch 9.110: Loss = 0.574463
Epoch 9.111: Loss = 0.549637
Epoch 9.112: Loss = 0.561264
Epoch 9.113: Loss = 0.562454
Epoch 9.114: Loss = 0.612335
Epoch 9.115: Loss = 0.572906
Epoch 9.116: Loss = 0.535019
Epoch 9.117: Loss = 0.628479
Epoch 9.118: Loss = 0.523788
Epoch 9.119: Loss = 0.579086
Epoch 9.120: Loss = 0.545776
TRAIN LOSS = 0.582672
TRAIN ACC = 81.6971 % (49020/60000)
Loss = 0.500214
Loss = 0.644577
Loss = 0.57605
Loss = 0.497833
Loss = 0.559677
Loss = 0.658218
Loss = 0.755753
Loss = 0.661179
Loss = 0.612167
Loss = 0.554779
Loss = 0.732239
Loss = 0.711456
Loss = 0.618256
Loss = 0.63176
Loss = 0.597443
Loss = 0.632309
Loss = 0.583817
Loss = 0.643433
Loss = 0.662064
Loss = 0.606827
TEST LOSS = 0.622002
TEST ACC = 490.199 % (8075/10000)
Reducing learning rate to 0.100006
Epoch 10.1: Loss = 0.560028
Epoch 10.2: Loss = 0.603439
Epoch 10.3: Loss = 0.616074
Epoch 10.4: Loss = 0.511978
Epoch 10.5: Loss = 0.607208
Epoch 10.6: Loss = 0.678131
Epoch 10.7: Loss = 0.585983
Epoch 10.8: Loss = 0.683685
Epoch 10.9: Loss = 0.473343
Epoch 10.10: Loss = 0.415298
Epoch 10.11: Loss = 0.644531
Epoch 10.12: Loss = 0.563919
Epoch 10.13: Loss = 0.604172
Epoch 10.14: Loss = 0.575485
Epoch 10.15: Loss = 0.607147
Epoch 10.16: Loss = 0.632492
Epoch 10.17: Loss = 0.511536
Epoch 10.18: Loss = 0.597717
Epoch 10.19: Loss = 0.54306
Epoch 10.20: Loss = 0.663208
Epoch 10.21: Loss = 0.498718
Epoch 10.22: Loss = 0.431915
Epoch 10.23: Loss = 0.56897
Epoch 10.24: Loss = 0.671783
Epoch 10.25: Loss = 0.572266
Epoch 10.26: Loss = 0.46434
Epoch 10.27: Loss = 0.553574
Epoch 10.28: Loss = 0.578644
Epoch 10.29: Loss = 0.572403
Epoch 10.30: Loss = 0.57692
Epoch 10.31: Loss = 0.644241
Epoch 10.32: Loss = 0.533813
Epoch 10.33: Loss = 0.506729
Epoch 10.34: Loss = 0.627625
Epoch 10.35: Loss = 0.616882
Epoch 10.36: Loss = 0.642715
Epoch 10.37: Loss = 0.647552
Epoch 10.38: Loss = 0.576691
Epoch 10.39: Loss = 0.637375
Epoch 10.40: Loss = 0.563583
Epoch 10.41: Loss = 0.608032
Epoch 10.42: Loss = 0.585403
Epoch 10.43: Loss = 0.577316
Epoch 10.44: Loss = 0.496918
Epoch 10.45: Loss = 0.596497
Epoch 10.46: Loss = 0.677414
Epoch 10.47: Loss = 0.574402
Epoch 10.48: Loss = 0.506653
Epoch 10.49: Loss = 0.606186
Epoch 10.50: Loss = 0.604279
Epoch 10.51: Loss = 0.461884
Epoch 10.52: Loss = 0.626328
Epoch 10.53: Loss = 0.634232
Epoch 10.54: Loss = 0.460312
Epoch 10.55: Loss = 0.585999
Epoch 10.56: Loss = 0.584503
Epoch 10.57: Loss = 0.641022
Epoch 10.58: Loss = 0.548401
Epoch 10.59: Loss = 0.678589
Epoch 10.60: Loss = 0.589981
Epoch 10.61: Loss = 0.544922
Epoch 10.62: Loss = 0.615295
Epoch 10.63: Loss = 0.487
Epoch 10.64: Loss = 0.464661
Epoch 10.65: Loss = 0.577408
Epoch 10.66: Loss = 0.492981
Epoch 10.67: Loss = 0.528748
Epoch 10.68: Loss = 0.757278
Epoch 10.69: Loss = 0.566422
Epoch 10.70: Loss = 0.583908
Epoch 10.71: Loss = 0.492462
Epoch 10.72: Loss = 0.592072
Epoch 10.73: Loss = 0.654572
Epoch 10.74: Loss = 0.616074
Epoch 10.75: Loss = 0.543945
Epoch 10.76: Loss = 0.567062
Epoch 10.77: Loss = 0.562973
Epoch 10.78: Loss = 0.588196
Epoch 10.79: Loss = 0.55835
Epoch 10.80: Loss = 0.518433
Epoch 10.81: Loss = 0.563324
Epoch 10.82: Loss = 0.528503
Epoch 10.83: Loss = 0.63765
Epoch 10.84: Loss = 0.521713
Epoch 10.85: Loss = 0.559937
Epoch 10.86: Loss = 0.624802
Epoch 10.87: Loss = 0.585968
Epoch 10.88: Loss = 0.516556
Epoch 10.89: Loss = 0.665039
Epoch 10.90: Loss = 0.60112
Epoch 10.91: Loss = 0.629181
Epoch 10.92: Loss = 0.577713
Epoch 10.93: Loss = 0.590256
Epoch 10.94: Loss = 0.55899
Epoch 10.95: Loss = 0.596664
Epoch 10.96: Loss = 0.549576
Epoch 10.97: Loss = 0.485657
Epoch 10.98: Loss = 0.566849
Epoch 10.99: Loss = 0.563751
Epoch 10.100: Loss = 0.581146
Epoch 10.101: Loss = 0.592636
Epoch 10.102: Loss = 0.623672
Epoch 10.103: Loss = 0.571411
Epoch 10.104: Loss = 0.507263
Epoch 10.105: Loss = 0.523132
Epoch 10.106: Loss = 0.66684
Epoch 10.107: Loss = 0.616638
Epoch 10.108: Loss = 0.711227
Epoch 10.109: Loss = 0.622772
Epoch 10.110: Loss = 0.566269
Epoch 10.111: Loss = 0.549057
Epoch 10.112: Loss = 0.559372
Epoch 10.113: Loss = 0.56871
Epoch 10.114: Loss = 0.611557
Epoch 10.115: Loss = 0.566757
Epoch 10.116: Loss = 0.527679
Epoch 10.117: Loss = 0.627441
Epoch 10.118: Loss = 0.521301
Epoch 10.119: Loss = 0.566574
Epoch 10.120: Loss = 0.536423
TRAIN LOSS = 0.577209
TRAIN ACC = 82.045 % (49229/60000)
Loss = 0.493652
Loss = 0.630188
Loss = 0.56488
Loss = 0.494781
Loss = 0.555466
Loss = 0.65184
Loss = 0.740509
Loss = 0.651672
Loss = 0.60231
Loss = 0.55014
Loss = 0.730408
Loss = 0.7052
Loss = 0.609879
Loss = 0.62648
Loss = 0.590912
Loss = 0.622437
Loss = 0.571747
Loss = 0.632751
Loss = 0.654068
Loss = 0.599365
TEST LOSS = 0.613934
TEST ACC = 492.29 % (8120/10000)
